{"/":{"title":"_index","content":"\n\n\n涉猎啊等啊等\n\n\n# 所有的文章\n```dataview\nlist from \"/\"\n```\n\n","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E4%BB%93/flink/%E4%B9%A6%E7%B1%8D":{"title":"书籍","content":"![[基于Apache Flink的流处理 流式应用基础、实现及操作 (比安‧霍斯克 瓦西里基‧卡拉夫里) (Z-Library).pdf]]","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E4%BB%93/flink/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4%E6%95%99%E7%A8%8B":{"title":"极客时间教程","content":"![[第一章：Apache Flink介绍.pdf]]![[第二章：Flink部署与应用.pdf]]![[第三章：Flink DataStream API实践原理.pdf]]![[第四章：Flink状态管理和容错.pdf]]![[第五章：Flink Table \u0026 SQL实践原理（上）.pdf]]![[第六章：Flink Runtime设计与实现.pdf]]![[第七章：Flink监控与性能优化.pdf]]![[第八章：Flink组件栈介绍与使用.pdf]]![[第九章：项目实战-使用Flink构建推荐系统实时数据流.pdf]]","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E4%BB%93/hadoop/Hadoop-%E7%94%9F%E6%80%81":{"title":"Hadoop 生态","content":"![[Pasted image 20230728000457.png]]","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%95%B0%E4%BB%93/spark/%E4%B9%A6%E7%B1%8D":{"title":"书籍","content":"","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%9F%BA%E7%A1%80":{"title":"消息队列基础","content":"  \n\n# 为什么要使用消息队列呢？\n\n消息队列主要有三大用途，我们拿一个电商系统的下单举例：\n\n**解耦**：引入消息队列之前，下单完成之后，需要订单服务去调用库存服务减库存，**调用营销服务加营销数据……引入消息队列之后，可以把订单完成的消息丢进队列里，下游服务自己去调用就行了，这样就完成了订单服务和其它服务的解耦合**。\n\n![](statistic/asynccode-513.png)\n\n**异步**：订单支付之后，我们要扣减库存、增加积分、发送消息等等**，这样一来这个链路就长了，链路一长，响应时间就变长了。引入消息队列，除了 更新订单状态 ，其它的都可以异步去做，这样一来就来，就能降低响应时间。**\n\n![](statistic/asynccode-512.png)\n\n**削峰**：消息队列合一用来削峰，**例如秒杀系统，平时流量很低，但是要做秒杀 活动，秒杀的时候流量疯狂怼进来，我们的服务器，Redis，MySQL各自的承受能力都不一样，直接全部流量照单全收肯定有问题啊，严重点可能直接打挂了。我们可以把请求扔到队列里面，只放出我们服务能处理的流量，这样就能抗住短时间的大流量了**\n\n![](statistic/asynccode-516.png)\n\n  \n\n  \n\n  \n\n# 主流消息队列的对比\n\n  \n\n![](statistic/asynccode-519.png)\n\n![](statistic/asynccode-517.png)\n\n![](statistic/asynccode-514.png)\n\n  \n\n![](statistic/asynccode-525.png)\n\n  \n\n总结下：\n\n选择中间件的可以从这些维度来考虑：可靠性，性能，功能，可运维行，可拓展性，社区活跃度。目前常用的几个中间件，ActiveMQ作为“老古董”，市面上用的已经不多，其它几种：\n\n**RabbitMQ：**\n\n优点：轻量，迅捷，容易部署和使用，拥有灵活的路由配置\n\n缺点：**性能和吞吐量不太理想，不易进行二次开发**\n\n**RocketMQ：**\n\n优点：性能好，高吞吐量，稳定可靠，有活跃的中文社区\n\n缺点：兼容性上不是太好\n\n**Kafka：**\n\n优点：拥有强大的性能及吞吐量，兼容性很好\n\n缺点：**由于“攒一波再处理”导致延迟比较高**\n\n  \n\n  \n\n# 消息队列有那些消息模型\n\n消息队列有两种模型：**队列模型和发布/订阅模型。**\n\n- 队列模型：\n    \n\n对应着消息队列“发-存-收”的模型。生产者往某个队列里面发送消息，一个队列可以存储多个生产者的消息，一个队列也可以有 多个消费者，但是消费者之间是竞争关系，也就是**说每条消息只能被一个消费者**\n\n**消费**\n\n![](statistic/asynccode-513.png)\n\n- 发布/订阅模型\n    \n\n如果**需要将一份消息数据分发给多个消费者，并且每个消费者都要求收到全量的消息**。很显然，队列模型无法满足这个需求。解决的方式就是发布/订阅模型。\n\n在发布 - 订阅模型中，\n\n- 消息的发送方称为发布者（Publisher）\n    \n- 消息的接收方称为订阅者（Subscriber），\n    \n- 服务存放消息的容器称为主题（Topic）。\n    \n\n**每份订阅中，订阅者都可以接收到主题的所有消息**\n\n![](statistic/asynccode-513.png)\n\n  \n\n# 消息的消费模式\n\n**Clustering**（集群消费）和**Broadcasting**（广播消费）。\n\n![](statistic/asynccode-515.png)\n\n- 集群消费：这种模式下 一**个消费者组共同消费一个主题的多个队列，一个队列只会被一个消费者消费** 。\n    \n- 广播消费：消息会发给消费者组中的每一个消费者进行消费\n    \n\n  \n\n  \n\n# 消息队列核心模型\n\n  \n\n![](statistic/asynccode-523.png)\n\n  \n\n# 推拉模型区别\n\n![](statistic/asynccode-524.png)\n\nx\n\n# 消费者消费模型\n\n![](statistic/asynccode-522.png)\n\n  \n\n  \n\n# 架构：kafka vs Pulsar vs RocketMQ\n\n## kafka\n\n![](statistic/asynccode-515.png)\n\n## Pulsar\n\n![](statistic/asynccode-518.png)\n\n## RockerMQ4.5 以前\n\n  \n\n![](statistic/asynccode-513.png)\n\n## RocketMQ4.5以后\n\n![](statistic/asynccode-521.png)\n\n  \n\n# kafka和RocketMQ数据组织对比\n\n  \n\n![](statistic/asynccode-514.png)\n\n  \n\n# Kafka 和RocketMQ存储结构对比\n\n![](statistic/asynccode-522.png)\n\n  \n\n# Kafka 和Pulsar存储结构对比\n\n  \n\n![](statistic/asynccode-520.png)\n\n  \n\n# RabbitMQ整体架构\n\n![](statistic/asynccode-526.png)","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E5%AE%A2%E6%88%B7%E7%AB%AF-%E6%97%A0%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE":{"title":"客户端-无消息丢失如何配置","content":"一句话概括，Kafka 只对**“已提交”的消息（committed message）做有限度的持久化保证。**\n\n  \n\n- 生产者\n    \n    - 要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用**带有回调通知的** send 方法。\n        \n- Broker\n    \n    - 设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。**如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。**这是最高等级的“已提交”定义\n        \n    - 设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，**对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries \u003e 0 的 Producer 能够自动重试消息发送，避免消息丢失**。\n        \n    - 设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如**果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。**故一般都要将该参数设置成 false，即不允许这种情况的发生。\n        \n    - 设置 replication.factor \u003e= 3。这也是 Broker 端的参数。其实这里想表**述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余**。\n        \n    - 设置 min.insync.replicas \u003e 1。这依然是 Broker 端参数，**控制的是消息至少要被写入到多少个副本才算是“已提交”。**设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。\n        \n    - 确保 replication.factor \u003e min.insync.replicas。**如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。**我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。\n        \n- 消费者\n    \n    - 确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的。","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E5%AE%A2%E6%88%B7%E7%AB%AF-%E7%94%9F%E4%BA%A7%E8%80%85%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95":{"title":"客户端-生产者压缩算法","content":"具体来说就是用 CPU 时间去换磁盘空间或网络 I/O 传输量，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I/O 传输。\n\n  \n\n  \n\n# 怎么压缩\n\nkafka 的消息层次都分为两层：\n\n- 消息集合（message set）\n    \n- 消息（message）。一个消息集合中包含若干条**日志项（record item**）,而日志项才是真正封装消息的地方\n    \n\n  \n\n目前 Kafka 共有两大类消息格式，社区分别称之为 **V1 版本和 V2 版本**。\n\n  \n\n## 引入V2版本的目的\n\n主要是针对 V1 版本的一些弊端做了修正，\n\n- CRC校验\n    \n    - 在 V1 版本中，**每条消息都需要执行 CRC 校验**，但有些情况下消息的 CRC 值是会发生变化的。比如在 Broker 端可能会对消息时间戳字段进行更新，那么重新计算之后的 CRC 值也会相应更新。\n        \n    - 在 V2 版本中，消息的 CRC 校验工作就被移到了消息集合这一层\n        \n- 压缩方式\n    \n    - V1 版本中保存压缩消息的方法是把**多条消息进行压缩然后保存到外层消息的消息体字段中**\n        \n    - V2 版本的做法是**对整个消息集合进行压缩**\n        \n\n  \n\n  \n\n# 何时压缩\n\n压缩可能发生在两个地方：**生产者端和 Broker 端**。\n\n  \n\n生产者程序中配置 **compression.type 参**数即表示启用指定类型的压缩算法\n\n```Go\n\n Properties props = new Properties();\n props.put(\"bootstrap.servers\", \"localhost:9092\");\n props.put(\"acks\", \"all\");\n props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n // 开启GZIP压缩\n props.put(\"compression.type\", \"gzip\");\n \n Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props);\n```\n\n**大部分情况下 Broker 从 Producer 端接收到消息后仅仅是原封不动地保存而不会对其进行任何修改**\n\n  \n\n有时 Broker端需要重新压缩\n\n- Broker 端指定了和 Producer 端不同的压缩算法。\n    \n- Broker 端发生了消息格式转换。\n    \n\n  \n\n# 何时解压缩\n\n**通常来说解压缩发生在消费者程序中，也就是说 Producer 发送压缩消息到 Broker 后，Broker 照单全收并原样保存起来**。当 Consumer 程序请求这部分消息时，Broker 依然原样发送出去，当消息到达 Consumer 端后，**由 Consumer 自行解压缩还原成之前的消息。**\n\n通常\n\n**Producer 端压缩、Broker 端保持、Consumer 端解压缩。**","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E5%AE%A2%E6%88%B7%E7%AB%AF-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E5%88%86%E5%8C%BA%E6%9C%BA%E5%88%B6":{"title":"客户端-生产者消息分区机制","content":"**切记分区是实现负载均衡以及高吞吐量的关键**\n\n# 为什么分区\n\n  \n\nKafka 有主题（Topic）的概念，**它是承载真实数据的逻辑容器**，而在主题之下还分为若干个分区，也就是说 Kafka 的消息组织方式实际上是三级结构：**主题 - 分区 - 消息。**主题下的每条消息只会保存在某一个分区中，**而不会在多个分区中被保存多份**\n\n![](statistic/asynccode-552.png)\n\n  \n\n## 为啥要使用多个分区而不是直接使用多个主题？\n\n- 其实分区的作用就是提供负载均衡的能力\n    \n- 为了实现系统的高伸缩性（Scalability）。不同的分区能够被放置到不同节点的机器上，而数据的读写操作也都是针对分区这个粒度而进行的，这样**每个节点的机器都能独立地执行各自分区的读写请求处理**。并且，我们还可以通过添加新的节点机器来增加整体系统的吞吐量。\n    \n\n  \n\n# 都有哪些分区策略\n\n所谓分区策略是决定生产者将消息发送到哪个分区的算法。\n\n  \n\n## 自定义分区策略\n\n在编写生产者程序时，你可以编写一个具体的类实现org.apache.kafka.clients.producer.Partitioner接口。这个接口也很简单，只定义了两个方法：partition()和close()，通常你只需要实现最重要的 partition 方法。我们来看看这个方法的方法签名：\n\n```Go\n\nint partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);\n```\n\ntopic、key、keyBytes、value和valueBytes都属于消息数据，cluster则是集群信息（比如当前 Kafka 集群共有多少主题、多少 Broker 等）。\n\n要你自己的实现类定义好了 partition 方法，同时设置partitioner.class参数为你自己实现类的 Full Qualified Name，\n\n  \n\n  \n\n## 轮训策略（默认的策略）\n\n  \n\n也称 Round-robin 策略，即顺序分配。比如一个主题下有 3 个分区，那么第一条消息被发送到分区 0，第二条被发送到分区 1，第三条被发送到分区 2，以此类推。当生产第 4 条消息时又会重新开始，即将其分配到分区 0，\n\n![](statistic/asynccode-553.png)\n\n轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上，故默认情况下它是最合理的分区策略，也是我们最常用的分区策略之一。\n\n  \n\n## 随机策略\n\n也称 Randomness 策略。所谓随机就是我们随意地将消息放置到任意一个分区上，如下面这张图所示。\n\n![](statistic/asynccode-554.png)\n\n随机策略也是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于轮询策略，所以如果追求数据的均匀分布，还是使用轮询策略比较好。事实上，随机策略是老版本生产者使用的分区策略，在新版本中已经改为轮询了。\n\n```Go\n\nList\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic);\nreturn ThreadLocalRandom.current().nextInt(partitions.size());\n```\n\n## 按消息键保序策略\n\nKafka 允许为每条消息定义消息键，**简称为 Key**。这个 Key 的作用非常大，它可**以是一个有着明确业务含义的字符串，比如客户代码、部门编号或是业务 ID 等**\n\n一旦消息被定义了 Key，那么你就**可以保证同一个 Key 的所有消息都进入到相同的分区里面**，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略，\n\n  \n\n![](statistic/asynccode-555.png)\n\n  \n\n```Go\n\nList\u003cPartitionInfo\u003e partitions = cluster.partitionsForTopic(topic);\nreturn Math.abs(key.hashCode()) % partitions.size();\n```\n\n  \n\n  \n\n### 如何实现消息的顺序问题。\n\n- 给 Kafka 主题设置单分区，也就是 1 个分区。这样所有的消息都只在这一个分区内读写，因此保证了全局的顺序性。这样做虽然实现了因果关系的顺序性，但也丧失了 Kafka 多分区带来的高吞吐量和负载均衡的优势。\n    \n- 标志位设定专门的分区策略，**保证同一标志位的所有消息都发送到同一分区**，这样既可以保证分区内的消息顺序，也可以享受到多分区带来的性能红利。\n    \n\n  \n\n  \n\n## 其他分区策略\n\n- 基于地理位置的分区策略","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E5%AE%A2%E6%88%B7%E7%AB%AF-kafka%E6%8B%A6%E6%88%AA%E5%99%A8":{"title":"客户端-kafka拦截器","content":"其基本思想就是允许应用程序在不修改逻辑的情况下，动态地实现一组可插拔的事件处理逻辑链。它能够在主业务操作的前后多个时间点上插入对应的“拦截”逻辑。springmvcde 拦截器的工作原理\n\n![](statistic/asynccode-551.png)\n\nKafka 拦截器借鉴了这样的设计思路。你可以在消息处理的前后多个时点动态植入不同的处理逻辑，比如在消息发送前或者在消息被消费后。\n\n# Kafka 拦截器\n\n  \n\nfka 拦截器分为生产者拦截器和消费者拦截器。生产者拦截器允许你在**发送消息前以及消息提交成功后**植入你的拦截器逻辑；而消费者拦截器支持在消费消息前以及提交位移后编写特定逻辑。值得一提的是，这两种拦截器都支持链的方式，即你可以将一组拦截器串连成一个大的拦截器，**Kafka 会按照添加顺序依次执行拦截器逻辑**。\n\n当前 Kafka 拦截器的设置方法是通过参数配置完成的。生产者和消费者两端有一个相同的参数，名字叫 interceptor.classes\n\n```Go\n\nProperties props = new Properties();\nList\u003cString\u003e interceptors = new ArrayList\u003c\u003e();\ninterceptors.add(\"com.yourcompany.kafkaproject.interceptors.AddTimestampInterceptor\"); // 拦截器1\ninterceptors.add(\"com.yourcompany.kafkaproject.interceptors.UpdateCounterInterceptor\"); // 拦截器2\nprops.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);\n……\n```\n\n  \n\n暂时无法在飞书文档外展示此内容\n\n我们应该怎么编写 AddTimeStampInterceptor 和 UpdateCounterInterceptor 类呢？其实很简单，这两个类以及你自己编写的所有 Producer 端拦截器实现类都要继承 **org.apache.kafka.clients.producer.ProducerInterceptor 接口。**\n\n- onSend：该方法会在消息发送之前被调用。如果你想在发送之前对消息“美美容”，这个方法是你唯一的机会。\n    \n- onAcknowledgement：该方法会在**消息成功提交或发送失败之后被调用**。还记得我在上一期中提到的发送回调通知 callback 吗？**onAcknowledgement 的调用要早于 callback 的调用**。值得注意的是，**这个方法和 onSend 不是在同一个线程中被调用的，因此如果你在这两个方法中调用了某个共享可变对象，一定要保证线程安全哦。**还有一点很重要，这个方法处在 Producer 发送的主路径中，所以最好别放一些太重的逻辑进去，否则你会发现你的 Producer TPS 直线下降","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%BE%85%E7%BB%AD":{"title":"服务端待续","content":"","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E6%B6%88%E6%81%AF%E5%BC%95%E6%93%8E":{"title":"消息引擎","content":"Apache Kafka 是一款开源的消息引擎系统。\n\n最基础的消息引擎就做两个重要的事实：\n\n- 消息引擎传输的对象是消息；\n    \n- 如何传输消息属于消息引擎设计机制的一部分。\n    \n\n\n\n\n# 消息的格式\n\n那么现在我告诉你 Kafka 的选择：**它使用的是纯二进制的字节序列**。\n\n  \n\n\n\n# **传输方法**\n\n消息引擎系统还要设定具体的传输协议，即我用什么方法把消息传输出去。常见的有两种方法：\n\n  \n\n- 点对点模型：也叫消息队列模型。如果拿上面那个“民间版”的定义来说，那么系统 A 发送的消息只能被系统 B 接收，其他任何系统都不能读取 A 发送的消息。日常生活的例子比如电话客服就属于这种模型：同一个客户呼入电话只能被一位客服人员处理，第二个客服人员不能为该客户服务。\n    \n- 发布 / 订阅模型：与上面不同的是，它有一个主题（Topic）的概念，你可以理解成逻辑语义相近的消息容器。该模型也有发送方和接收方，只不过提法不同。发送方也称为发布者（Publisher），接收方称为订阅者（Subscriber）。和点对点模型不同的是，这个模型可能存在多个发布者向相同的主题发送消息，而订阅者也可能存在多个，它们都能接收到相同主题的消息。生活中的报纸订阅就是一种典型的发布 / 订阅模型。\n    \n\n  \n\n# Kafka 的作用\n\n削峰填谷，所谓的“削峰填谷”就是指缓冲上下游瞬时突发流量，使其更平滑。特别是对于那种发送能力很强的上游系统，如果没有消息引擎的保护，“脆弱”的下游系统可能会直接被压垮导致全链路服务“雪崩”。\n\n\n# kafka 整体架构\n\n![](statistic/asynccode-507.png)\n\n  \n\n# Kafka 主体核心流程\n\n![](statistic/asynccode-509.png)\n\n# 数据如何存储\n\n![](statistic/asynccode-510.png)\n\n# 如何查找消息，旧数据如何清理\n\n![](statistic/asynccode-511.png)\n\n# 如何让多个消费者重复消费\n\n![](statistic/asynccode-508.png)","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E7%AE%A1%E7%90%86%E7%9B%91%E6%8E%A7%E5%BE%85%E7%BB%AD":{"title":"管理监控待续","content":"","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/kafka-%E6%9C%AF%E8%AF%AD":{"title":"kafka 术语","content":"\n- 消息：Record。Kafka 是消息引擎嘛，这里的消息就是指 Kafka 处理的主要对象。\n    \n- 主题：Topic。主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。\n    \n- 分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。\n    \n- 消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。\n    \n- 副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。\n    \n- 生产者：Producer。向主题发布新消息的应用程序。\n    \n- 消费者：Consumer。从主题订阅新消息的应用程序。\n    \n- 消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。\n    \n    - 每个消费者在消费消息的过程中必然需要有个字段记录它当前消费到了分区的哪个位置上，这个字段就是消费者位移（Consumer Offset）\n        \n    - 这和上面所说的位移完全不是一个概念。上面的“位移”表征的是分区内的消息位置，它是不变的，即一旦消息被成功写入到一个分区上，它的位移值就是固定的了\n        \n    - 而消费者位移则不同，它可能是随时变化的，毕竟它是消费者消费进度的指示器嘛。另外每个消费者有着自己的消费者位移，因此一定要区分这两类位移的区别。我个人把消息在分区中的位移称为分区位移，而把消费者端的位移称为消费者位移。\n        \n- 消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐\n    \n- 重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。\n    \n- 最后我用一张图来展示上面提到的这些概念，希望这张图能够帮助你形象化地理解所有这些概念：\n    \n\n![](statistic/asynccode-556.png)\n\n# 高可用\n\n## 部署\n\nKafka 的服务器端由被称为 **Broker 的服务进程**构成，即一个 Kafka 集群由多个 Broker 组成，B**roker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化**。将不同的 Broker 分散运行在不同的机器上，这样如果集群中某一台机器宕机，即使在它上面运行的所有 Broker 进程都挂掉了，其他机器上的 Broker 也依然能够对外提供服务。\n\n  \n\n## 备份\n\n实现高可用的另一个手段就是备份机制（Replication）。备份的思想很简单，就是把相同的数据拷贝到多台机器上，而这些相同的数据拷贝在 Kafka 中被称为副本（Replica）\n\nKafka 定义了两类副本：领导者副本（Leader Replica）和追随者副本（Follower Replica）。\n\n- 前者对外提供服务，这里的对外指的是与客户端程序进行交互；生产者总是向领导者副本写消息；而消费者总是从领导者副本读消息\n    \n- 而后者只是被动地追随领导者副本而已，不能与外界进行交互。向领导者副本发送请求，请求领导者把最新生产的消息发给它，这样它能保持与领导者的同步。\n    \n\n  \n\n  \n\n# 伸缩性\n\n拿副本来说，虽然现在有了领导者副本和追随者副本，但倘若**领导者副本积累了太多的数据以至于单台 Broker 机器都无法容纳了**，此时应该怎么办呢？一个很自然的想法就是，能否把数据**分割成多份保存在不同的 Broker** 上？如果你就是这么想的，那么恭喜你，Kafka 就是这么设计的。\n\n  \n\nKafka 中的分区机制指的是将**每个主题划分成多个分区（Partition）**，每个分区是一组有序的消息日志。生产者生产的每条消息只会被发送到一个分区中，也就是说**如果向一个双分区的主题发送一条消息，这条消息要么在分区 0 中，要么在分区 1 中**。如你所见，Kafka 的分区编号是从 0 开始的，如果 Topic 有 100 个分区，那么它们的分区号就是从 0 到 99\n\n  \n\n每个分区下可以配置若干个副本，其中只能有 1 个领导者副本和 N-1 个追随者副本\n\n  \n\nKafka 的三层消息架构\n\n- 第一层是主题层，每个主题可以配置 M 个分区，而每个分区又可以配置 N 个副本。\n    \n- 第二层是分区层，每个分区的 N 个副本中只能有一个充当领导者角色，对外提供服务；其他 N-1 个副本是追随者副本，只是提供数据冗余之用。\n    \n- 第三层是消息层，分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。最后，客户端程序只能与分区的领导者副本进行交互。\n    \n\n  \n\n  \n\n# 持久性\n\nKafka 使用消息日志（Log）来保存数据，一个日志就是磁盘上一个只能追加写（Append-only）消息的物理文件。\n\nKafka 必然要定期地删除消息以回收磁盘。怎么删除呢？\n\n简单来说就是通过**日志段（Log Segment）机制。**在 Kafka 底层，一个日志又进一步细分成多个日志段，消息被追加写到当前最新的日志段中，当写满了一个日志段后，Kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。\n\n  \n\n  \n\n  \n\n# Kafka 实现两种消息模型\n\n点对点模型（Peer to Peer，P2P）和发布订阅模型。这里面的点对点指的是**同一条消息只能被下游的一个消费者消费，其他消费者则不能染指**。在 Kafka 中实现这种 P2P 模型的方法就是引入了**消费者组（Consumer Group）**。所谓的消费者组，**指的是多个消费者实例共同组成一个组来消费一组主题。这组主题中的每个分区都只会被组内的一个消费者实例消费**，其他消费者实例不能消费它。为什么要引入消费者组呢？主要是为了提升消费者端的吞吐量。多个消费者实例同时消费，加速整个消费端的吞吐量（TPS）\n\n  \n\n  \n\n# 重平衡\n\n消费者组里面的所有消费者实例不仅“瓜分”订阅主题的数据，而且更酷的是它们还能彼此协助。假设组内某个实例挂掉了，Kafka 能够自动检测到，然后**把这个 Failed 实例之前负责的分区转移给其他活着的消费者**。这个过程就是 Kafka 中大名鼎鼎的“重平衡”（Rebalance）","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RockerMQ/RockerMQ%E4%BB%8B%E7%BB%8D":{"title":"RockerMQ介绍","content":"  \n\n# RocketMQ 的优缺点\n\n- RocketMQ优点：\n    \n    - 单机吞吐量：十万级\n        \n    - 可用性：非常高，分布式架构\n        \n    - 消息可靠性：经过参数优化配置，**消息可以做到0丢失**\n        \n    - 功能支持：MQ功能较为完善，还是分布式的，扩展性好\n        \n    - 支持10亿级别的消息堆积，不会因为堆积导致性能下降\n        \n    - 源码是Java，方便结合公司自己的业务二次开发\n        \n    - 天生为金融互联网领域而生，对于可靠性要求很高的场景，尤其是电商里面的订单扣款，以及业务削峰，在大量交易涌入时，后端可能无法及时处理的情况RoketMQ 在稳定性上可能更值得信赖，这些业务场景在阿里双11已经经历了次考验，如果你的业务有上述并发场景，建议可以选择 RocketMQ\n        \n- RocketMQ缺点：\n    \n    - 支持的客户端语言不多，目前是Java及c++，其中c++不成熟\n        \n    - 没有在 MQ核心中去实现 JMS 等接口，有些系统要迁移需要修改大量代码\n        \n\n  \n\n  \n\n# RocketMQ 的消息模型\n\nRocketMQ使用的消息模型是标准的**发布-订阅模型**\n\n![](statistic/asynccode-527.png)\n\n  \n\n# RocketMQ 的名词介绍\n\n## Message\n\n**Message**（消息）就是要传输的信息。\n\n- 一条消息必须有一个主题（Topic），主题可以看做是你的信件要邮寄的地址。\n    \n- 一条消息也可以拥有一个可选的标签（Tag）和额处的键值对，它们可以用于设置一个业务 Key 并在 Broker 上查找此消息以便在开发期间查找问题。\n    \n\n## Topic\n\n**Topic**（主题）可以看做消息的归类，它是消息的第一级类型。比如一个电商系统可\n\n- **Topic** 与生产者和消费者的关系非常松散，一个 Topic 可以有0个、1个、多个生产者向其发送消息，一个生产者也可以同时向不同的 Topic 发送消息。\n    \n- 一个 Topic 也可以被 0个、1个、多个消费者订阅。\n    \n\n## Tag\n\n**Tag**（标签）可以看作子主题，**它是消息的第二级类型**，用于为用户提供额外的灵活性。使**用标签，同一业务模块不同目的的消息就可以用相同 Topic 而不同的 Tag 来标识**。\n\n- 一条消息可以没有 **Tag** 。\n    \n- 标签有助于保持你的代码干净和连贯，并且还可以为 **RocketMQ** 提供的查询系统提 供帮助。\n    \n\n## Group\n\nRocketMQ中，**订阅者的概念是通过消费组（Consumer Group）来体现的**。\n\n- **每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响**，也就是说，一条消息被Consumer Group1消费过，也会再给Consumer Group2消费。\n    \n- 消费组中包含多个消费者，**同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息**。默认情况，如果一条消息被消费者Consumer1消费了，那同组的其他消费者就不会再收到这条消息。\n    \n\n## Message Queue\n\n**Message Queue**（消息队列），**一个 Topic 下可以设置多个消息队列，Topic 包括多个 Message Queue ，如果一个 Consumer 需要获取 Topic下所有的消息**，就要遍历所有的 Message Queue。\n\nRocketMQ还有一些其它的Queue——例如ConsumerQueue。\n\n## Offset\n\n在Topic的消费过程中，由于消息需要被不同的组进行多次消费，**所以消费完的消息并不会立即被删除，这就需要RocketMQ为每个消费组在每个队列上维护一个消费位置（Consumer Offset）**，这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。\n\n![](statistic/asynccode-547.png)\n\n  \n\n# RocketMQ 的基本架构\n\n![](statistic/asynccode-533.png)\n\nRocketMQ 一共有是个部分\n\n- NameServer--发现\n    \n- Broker--存\n    \n- Producer生产者-发\n    \n- Consumer消费者-收\n    \n\n![](statistic/asynccode-528.png)\n\n## **NameServer**\n\nNameServer 是一**个无状态的服务器，角色类似于 Kafka使用的 Zookeeper**，但比Zookeeper 更轻量。\n\n- 特点：\n    \n    - **每个 NameServer 结点之间是相互独立，彼此没有任何信息交互**。\n        \n    - Nameserver 被设计成几乎是无状态的，通过部署多个结点来标识自己是一个伪集群，**Producer 在发送消息前从 NameServer 中获取 Topic 的路由信息也就是发往哪个 Broker，Consumer 也会定时从 NameServer 获取 Topic 的路由信息**，Broker在启动时会向 NameServer 注册，并定时进行心跳连接，且定时同步维护的 Topic到 NameServer。\n        \n- 功能主要有两个：\n    \n    - 和Broker 结点保持长连接。\n        \n    - 维护 Topic 的路由信息\n        \n\n## **Broker**\n\n消息存储和中转角色，**负责存储和转发消息。**\n\n- Broker 内部维护着一个个 Consumer Queue，用来存储消息的索引，真正存储消息的地方**是 CommitLog（日志文件）**。\n    \n- 单个 Broker **与所有的 Nameserver 保持着长连接和心跳**，并会定时将 Topic 信息同步到 NameServer，**和 NameServer 的通信底层是通过 Netty 实现的。**\n    \n\n![](statistic/asynccode-543.png)\n\n## Producer\n\n消息生产者，业务端负责发送消息，由用户自行实现和分布式部署。\n\n- **Producer**由用户进行分布式部署，消息由**Producer**通过多种负载均衡模式发送到**Broker**集群，发送低延时，支持快速失败。\n    \n- **RocketMQ** 提供了三种方式发送消息：同步、异步和单向\n    \n    - 同步发送 ：**同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包**。一般用于重要通知消息，例如重要通知邮件、营销短信。\n        \n    - 异步发送 ：**异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包**，**有回调**。一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。\n        \n    - 单向发送 ：单向发送是指只负责发送消息**而不等待服务器回应且没有回调函数触发**，适用于某些耗时非常短但对可靠性要求并不高的场景，例如**日志收集**。\n        \n\n## **Consumer**\n\n消息消费者，**负责消费消息，一般是后台系统负责异步消费。**\n\n- Consumer 也由用户部署，**支持PUSH和PULL两种消费模式，支持集群消费和广播消费** ，**提供实时的消息订阅机制 。**\n    \n- Pull ：拉取型消费者（Pull Consumer）**主动从消息服务器拉取信息**，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。\n    \n- Push ：推**送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作**，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型**，但其实从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器**，当监听器处触发后才开始消费消息\n\n\n# 如何保证消息的可用性**/**可靠性**/**不丢失呢\n\n消息可能在哪些阶段丢失呢？可能会在这三个阶段发生丢失：生**产阶段、存储阶 段、消费阶段。**\n\n  \n\n![](statistic/asynccode-529.png)\n\n## 生**产阶段**\n\n在生产阶段，主**要通过请求确认机制，来保证消息的可靠传递。**\n\n1. 同步发送的时候，**要注意处理响应结果和异常**。如果返回响应OK，表示消息成功发送到了Broker，如果响应失败，或者发生其它异常，都应该重试。\n    \n2. 异步发送的时候，**应该在回调方法里检查，如果发送失败或者异常，都应该进行重试**。\n    \n3. 如果发生超时的情况，**也可以通过查询日志的API，来检查是否在Broker存储成功**。\n    \n\n## **存储阶段**\n\n可以通过**配置可靠性优先的 Broker 参数来避免因为宕机丢消息**，简单说就是可靠性优先的场景都应该使用同步\n\n1. **消息只要持久化到CommitLog（日志文件）中**，即使Broker宕机，**未消费的消息也能重新恢复再消费**。\n    \n2. **Broker的刷盘机制：同步刷盘和异步刷盘，**不管哪种刷盘都可以保证消息**一定存储在pagecache中（内存中）**，**但是同步刷盘更可靠**，它是Producer发送消息后等数据持久化到磁盘之后再返回响应给Producer。\n    \n\n  \n\n![](statistic/asynccode-530.png)\n\n  \n\n3. Broker通过主从模式来保证高可用，**Broker支持Master和Slave同步复制、Master和Slave异步复制模式**，生产者的消息都是发送给Master，但是消费既可以从Master消费，也可以从Slave消费。**同步复制模式可以保证即使Master宕机，消息肯定在Slave中有备份，保证了消息不会丢失。**\n    \n\n  \n\n## **消费阶段**\n\nConsumer保证消息成功消费的**关键在于确认的时机**，**不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确**认。\n\n因 为消息队列维护了消费的位置，逻辑执行失败了，没有确认，再去队列拉取消\n\n息，就还是之前的一条\n\n  \n\n# 如何处理消息重复的问题呢\n\n处理消息重复问题，**主要有业务端自己保证，主要的方式有两种：业务幂等和消息去重**。\n\n- 业务幂等：**第一种是保证消费逻辑的幂等性，也就是多次调用和一次调用的效果是一样的**。这样一来，不管消息消费多少次，对业务都没有影响\n    \n- 消息去重：第二种是业务端，**对重复的消息就不再消费了。这种方法，需要保证每 条消息都有一个惟一的编号，通常是业务相关的，比如订单号，消费的记录需要落库，而且需要保证和消息确认这一步的原子性。**\n    \n\n  \n\n# 如何处理消息积压\n\n发生了消息积压，这时候就得想办法赶紧把积压的消息消费完，就得考虑提高消费能力，一般有两种办法\n\n![](statistic/asynccode-536.png)\n\n- 消费者扩容 **：如果当前Topic的Message Queue的数量大于消费者数量，就可以对 消费者进行扩容，增加消费者，**来提高消费能力，尽快把积压的消息消费玩。\n    \n- 消息迁移Queue扩容 ：如果当前Topic的Message Queue的数量小于或者等于消费者数量，这种情况，再扩容消费者就没什么用，**就得考虑扩容Message Queue**。可以新建一个临时的Topic，**临时的Topic多设置一些Message Queue，然后先用一些消费者把消费的数据丢到临时的Topic**，因为不用业务处理，**只是转发一下消息，还是很快的。接下来用扩容的消费者去消费新的Topic里的数据，消费完了之后，恢复原状。**\n    \n\n![](statistic/asynccode-546.png)\n\n# 顺序消息如何实现\n\n顺序消息分为**全局顺序消息和部分顺序消息**，\n\n- 全局顺序消息指某个 Topic 下的所有消息都要保证顺序\n    \n- 部分顺序消息只要保证每一组消息被顺序消费即可\n    \n\n![](statistic/asynccode-545.png)\n\n  \n\n  \n\n## 部分顺序消息\n\n- **生产端需要做到把同 ID 的消息发送到同一个 Message Queue**\n    \n- 在消费过程中，要做到从同一个Message Queue读取的消息顺序处理——**消费端不能并发处理顺序消息，这样才能达到部分有序。**\n    \n\n![](statistic/asynccode-535.png)\n\n发送端使用 MessageQueueSelector 类来控制 把消息发往哪个 Message Queue 。\n\n![](statistic/asynccode-543.png)\n\n![](statistic/asynccode-550.png)\n\n消费端通过使用 **MessageListenerOrderly** 来解决单 Message Queue 的消息被并发处理的问题\n\n![](statistic/asynccode-542.png)\n\n## 全局顺序消息\n\nRocketMQ 默认情况下不保证顺序，\n\n- 比如创建一个 Topic ，默认八个写队列，八个读 队列，**这时候一条消息可能写入任意一个队列里**；\n    \n- 在数据的读取过程中，可能有多个 Consumer ，**每个 Consumer 也可能启动多个线程并行处理，所以消息被哪个 Consumer 消费，被消费的顺序和写人的顺序是否一致是不确定的**\n    \n\n  \n\n要保证全局顺序消息， 需要先把 Topic 的**读写队列数设置为 一**，**然后Producer Consumer 的并发设置，也要是一**。简单来说，为了保证整个 Topic全局消息有序， 只能消除所有的并发处理，各部分都设置成单线程处理 ，这时候就完全牺牲 RocketMQ的高并发、高吞吐的特性了。\n\n  \n\n  \n\n  \n\n# 如何实现消息过滤\n\n- **一种是在 Broker 端按照 Consumer 的去重逻辑进行过滤**，这样做的好处是避免了无用的消息传输到 Consumer 端，缺点是加重了 Broker 的负担，实现起来相对复杂。\n    \n- 另一种是在 Consumer 端过滤，比如按照消息设置的 tag 去重，这样的好处是实现起来简单，**缺点是有大量无用的消息到达了 Consumer 端只能丢弃不处理**\n    \n\n![](statistic/asynccode-531.png)\n\n- 根据Tag过滤：这是最常见的一种，用起来高效简单\n    \n    ![](statistic/asynccode-535.png)\n    \n- SQL 表达式过滤：SQL表达式过滤更加灵活\n    \n\n![](statistic/asynccode-550.png)\n\n- Filter Server 方式：最灵活，也是最复杂的一种方式，允许用户自定义函数进行 过滤\n    \n\n  \n\n  \n\n# 延时消息如何实现\n\n用户提交了一个订单，就可以发送一个延时消息，1h后去检查这个订单的状态，如果还是未付款就\n\n取消订单释放库存。\n\nRocketMQ是支持延时消息的，**只需要在生产消息的时候设置消息的延时级别**：\n\n![](statistic/asynccode-546.png)\n\nRocketMQ支持的延时级别是有限的\n\n![](statistic/asynccode-532.png)\n\n## RocketMQ怎么实现延时消息？\n\n简单，八个字： 临时存储 + 定时任务\n\nBroker收到延时消息了，**会先发送到主题（SCHEDULE_TOPIC_XXXX）的相应时间**\n\n**段的Message Queue中**，然后通过一个定时任务轮询这些队列，到期后，把消息投递到目标Topic的队列中，然后消费者就可以正常消费这些消息\n\n![](statistic/asynccode-538.png)\n\n  \n\n# 如何实现分布式消息事务？半消息\n\n半消息：是指暂时还不能被 Consumer 消费的消息，**Producer 成功发送到 Broker 端 的消息，但是此消息被标记为 “暂不可投递” 状态**，只有等 Producer 端执行完**本地事务后经过二次确认了之后，Consumer 才能消费此条消息**\n\n![](statistic/asynccode-538.png)\n\n1. Producer 向 broker 发送半消息\n    \n2. Producer 端收到响应，消息发送成功，此时消息是半消息，标记为 “不可投递” 状态，Consumer 消费不了。\n    \n3. Producer 端执行本地事务。\n    \n4. 正常情况本地事务执行完成，Producer 向 Broker 发送 Commit/Rollback，如果是 Commit，Broker 端将半消息标记为正常消息，Consumer 可以消费，如果是Rollback，Broker 丢弃此消息。\n    \n5. 异常情况，Broker 端迟迟等不到二次确认。在一定时间后，会查询所有的半消息，然后到 Producer 端查询半消息的执行情况。\n    \n6. Producer 端查询本地事务的状态\n    \n7. 根据事务的状态提交 commit/rollback 到 broker 端。（5，6，7 是消息回查）\n    \n8. 消费者段消费到消息之后，执行本地事务，执行本地事务。\n    \n\n  \n\n# 死信队列知道吗？\n\n死信队列用于处理无法被正常消费的消息，即死信消息。\n\n当一条消息初次消费失败，消息队列 **RocketMQ** 会自动进行消息重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消 息，此时，消息队列 RocketMQ 不会立刻将消息丢弃，**而是将其发送到该消费者对应的特殊队列中，该特殊队列称为死信队列**。\n\n死信消息的特点：\n\n1. **不会再被消费者正常消费。**\n    \n2. 有效期与正常消息相同，均为 3 天，3 天后会被自动删除。因此，需要在死信消息产生后的 3 天内及时处理。\n    \n\n死信队列的特点：\n\n1. **一个死信队列对应一个 Group ID， 而不是对应单个消费者实例。**\n    \n2. **如果一个 Group ID 未产生死信消息，消息队列 RocketMQ 不会为其创建相应的死信队列。**\n    \n3. 一个死信队列包含了对应 Group ID 产生的所有死信消息**，不论该消息属于哪个 Topic。**\n    \n\nRocketMQ 控制台提供对死信消息的查询、导出和重发的功能。\n\n  \n\n# 如何保证RocketMQ 的高可用\n\n## NameServer的高可用\n\n**NameServer因为是无状态，且不相互通信的**，所以只要集群部署就可以保证高可用。\n\nRocketMQ的高可用主要是在体现在Broker的读和写的高可用，**Broker的高可用是通过 集群 和 主从 实现的**\n\n![](statistic/asynccode-546.png)\n\nBroker可以配置两种角色：**Master和Slave，Master角色的Broker支持读和写，Slave角色的Broker只支持读，Master会向Slave同步消息。**也就是说Producer只能向Master角色的Broker写入消息，Cosumer可以从Master和Slave 角色的Broker读取消息。\n\n## Broker读的高可用\n\nConsumer 的配置文件中，**并不需要设置是从 Master 读还是从 Slave读，当 Master 不可用或者繁忙的**时候， **Consumer 的读请求会被自动切换到从 Slave**。有了自动切换Consumer 这种机制，当一个 Master 角色的机器出现故障后，**Consumer 仍然可以从Slave 读取消息，不影响 Consumer 读取消息，这就实现了读的高可用**。\n\n  \n\n## Broker写的高可用\n\n如何达到发送端写的高可用性呢？**在创建 Topic 的时候，把 Topic 的多个Message Queue 创建在多个 Broker 组上（相同 Broker 名称，不同 brokerId机器组成 Broker 组），这样当 Broker 组的 Master 不可用后，其他组Master 仍然可用， Producer 仍然可以发送消息 RocketMQ 目前还不支持把Slave自动转成 Master** ，如果机器资源不足，需要把 Slave 转成 Master ，则要手动停止 Slave 色的 Broker ，更改配置文件，用新的配置文件启动 Broker\n\n\n\n# 说下 RocketMQ 的整体流程\n\n简单来说，RocketMQ是一个分布式消息队列，也就是 消息队列 + 分布式系统 。\n\n- 作为消息队列，它是 **发 - 存 - 收 的一个模型，对应的就是Producer、Broker、 Cosumer**；\n    \n- 作为分布式系统，**它要有服务端、客户端、注册中心，对应的就是 Broker、Producer/Consumer、NameServer**\n    \n\n所以我们看一下它主要的工作流程：RocketMQ由NameServer注册中心集群、Producer生产者集群、Consumer消费者集群和若干Broker（RocketMQ进程）组成：\n\n1. Broker在启动的时候去向所有的NameServer注册，并保持长连接，每30s发送一次心跳\n    \n2. Producer在发送消息的时候从NameServer获取Broker服务器地址，根据负载均衡算法选择一台服务器来发送消息\n    \n3. Conusmer消费消息的时候同样从NameServer获取Broker地址，然后主动拉取消息 来消费\n    \n\n![](statistic/asynccode-538.png)\n\n## 为啥RockerMQ 不使用zookeeper 作为注册中心\n\nKafka我们都知道采用Zookeeper作为注册中心——当然也开始逐渐去Zookeeper， RocketMQ不使用Zookeeper其实主要可能从这几方面来考虑：\n\n1. 基于可用性的考虑，根据CAP理论，**同时最多只能满足两个点，而Zookeeper满 足的是CP，也就是说Zookeeper并不能保证服务的可用性，Zookeeper在进行选举 的时候，整个选举的时间太长**，期**间整个集群都处于不可用的状态，而这对于一 个注册中心来说肯定是不能接受的，**作为服务发现来说就应该是为可用性而设 计。\n    \n2. **基于性能的考虑，NameServer本身的实现非常轻量，而且可以通过增加机器的方式水平扩展，增加集群的抗压能力，而Zookeeper的写是不可扩展的，Zookeeper 要解决这个问题只能通过划分领域，**划分多个Zookeeper集群来解决，首先操作 起来太复杂，其次这样还是又违反了CAP中的A的设计，导致服务之间是不连通 的。\n    \n3. 持久化的机制来带的问题，**ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像 （Snapshot）到磁盘来保证数据的一致性和持久性，而对于一个简单的服务发现 的场景来说，这其实没有太大的必要，这个实现方案太重了。**而且本身存储的数 据应该是高度定制化的。\n    \n4. **消息发送应该弱依赖注册中心，而RocketMQ的设计理念也正是基于此，生产者 在第一次发送消息的时候从NameServer获取到Broker地址后缓存到本地**，如果 NameServer整个集群不可用，短时间内对于生产者和消费者并不会产生太大影 响。\n    \n\n  \n\n# Broker 是如何保存数据的\n\nRocketMQ主要的存储文件包括CommitLog文件、ConsumeQueue文件、Indexfile文件。\n\n![](statistic/asynccode-537.png)\n\n消息的整体设计\n\n![](statistic/asynccode-550.png)\n\n## CommitLog\n\n**消息主体以及元数据的存储主体，存储Producer端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G, 文件名长度为20位，左边补零，剩余为起始偏移量**，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。**消息主要是顺序写入日志文件，当文件满了，写入下一个文件**。\n\nCommitLog文件保存于${Rocket_Home}/store/commitlog目录中，从图中我们可以 明显看出来文件名的偏移量，每个文件默认1G，写满后自动生成一个新的文件\n\n![](statistic/asynccode-543.png)\n\n## **ConsumeQueue - 可以看做是CommitLog的索引文件**\n\n消息消费队列，引入的目的主要是提高消息消费的性能，**由于RocketMQ是基于主题topic的订阅模式，消息消费是针对主题进行的，如果要遍历commitlog文件中根据topic检索消息是非常低效的**。\n\n**Consumer即可根据ConsumeQueue来查找待消费的消息**。其中，C**onsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量offset，消息大小size和消息Tag的 HashCode值**。\n\nConsumeQueue文件可以看成是基于Topic的CommitLog索引文件，故\n\nConsumeQueue文件夹的组织方式如下：**topic/queue/file三层组织结构，具体存储 路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}**。\n\n同样 ConsumeQueue文件采取定长设计，每一个条目共20个字节，分别为8字节的\n\nCommitLog物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个ConsumeQueue文 件大小约5.72M；\n\n![](statistic/asynccode-535.png)\n\n## **IndexFile**\n\n**IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法**。Index文件的存储位置是：$HOME \\store\\index${fileName}，文件名 fileName是以创建时的时间戳命名的，固定的单个IndexFile文件大小约为400M， 一个IndexFile可以保存 2000W个索引**，IndexFile的底层存储设计为在文件系统中**\n\n**实现HashMap结构，故RocketMQ的索引文件其底层实现为hash索引**\n\n![](statistic/asynccode-540.png)\n\n  \n\n  \n\n## 总结\n\n- RocketMQ采用的是混合型的存储结构，即为**Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。**\n    \n- RocketMQ的混合型存储结构(多个Topic的消息实体内容都存储于一个CommitLog中) **针对Producer和Consumer分别采用了数据和索引部分相分离的存储结构**，Producer发 送消息至Broker端，然后Broker端使用同步或者异步的方式对消息刷盘持久化，保存至CommitLog中。\n    \n- 只要消息被刷盘持久化至磁盘文件CommitLog中，那么Producer发送的消息就不会丢失。正因为如此，Consumer也就肯定有机会去消费这条消息。\n    \n    - 当无法拉取到消息后，可以等下一次消息拉取，同时服务端也支持长轮询模式，\n        \n    - 如果一个消息拉取请未拉取到消息，Broker允许等待30s的时间，只要这段时间内有新消息到达，将直接返回给消费端。\n        \n        - 这里，RocketMQ的具体做法是，**使用Broker端的后台服务线程— ReputMessageService不停地分发请求并异步构建ConsumeQueue（逻辑消费队列）和 IndexFile（索引文件）数据。**\n            \n\n  \n\n  \n\n# RocketMQ 怎么对文件进行读写的\n\nRocketMQ对文件的读写巧妙地利用了操作系统的一些高效文件读写方式—— PageCache 、 顺序读写 、 零拷贝\n\n  \n\n## PageCache、顺序读取\n\n  \n\n- ConsumeQueue逻辑消费队列存储的数据较少，并且是顺序读取，在page cache机制的预读取作用下，Consume Queue文件的读性能几乎接近读内存，即使在有消息堆积情况下也不会影响性能。\n    \n\n  \n\n而对于**CommitLog消息存储的日志数据文件来说，读取消息内容时候会产生较多的随机访问读取**，严重影响性能。\n\n- 如果选择**合适的系统IO调度算法**，比如设置调度算法为“Deadline”（此时块存储采用SSD的话），随机读的性能也会有所提升。\n    \n- **页缓存（PageCache)是OS对文件的缓存**，用于加速对文件的读写。一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写速度，主要原因就是由于OS使用**PageCache机制对读写访问操作进行了性能优化**，将一部分的内存用作PageCache**。**\n    \n    - **对于数据的写入，OS会先写入至Cache内，随后通过异步的方式由pdflush内核线程将Cache内的数据刷盘至物理磁盘上**。\n        \n    - 对于数据的读取，如果一次读取文件时出现未命中PageCache的情况，OS从物理磁盘上访问读取文件的同时，**会顺序对其他相邻块的数据文件进行预读取**。\n        \n\n## 零拷贝\n\n另外**，RocketMQ主要通过MappedByteBuffer对文件进行读写操作**。\n\n- 其中，利用了NIO中的FileChannel模型**将磁盘上的物理文件直接映射到用户态的内存地址中**（这种Mmap的方式减少了传统IO，将磁盘文件数据在操作系统内核地址空间的缓冲区，和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），**将对文件的操作转化为直接对内存地址进行操作**，从而极大地提高了文件的读写效率（**正因为需要使用内存映射机制，故RocketMQ的文件存储都使用定长结构来存储，方便一次将整个文件映射至内存**）。\n    \n\n  \n\n### 啥是零拷贝\n\n在操作系统中，使用传统的方式，数据需要经历几次拷贝，还要经历用户态/内核态切换。\n\n![](statistic/asynccode-539.png)\n\n1. 从磁盘复制数据到内核态内存；\n    \n2. 从内核态内存复制到用户态内存；\n    \n3. 然后从用户态内存复制到网络驱动的内核态内存；\n    \n4. 最后是从网络驱动的内核态内存复制到网卡中进行传输。\n    \n\n可以通过零拷贝的方式，**减少用户态与内核态的上下文切换和内存拷贝次数**，用来提升I/O的性能。零拷贝比较常见的实现方式是**mmap**，这种机制在Java中是通过MappedByteBuffer实现的\n\n![](statistic/asynccode-538.png)\n\n# 消息刷盘怎么实现的呢\n\nRocketMQ提供了两种刷盘策略：同步刷盘和异步刷盘\n\n- 同步刷盘：**在消息达到Broker的内存之后，必须刷到commitLog日志文件中才算成功**，然后返回Producer数据已经发送成功。\n    \n- 异步刷盘：**异步刷盘是指消息达到Broker内存后就返回Producer数据已经发送成功，会唤醒一个线程去将数据持久化到CommitLog日志文件中。**\n    \n\n**Broker** 在消息的存取时直接操作的是内存（内存映射文件），这可以提供系统的吞吐量，但是无法避免机器掉电时数据丢失，所以需要持久化到磁盘中。\n\n刷盘的最终实现都是使用**NIO**中的 MappedByteBuffer.force() 将映射区的数据写入到磁盘，\n\n- 如果是同步刷盘的话，在**Broker把消息写到CommitLog映射区后**，就会等待写入完成。\n    \n- 异步而言，只是唤醒对应的线程，不保证执行的时机，流程如图所示。\n    \n\n![](statistic/asynccode-534.png)\n\n# **RocketMQ** 的负载均衡是如何实现的\n\nRocketMQ中的负载均衡都在Client端完成，具体来说的话，主要可以分为Producer端发送消息时候的负载均衡和Consumer端订阅消息的负载均衡。\n\n  \n\n## **Producer**的负载均衡\n\nProducer端在发送消息的时候，会先根据Topic找到指定的TopicPublishInfo，**在获取了**\n\n**TopicPublishInfo路由信息后**，RocketMQ的客户端在默认方式下 selectOneMessageQueue()方法会从TopicPublishInfo中的messageQueueList中选择一个队列（MessageQueue）进行发送消息。具这里有一个sendLatencyFaultEnable开关变量，如果开启，在随机递增取模的基础上，再过滤掉not available的Broker代理。\n\n![](statistic/asynccode-548.png)\n\n## **Consumer**的负载均衡\n\n  \n\n消息消费队列在同一消费组不同消费者之间的负载均衡，其核心设计理念是在一个 消息消费队列在同一时间只允许被同一消费组内的一个消费者消费，一个消息消费者能同时消费多个消息队列。\n\n  \n\n在RocketMQ中，C**onsumer端的两种消费模式（Push/Pull）都是基于拉模式来获取消 息的**，\n\n- 而在**Push模式只是对pull模式的一种封装**，其本质实现为消息拉取线程在从服务器拉取到一批消息后，然后提交到消息消费线程池后，\n    \n    - 又“马不停蹄”的继续向服务器再次尝试拉取消息。\n        \n    - 如果未拉取到消息，则延迟一下又继续拉取。\n        \n- 在两种基于拉模式的消费方式（Push/Pull）中，**均需要Consumer端知道从Broker端的哪一个消息队列中去获取消息。**因此，有必要在Consumer端来做负载均衡，即Broker端中多个MessageQueue分配给同一个ConsumerGroup中的哪些Consumer消费。\n    \n\n  \n\n  \n\n1. Consumer端的心跳包发送\n    \n\n它就会通过定时任务不断地向RocketMQ集群中的所有Broker实例发送心跳包（其中包含了，消息消费分组名称、订阅关系集合、消息通信模式和客户端id的值等信息）。B**roker端在收到Consumer的心跳消息后，会将它维护在 ConsumerManager的本地缓存变量—consumerTable**，同时并将封装后的客户端网络通道信息保存在本地缓存变量—channelInfoTable中，为之后做Consumer端的负载均衡提供可以依据的元数据信息。\n\n2. Consumer端实现负载均衡的核心类—RebalanceImpl\n    \n\n在Consumer实例的启动流程中的启动MQClientInstance实例部分，**会完成负载均衡服务线程—RebalanceService的启动**（每隔20s执行一次）。\n\n通过查看源码可以发现，**RebalanceService线程的run()方法最终调用的是RebalanceImpl类的rebalanceByTopic()方法，这个方法是实现Consumer端负载均 衡的核心**。\n\nrebalanceByTopic()方法会根据消费者通信类型为“广播模式”还是“集群模式”做不同的逻辑处理。这里主要来看下集群模式下的主要处理流程：\n\n![](statistic/asynccode-544.png)\n\n- 从rebalanceImpl实例的本地缓存变量—topicSubscribeInfoTable中**，获取该Topic主题下的消息消费队列集合（mqSet）**\n    \n- 根据topic和consumerGroup为参数调用mQClientFactory.findConsumerIdList()方法向Broker端发送通信请求，获取该消费组下消费者Id列表；\n    \n- 先对Topic下的消息消费队列、消费者Id排序，然后用消息队列分配策略算法（默认为：消息队列的平均分配算法），计算出待拉取的消息队列。\n    \n    - 这里的平均分配算法，类似于分页的算法，将所有MessageQueue排好序类似于记录，将所有消费端Consumer排好序类似页数，并求出每一页需要包含的平均size和每个页面记录的范围range，最后遍历整个range而计算出当前Consumer端应该分配到的的MessageQueue\n        \n\n![](statistic/asynccode-544.png)\n\n- 然后，调用updateProcessQueueTableInRebalance()方法，具体的做法是，**先将分配到的消息队列集合（mqSet）与processQueueTable做一个过滤比对**\n    \n\n![](statistic/asynccode-541.png)\n\n- 上图中processQueueTable标注的红色部分，表示与分配到的消息队列集合mqSet互不包含。将这些队列设置Dropped属性为true，然后查看这些队列是否可以移除出processQueueTable缓存变量，这里具体执行removeUnnecessaryMessageQueue()方法，即每隔1s 查看是否可以获取当前消费处理队列的锁，拿到的话返回true。 如果等待1s后，仍然拿不到当前消费处理队列的锁则返回false。如果返回true， 则从processQueueTable缓存变量中移除对应的Entry；\n    \n- 上图中processQueueTable的绿色部分，表示与分配到的消息队列集合mqSet的交 集。判断该ProcessQueue是否已经过期了，在Pull模式的不用管，如果是Push模式 的，设置Dropped属性为true，并且调用removeUnnecessaryMessageQueue()方法，像上面一样尝试移除Entry；\n    \n- 最后，为过滤后的消息队列集合（mqSet）中的每个MessageQueue创建一个 ProcessQueue对象并存入RebalanceImpl的processQueueTable队列中（其中调用RebalanceImpl实例的computePullFromWhere(MessageQueue mq)方法获取该MessageQueue对象的下一个进度消费值offset，随后填充至接下来要创建的pullRequest对象属性中），并创建拉取请求对象—pullRequest添加到拉取列表—pullRequestList中，最后执行dispatchPullRequest()方法，将Pull消息的请求对象PullRequest依次放入PullMessageService服务线程的阻塞队列pullRequestQueue中，待该服务线程取出后向Broker端发起Pull消息的请求。其中，可以重点对比下，RebalancePushImpl和RebalancePullImpl两个实现类的dispatchPullRequest()方法不同，RebalancePullImpl类里面的该方法为空。\n    \n\n  \n\n## RocketMQ消息长轮训\n\n所谓的长轮询，就是Consumer 拉取消息**，如果对应的 Queue 如果没有数据，Broker不会立即返回，而是把 PullReuqest hold起**来，等待 queue 有了消息后，或者长轮询阻塞时间到了，**再重新处理该 queue 上的所有 PullRequest**。\n\n![](statistic/asynccode-549.png)\n\n- PullMessageProcessor#processRequest\n    \n\n![](statistic/asynccode-544.png)\n\n挂起的请求，有一个服务线程会不停地检查，看queue中是否有数据，或者超时。\n\n- PullRequestHoldService#run()\n    \n\n![](statistic/asynccode-549.png)","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/Restful":{"title":"Restful","content":"## 何为 API？\n\n![](statistic/asynccode-209.png)\n\n**API（Application Programming Interface）** 翻译过来是应用程序编程接口的意思。\n\n我们在进行后端开发的时候，主要的工作就是为前端或者其他后端服务提供 API 比如查询用户数据的 API 。\n\n![](statistic/asynccode-208.png)\n\n但是， API 不仅仅代表后端系统暴露的接口，像框架中提供的方法也属于 API 的范畴。\n\n为了方便大家理解，我再列举几个例子 🌰：\n\n1. 你通过某电商网站搜索某某商品，电商网站的前端就调用了后端提供了搜索商品相关的 API。\n    \n2. 你使用 JDK 开发 Java 程序，想要读取用户的输入的话，你就需要使用 JDK 提供的 IO 相关的 API。\n    \n3. ......\n    \n\n你可以把 API 理解为程序与程序之间通信的桥梁，其本质就是一个函数而已。另外，API 的使用也不是没有章法的，它的规则由（比如数据输入和输出的格式）API 提供方制定。\n\n# 何为 RESTful API？\n\n**RESTful API** 经常也被叫做 **REST API**，它是基于 REST 构建的 API。这个 REST 到底是什么，我们后文在讲，涉及到的概念比较多。\n\n如果你看 RESTful API 相关的文章的话一般都比较晦涩难懂，主要是因为 REST 涉及到的一些概念比较难以理解。但是，实际上，我们平时开发用到的 RESTful API 的知识非常简单也很容易概括！\n\n举个例子，如果我给你下面两个 API 你是不是立马能知道它们是干什么用的！这就是 RESTful API 的强大之处！\n\n```Plain\nGET    /classes：列出所有班级\nPOST   /classes：新建一个班级\n```\n\n**RESTful API 可以让你看到 URL+Http Method 就知道这个 URL 是干什么的，让你看到了 HTTP 状态码（status code）就知道请求结果如何。**\n\n像咱们在开发过程中设计 API 的时候也应该至少要满足 RESTful API 的最基本的要求（比如接口中尽量使用名词，使用 `POST` 请求创建资源，`DELETE` 请求删除资源等等，示例：`GET /notes/id`：获取某个指定 id 的笔记的信息）。\n\n# 解读 REST\n\n**REST** 是 `REpresentational State Transfer` 的缩写。这个词组的翻译过来就是“**表现层状态转化**”。\n\n这样理解起来甚是晦涩，实际上 REST 的全称是 **Resource Representational State Transfer** ，直白地翻译过来就是 **“资源”在网络传输中以某种“表现形式”进行“状态转移”** 。如果还是不能继续理解，请继续往下看，相信下面的讲解一定能让你理解到底啥是 REST 。\n\n我们分别对上面涉及到的概念进行解读，以便加深理解，实际上你不需要搞懂下面这些概念，也能看懂我下一部分要介绍到的内容。不过，为了更好地能跟别人扯扯 “RESTful API”我建议你还是要好好理解一下！\n\n- **资源（Resource）** ：我们可以把真实的对象数据称为资源。一个资源既可以是一个集合，也可以是单个个体。比如我们的班级 classes 是代表一个集合形式的资源，而特定的 class 代表单个个体资源。每一种资源都有特定的 URI（统一资源标识符）与之对应，如果我们需要获取这个资源，访问这个 URI 就可以了，比如获取特定的班级：`/class/12`。另外，资源也可以包含子资源，比如 `/classes/classId/teachers`：列出某个指定班级的所有老师的信息\n    \n- **表现形式（Representational）**：\"资源\"是一种信息实体，它可以有多种外在表现形式。我们把\"资源\"具体呈现出来的形式比如 `json`，`xml`，`image`,`txt` 等等叫做它的\"表现层/表现形式\"。\n    \n- **状态转移（State Transfer）** ：大家第一眼看到这个词语一定会很懵逼？内心 BB：这尼玛是啥啊？ 大白话来说 REST 中的状态转移更多地描述的服务器端资源的状态，比如你通过增删改查（通过 HTTP 动词实现）引起资源状态的改变。ps:互联网通信协议 HTTP 协议，是一个无状态协议，所有的资源状态都保存在服务器端。\n    \n\n综合上面的解释，我们总结一下什么是 RESTful 架构：\n\n1. 每一个 URI 代表一种资源；\n    \n2. 客户端和服务器之间，传递这种资源的某种表现形式比如 `json`，`xml`，`image`,`txt` 等等；\n    \n3. 客户端通过特定的 HTTP 动词，对服务器端资源进行操作，实现\"表现层状态转化\"。\n    \n\n# RESTful API 规范\n\n![](statistic/asynccode-207.png)\n\n# 动作\n\n- `GET`：请求从服务器获取特定资源。举个例子：`GET /classes`（获取所有班级）\n    \n- `POST` ：在服务器上创建一个新的资源。举个例子：`POST /classes`（创建班级）\n    \n- `PUT` ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：`PUT /classes/12`（更新编号为 12 的班级）\n    \n- `DELETE` ：从服务器删除特定的资源。举个例子：`DELETE /classes/12`（删除编号为 12 的班级）\n    \n- `PATCH` ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。\n    \n\n# 路径（接口命名）\n\n路径又称\"终点\"（endpoint），表示 API 的具体网址。实际开发中常见的规范如下：\n\n1. **网址中不能有动词，只能有名词，API 中的名词也应该使用复数。** 因为 REST 中的资源往往和数据库中的表对应，而数据库中的表都是同种记录的\"集合\"（collection）。如果 API 调用并不涉及资源（如计算，翻译等操作）的话，可以用动词。比如：`GET /calculate?param1=11\u0026param2=33` 。\n    \n2. **不用大写字母，建议用中杠 - 不用下杠 _** 。比如邀请码写成 `invitation-code`而不是 ~~invitation_code~~ 。\n    \n3. **善用版本化 API**。当我们的 API 发生了重大改变而不兼容前期版本的时候，我们可以通过 URL 来实现版本化，比如 `http://api.example.com/v1`、`http://apiv1.example.com` 。版本不必非要是数字，只是数字用的最多，日期、季节都可以作为版本标识符，项目团队达成共识就可。\n    \n4. **接口尽量使用名词，避免使用动词。** RESTful API 操作（HTTP Method）的是资源（名词）而不是动作（动词）。\n    \n\nTalk is cheap！来举个实际的例子来说明一下吧！现在有这样一个 API 提供班级（class）的信息，还包括班级中的学生和教师的信息，则它的路径应该设计成下面这样。\n\n```Plain\nGET    /classes：列出所有班级\nPOST   /classes：新建一个班级\nGET    /classes/{classId}：获取某个指定班级的信息\nPUT    /classes/{classId}：更新某个指定班级的信息（一般倾向整体更新）\nPATCH  /classes/{classId}：更新某个指定班级的信息（一般倾向部分更新）\nDELETE /classes/{classId}：删除某个班级\nGET    /classes/{classId}/teachers：列出某个指定班级的所有老师的信息\nGET    /classes/{classId}/students：列出某个指定班级的所有学生的信息\nDELETE /classes/{classId}/teachers/{ID}：删除某个指定班级下的指定的老师的信息\n```\n\n反例：\n\n```Plain\n/getAllclasses\n/createNewclass\n/deleteAllActiveclasses\n```\n\n理清资源的层次结构，比如业务针对的范围是学校，那么学校会是一级资源:`/schools`，老师: `/schools/teachers`，学生: `/schools/students` 就是二级资源。\n\n# 过滤信息（Filtering）\n\n如果我们在查询的时候需要添加特定条件的话，建议使用 url 参数的形式。比如我们要查询 state 状态为 active 并且 name 为 guidegege 的班级：\n\n```Plain\nGET    /classes?state=active\u0026name=guidegege\n```\n\n比如我们要实现分页查询：\n\n```Plain\nGET    /classes?page=1\u0026size=10 //指定第1页，每页10个数据\n```\n\n# 状态码（Status Codes）\n\n**状态码范围：**\n\n|   |   |   |   |\n|---|---|---|---|\n|2xx：成功|3xx：重定向|4xx：客户端错误|5xx：服务器错误|\n|200 成功|301 永久重定向|400 错误请求|500 服务器错误|\n|201 创建|304 资源未修改|401 未授权|502 网关错误|\n|||403 禁止访问|504 网关超时|\n|||404 未找到||\n|||405 请求方法不对||\n\n# RESTful 的极致 HATEOAS\n\n\u003e **RESTful 的极致是 hateoas ，但是这个基本不会在实际项目中用到。**\n\n上面是 RESTful API 最基本的东西，也是我们平时开发过程中最容易实践到的。实际上，RESTful API 最好做到 Hypermedia，即返回结果中提供链接，连向其他 API 方法，使得用户不查文档，也知道下一步应该做什么。\n\n比如，当用户向 `api.example.com` 的根目录发出请求，会得到这样一个返回结果\n\n```JavaScript\n{\"link\": {\"rel\":   \"collection https://www.example.com/classes\",\"href\":  \"https://api.example.com/classes\",\"title\": \"List of classes\",\"type\":  \"application/vnd.yourformat+json\"}}\n```\n\n上面代码表示，文档中有一个 `link` 属性，用户读取这个属性就知道下一步该调用什么 API 了。`rel` 表示这个 API 与当前网址的关系（collection 关系，并给出该 collection 的网址），`href` 表示 API 的路径，title 表示 API 的标题，`type` 表示返回类型 `Hypermedia API` 的设计被称为[HATEOASopen in new window](http://en.wikipedia.org/wiki/HATEOAS)。\n\n在 Spring 中有一个叫做 HATEOAS 的 API 库，通过它我们可以更轻松的创建出符合 HATEOAS 设计的 API。相关文章：\n\n- [在 Spring Boot 中使用 HATEOASopen in new window](https://blog.aisensiy.me/2017/06/04/spring-boot-and-hateoas/)\n    \n- [Building REST services with Springopen in new window](https://spring.io/guides/tutorials/rest/) (Spring 官网 )\n    \n- [An Intro to Spring HATEOASopen in new window](https://www.baeldung.com/spring-hateoas-tutorial)\n    \n- [spring-hateoas-examplesopen in new window](https://github.com/spring-projects/spring-hateoas-examples/tree/master/hypermedia)\n    \n- [Spring HATEOASopen in new window](https://spring.io/projects/spring-hateoas#learn) (Spring 官网\n    \n\n  \n\n  \n\n参考\n\nhttps://RESTfulapi.net/\n\nhttps://www.ruanyifeng.com/blog/2014/05/restful_api.html\n\nhttps://juejin.im/entry/59e460c951882542f578f2f0\n\nhttps://phauer.com/2016/testing-RESTful-services-java-best-practices/\n\nhttps://www.seobility.net/en/wiki/REST_API\n\nhttps://dev.to/duomly/rest-api-vs-graphql-comparison-3j6g","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E7%BD%91%E7%BB%9C/%E5%95%8A%E5%95%8A%E5%95%8A%E5%95%8A%E5%95%8A%E5%95%8A%E5%95%8A%E5%95%8Aasdf":{"title":"啊啊啊啊啊啊啊啊asdf","content":"阿塞阀等啊懂法啊啊啊","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E7%BD%91%E7%BB%9C/%E5%9F%BA%E7%A1%80":{"title":"基础","content":"","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E7%BD%91%E7%BB%9C/HTTP":{"title":"HTTP","content":"","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E7%BD%91%E7%BB%9C/IP":{"title":"IP","content":"","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E7%BD%91%E7%BB%9C/TCP":{"title":"TCP","content":"涉猎啊等啊等","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/%E9%AB%98%E5%B9%B6%E5%8F%91%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F/%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"面试题","content":"![[分布式相关面试题汇总.pdf]]","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]},"/ElasticSearch/%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"面试题","content":"![[精选7道Elastic Search面试题！.pdf]]","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E5%8F%8D%E5%B0%84":{"title":"Java 反射","content":"\n\n关于反射的详细解读，请看这篇文章 Java 反射机制详解。\n\n# 何谓反射？\n\n主要是因为**它赋予了我们在运行时分析类以及执行类中方法的能力。通过反射你可以获取任意一个类的所有属性和方法，你还可以调用这些方法和属性**。\n\n  \n\n# 反射的优缺点？\n\n优点：反射可以让我们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利。\n\n缺点：反射让我们在运行时有了分析操作类的能力的同时，**也增加了安全题，比如可以无视泛型参数的安全检查**（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。\n\n相关阅读：[Java Reflection: Why is it so slow?open in new window](https://stackoverflow.com/questions/1392351/java-reflection-why-is-it-so-slow) 。\n\n# 反射的应用场景？\n\n像咱们平时大部分时候都是在写业务代码，很少会接触到直接使用反射机制的场景。但是！这并不代表反射没有用。相反，正是因为反射，你才能这么轻松地使用各种框架。像 Spring/Spring Boot、MyBatis 等等框架中都大量使用了反射机制。\n\n- **这些框架中也大量使用了动态代理，而动态代理的实现也依赖反射。**\n    \n\n比如下面是通过 JDK 实现动态代理的示例代码，其中就使用了反射类 `Method` 来调用指定的方法。\n\n```Java\npublic class DebugInvocationHandler implements InvocationHandler {\n    /**\n     * 代理类中的真实对象\n     */\n    private final Object target;\n\n    public DebugInvocationHandler(Object target) {\n        this.target = target;\n    }\n\n\n    public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException {\n        System.out.println(\"before method \" + method.getName());\n        Object result = method.invoke(target, args);\n        System.out.println(\"after method \" + method.getName());\n        return result;\n    }\n}\n\n```\n\n- Java 中的一大利器 **注解** 的实现也用到了反射。\n    \n\n为什么你使用 Spring 的时候 ，一个`@Component`注解就声明了一个类为 Spring Bean 呢？为什么你通过一个 `@Value`注解就读取到配置文件中的值呢？究竟是怎么起作用的呢？\n\n这些都是因为你可以基于反射分析类，然后获取到类/属性/方法/方法的参数上的注解。你获取到注解之后，就可以做进一步的处理。\n\n  \n\n## 反射相关类\n\n![](statistic/asynccode-17.png)\n\n## 获取class 对象的四种方式\n\n- 知道具体类的情况下可以使用\n    \n\n```Go\nClass alunbarClass = TargetObject.class;\n```\n\n- 通过 `Class.forName()`传入类的全路径获取\n    \n\n```Go\nClass alunbarClass1 = Class.forName(\"cn.javaguide.TargetObject\");\n```\n\n- 通过 `Class.forName()`传入类的全路径获取\n    \n\n```Go\nTargetObject o = new TargetObject();\nClass alunbarClass2 = o.getClass();\n```\n\n- .通过类加载器`xxxClassLoader.loadClass()`传入类路径获取\n    \n\n```Go\nClassLoader.getSystemClassLoader().loadClass(\"cn.javaguide.TargetObject\");\n```\n\n## 反射的基本操作\n\n1. 创建一个我们要使用反射操作的类 `TargetObject`。\n    \n\n```Java\npackage cn.javaguide;\n\npublic class TargetObject {\n    private String value;\n\n    public TargetObject() {\n        value = \"JavaGuide\";\n    }\n\n    public void publicMethod(String s) {\n        System.out.println(\"I love \" + s);\n    }\n\n    private void privateMethod() {\n        System.out.println(\"value is \" + value);\n    }\n}\n```\n\n1. 使用反射操作这个类的方法以及参数\n    \n\n```Java\npackage cn.javaguide;\n\nimport java.lang.reflect.Field;\nimport java.lang.reflect.InvocationTargetException;\nimport java.lang.reflect.Method;\n\npublic class Main {\n    public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException {\n        /**\n         * 获取 TargetObject 类的 Class 对象并且创建 TargetObject 类实例\n         */\n        Class\u003c?\u003e targetClass = Class.forName(\"cn.javaguide.TargetObject\");\n        TargetObject targetObject = (TargetObject) targetClass.newInstance();\n        /**\n         * 获取 TargetObject 类中定义的所有方法\n         */\n        Method[] methods = targetClass.getDeclaredMethods();\n        for (Method method : methods) {\n            System.out.println(method.getName());\n        }\n\n        /**\n         * 获取指定方法并调用\n         */\n        Method publicMethod = targetClass.getDeclaredMethod(\"publicMethod\",\n                String.class);\n\n        publicMethod.invoke(targetObject, \"JavaGuide\");\n\n        /**\n         * 获取指定参数并对参数进行修改\n         */\n        Field field = targetClass.getDeclaredField(\"value\");\n        //为了对类中的参数进行修改我们取消安全检查\n        field.setAccessible(true);\n        field.set(targetObject, \"JavaGuide\");\n\n        /**\n         * 调用 private 方法\n         */\n        Method privateMethod = targetClass.getDeclaredMethod(\"privateMethod\");\n        //为了调用private方法我们取消安全检查\n        privateMethod.setAccessible(true);\n        privateMethod.invoke(targetObject);\n    }\n}\n\n```\n\n输出内容：\n\n```Plain\npublicMethod\nprivateMethod\nI love JavaGuide\nvalue is JavaGuide\n```\n\n---\n\n著作权归所有 原文链接：https://javaguide.cn/java/basis/reflection.html","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95":{"title":"Java 基础语法","content":"\n\n# 注释有哪几种形式？\n\nJava 中的注释有三种：\n\n1. **单行注释** ：通常用于解释方法内某单行代码的作用。\n    \n2. **多行注释** ：通常用于解释一段代码的作用。\n    \n3. **文档注释** ：通常用于生成 Java 开发文档。\n    \n\n用的比较多的还是单行注释和文档注释，多行注释在实际开发中使用的相对较少。\n\n![](statistic/asynccode-9.png)\n\n# 标识符和关键字\n\n## 标识符和关键字的区别是什么？\n\n需要大量地为程序、类、变量、方法等取名字，于是就有了 标识符 。简单来说， **标识符就是一个名字**\n\nJava 语言已经赋予了其特殊的含义，只能用于特定的地方，**这些特殊的标识符就是 关键字 。简单来说，关键字是被赋予特殊含义的标识符**\n\n## Java 语言关键字有哪些？\n\n|   |   |   |   |   |   |   |   |\n|---|---|---|---|---|---|---|---|\n|分类|关键字|   |   |   |   |   |   |\n|访问控制|private|protected|public|||||\n|类，方法和变量修饰符|abstract|class|extends|final|implements|interface|native|\n||new|static|strictfp|synchronized|transient|volatile|enum|\n|程序控制|break|continue|return|do|while|if|else|\n||for|instanceof|switch|case|default|assert||\n|错误处理|try|catch|throw|throws|finally|||\n|包相关|import|package||||||\n|基本类型|boolean|byte|char|double|float|int|long|\n||short|||||||\n|变量引用|super|this|void|||||\n|保留字|goto|const||||||\n\n  \n\n\u003e Tips：所有的关键字都是小写的，在 IDE 中会以特殊颜色显示。\n\u003e \n\u003e `default` 这个关键字很特殊，既属于程序控制，也属于类，方法和变量修饰符，还属于访问控制。\n\u003e \n\u003e - 在程序控制中，当在 `switch` 中匹配不到任何情况时，可以使用 `default` 来编写默认匹配的情况。\n\u003e     \n\u003e - 在类，方法和变量修饰符中，从 JDK8 开始引入了默认方法，可以使用 `default` 关键字来定义一个方法的默认实现。\n\u003e     \n\u003e - 在访问控制中，如果一个方法前没有任何修饰符，则默认会有一个修饰符 `default`，但是这个修饰符加上了就会报错。\n\u003e     \n\n⚠️ 注意 ：虽然 `true`, `false`, 和 `null` 看起来像关键字但实际上他们是字面值，同时你也不可以作为标识符来使用。\n\n官方文档：[https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.htmlopen in new window](https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html)\n\n## continue、break 和 return 的区别是什么？\n\n在循环结构中，当循环条件不满足或者循环次数达到要求时，循环会正常结束。但是，有时候可能需要在循环的过程中，当发生了某种条件之后 ，提前终止循环，这就需要用到下面几个关键词：\n\n1. `continue` ：指跳出当前的这一次循环，继续下一次循环。\n    \n2. `break` ：指跳出整个循环体，继续执行循环下面的语句。\n    \n\n`return` 用于跳出所在方法，结束该方法的运行。return 一般有两种用法：\n\n1. `return;` ：直接使用 return 结束方法执行，用于没有返回值函数的方法\n    \n2. `return value;` ：return 一个特定值，用于有返回值函数的方法\n    \n\n# 运算符\n\n## 自增自减运算符\n\n在写代码的过程中，常见的一种情况是需要某个整数类型变量增加 1 或减少 1，Java 提供了一种特殊的运算符，用于这种表达式，叫做自增运算符（++)和自减运算符（--）。“**符号在前就先加/减，符号在后就后加/减”**。\n\n## 移位运算符\n\n移位运算符是最基本的运算符之一，几乎每种编程语言都包含这一运算符。移位操作中，被操作的数据被视为二进制数，移位就是将其向左或向右移动若干位的运算。\n\n移位运算符在各种框架以及 JDK 自身的源码中使用还是挺广泛的，`HashMap`（JDK1.8） 中的 `hash` 方法的源码就用到了移位运算符：\n\n```Java\nstatic final int hash(Object key) {\n\tint h;// key.hashCode()：返回散列值也就是hashcode// ^ ：按位异或// \u003e\u003e\u003e:无符号右移，忽略符号位，空位都以0补齐return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u003e\u003e\u003e 16);}\n著作权归所有 原文链接：https://javaguide.cn/java/basis/java-basic-questions-01.html\n```\n\n在 Java 代码里使用 `\u003c\u003c` 、 `\u003e\u003e` 和`\u003e\u003e\u003e`转换成的指令码运行起来会更高效些。\n\n掌握最基本的移位运算符知识还是很有必要的，这不光可以帮助我们在代码中使用，还可以帮助我们理解源码中涉及到移位运算符的代码。\n\nJava 中有三种移位运算符：\n\n- `\u003c\u003c` :左移运算符，向左移若干位，高位丢弃，低位补零。`x \u003c\u003c 1`,相当于 x 乘以 2(不溢出的情况下)。\n    \n- `\u003e\u003e` :带符号右移，向右移若干位，高位补符号位，低位丢弃。正数高位补 0,负数高位补 1。`x \u003e\u003e 1`,相当于 x 除以 2。\n    \n- `\u003e\u003e\u003e` :无符号右移，忽略符号位，空位都以 0 补齐。\n    \n\n由于 `double`，`float` 在二进制中的表现比较特殊，因此不能来进行移位操作。\n\n移位操作符实际上支持的类型只有`int`和`long`，编译器在对`short`、`byte`、`char`类型进行移位前，都会将其转换为`int`类型再操作。\n\n**如果移位的位数超过数值所占有的位数会怎样？**\n\n当 int 类型左移/右移位数大于等于 32 位操作时，会先求余（%）后再进行左移/右移操作。也就是说左移/右移 32 位相当于不进行移位操作（32%32=0），左移/右移 42 位相当于左移/右移 10 位（42%32=10）。当 long 类型进行左移/右移操作时，由于 long 对应的二进制是 64 位，因此求余操作的基数也变成了 64。\n\n也就是说：`x\u003c\u003c42`等同于`x\u003c\u003c10`，`x\u003e\u003e42`等同于`x\u003e\u003e10`，`x \u003e\u003e\u003e42`等同于`x \u003e\u003e\u003e 10`。\n\n## \u0026 和 \u0026\u0026 的区别\n\n\u0026运算符有两种用法： 短路与 、 逻辑与 。\n\n\u0026\u0026运算符是短路与运算。逻辑与跟短路与的差别是非常巨大的，虽然二者都要求运 算符左右两端的布尔值都是true 整个表达式的值才是 true。\n\n\u0026\u0026之所以称为短路运算是因为，如果\u0026\u0026左边的表达式的值是 false，右边的表达式 会被直接短路掉，不会进行运算。很多时候我们可能都需要用\u0026\u0026而不是\u0026。\n\n例如在验证用户登录时判定用户名不是 null 而且不是空字符串，应当写为 username != null \u0026\u0026!username.equals(\"\") ，二者的顺序不能交换，更不能 用\u0026运算符，因为第一个条件如果不成立，根本不能进行字符串的 equals 比较，否 则会产生 NullPointerException 异常。\n\n注意：逻辑或运算符（|）和短路或运算符（||）的差别也是如此。\n\n## **switch** 是否能作用在 **byte/long/String**上？\n\nJava5 以前 switch(expr)中，expr 只能是 byte、short、char、int。\n\n从 Java 5 开始，Java 中引入了枚举类型， expr 也可以是 enum 类型。\n\n从 Java 7 开始，expr还可以是字符串(String)，但是长整型(long)在目前所有的版本中都是不可以的。\n\n# 变量\n\n## 成员变量与局部变量的区别？\n\n**语法形式** ：从语法形式上看，成员变量是属于类的，而局部变量是在代码块或方法中定义的变量或是方法的参数；成员变量可以被 `public`,`private`,`static` 等修饰符所修饰，而局部变量不能被访问控制修饰符及 `static` 所修饰；但是，成员变量和局部变量都能被 `final` 所修饰。\n\n**存储方式** ：从变量在内存中的存储方式来看,如果成员变量是使用 `static` 修饰的，那么这个成员变量是属于类的，如果没有使用 `static` 修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。\n\n**生存时间** ：从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动生成，随着方法的调用结束而消亡。\n\n**默认值** ：从变量是否有默认值来看，成员变量如果没有被赋初始值，则会自动以类型的默认值而赋值（一种情况例外:被 `final` 修饰的成员变量也必须显\n\n## 静态变量有什么作用？\n\n静态变量可以被类的所有实例共享。无论一个类创建了多少个对象，它们都共享同一份静态变量。\n\n通常情况下，静态变量会被 `final` 关键字修饰成为常量。\n\n## 字符型常量和字符串常量的区别?\n\n1. **形式** : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。\n    \n2. **含义** : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。\n    \n3. **占内存大小** ： 字符常量只占 2 个字节; 字符串常量占若干个字节。\n    \n\n(**注意：** **`char`** **在 Java 中占两个字节**)\n\n  \n\n# 方法\n\n## 静态方法\n\n### 静态方法为什么不能调用非静态成员?：\n\n1. 静态方法是属于类的，在类加载的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，需要通过类的实例对象去访问。\n    \n2. 在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作\n    \n\n### 静态方法和实例方法有何不同？\n\n**1、调用方式**\n\n在外部调用静态方法时，可以使用 `类名.方法名` 的方式，也可以使用 `对象.方法名` 的方式，而实例方法只有后面这种方式。也就是说，**调用静态方法可以无需创建对象** 。\n\n不过，需要注意的是一般不建议使用 `对象.方法名` 的方式来调用静态方法。这种方式非常容易造成混淆，静态方法不属于类的某个对象而是属于这个类。\n\n因此，一般建议使用 `类名.方法名` 的方式来调用静态方法。\n\n  \n\n**2、访问类成员是否存在限制**\n\n静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法），而实例方法不存在这个限制\n\n  \n\n  \n\n## 重载和重写\n\n\u003e 重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理\n\u003e \n\u003e 重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法\n\n重载\n\n发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同**。重载就是同一个类中多个同名方法根据不同的传参来执行不同的逻辑处理。**\n\n**重写**\n\n重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。\n\n1. 方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。\n    \n2. 如果父类方法访问修饰符为 `private/final/static` 则子类就不能重写该方法，但是被 `static` 修饰的方法能够被再次声明。\n    \n3. 构造方法无法被重写\n    \n\n综上：**重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变**\n\n  \n\n**方法的重写要遵循“两同两小一大”**（以下内容摘录自《疯狂 Java 讲义》，[issue#892open in new window](https://github.com/Snailclimb/JavaGuide/issues/892) ）：\n\n- “两同”即方法名相同、形参列表相同；\n    \n- “两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等；\n    \n- “一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。\n    \n\n⭐️ 关于 **重写的返回值类型** 这里需要额外多说明一下，上面的表述不太清晰准确：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。\n\n  \n\n## 什么是可变长参数\n\n所谓可变长参数就是允许在调用方法时传入不定长度的参数。就比如下面的这个 `printVariable` 方法就可以接受 0 个或者多个参数。\n\n```Go\npublic static void method1(String... args) {\n   //......\n}\n```\n\n另外，可变参数只能作为函数的最后一个参数，但其前面可以有也可以没有任何其他参数。\n\n```Go\npublic static void method2(String arg1, String... args) {//......}\n```\n\n遇到方法重载的情况怎么办呢？会优先匹配固定参数还是可变参数的方法呢？\n\n答案是**会优先匹配固定参数的方法，**因为固定参数的方法匹配度更高。\n\n我们通过下面这个例子来证明一下。\n\n```Go\n/**\n * 微信搜 JavaGuide 回复\"面试突击\"即可免费领取个人原创的 Java 面试手册\n *\n * @author Guide哥\n * @date 2021/12/13 16:52\n **/\npublic class VariableLengthArgument {\n\n    public static void printVariable(String... args) {\n        for (String s : args) {\n            System.out.println(s);\n        }\n    }\n\n    public static void printVariable(String arg1, String arg2) {\n        System.out.println(arg1 + arg2);\n    }\n\n    public static void main(String[] args) {\n        printVariable(\"a\", \"b\");\n        printVariable(\"a\", \"b\", \"c\", \"d\");\n    }\n}\n\n\n\nab\na\nb\nc\nd\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":["Java","基础语法"]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E5%B8%B8%E7%94%A8%E7%B1%BB":{"title":"Java 常用类","content":"\n\n# Object\n\n## Object 类的常见方法有哪些？\n\nObject 类是一个特殊的类，是所有类的父类。它主要提供了以下 11 个方法：\n\n```Java\n/**\n * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。\n */public final native Class\u003c?\u003e getClass()/**\n * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。\n */public native int hashCode()/**\n * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。\n */public boolean equals(Object obj)/**\n * naitive 方法，用于创建并返回当前对象的一份拷贝。\n */protected native Object clone() throws CloneNotSupportedException/**\n * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。\n */public String toString()/**\n * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。\n */public final native void notify()/**\n * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。\n */public final native void notifyAll()/**\n * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。\n */public final native void wait(long timeout) throws InterruptedException/**\n * 多了 nanos 参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 毫秒。。\n */public final void wait(long timeout, int nanos) throws InterruptedException/**\n * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念\n */public final void wait() throws InterruptedException/**\n * 实例被垃圾回收器回收的时候触发的操作\n */protected void finalize() throws Throwable { }\n```\n\n![](statistic/asynccode-5.png)\n\n## == 和 equals() 的区别\n\n**==** 对于基本类型和引用类型的作用效果是不同的：\n\n- 对于基本数据类型来说，== 比较的是值。\n    \n- 对于引用数据类型来说，== 比较的是对象的内存地址。\n    \n\n\u003e 因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。\n\n**`equals()`** 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。`equals()`方法存在于`Object`类中，而`Object`类是所有类的直接或间接父类，因此所有的类都有`equals()`方法。\n\n`Object` 类 `equals()` 方法：\n\n```Java\npublic boolean equals(Object obj) {return (this == obj);}\n```\n\n`equals()` 方法存在两种使用情况：\n\n- **类没有重写** **`equals()`****方法** ：通过`equals()`比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 `Object`类`equals()`方法。\n    \n- **类重写了** **`equals()`****方法** ：一般我们都重写 `equals()`方法来比较两个对象中的属性是否相等；若它们的属性相等，则返回 true(即，认为这两个对象相等)。\n    \n\n举个例子（这里只是为了举例。实际上，你按照下面这种写法的话，像 IDEA 这种比较智能的 IDE 都会提示你将 `==` 换成 `equals()` ）：\n\n```Java\nString a = new String(\"ab\"); // a 为一个引用String b = new String(\"ab\"); // b为另一个引用,对象的内容一样String aa = \"ab\"; // 放在常量池中String bb = \"ab\"; // 从常量池中查找System.out.println(aa == bb);// trueSystem.out.println(a == b);// falseSystem.out.println(a.equals(b));// trueSystem.out.println(42 == 42.0);// true\n```\n\n`String` 中的 `equals` 方法是被重写过的，因为 `Object` 的 `equals` 方法是比较的对象的内存地址，而 `String` 的 `equals` 方法比较的是对象的值。\n\n当创建 `String` 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 `String` 对象。\n\n`String`类`equals()`方法：\n\n```Java\npublic boolean equals(Object anObject) {if (this == anObject) {return true;}if (anObject instanceof String) {String anotherString = (String)anObject;int n = value.length;if (n == anotherString.value.length) {char v1[] = value;char v2[] = anotherString.value;int i = 0;while (n-- != 0) {if (v1[i] != v2[i])return false;\n                i++;}return true;}}return false;}\n```\n\n## hashCode() 有什么用？\n\n`hashCode()` 的作用是获取哈希码（`int` 整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。\n\n`hashCode()`定义在 JDK 的 `Object` 类中，这就意味着 Java 中的任何类都包含有 `hashCode()` 函数。另外需要注意的是： `Object` 的 `hashCode()` 方法是本地方法，也就是用 C 语言或 C++ 实现的，该方法通常用来将对象的内存地址转换为整数之后返回。\n\n```Java\npublic native int hashCode();\n```\n\n散列表存储的是键值对(key-value)，它的特点是：**能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象）**\n\n## 为什么要有 hashCode？\n\n我们以“`HashSet` 如何检查重复”为例子来说明为什么要有 `hashCode`？\n\n下面这段内容摘自我的 Java 启蒙书《Head First Java》:\n\n\u003e 当你把对象加入 `HashSet` 时，`HashSet` 会先计算对象的 `hashCode` 值来判断对象加入的位置，同时也会与其他已经加入的对象的 `hashCode` 值作比较，如果没有相符的 `hashCode`，`HashSet` 会假设对象没有重复出现。但是如果发现有相同 `hashCode` 值的对象，这时会调用 `equals()` 方法来检查 `hashCode` 相等的对象是否真的相同。如果两者相同，`HashSet` 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 `equals` 的次数，相应就大大提高了执行速度。\n\n其实， `hashCode()` 和 `equals()`都是用于比较两个对象是否相等。\n\n**那为什么 JDK 还要同时提供这两个方法呢？**\n\n这是因为在一些容器（比如 `HashMap`、`HashSet`）中，有了 `hashCode()` 之后，判断元素是否在对应容器中的效率会更高（参考添加元素进`HashSet`的过程）！\n\n我们在前面也提到了添加元素进`HashSet`的过程，如果 `HashSet` 在对比的时候，同样的 `hashCode` 有多个对象，它会继续使用 `equals()` 来判断是否真的相同。也就是说 `hashCode` 帮助我们大大缩小了查找成本。\n\n**那为什么不只提供** **`hashCode()`** **方法呢？**\n\n这是因为两个对象的`hashCode` 值相等并不代表两个对象就相等。\n\n**那为什么两个对象有相同的** **`hashCode`** **值，它们也不一定是相等的？**\n\n因为 `hashCode()` 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓哈希碰撞也就是指的是不同的对象得到相同的 `hashCode` )。\n\n总结下来就是 ：\n\n- 如果两个对象的`hashCode` 值相等，那这两个对象不一定相等（哈希碰撞）。\n    \n- 如果两个对象的`hashCode` 值相等并且`equals()`方法也返回 `true`，我们才认为这两个对象相等。\n    \n- 如果两个对象的`hashCode` 值不相等，我们就可以直接认为这两个对象不相等。\n    \n\n相信大家看了我前面对 `hashCode()` 和 `equals()` 的介绍之后，下面这个问题已经难不倒你们了。\n\n## 为什么重写 equals() 时必须重写 hashCode() 方法？\n\n因为两个相等的对象的 `hashCode` 值必须是相等。也就是说如果 `equals` 方法判断两个对象是相等的，那这两个对象的 `hashCode` 值也要相等。\n\n如果重写 `equals()` 时没有重写 `hashCode()` 方法的话就可能会导致 `equals` 方法判断是相等的两个对象，`hashCode` 值却不相等。\n\n**思考** ：重写 `equals()` 时没有重写 `hashCode()` 方法的话，使用 `HashMap` 可能会出现什么问题。\n\n**总结** ：\n\n- `equals` 方法判断两个对象是相等的，那这两个对象的 `hashCode` 值也要相等。\n    \n- 两个对象有相同的 `hashCode` 值，他们也不一定是相等的（哈希碰撞）。\n    \n\n更多关于 `hashCode()` 和 `equals()` 的内容可以查看：[Java hashCode() 和 equals()的若干问题解答open in new window](https://www.cnblogs.com/skywang12345/p/3324958.html)\n\n# String\n\n## String 是javaa基本数据类型吗？可以被继承吗？\n\nString 不是基本类型\n\nString 使用final 修饰，是不可变类\n\n## String、StringBuffer、StringBuilder 的区别？\n\n**可变性**\n\n`String` 是不可变的（后面会详细分析原因）。\n\n`StringBuilder` 与 `StringBuffer` 都继承自 `AbstractStringBuilder` 类，在 `AbstractStringBuilder` 中也是使用字符数组保存字符串，不过没有使用 `final` 和 `private` 关键字修饰，最关键的是这个 `AbstractStringBuilder` 类还提供了很多修改字符串的方法比如 `append` 方法。\n\n```Java\nabstract class AbstractStringBuilder implements Appendable, CharSequence {char[] value;public AbstractStringBuilder append(String str) {if (str == null)return appendNull();int len = str.length();ensureCapacityInternal(count + len);\n        str.getChars(0, len, value, count);\n        count += len;return this;}//...}\n```\n\n**线程安全性**\n\n`String` 中的对象是不可变的，也就可以理解为常量，线程安全。`AbstractStringBuilder` 是 `StringBuilder` 与 `StringBuffer` 的公共父类，定义了一些字符串的基本操作，如 `expandCapacity`、`append`、`insert`、`indexOf` 等公共方法。`StringBuffer` 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。`StringBuilder` 并没有对方法进行加同步锁，所以是非线程安全的。\n\n**性能**\n\n每次对 `String` 类型进行改变的时候，都会生成一个新的 `String` 对象，然后将指针指向新的 `String` 对象。`StringBuffer` 每次都会对 `StringBuffer` 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 `StringBuilder` 相比使用 `StringBuffer` 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。\n\n**对于三者使用的总结：**\n\n1. **操作少量的数据: 适用** **`String`**\n    \n2. **单线程操作字符串缓冲区下操作大量数据: 适用** **`StringBuilder`**\n    \n3. **多线程操作字符串缓冲区下操作大量数据: 适用** **`StringBuffer`**\n    \n\n## String 为什么是不可变的?\n\n`String` 类中使用 `final` 关键字修饰字符数组来保存字符串，~~所以~~~~`String`~~ ~~对象是不可变的。~~\n\n```Java\npublic final class String implements java.io.Serializable, Comparable\u003cString\u003e, CharSequence {private final char value[];//...}\n```\n\n\u003e 🐛 修正 ： 我们知道被 `final` 关键字修饰的类不能被继承，修饰的方法不能被重写，修饰的变量是基本数据类型则值不能改变，修饰的变量是引用类型则不能再指向其他对象。因此，`final` 关键字修饰的数组保存字符串并不是 `String` 不可变的根本原因，因为这个数组保存的字符串是可变的（`final` 修饰引用类型变量的情况）。\n\u003e \n\u003e `String` 真正不可变有下面几点原因：\n\u003e \n\u003e 1. 保存字符串的数组被 `final` 修饰且为私有的，并且`String` 类没有提供/暴露修改这个字符串的方法。\n\u003e     \n\u003e 2. `String` 类被 `final` 修饰导致其不能被继承，进而避免了子类破坏 `String` 不可变。\n\u003e     \n\u003e \n\u003e 相关阅读：[如何理解 String 类型值的不可变？ - 知乎提问open in new window](https://www.zhihu.com/question/20618891/answer/114125846)\n\u003e \n\u003e 补充（来自[issue 675open in new window](https://github.com/Snailclimb/JavaGuide/issues/675)）：在 Java 9 之后，`String` 、`StringBuilder` 与 `StringBuffer` 的实现改用 `byte` 数组存储字符串。\n\u003e \n\n```\npublic final class String implements java.io.Serializable,Comparable\u003cString\u003e, CharSequence {// @Stable 注解表示变量最多被修改一次，称为“稳定的”。@Stableprivate final byte[] value;}abstract class AbstractStringBuilder implements Appendable, CharSequence {byte[] value;}\n```\n\n\u003e \n\u003e **Java 9 为何要将** **`String`** **的底层实现由** **`char[]`** **改成了** **`byte[]`** **?**\n\u003e \n\u003e 新版的 String 其实支持两个编码方案： Latin-1 和 UTF-16。如果字符串中包含的汉字没有超过 Latin-1 可表示范围内的字符，那就会使用 Latin-1 作为编码方案。Latin-1 编码方案下，`byte` 占一个字节(8 位)，`char` 占用 2 个字节（16），`byte` 相较 `char` 节省一半的内存空间。\n\u003e \n\u003e JDK 官方就说了绝大部分字符串对象只包含 Latin-1 可表示的字符。\n\u003e \n\u003e ![](statistic/asynccode-2.png)\n\u003e \n\u003e 如果字符串中包含的汉字超过 Latin-1 可表示范围内的字符，`byte` 和 `char` 所占用的空间是一样的。\n\u003e \n\u003e 这是官方的介绍：https://openjdk.java.net/jeps/254 。\n\n## 字符串拼接用“+” 还是 StringBuilder?\n\nJava 语言本身并不支持运算符重载**，“+”和“+=”是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。**\n\n```Java\nString str1 = \"he\";String str2 = \"llo\";String str3 = \"world\";String str4 = str1 + str2 + str3;\n```\n\n上面的代码对应的字节码如下：\n\n![](statistic/asynccode-6.png)\n\n可以看出，字符串对象通过“+”的字符串拼接方式，实际上是通过 `StringBuilder` 调用 `append()` 方法实现的，拼接完成之后调用 `toString()` 得到一个 `String` 对象 。\n\n不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：**编译器不会创建单个** **`StringBuilder`** **以复用，会导致创建过多的** **`StringBuilder`** **对象**。\n\n```Java\nString[] arr = {\"he\", \"llo\", \"world\"};String s = \"\";for (int i = 0; i \u003c arr.length; i++) {\n    s += arr[i];}System.out.println(s);\n```\n\n`StringBuilder` 对象是在循环内部被创建的，这意味着每循环一次就会创建一个 `StringBuilder` 对象。\n\n![](statistic/asynccode-4.png)\n\n如果直接使用 `StringBuilder` 对象进行字符串拼接的话，就不会存在这个问题了。\n\n```Java\nString[] arr = {\"he\", \"llo\", \"world\"};StringBuilder s = new StringBuilder();for (String value : arr) {\n    s.append(value);}System.out.println(s);\n```\n\n![](statistic/asynccode-3.png)\n\n如果你使用 IDEA 的话，IDEA 自带的代码检查机制也会提示你修改代码。\n\n  \n\n## String str1 = new String(\"abc\" ) 和 String str2 =\"abc\" 的区别\n\n相同：两个语句都会去字符串常量池中检查是否已经存在 “abc”，如果有则直接使用，如果没有则会在常量池中创建 “abc” 对象。\n\n不同： String str1 = new String(\"abc\") 还会通过 new String() 在堆里创建一个 \"abc\" 字符串对象实例。所以后者可以理解为被前者包含\n\n![](statistic/asynccode-4.png)\n\n  \n\n## String s = new String(\"abc\")创建了几个对象\n\n一个或者两个\n\n如果字符串常量池已经有“abc”，则是一个；否则，两个。 当字符创常量池没有 “abc”\n\n- 一个是字符串字面量 \"abc\" 所对应的、字符串常量池中的实例\n    \n- 另一个是通过 new String() 创建并初始化的，内容与\"abc\"相同的实例，在堆中。\n    \n\n## String#equals() 和 Object#equals() 有何区别？\n\n`String` 中的 `equals` 方法是被重写过的，比较的是 String 字符串的值是否相等。 `Object` 的 `equals` 方法是比较的对象的内存地址。\n\n## 字符串常量池的作用了解吗？\n\n**字符串常量池** 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。\n\n```Java\n// 在堆中创建字符串对象”ab“\n// 将字符串对象”ab“的引用保存在字符串常量池中\nString aa = \"ab\";\n// 直接返回字符串常量池中字符串对象”ab“的引用\nString bb = \"ab\";\nSystem.out.println(aa==bb);// true\n```\n\n更多关于字符串常量池的介绍可以看一下 [Java 内存区域详解open in new window](https://javaguide.cn/java/jvm/memory-area.html) 这篇文章。\n\n## String s1 = new String(\"abc\");这句话创建了几个字符串对象？\n\n会创建 1 或 2 个字符串对象。\n\n1、如果字符串常量池中不存在字符串对象“abc”的引用，那么会在堆中创建 2 个字符串对象“abc”。\n\n示例代码（JDK 1.8）：\n\n```Java\nString s1 = new String(\"abc\");\n```\n\n对应的字节码：\n\n![](statistic/asynccode-4.png)\n\n`ldc` 命令用于判断字符串常量池中是否保存了对应的字符串对象的引用，如果保存了的话直接返回，如果没有保存的话，会在堆中创建对应的字符串对象并将该字符串对象的引用保存到字符串常量池中。\n\n2、如果字符串常量池中已存在字符串对象“abc”的引用，则只会在堆中创建 1 个字符串对象“abc”。\n\n示例代码（JDK 1.8）：\n\n```Java\n// 字符串常量池中已存在字符串对象“abc”的引用String s1 = \"abc\";// 下面这段代码只会在堆中创建 1 个字符串对象“abc”String s2 = new String(\"abc\");\n```\n\n对应的字节码：\n\n![](statistic/asynccode-4.png)\n\n这里就不对上面的字节码进行详细注释了，7 这个位置的 `ldc` 命令不会在堆中创建新的字符串对象“abc”，这是因为 0 这个位置已经执行了一次 `ldc` 命令，已经在堆中创建过一次字符串对象“abc”了。7 这个位置执行 `ldc` 命令会直接返回字符串常量池中字符串对象“abc”对应的引用。\n\n## intern 方法有什么作用?\n\n`String.intern()` 是一个 native（本地）方法，其作用是将指定的字符串对象的引用保存在字符串常量池中，可以简单分为两种情况：\n\n- 如果字符串常量池中保存了对应的字符串对象的引用，就直接返回该引用。\n    \n- 如果字符串常量池中没有保存了对应的字符串对象的引用，那就在常量池中创建一个指向该字符串对象的引用并返回。\n    \n\n示例代码（JDK 1.8） :\n\n```Java\n// 在堆中创建字符串对象”Java“\n// 将字符串对象”Java“的引用保存在字符串常量池中\nString s1 = \"Java\";\n// 直接返回字符串常量池中字符串对象”Java“对应的引用\nString s2 = s1.intern();\n// 会在堆中在单独创建一个字符串对象\nString s3 = new String(\"Java\");\n// 直接返回字符串常量池中字符串对象”Java“对应的引用\nString s4 = s3.intern();\n// s1 和 s2 指向的是堆中的同一个对象\nSystem.out.println(s1 == s2); // true\n// s3 和 s4 指向的是堆中不同的对象\nSystem.out.println(s3 == s4); // false\n// s1 和 s4 指向的是堆中的同一个对象\nSystem.out.println(s1 == s4); //true\n```\n\n## String 类型的变量和常量做“+”运算时发生了什么？\n\n先来看字符串不加 `final` 关键字拼接的情况（JDK1.8）：\n\n```Java\nString str1 = \"str\";\nString str2 = \"ing\";\nString str3 = \"str\" + \"ing\";\nString str4 = str1 + str2;\nString str5 = \"string\";\nSystem.out.println(str3 == str4);//false\nSystem.out.println(str3 == str5);//true\nSystem.out.println(str4 == str5);//false\n```\n\n\u003e **注意** ：比较 String 字符串的值是否相等，可以使用 `equals()` 方法。 `String` 中的 `equals` 方法是被重写过的。 `Object` 的 `equals` 方法是比较的对象的内存地址，而 `String` 的 `equals` 方法比较的是字符串的值是否相等。如果你使用 `==` 比较两个字符串是否相等的话，IDEA 还是提示你使用 `equals()` 方法替换。\n\n![](statistic/asynccode-7.png)\n\n**对于编译期可以确定值的字符串，也就是常量字符串 ，jvm 会将其存入字符串常量池。并且，字符串常量拼接得到的字符串常量在编译阶段就已经被存放字符串常量池，这个得益于编译器的优化。**\n\n在编译过程中，Javac 编译器（下文中统称为编译器）会进行一个叫做 **常量折叠(Constant Folding)** 的代码优化。《深入理解 Java 虚拟机》中是也有介绍到：\n\n![[Pasted image 20230726233943.png]]\n\n常量折叠会把常量表达式的值求出来作为常量嵌在最终生成的代码中，这是 Javac 编译器会对源代码做的极少量优化措施之一(代码优化几乎都在即时编译器中进行)。\n\n**对于** **`String str3 = \"str\" + \"ing\";`** **编译器会给你优化成** **`String str3 = \"string\";`** 。\n\n并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以：\n\n- 基本数据类型( `byte`、`boolean`、`short`、`char`、`int`、`float`、`long`、`double`)以及字符串常量。\n    \n- `final` 修饰的基本数据类型和字符串变量\n    \n- 字符串通过 “+”拼接得到的字符串、基本数据类型之间算数运算（加减乘除）、基本数据类型的位运算（\u003c\u003c、\u003e\u003e、\u003e\u003e\u003e ）\n    \n\n**引用的值在程序编译期是无法确定的，编译器无法对其进行优化。**\n\n**对象引用和“+”的字符串拼接方式，实际上是通过** **`StringBuilder`** **调用** **`append()`** **方法实现的，拼接完成之后调用** **`toString()`** **得到一个** **`String`** **对象** 。\n\n```Java\nString str4 = new StringBuilder().append(str1).append(str2).toString();\n```\n\n我们在平时写代码的时候，尽量避免多个字符串对象拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 `StringBuilder` 或者 `StringBuffer`。\n\n不过，字符串使用 `final` 关键字声明之后，可以让编译器当做常量来处理。\n\n示例代码：\n\n```Java\nfinal String str1 = \"str\";\nfinal String str2 = \"ing\";// 下面两个表达式其实是等价的String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 常量池中的对象System.out.println(c == d);// true\n```\n\n被 `final` 关键字修改之后的 `String` 会被编译器当做常量来处理，编译器在程序编译期就可以确定它的值，其效果就相当于访问常量。\n\n如果 ，编译器在运行时才能知道其确切值的话，就无法对其优化。\n\n示例代码（`str2` 在运行时才能确定其值）：\n\n```Java\nfinal String str1 = \"str\";\nfinal String str2 = getStr();\nString c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 在堆上创建的新的对象System.out.println(c == d);// falsepublic static String getStr() {return \"ing\";}\n```\n\n## String 怎么转化成Integer ？原理？\n\n- Integer.parseInt(String s)\n    \n- Integer.valueOf(String s)\n    \n\n不管哪一种，最终还是会调用**Integer类内中的 parseInt(String s, int radix) 方法**。\n\n![](statistic/asynccode-8.png)","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96":{"title":"Java 序列化与反序列化","content":"# 什么是序列化?什么是反序列化?\n\n如果我们需要持久化 Java 对象比如将 Java 对象保存在文件中，或者在网络传输 Java 对象，这些场景都需要用到序列化。\n\n简单来说：\n\n- **序列化**： **将数据结构或对象转换成二进制字节流的过程**\n    \n- **反序列化**：**将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过**程\n    \n\n对于 Java 这种面向对象编程语言来说，我们序列化的都是对象（Object）也就是实例化后的类(Class)，但是在 C++这种半面向对象的语言中，struct(结构体)定义的是数据结构类型，而 class 对应的是对象类型。\n\n下面是序列化和反序列化常见应用场景：\n\n- 对象在进行网络传输（比如远程方法调用 RPC 的时候）之前需要先被序列化，接收到序列化的对象之后需要再进行反序列化；\n    \n- 将对象存储到文件之前需要进行序列化，将对象从文件中读取出来需要进行反序列化；\n    \n- 将对象存储到数据库（如 Redis）之前需要用到序列化，将对象从缓存数据库中读取出来需要反序列化；\n    \n- 将对象存储到内存之前需要进行序列化，从内存中读取出来之后需要进行反序列化。\n    \n\n维基百科是如是介绍序列化的：\n\n\u003e **序列化**（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换成可取用格式（例如存成文件，存于缓冲，或经由网络中发送），以留待后续在相同或另一台计算机环境中，能恢复原先状态的过程。依照序列化格式重新获取字节的结果时，可以利用它来产生与原始对象相同语义的副本。对于许多对象，像是使用大量引用的复杂对象，这种序列化重建的过程并不容易。面向对象中的对象序列化，并不概括之前原始对象所关系的函数。这种过程也称为对象编组（marshalling）。从一系列字节提取数据结构的反向操作，是反序列化（也称为解编组、deserialization、unmarshalling）。\n\n综上：**序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中。**\n\n![](statistic/asynccode-19.png)\n\nhttps://www.corejavaguru.com/java/serialization/interview-questions-1\n\n**序列化协议对应于 TCP/IP 4 层模型的哪一层？**\n\n我们知道网络通信的双方必须要采用和遵守相同的协议。TCP/IP 四层模型是下面这样的，序列化协议属于哪一层呢？\n\n1. 应用层\n    \n2. 传输层\n    \n3. 网络层\n    \n4. 网络接口层\n    \n\n![](statistic/asynccode-21.png)\n\n- OSI 七层协议模型中，**表示层做的事情主要就是对应用层的用户数据进行处理转换为二进制流**。\n    \n- **TCP/IP 协议:序列化协议属于 TCP/IP 协议应用层的一部分**。\n    \n\n  \n\n  \n\n# JDK 自带的序列化方式\n\nJDK 自带的序列化，只需实现 `java.io.Serializable`接口即可\n\n```Go\n@AllArgsConstructor\n@NoArgsConstructor\n@Getter\n@Builder\n@ToString\npublic class RpcRequest implements Serializable {\n    private static final long serialVersionUID = 1905122041950251207L;\n    private String requestId;\n    private String interfaceName;\n    private String methodName;\n    private Object[] parameters;\n    private Class\u003c?\u003e[] paramTypes;\n    private RpcMessageTypeEnum rpcMessageTypeEnum;\n}\n```\n\n  \n\n## serialVersionUID 有什么作用？\n\n序列化号 `serialVersionUID` 属于版本控制的作用。**反序列化时，会检查** **`serialVersionUID`** **是否和当前类的** **`serialVersionUID`** **一致。**如果 `serialVersionUID` 不一致则会抛出 `InvalidClassException` 异常。强烈推荐每个序列化类都手动指定其 `serialVersionUID`，如果不手动指定，那么编译器会动态生成默认的 `serialVersionUID`。\n\n## **serialVersionUID 不是被 static 变量修饰了吗？为什么还会被“序列化”？**\n\n`static` 修饰的变量是静态变量，位于方法区，本身是不会被序列化的。 `static` 变量是属于类的而不是对象。你反序列之后，`static` 变量的值就像是默认赋予给了对象一样，看着就像是 `static` 变量被序列化，实际只是假象罢了。\n\n## 如果有些字段不想进行序列化怎么办？\n\n对于不想进行序列化的变量，使用 `transient` 关键字修饰。\n\n`transient` 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被 `transient` 修饰的变量值不会被持久化和恢复。\n\n关于 `transient` 还有几点注意：\n\n- `transient` 只能修饰变量，不能修饰类和方法。\n    \n- `transient` 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 `int` 类型，那么反序列后结果就是 `0`。\n    \n- `static` 变量因为不属于任何对象(Object)，所以无论有没有 `transient` 关键字修饰，均不会被序列化。\n    \n\n## 为什么不推荐使用 JDK 自带的序列化？\n\n我们很少或者说**几乎不会直接使用 JDK 自带的序列化方式**，主要原因有下面这些原因：\n\n- **不支持跨语言调用** : 如果调用的是其他语言开发的服务的时候就不支持了。\n    \n- **性能差** ：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。\n    \n- **存在安全问题** ：序列化和反序列化本身并不存在问题。但当输入的反序列化的数据可被用户控制，那么攻击者即可通过构造恶意输入，让反序列化产生非预期的对象，在此过程中执行构造的任意代码。相关阅读：[应用安全：JAVA反序列化漏洞之殇open in new window](https://cryin.github.io/blog/secure-development-java-deserialization-vulnerability/) 。\n    \n\n# 常见序列化协议有哪些？\n\nJDK 自带的序列化方式一般不会用 ，因为序列化效率低并且存在安全问题。比较常用的序列化协议有 Hessian、Kryo、Protobuf、ProtoStuff，这些都是基于二进制的序列化协议。\n\n像 JSON 和 XML 这种属于文本类序列化方式。虽然可读性比较好，但是性能较差，一般不会选择。\n\n## Kryo\n\nKryo 是一个高性能的序列化/反序列化工具，由于其变长存储特性并使用了字节码生成机制，拥有较高的运行速度和较小的字节码体积。\n\n另外，Kryo 已经是一种非常成熟的序列化实现了，已经在 Twitter、Groupon、Yahoo 以及多个著名开源项目（如 Hive、Storm）中广泛的使用。\n\n[guide-rpc-frameworkopen in new window](https://github.com/Snailclimb/guide-rpc-framework) 就是使用的 kryo 进行序列化，序列化和反序列化相关的代码如下：\n\n```Java\n/**\n * Kryo serialization class, Kryo serialization efficiency is very high, but only compatible with Java language\n *\n * @author shuang.kou\n * @createTime 2020年05月13日 19:29:00\n */@Slf4jpublic class KryoSerializer implements Serializer {/**\n     * Because Kryo is not thread safe. So, use ThreadLocal to store Kryo objects\n     */private final ThreadLocal\u003cKryo\u003e kryoThreadLocal = ThreadLocal.withInitial(() -\u003e {Kryo kryo = new Kryo();\n        kryo.register(RpcResponse.class);\n        kryo.register(RpcRequest.class);return kryo;});@Overridepublic byte[] serialize(Object obj) {try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();Output output = new Output(byteArrayOutputStream)) {Kryo kryo = kryoThreadLocal.get();// Object-\u003ebyte:将对象序列化为byte数组\n            kryo.writeObject(output, obj);\n            kryoThreadLocal.remove();return output.toBytes();} catch (Exception e) {throw new SerializeException(\"Serialization failed\");}}@Overridepublic \u003cT\u003e T deserialize(byte[] bytes, Class\u003cT\u003e clazz) {try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes);Input input = new Input(byteArrayInputStream)) {Kryo kryo = kryoThreadLocal.get();// byte-\u003eObject:从byte数组中反序列化出对象Object o = kryo.readObject(input, clazz);\n            kryoThreadLocal.remove();return clazz.cast(o);} catch (Exception e) {throw new SerializeException(\"Deserialization failed\");}}}\n```\n\nGithub 地址：[https://github.com/EsotericSoftware/kryo](https://github.com/EsotericSoftware/kryo)\n\n## Protobuf\n\nProtobuf 出自于 Google，**性能还比较优秀，也支持多种语言，同时还是跨平台的**。就是在使用中过于繁琐，因为你需要自己定义 IDL 文件和生成对应的序列化代码。这样虽然不灵活，但是，另一方面导致 protobuf 没有序列化漏洞的风险。\n\n\u003e Protobuf 包含序列化格式的定义、各种语言的库以及一个 IDL 编译器。正常情况下你需要定义 proto 文件，然后使用 IDL 编译器编译成你需要的语言\n\n一个简单的 proto 文件如下：\n\n```ProtoBuf\n// protobuf的版本\nsyntax = \"proto3\";\n// SearchRequest会被编译成不同的编程语言的相应对象，比如Java中的class、Go中的struct\nmessage Person {\n  //string类型字段\n  string name = 1;\n  // int 类型字段\n  int32 age = 2;\n}\n```\n\nGithub 地址：[https://github.com/protocolbuffers/protobufopen in new window](https://github.com/protocolbuffers/protobuf)。\n\n## ProtoStuff\n\n由于 Protobuf 的易用性，它的哥哥 Protostuff 诞生了。\n\nprotostuff 基于 Google protobuf，但是提供了更多的功能和更简易的用法。虽然更加易用，但是不代表 ProtoStuff 性能更差。\n\nGithub 地址：[https://github.com/protostuff/protostuffopen in new window](https://github.com/protostuff/protostuff)。\n\n## Hessian\n\nHessian 是一个轻量级的，自定义描述的二进制 RPC 协议。Hessian 是一个比较老的序列化实现了，并且同样也是跨语言的。\n\n![](statistic/asynccode-21.png)\n\nDubbo2.x 默认启用的序列化方式是 Hessian2 ,但是，Dubbo 对 Hessian2 进行了修改，不过大体结构还是差不多。\n\n## 总结\n\nKryo 是专门针对 Java 语言序列化方式并且性能非常好，如果你的应用是专门针对 Java 语言的话可以考虑使用，并且 Dubbo 官网的一篇文章中提到说推荐使用 Kryo 作为生产环境的序列化方式。(文章地址：[https://dubbo.apache.org/zh/docs/v2.7/user/references/protocol/rest/open in new window](https://dubbo.apache.org/zh/docs/v2.7/user/references/protocol/rest/))\n\n![](statistic/asynccode-20.png)\n\n像 Protobuf、 ProtoStuff、hessian 这类都是跨语言的序列化方式，如果有跨语言需求的话可以考虑使用。\n\n除了我上面介绍到的序列化方式的话，还有像 Thrift，Avro 这些。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E5%BC%82%E5%B8%B8":{"title":"Java 异常","content":"\n\n\n**Java 异常类层次结构图概览** ：\n\n![](statistic/asynccode-11.png)\n\n# Exception 和 Error 有什么区别？\n\n在 Java 中，所有的异常都有一个共同的祖先 `java.lang` 包中的 `Throwable` 类。`Throwable` 类有两个重要的子类:\n\n- **`Exception`** :程序本身可以处理的异常，可以通过 `catch` 来进行捕获。\n    \n    - CheckedException受检异常：编译器会强制检查并要求处理的异常。\n        \n    - RuntimeExcepiton：程序运行中出现异常，比如我们熟悉的空指针、 数组下标越界等等\n        \n- **`Error`** ：`Error` 属于程序无法处理的错误 ，~~我们没办法通过~~ ~~`catch`~~ ~~来进行捕获~~不建议通过`catch`捕获 。例如 Java 虚拟机运行错误（`Virtual MachineError`）、虚拟机内存不够错误(`OutOfMemoryError`)、类定义错误（`NoClassDefFoundError`）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。\n    \n\n# Checked Exception 和 Unchecked Exception 有什么区别？\n\n**Checked Exception** 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 `catch`或者`throws` 关键字处理的话，就没办法通过编译。\n\n比如下面这段 IO 操作的代码：\n\n![](statistic/asynccode-12.png)\n\n除了`RuntimeException`及其子类以外，其他的`Exception`类及其子类都属于受检查异常 。常见的受检查异常有： IO 相关的异常、`ClassNotFoundException` 、`SQLException`...。\n\n**Unchecked Exception** 即 **不受检查异常** ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。\n\n`RuntimeException` 及其子类都统称为非受检查异常，常见的有（建议记下来，日常开发中会经常用到）：\n\n- `NullPointerException`(空指针错误)\n    \n- `IllegalArgumentException`(参数错误比如方法入参类型错误)\n    \n- `NumberFormatException`（字符串转换为数字格式错误，`IllegalArgumentException`的子类）\n    \n- `ArrayIndexOutOfBoundsException`（数组越界错误）\n    \n- `ClassCastException`（类型转换错误）\n    \n- `ArithmeticException`（算术错误）\n    \n- `SecurityException` （安全错误比如权限不够）\n    \n- `UnsupportedOperationException`(不支持的操作错误比如重复创建同一用户)\n    \n- ......\n    \n\n![](statistic/asynccode-16.png)\n\n# 异常的处理方式\n\n![](statistic/asynccode-13.png)\n\n  \n\n  \n\n# Throwable 类常用方法有哪些？\n\n- `String getMessage()`: 返回异常发生时的简要描述\n    \n- `String toString()`: 返回异常发生时的详细信息\n    \n- `String getLocalizedMessage()`: 返回异常对象的本地化信息。使用 `Throwable` 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 `getMessage()`返回的结果相同\n    \n- `void printStackTrace()`: 在控制台上打印 `Throwable` 对象封装的异常信息\n    \n\n# try-catch-finally 如何使用？\n\n- `try`块 ： 用于捕获异常。其后可接零个或多个 `catch` 块，如果没有 `catch` 块，则必须跟一个 `finally` 块。\n    \n- `catch`块 ： 用于处理 try 捕获到的异常。\n    \n- `finally` 块 ： 无论是否捕获或处理异常，`finally` 块里的语句都会被执行。当在 `try` 块或 `catch` 块中遇到 `return` 语句时，`finally` 语句块将在方法返回之前被执行。\n    \n\n代码示例：\n\n```Java\ntry {\n    System.out.println(\"Try to do something\");\n    throw new RuntimeException(\"RuntimeException\");\n} catch (Exception e) {\n    System.out.println(\"Catch Exception -\u003e \" + e.getMessage());\n} finally {\n     System.out.println(\"Finally\");\n}\n```\n\n输出：\n\n```Plain\nTry to do something\nCatch Exception -\u003e RuntimeException\nFinally\n```\n\n**注意：不要在 finally 语句块中使用 return!** 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值。\n\n[jvm 官方文档open in new window](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.10.2.5)中有明确提到：\n\n\u003e If the `try` clause executes a _return_, the compiled code does the following:\n\u003e \n\u003e 1. Saves the return value (if any) in a local variable.\n\u003e     \n\u003e 2. Executes a _jsr_ to the code for the `finally` clause.\n\u003e     \n\u003e 3. Upon return from the `finally` clause, returns the value saved in the local variable.\n\u003e     \n\n代码示例：\n\n```Java\npublic static void main(String[] args) {\n    System.out.println(f(2));\n}\n\npublic static int f(int value) {\n    try {\n        return value * value;\n    } finally {\n        if (value == 2) {\n            return 0;\n        }\n    }\n}\n```\n\n输出：\n\n```Plain\n0\n```\n\n# finally 中的代码一定会执行吗？\n\n不一定的！在某些情况下，finally 中的代码不会被执行。\n\n就比如说 finally 之前虚拟机被终止运行的话，finally 中的代码就不会被执行。\n\n```Java\ntry {\n    System.out.println(\"Try to do something\");\n    throw new RuntimeException(\"RuntimeException\");\n} catch (Exception e) {\n    System.out.println(\"Catch Exception -\u003e \" + e.getMessage());\n    // 终止当前正在运行的Java虚拟机\n    System.exit(1);\n} finally {\n    System.out.println(\"Finally\");\n}\n```\n\n输出：\n\n```Plain\nTry to do something\nCatch Exception -\u003e RuntimeException\n```\n\n另外，在以下 2 种特殊情况下，`finally` 块的代码也不会被执行：\n\n1. 程序所在的线程死亡。\n    \n2. 关闭 CPU。\n    \n\n相关 issue： [https://github.com/Snailclimb/JavaGuide/issues/190open in new window](https://github.com/Snailclimb/JavaGuide/issues/190)。\n\n🧗🏻 进阶一下：从字节码角度分析`try catch finally`这个语法糖背后的实现原理。\n\n# 如何使用 `try-with-resources` 代替`try-catch-finally`？\n\n1. **适用范围（资源的定义）：** 任何实现 `java.lang.AutoCloseable`或者 `java.io.Closeable` 的对象\n    \n2. **关闭资源和 finally 块的执行顺序：** 在 `try-with-resources` 语句中，任何 catch 或 finally 块在声明的资源关闭后运行\n    \n\n《Effective Java》中明确指出：\n\n\u003e 面对必须要关闭的资源，我们总是应该优先使用 `try-with-resources` 而不是`try-finally`。随之产生的代码更简短，更清晰，产生的异常对我们也更有用。`try-with-resources`语句让我们更容易编写必须要关闭的资源的代码，若采用`try-finally`则几乎做不到这点。\n\nJava 中类似于`InputStream`、`OutputStream` 、`Scanner` 、`PrintWriter`等的资源都需要我们调用`close()`方法来手动关闭，一般情况下我们都是通过`try-catch-finally`语句来实现这个需求，如下：\n\n```Java\n    Scanner scanner\n    try{\n    //读取文本文件的内容Scanner scanner = null;try {\n    scanner = new Scanner(new File(\"D://read.txt\"));\n        while (scanner.hasNext()) {\n            System.out.println(scanner.nextLine());\n        }\n    } catch (FileNotFoundException e) {\n        e.printStackTrace();\n    }finally {\n        if (scanner != null) {\n            scanner.close();\n        }\n    }\n```\n\n使用 Java 7 之后的 `try-with-resources` 语句改造上面的代码:\n\n```Java\ntry (Scanner scanner = new Scanner(new File(\"test.txt\"))) {\n    while (scanner.hasNext()) {\n        System.out.println(scanner.nextLine());\n    }\n} catch (FileNotFoundException fnfe) {\n    fnfe.printStackTrace();\n}\n```\n\n当然多个资源需要关闭的时候，使用 `try-with-resources` 实现起来也非常简单，如果你还是用`try-catch-finally`可能会带来很多问题。\n\n通过使用分号分隔，可以在`try-with-resources`块中声明多个资源。\n\n```Java\ntry (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(\"test.txt\")));\n    BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(\"out.txt\")))) {\n    int b;\n    while ((b = bin.read()) != -1) {\n        bout.write(b);\n    }\n}catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n# 异常使用有哪些需要注意的地方？\n\n- 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。\n    \n- 抛出的异常信息一定要有意义。\n    \n- 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出`NumberFormatException`而不是其父类`IllegalArgumentException`。\n    \n- 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在一段代码逻辑中）。\n    \n- ......\n    \n\n  \n\n# 三道经典异常面试题\n\n![](statistic/asynccode-15.png)\n\n  \n\n执行结果：31。\n\ntry、catch。finally 的基础用法，在return 前会先执行finally 语句块，所以是先输出finally 里的3，再输出return 的1。\n\n![](statistic/asynccode-14.png)\n\n执行结果：3。\n\ntry 返回前先执行 finally，结果 finally 里不按套路出牌，直接 return 了，自然也就走 不到 try 里面的 return 了。\n\n![](statistic/asynccode-12.png)\n\n执行结果：2。\n\n大家可能会以为结果应该是 3，因为在 return 前会执行 finally，而 i 在 finally 中被修 改为 3 了，那最终返回 i 不是应该为 3 吗？\n\n但其实，在执行 finally 之前，JVM 会先将 i 的结果暂存起来，**然后 finally 执行完毕**\n\n**后，会返回之前暂存的结果，而不是返回 i，**所以即使 i 已经被修改为 3，最终返回\n\n的还是之前暂存起来的结果 2","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E6%B3%9B%E5%9E%8B":{"title":"Java 泛型","content":"\n\n# 什么是泛型？有什么作用？\n\n**Java 泛型（Generics）** 是 JDK 5 中引入的一个新特性。使用泛型参数，可以增强代码的可读性以及稳定性。\n\n编译器可以对泛型参数进行检测，并且通过泛型参数可以指定传入的对象类型。比如 `ArrayList\u003cPerson\u003e persons = new ArrayList\u003cPerson\u003e()` 这行代码就指明了该 `ArrayList` 对象只能传入 `Person` 对象，如果传入其他类型的对象就会报错。\n\n```Java\nArrayList\u003cE\u003e extends AbstractList\u003cE\u003e\n```\n\n并且，原生 `List` 返回类型是 `Object` ，需要手动转换类型才能使用，使用泛型后编译器自动转换。\n\n# 泛型的使用方式有哪几种？\n\n泛型一般有三种使用方式:**泛型类**、**泛型接口**、**泛型方法**。\n\n**1.泛型类**：\n\n```Java\n//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型//在实例化泛型类时，必须指定T的具体类型\npublic class Generic\u003cT\u003e{\n    private T key;\n    public Generic(T key) {\n        this.key = key;\n    }\n    public T getKey(){\n        return key;\n    }\n}\n```\n\n如何实例化泛型类：\n\n```Java\nGeneric\u003cInteger\u003e genericInteger = new Generic\u003cInteger\u003e(123456);\n```\n\n**2.泛型接口** ：\n\n```Java\npublic interface Generator\u003cT\u003e {public T method();}\n```\n\n实现泛型接口，不指定类型：\n\n```Java\nclass GeneratorImpl\u003cT\u003e implements Generator\u003cT\u003e{\n    @Override\n    public T method() {\n        return null;\n    }\n}\n```\n\n实现泛型接口，指定类型：\n\n```Java\nclass GeneratorImpl\u003cT\u003e implements Generator\u003cString\u003e{\n    @Overridepublic String method() {\n        return \"hello\";\n    }\n}\n```\n\n**3.泛型方法** ：\n\n```Java\n   public static \u003c E \u003e void printArray( E[] inputArray ){\n       for ( E element : inputArray ){\n           System.out.printf( \"%s \", element );\n       }\n       System.out.println();\n   }\n```\n\n使用：\n\n```Java\n// 创建不同类型数组： Integer, Double 和 CharacterInteger[] intArray = { 1, 2, 3 };String[] stringArray = { \"Hello\", \"World\" };printArray( intArray  );printArray( stringArray  );\n```\n\n\u003e 注意: `public static \u003c E \u003e void printArray( E[] inputArray )` 一般被称为静态泛型方法;在 java 中泛型只是一个占位符，必须在传递类型后才能使用。类在实例化时才能真正的传递类型参数，由于静态方法的加载先于类的实例化，也就是说类中的泛型还没有传递真正的类型参数，静态的方法的加载就已经完成了，所以静态泛型方法是没有办法使用类上声明的泛型的。只能使用自己声明的 `\u003cE\u003e`\n\n# 项目中哪里用到了泛型？\n\n- 自定义接口通用返回结果 `CommonResult\u003cT\u003e` 通过参数 `T` 可根据具体的返回类型动态指定结果的数据类型\n    \n- 定义 `Excel` 处理类 `ExcelUtil\u003cT\u003e` 用于动态指定 `Excel` 导出的数据类型\n    \n- 构建集合工具类（参考 `Collections` 中的 `sort`, `binarySearch` 方法）。\n    \n- ......\n    \n\n  \n\n# 什么是泛型擦除\n\n所谓的泛型擦除，官方名叫“类型擦除”。\n\nJava 的泛型是伪泛型，这是因为 Java 在编译期间，所有的类型信息都会被擦掉。\n\n  \n\n![](statistic/asynccode-10.png)\n\n![](statistic/asynccode-10.png)\n\n上面两段代码在JRE（Java运行环境）看来没区别\n\n## 为啥要有类型擦除？\n\n主要是为了向下兼容，因为JDK5之前是没有泛型的，为了让JVM保持向下兼容，就出了类型擦除这个策略。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E6%B3%A8%E8%A7%A3":{"title":"Java 注解","content":"\n\n# 注解\n\n# 何谓注解？\n\n`Annotation` （注解） 是 Java5 开始引入的新特性，可以看作是一种**特殊的注释，主要用于修饰类、方法或者变量，提供某些信息供程序在编译或者运行时使用**。\n\n注解本质是一个继承了`Annotation` 的特殊接口：\n\n```Java\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.SOURCE)\npublic @interface Override {\n\n}\n\npublic interface Override extends Annotation{\n\n}\n```\n\nJDK 提供了很多内置的注解（比如 `@Override` 、`@Deprecated`），同时，我们还可以自定义注解。\n\n# 注解的解析方法有哪几种？\n\n注解只有被解析之后才会生效，常见的解析方法有两种：\n\n- **编译期直接扫描** ：编译器在编译 Java 代码的时候扫描对应的注解并处理，比如某个方法使用`@Override` 注解，编译器在编译的时候就会检测当前的方法是否重写了父类对应的方法。\n    \n- **运行期通过反射处理** ：像框架中自带的注解(比如 Spring 框架的 `@Value` 、`@Component`)都是通过反射来进行处理的。\n    \n\n  \n\n# 注解的生命周期\n\n- RetentionPolicy.SOURCE：给编译器用的，不会写入 class 文件\n    \n- RetentionPolicy.CLASS：会写入 class 文件，在类加载阶段丢弃，也就是运行的时候就没这个信息了\n    \n- RetentionPolicy.RUNTIME：会写入 class 文件，永久保存，可以通过反射获取注信息","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84":{"title":"Java 的数据结构","content":"\n# Java 的数据类型\n定义：Java 语言是强类型语言，对于每一种数据都定义了明确的具体的数据类型，在内存中分配了不同大小的内存空间。 \nJava 语言数据类型分为两种：基本数据类型和引用数据类型\n![[Pasted image 20230726230002.png]]\n# 基本数据类型\nAva 中有 8 种基本数据类型，分别为：\n- 6 种数字类型： \n  - 4 种整数型：byte、short、int、long\n  - 2 种浮点型：float、double\n- 1 种字符类型：char\n- 1 种布尔型：boolean\n这 8 种基本数据类型的默认值以及所占空间的大小如下：\n\n\n| 基本类型 | 位数 | 字节 | 默认值  | 取值范围                                   |\n| -------- | ---- | ---- | ------- | ------------------------------------------ |\n| byte     | 8    | 1    | 0       | -128 ~ 127                                 |\n| short    | 16   | 2    | 0       | -32768 ~ 32767                             |\n| int      | 32   | 4    | 0       | -2147483648 ~ 2147483647                   |\n| long     | 64   | 8    | 0 L      | -9223372036854775808 ~ 9223372036854775807 |\n| char     | 16   | 2    | 'u 0000' | 0 ~ 65535                                  |\n| float    | 32   | 4    | 0 f      | 1.4 E-45 ~ 3.4028235 E 38                     |\n| double   | 64   | 8    | 0 d      | 4.9 E-324 ~ 1.7976931348623157 E 308          |\n| boolean  | 1    |      | FALSE   | true、false                                |\n\n\n\n对于 boolean，官方文档未明确定义，它依赖于 JVM 厂商的具体实现。逻辑上理解是占用 1 位，但是实际中会考虑计算机高效存储因素。\n另外，Java 的每种基本类型所占存储空间的大小不会像其他大多数语言那样随机器硬件架构的变化而变化。这种所占存储空间大小的不变性是 Java 程序比用其他大多数语言编写的程序更具可移植性的原因之一（《Java 编程思想》2.2 节有提到）。\n**注意**：\n1. Java 里使用 long 类型的数据一定要在数值后面加上 L，否则将作为整型解析。\n2. Char a = 'h'char : 单引号，String a = \"hello\" : 双引号。\n这八种基本类型都有对应的包装类分别为：Byte、Short、Integer、Long、Float、Double、Character、Boolean \n\n# 自动类型转换、强制类型转换\nJava 所有的数值型变量可以相互转换，当把一个表数范围小的数值或变量直接赋给另一个表数范围大的变量时，可以进行自动类型转换；反之，需要强制转换。\n\n![[Pasted image 20230726230138.png]]\n这就好像，小杯里的水倒进大杯没问题，但大杯的水倒进小杯就不行了，可能会溢出。 \nFloat f=3.4 ，对吗？ \n不正确。3.4 是单精度数，将双精度型（double）赋值给浮点型（float）属于下转型 down-casting，也称为窄化）会造成精度损失，因此需要强制类型转换 float f = (float) 3.4; 或者写成 float f =3.4 F \nShort s 1 = 1; s 1 = s 1 + 1；对吗？ Short s 1 = 1; s 1 += 1; 对吗？ \n- 对于 short s 1 = 1; s 1 = s 1 + 1; 编译出错，由于 1 是 int 类型，因此 s 1+1 运算结果也是 int 型，需要强制转换类型才能赋值给 short 型。 \n- 而 short s 1 = 1; s 1 += 1; 可以正确编译，因为 s 1+= 1; 相当于 s 1 = (short (s 1 + 1); 其中有隐含的强制类型转换。 \n# 基本类型和包装类型的区别 ？\n- 成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null。\n- 包装类型可用于泛型，而基本类型不可以。\n- 基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。\n- 相比于对象类型，基本数据类型占用的空间非常小。\n为什么说是几乎所有对象实例呢？ 这是因为 HotSpot 虚拟机引入了 JIT 优化之后，会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配，而避免堆上分配内存\n⚠️ 注意 ： 基本数据类型存放在栈中是一个常见的误区！ 基本数据类型的成员变量如果没有被 static 修饰的话（不建议这么使用，应该要使用基本数据类型对应的包装类型），就存放在堆中。\n\n```\nClass BasicTypeVar{\n    Private int x;\n}\n```\n\n# 包装类型的缓存机制\nJava 基本数据类型的包装类型的大部分都用到了缓存机制来提升性能。\n\nByte, Short, Integer, Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。\nInteger 缓存源码：\n\n```\nPublic static Integer valueOf (int i) {\n    If (i \u003e= IntegerCache. Low \u0026\u0026 i \u003c= IntegerCache. High)\n        Return IntegerCache. Cache[i + (-IntegerCache. Low)];\n    Return new Integer (i);\n}\nPrivate static class IntegerCache {\n    Static final int low = -128;\n    Static final int high;\n    Static {\n        // high value may be configured by property\n        Int h = 127;\n    }\n}\n```\n\n\nCharacter 缓存源码:\n\n```\nPublic static Character valueOf (char c) {\n    If (c \u003c= 127) { // must cache\n      Return CharacterCache. Cache[(int) c];\n    }\n    Return new Character (c);\n}\n\nPrivate static class CharacterCache {\n    Private CharacterCache (){}\n    Static final Character cache[] = new Character[127 + 1];\n    Static {\n        For (int i = 0; i \u003c cache. Length; i++)\n            Cache[i] = new Character ((char) i);\n    }\n\n}\n\n```\n\nBoolean 缓存源码：\nPublic static Boolean valueOf (boolean b) {\n    Return (b ? TRUE : FALSE);\n}\n\n如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。\n两种浮点数类型的包装类 Float, Double 并没有实现缓存机制。\n\n```\nInteger i 1 = 33;\nInteger i 2 = 33;\nSystem. Out. Println (i 1 == i 2);// 输出 true\n\nFloat i 11 = 333 f;\nFloat i 22 = 333 f;\nSystem. Out. Println (i 11 == i 22);// 输出 false\n\nDouble i 3 = 1.2;\nDouble i 4 = 1.2;\nSystem. Out. Println (i 3 == i 4);// 输出 false\n\nInteger i 1 = 40;\nInteger i 2 = new Integer (40);\nSystem. Out. Println (i 1==i 2);\n//Integer i 1=40 这一行代码会发生装箱，也就是说这行代码等价于 Integer i 1=Integer. ValueOf (40) 。因此，i 1 直接使用的是缓存中的对象。而 Integer i 2 = new Integer (40) 会直接创建新的对象。\n```\n\n所有整型包装类对象之间值的比较，全部使用 equals 方法比较。\n\n\n# 自动装箱与拆箱了解吗 ？原理是什么？\n- 装箱：将基本类型用它们对应的引用类型包装起来；\n- 拆箱：将包装类型转换为基本数据类型；\n\n```\nInteger i = 10;  //装箱\nInt n = i;   //拆箱\n```\n\n装箱其实就是调用了包装类的 valueOf ()方法，拆箱其实就是调用了 xxxValue ()方法。\n如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作。\n# 为什么浮点数运算的时候会有精度丢失的风险 ？\n\n```\nFloat a = 2.0 f - 1.9 f;\nFloat b = 1.8 f - 1.7 f;\nSystem. Out. Println (a);// 0.100000024\nSystem. Out. Println (b);// 0.099999905\nSystem. Out. Println (a == b);// false\n\n```\n\n这个和计算机保存浮点数的机制有很大关系。我们知道计算机是二进制的，而且计算机在表示一个数字时，**宽度是有限的，无限循环的小数存储在计算机时，只能被截断，所以就会导致小数精度发生损失的情况。** 这也就是解释了为什么浮点数没有办法用二进制精确表示。\n就比如说十进制下的 0.2 就没办法精确转换成二进制小数：\n\n```\n// 0.2 转换为二进制数的过程为，不断乘以 2，直到不存在小数为止，\n// 在这个计算过程中，得到的整数部分从上到下排列就是二进制的结果。\n0.2 * 2 = 0.4 -\u003e 0\n0.4 * 2 = 0.8 -\u003e 0\n0.8 * 2 = 1.6 -\u003e 1\n0.6 * 2 = 1.2 -\u003e 1\n0.2 * 2 = 0.4 -\u003e 0（发生循环）\n...\n```\n\n\n# 如何解决浮点数的精度丢失\nBigDecimal 可以实现对浮点数的运算，不会造成精度丢失。通常情况下，大部分需要浮点数精确运算结果的业务场景（比如涉及到钱的场景）都是通过 BigDecimal 来做的。\n超过 long 整型的数据应该如何表示？\nBigInteger \n\n","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":["Java","数据结构"]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1":{"title":"Java 面向对象","content":"\n\n## 面向对象和面向过程的区别\n\n两者的主要区别在于解决问题的方式不同：\n\n- 面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。\n    \n- 面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。\n    \n\n另外，面向对象开发的程序一般更易维护、易复用、易扩展。\n\n相关 issue : [面向过程 ：面向过程性能比面向对象高？？open in new window](https://github.com/Snailclimb/JavaGuide/issues/431)\n\n## 创建一个对象用什么运算符?对象实体与对象引用有何不同?\n\nnew 运算符，new 创建对象实例（对象实例在堆内存中），**对象引用指向对象实例**（对象引用存放在栈内存中）。\n\n- 一个对象引用可以指向 0 个或 1 个对象（一根绳子可以不系气球，也可以系一个气球）;\n    \n- 一个对象可以有 n 个引用指向它（可以用 n 条绳子系住一个气球）。\n    \n\n## 类的构造方法的作用是什么?\n\n构造方法是一种特殊的方法，主要作用是完成对象的初始化工作。\n\n## 如果一个类没有声明构造方法，该程序能正确执行吗?\n\n如果一个类没有声明构造方法，也可以执行！因为**一个类即使没有声明构造方法也会有默认的不带参数的构造方法**。如果我们自己添加了类的构造方法（无论是否有参），Java 就不会再添加默认的无参数的构造方法了，我们一直在不知不觉地使用构造方法，这也是为什么我们在创建对象的时候后面要加一个括号（因为要调用无参的构造方法）**。如果我们重载了有参的构造方法，记得都要把无参的构造方法也写出来（无论是否用到）**，因为这可以帮助我们在创建对象的时候少踩坑。\n\n## 构造方法有哪些特点？是否可被 override?\n\n构造方法特点如下：\n\n- 名字与类名相同。\n    \n- 没有返回值，但不能用 void 声明构造函数。\n    \n- 生成类的对象时自动执行，无需调用。\n    \n\n**构造方法不能被 override（重写）,但是可以 overload（重载）**,所以你可以看到一个类中有多个构造函数的情况。\n\n## 面向对象三大特征\n\n### 封装\n\n**封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部**，不允许外部对象直接访问对象的内部信息。\n\n```Java\npublic class Student {\n    private int id;//id属性私有化\n    private String name;//name属性私有化\n\n    //获取id的方法\n    public int getId() {\n        return id;\n    }\n\n    //设置id的方法\n    public void setId(int id) {\n        this.id = id;\n    }\n\n    //获取name的方法\n    public String getName() {\n        return name;\n    }\n\n    //设置name的方法\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n```\n\n### 继承\n\n不同类型的对象，相互之间经常有一定数量的共同点。继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间 ，提高我们的开发效率。\n\n**关于继承如下 3 点请记住：**\n\n1. **子类拥有父类对象所有的属性和方法（包括私有属性和私有方法）**，但是父类中的私有属性和方法子类是无法访问，**只是拥有**。\n    \n2. 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。\n    \n3. 子类可以用自己的方式实现父类的方法。（以后介绍）。\n    \n\n### 多态\n\n多态，顾名思义，表示一个对象具有多种的状态，具体表现为父类的引用指向子类的实例。\n\n**多态的特点:**\n\n- 对象类型和引用类型之间具有继承（类）/实现（接口）的关系；\n    \n- 引用类型变量发出的方法调用的到底是哪个类中的方法，必须在程序运行期间才能确定；\n    \n- 多态不能调用“只在子类存在但在父类不存在”的方法；\n    \n- 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法。\n    \n\n## 重载（**overload**）和重写（**override**）的区别？\n\n  \n\n方法的重载和重写都是实现多态的方式，区别在于**前者实现的是编译时的多态性， 而后者实现的是运行时的多态性**\n\n- 重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；\n    \n- 重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异 常（里氏代换原则）。\n    \n\n### 方法重载的原则\n\n1. 方法名一致，参数列表中参数的顺序，类型，个数不同。\n    \n2. 重载与方法的返回值无关，存在于父类和子类，同类中。\n    \n3. 可以抛出不同的异常，可以有不同修饰符。\n    \n\n  \n\n## 访问修饰符 public、private、protected、以及不写（默认）时的区别\n\n- default (即默认，什么也不写）: 在同一包内可见，不使用任何修饰符。可以修 饰在类、接口、变量、方法。*\n    \n- private : 在同一类内可见。可以修饰变量、方法。 注意：不能修饰类（外部 类）\n    \n- public : 对所有类可见。可以修饰类、接口、变量、方法\n    \n- protected : 对同一包内的类和所有子类可见。可以修饰变量、方法。 注意：不能修饰类（外部类） 。\n    \n![[Pasted image 20230726232059.png]]\n\n\n## this 关键字有什么作用\n\nthis是自身的一个对象，代表对象本身，可以理解为：**指向对象本身的一个指针**。\n\n- 普通的直接引用，this相当于是指向当前对象本身\n    \n- 形参与成员变量名字重名，用this来区分：\n    \n- 引用本类的构造函数\n    \n\n## 接口(interface)和抽象类(abstract class)有什么共同点和区别？\n\n**共同点** ：\n\n- 都不能被实例化。\n    \n- 都可以包含抽象方法。\n    \n- 都可以有默认实现的方法（Java 8 可以用 `default` 关键字在接口中定义默认方法）。\n    \n\n**区别** ：\n\n- 接⼝的⽅法默认是 public ，所有⽅法在接⼝中不能有实现(Java 8 开始接⼝⽅法 可以有默认实现），⽽抽象类可以有⾮抽象的⽅法\n    \n- 接⼝中除了 static 、 final 变量，不能有其他变量，⽽抽象类中则不⼀定。\n    \n- 接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系。\n    \n- 一个类只能继承一个类，但是可以实现多个接口。\n    \n- 接口中的成员变量只能是 `public static final` 类型的，不能被修改且必须有初始值，而抽象类的成员变量默认 default，可在子类中被重新定义，也可被重新赋值。\n    \n\n## 成员变量与局部变量的区别有哪些\n\n- 从语法形式上看 ：成员变量是属于类的，⽽局部变量是在⽅法中定义的变量或是⽅法的参数；\n    \n- 从变量在内存中的存储⽅式来看 ：如果成员变量是使⽤ static 修饰的，那么这个 成员变量是属于类的，如果没有使⽤ static 修饰，这个成员变量是属于实例的。 对象存于堆内存，如果局部变量类型为基本数据类型，那么存储在栈内存，如果 为引⽤数据类型，那存放的是指向堆内存对象的引⽤或者是指向常量池中的地 址。\n    \n- 从变量在内存中的⽣存时间上看 ：成员变量是对象的⼀部分，它随着对象的创建⽽存在，⽽局部变量随着⽅法的调⽤⽽⾃动消失。\n    \n- 成员变量如果没有被赋初值 ：则会⾃动以类型的默认值⽽赋值（⼀种情况例外:被 final 修饰的成员变量也必须显式地赋值），⽽局部变量则不会⾃动赋值\n    \n\n  \n\n## 静态变量和实例变量的区别？静态方法、实例方法呢\n\n### 静态变量和实例变量\n\n  \n\n- 静态变量**:** 是被 static 修饰符修饰的变量，也称为类变量，它属于类，不属于类的任 何一个对象，一个类不管创建多少个对象，静态变量在内存中有且仅有一个副本。\n    \n- 实例变量**:** 必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。\n    \n\n  \n\n### 静态⽅法和实例⽅法有何不同?\n\n- 静态方法：static修饰的方法，也被称为类方法。在外部调⽤静态⽅法时，可以使 ⽤\"类名**.⽅**法名\"的⽅式，也可以使⽤\"对象名**.⽅**法名\"的⽅式。静态方法里不能访 问类的非静态成员变量和方法。\n    \n- 实例**⽅**法：依存于类的实例，需要使用\"对象名**.⽅**法名\"的⽅式调用；可以访问类 的所有成员变量和方法\n    \n\n  \n\n## final关键字的作用\n\nfinal表示不可变的意思，可用于修饰类、属性和方法：\n\n- 被final修饰的类不可以被继承\n    \n- 被final修饰的方法不可以被重写\n    \n- 被final修饰的变量不可变，被final修饰的变量必须被显式第指定初始值，还得注 意的是，这里的不可变指的是变量的引用不可变，不是引用指向的内容的不可变。\n    \n![[Pasted image 20230726232124.png]]\n![[Pasted image 20230726232130.png]]\n\njava.lang.Object 里定义的方法，也就是说每一个对象都有这么个方法，这个方法在 gc 启动，该对象被回收的时候被调用。\n    \n\n一个对象的 **finalize 方法只会被调用一次，finalize 被调用不一定会立即回收该对象，所以有可能调用 finalize 后，该对象又不需要被回收了**，然后到了真正要被 回收的时候，因为前面调用过一次，所以不会再次调用 finalize 了，进而产生问题，因此不推荐使用 finalize 方法。\n\n## 对象的相等和引用相等的区别\n\n- 对象的相等一般比较的是内存中存放的内容是否相等。\n    \n- 引用相等一般比较的是他们指向的内存地址是否相等。\n    \n\n  \n\n## **==**和 **equals** 的区别？\n\n- **==** : 它的作⽤是判断两个对象的地址是不是相等。即，判断两个对象是不是同⼀个 对象(基本数据类型**==**比较的是值，引⽤数据类型**==**比较的是内存地址)。\n    \n- **equals()** : 它的作⽤也是判断两个对象是否相等。但是这个“相等”一般也分两种情 况：\n    \n    - 默认情况：类没有覆盖 equals() ⽅法。则通过 equals() 比较该类的两个对象时， 等价于通过“ == ”比较这两个对象，还是相当于比较内存地址。\n        \n    - 自定义情况：类覆盖了 equals() ⽅法。我们平时覆盖的 equals()方法一般是比较两个对象的内容是否相同，自定义了一个相等的标准，也就是两个对象的值是否 相等\n        \n\n![](statistic/asynccode-1.png)\n\n  \n\n## hasCode 与equals\n\n### 什么是hashcode\n\nhashCode() 的作⽤是获取哈希码，也称为散列码；它实际上是返回⼀个 int 整数，定 义在 Object 类中， 是一个**本地⽅法**，这个⽅法通常⽤来将**对象的内存地址转换为整 数之后**返回\n\n### 为什么要有 hashCode？\n\n主要是在哈希表这种结构中用的到，例如HashMap怎么**把key映射到对应的value**上呢？用的就是哈希取余法，也就是拿哈 希码和存储元素的数组的长度取余，获取key对应的value所在的下标位置。详\n\n### 为什么重写 quals 时必须重写 hashCode ⽅法？\n\n如果两个对象相等，**则 hashcode ⼀定也是相同的**。两**个对象相等，对两个对象分别 调⽤ equals⽅法都返回 true。**反之，两个对象有相同的 hashcode 值，它们也不⼀定 是相等的 。因此，**equals** ⽅法被覆盖过，则 **hashCode** ⽅法也必须被覆盖。\n\n### 为什么两个对象有相同的 hashcode值，它们也不⼀定是相等的？\n\n因为可能会碰撞， hashCode() 所使⽤的散列算法也许刚好会让多个对象传回相同的 散列值。越糟糕的散列算法越容易碰撞，但这也与数据值域分布的特性有关\n\n## Java 是值传递还是引用传递？\n\nJava语言是值传递。Java 语言的**方法调用只支持参数的值传递**。当一个对象实例**作 为一个参数被传递到方法中时，参数的值就是对该对象的引用**。对象的属性可以在 被调用过程中被改变，但对对象引用的改变是不会影响到调用者的。\n\n## 深拷贝和浅拷贝区别了解吗？什么是引用拷贝？\n\n关于深拷贝和浅拷贝区别，我这里先给结论：\n\n- **浅拷贝**：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象。\n    \n- **深拷贝** ：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象。\n    \n\n上面的结论没有完全理解的话也没关系，我们来看一个具体的案例！\n\n**浅拷贝**\n\n浅拷贝的示例代码如下，我们这里实现了 `Cloneable` 接口，并重写了 `clone()` 方法。\n\n`clone()` 方法的实现很简单，**直接调用的是父类** **`Object`** **的** **`clone()`** 方法。\n\n```Java\npublic class Address implements Cloneable{\n\tprivate String name;// 省略构造函数、Getter\u0026Setter方法\n\t@Override\n\tpublic Address clone() {\n\t\ttry {\n\t\t\treturn (Address) super.clone();\n\t\t} catch (CloneNotSupportedException e) {\n\t\t\tthrow new AssertionError();\n\t\t}\n\t}\n}\n\npublic class Person implements Cloneable {\n\tprivate Address address;// 省略构造函数、Getter\u0026Setter方法\n\t\n\t@Override\n\tpublic Person clone() {\n\t\n\ttry {\n\t\tPerson person = (Person) super.clone();return person;} catch (CloneNotSupportedException e) {throw new AssertionError();}}}\n```\n\n测试 ：\n\n```Java\nPerson person1 = new Person(new Address(\"武汉\"));Person person1Copy = person1.clone();// trueSystem.out.println(person1.getAddress() == person1Copy.getAddress());\n```\n\n从输出结构就可以看出， `person1` 的克隆对象和 `person1` 使用的仍然是同一个 `Address` 对象。\n\n**深拷贝**\n\n这里我们简单对 `Person` 类的 `clone()` 方法进行修改，连带着要把 `Person` 对象内部的 `Address` 对象一起复制。\n\nsfhuyu\n\n```Java\n@Overridepublic Person clone() {try {Person person = (Person) super.clone();\n        person.setAddress(person.getAddress().clone());return person;} catch (CloneNotSupportedException e) {throw new AssertionError();}}\n```\n\n测试 ：\n\n```Java\nPerson person1 = new Person(new Address(\"武汉\"));Person person1Copy = person1.clone();// falseSystem.out.println(person1.getAddress() == person1Copy.getAddress());\n```\n\n从输出结构就可以看出，虽然 `person1` 的克隆对象和 `person1` 包含的 `Address` 对象已经是不同的了。\n\n**那什么是引用拷贝呢？** 简单来说，引用拷贝就是两个不同的引用指向同一个对象。\n\n我专门画了一张图来描述浅拷贝、深拷贝、引用拷贝：\n\n![](statistic/asynccode.png)\n\n## Java 创建对象的方式？\n\n![](statistic/asynccode.png)","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":["Java","面向对象"]},"/Java/%E5%9F%BA%E7%A1%80/Java-%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"Java 面试题","content":"\n\n![[Java面试题、面经.pdf]]\n\n\n\n\n![[Java 面试知识点总结.pdf]]\n\n\n\n\n![[Java 面试知识点总结(1).pdf]]\n\n\n\n\n![[Java面试题、面经(1).pdf]]\n\n\n![[阿里大佬总结的Java面试资料.pdf]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-SPI":{"title":"Java SPI","content":"\n\n# 何谓 SPI?\n\nSPI 即 Service Provider Interface ，字面意思就是：“服务提供者的接口”，我的理解是：专门提供给服务提供者或者扩展框架功能的开发者去使用的一个接口。\n\nSPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。\n\n很多框架都使用了 Java 的 SPI 机制，比如：Spring 框架、数据库加载驱动、日志接口、以及 Dubbo 的扩展实现等等。\n\n![](statistic/asynccode.jpg)\n\n# SPI 和 API 有什么区别？\n\n**那 SPI 和 API 有啥区别？**\n\n说到 SPI 就不得不说一下 API 了，从广义上来说它们都属于接口，而且很容易混淆。下面先用一张图说明一下：\n\n![](statistic/asynccode-18.png)\n\n一般模块之间都是通过接口进行通讯，那我们在服务调用方和服务实现方（也称服务提供者）之间引入一个“接口”。\n\n当实现方提供了接口和实现，我们可以通过调用实现方的接口从而拥有实现方给我们提供的能力，这就是 API ，这种接口和实现都是放在实现方的。\n\n当接口存在于调用方这边时，就是 SPI ，由接口调用方确定接口规则，然后由不同的厂商去根据这个规则对这个接口进行实现，从而提供服务。\n\n举个通俗易懂的例子：公司 H 是一家科技公司，新设计了一款芯片，然后现在需要量产了，而市面上有好几家芯片制造业公司，这个时候，只要 H 公司指定好了这芯片生产的标准（定义好了接口标准），那么这些合作的芯片公司（服务提供者）就按照标准交付自家特色的芯片（提供不同方案的实现，但是给出来的结果是一样的）。\n\n# SPI 的优缺点？\n\n通过 SPI 机制能够大大地提高接口设计的灵活性，但是 SPI 机制也存在一些缺点，比如：\n\n- 需要遍历加载所有的实现类，不能做到按需加载，这样效率还是相对较低的。\n    \n- 当多个 `ServiceLoader` 同时 `load` 时，会有并发问题。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%9F%BA%E7%A1%80/Java-overview":{"title":"Java overview","content":"\n\n\n# 什么是 Java\nJava是一门面向对象的编程语言，不仅吸收了C++语言的各种优点，还摒弃了C++里 难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。 Java语言作为静态面向对象编程语言的优秀代表，极好地实现了面向对象理论，允许 程序员以优雅的思维方式进行复杂的编程\nJava 语言有哪些特点?\n1. 简单易学；\n2. 面向对象（封装，继承，多态）；\n3. 平台无关性（ Java 虚拟机实现平台无关性）；\n4. 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）；\n5. 可靠性；\n6. 安全性；\n7. 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）；\n8. 编译与解释并存；\n🐛 修正（参见： issue#544open in new window） ：C++11 开始（2011 年的时候）,C++就引入了多线程库，在 windows、linux、macos 都可以使用std::thread和std::async来创建线程。参考链接：http://www.cplusplus.com/reference/thread/thread/?kw=thread\n🌈 拓展一下：\n“Write Once, Run Anywhere（一次编写，随处运行）”这句宣传口号，真心经典，流传了好多年！以至于，直到今天，依然有很多人觉得跨平台是 Java 语言最大的优势。实际上，跨平台已经不是 Java 最大的卖点了，各种 JDK 新特性也不是。目前市面上虚拟化技术已经非常成熟，比如你通过 Docker 就很容易实现跨平台了。在我看来，Java 强大的生态才是\n# JVM vs JDK vs JRE\n## JVM\nJava 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。\nJVM 并不是只有一种！只要满足 JVM 规范，每个公司、组织或者个人都可以开发自己的专属 JVM。 也就是说我们平时接触到的 HotSpot VM 仅仅是是 JVM 规范的一种实现而已。\n## JDK\nJDK 是 Java Development Kit 缩写，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。\n## JRE\nJRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。\n如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装 JDK 了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何 Java 开发，仍然需要安装 JDK。例如，如果要使用 JSP 部署 Web 应用程序，那么从技术上讲，您只是在应用程序服务器中运行 Java 程序。那你为什么需要 JDK 呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。\n\n![[Pasted image 20230726225043.png]]\n# 什么是跨平台性 ？原理是啥？\n所谓跨平台性，是指Java语言编写的程序，一次编译后，可以在多个系统平台上运 行。 \n实现原理：Java程序是通过Java虚拟机在系统平台上运行的，只要该系统可以安装相 、应的Java虚拟机，该系统就可以运行java程序。\n\n# 什么是字节码 ?采用字节码的好处是什么?\n在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以， Java 程序运行时相对来说还是高效的（不过，和 C++，Rust，Go 等语言还是有一定差距的），而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。\nJava 程序从源代码到运行的过程如下图所示：\n![[Pasted image 20230726225054.png]]\n我们需要格外注意的是 .class-\u003e机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT（just-in-time compilation） 编译器.\nJIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言 。\nHotSpot 采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是 JIT 所需要编译的部分。JVM 会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9 引入了一种新的编译模式 AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了 JIT 预热等各方面的开销。JDK 支持分层编译和 AOT 协作使用。\n# 为什么不全部使用 AOT 呢？\nAOT 可以提前编译节省启动时间，那为什么不全部使用这种编译方式呢？\n长话短说，这和 Java 语言的动态特性有千丝万缕的联系了。举个例子，CGLIB 动态代理使用的是 ASM 技术，而这种技术大致原理是运行时直接在内存中生成并加载修改后的字节码文件也就是 .class 文件，如果全部使用 AOT 提前编译，也就不能使用 ASM 技术了。为了支持类似的动态特性，所以选择使用 JIT 即时编译器。\n# 为什么说 Java 语言“编译与解释并存”？\n- 编译型 ：编译型语言open in new window 会通过编译器open in new window将源代码一次性翻译成可被该平台执行的机器码。一般情况下，编译语言的执行速度比较快，开发效率比较低。常见的编译性语言有 C、C++、Go、Rust 等等。\n- 解释型 ：解释型语言open in new window会通过解释器open in new window一句一句的将代码解释（interpret）为机器代码后再执行。解释型语言开发效率比较快，执行速度比较慢。常见的解释性语言有 Python、JavaScript、PHP 等等。\n![[Pasted image 20230726225103.png]]\n根据维基百科介绍：\n为了改善编译语言的效率而发展出的即时编译技术，已经缩小了这两种语言间的差距。这种技术混合了编译语言与解释型语言的优点，它像编译语言一样，先把程序源代码编译成字节码window。到执行期时，再将字节码直译，之后执行。Java与LLVM是这种技术的代表产物。\n相关阅读：基本功 | Java 即时编译器原理解析及实践\n## 为什么说 Java 语言“编译与解释并存”？\n这是因为 Java 语言既具有编译型语言的特征，也具有解释型语言的特征。因为 Java 程序要经过先编译，后解释两个步骤，由 Java 编写的程序需要先经过编译步骤，生成字节码（.class 文件），这种字节码必须由 Java 解释器来解释执行。\n\n# Oracle JDK vs OpenJDK\n可能在看这个问题之前很多人和我一样并没有接触和使用过 OpenJDK 。那么 Oracle JDK 和 OpenJDK 之间是否存在重大差异？下面我通过收集到的一些资料，为你解答这个被很多人忽视的问题。\n对于 Java 7，没什么关键的地方。OpenJDK 项目主要基于 Sun 捐赠的 HotSpot 源代码。此外，OpenJDK 被选为 Java 7 的参考实现，由 Oracle 工程师维护。关于 JVM，JDK，JRE 和 OpenJDK 之间的区别，Oracle 博客帖子在 2012 年有一个更详细的答案：\n问：OpenJDK 存储库中的源代码与用于构建 Oracle JDK 的代码之间有什么区别？\n答：非常接近 - 我们的 Oracle JDK 版本构建过程基于 OpenJDK 7 构建，只添加了几个部分，例如部署代码，其中包括 Oracle 的 Java 插件和 Java WebStart 的实现，以及一些闭源的第三方组件，如图形光栅化器，一些开源的第三方组件，如 Rhino，以及一些零碎的东西，如附加文档或第三方字体。展望未来，我们的目的是开源 Oracle JDK 的所有部分，除了我们考虑商业功能的部分。\n总结：（提示：下面括号内的内容是基于原文补充说明的，因为原文太过于晦涩难懂，用人话重新解释了下，如果你看得懂里面的术语，可以忽略括号解释的内容）\n1. Oracle JDK 大概每 6 个月发一次主要版本（从 2014 年 3 月 JDK 8 LTS 发布到 2017 年 9 月 JDK 9 发布经历了长达 3 年多的时间，所以并不总是 6 个月），而 OpenJDK 版本大概每三个月发布一次。但这不是固定的，我觉得了解这个没啥用处。详情参见：https://blogs.oracle.com/java-platform-group/update-and-faq-on-the-java-se-release-cadenceopen in new window 。\n2. OpenJDK 是一个参考模型并且是完全开源的，而 Oracle JDK 是 OpenJDK 的一个实现，并不是完全开源的；（个人观点：众所周知，JDK 原来是 SUN 公司开发的，后来 SUN 公司又卖给了 Oracle 公司，Oracle 公司以 Oracle 数据库而著名，而 Oracle 数据库又是闭源的，这个时候 Oracle 公司就不想完全开源了，但是原来的 SUN 公司又把 JDK 给开源了，如果这个时候 Oracle 收购回来之后就把他给闭源，必然会引起很多 Java 开发者的不满，导致大家对 Java 失去信心，那 Oracle 公司收购回来不就把 Java 烂在手里了吗！然后，Oracle 公司就想了个骚操作，这样吧，我把一部分核心代码开源出来给你们玩，并且我要和你们自己搞的 JDK 区分下，你们叫 OpenJDK，我叫 Oracle JDK，我发布我的，你们继续玩你们的，要是你们搞出来什么好玩的东西，我后续发布 Oracle JDK 也会拿来用一下，一举两得！）OpenJDK 开源项目：https://github.com/openjdk/jdkopen in new window\n3. Oracle JDK 比 OpenJDK 更稳定（肯定啦，Oracle JDK 由 Oracle 内部团队进行单独研发的，而且发布时间比 OpenJDK 更长，质量更有保障）。OpenJDK 和 Oracle JDK 的代码几乎相同（OpenJDK 的代码是从 Oracle JDK 代码派生出来的，可以理解为在 Oracle JDK 分支上拉了一条新的分支叫 OpenJDK，所以大部分代码相同），但 Oracle JDK 有更多的类和一些错误修复。因此，如果您想开发企业/商业软件，我建议您选择 Oracle JDK，因为它经过了彻底的测试和稳定。某些情况下，有些人提到在使用 OpenJDK 可能会遇到了许多应用程序崩溃的问题，但是，只需切换到 Oracle JDK 就可以解决问题；\n4. 在响应性和 JVM 性能方面，Oracle JDK 与 OpenJDK 相比提供了更好的性能；\n5. Oracle JDK 不会为即将发布的版本提供长期支持（如果是 LTS 长期支持版本的话也会，比如 JDK 8，但并不是每个版本都是 LTS 版本），用户每次都必须通过更新到最新版本获得支持来获取最新版本；\n6. Oracle JDK 使用 BCL/OTN 协议获得许可，而 OpenJDK 根据 GPL v2 许可获得许可。\n既然 Oracle JDK 这么好，那为什么还要有 OpenJDK？\n答：\n1. OpenJDK 是开源的，开源意味着你可以对它根据你自己的需要进行修改、优化，比如 Alibaba 基于 OpenJDK 开发了 Dragonwell8：https://github.com/alibaba/dragonwell8open in new window\n2. OpenJDK 是商业免费的（这也是为什么通过 yum 包管理器上默认安装的 JDK 是 OpenJDK 而不是 Oracle JDK）。虽然 Oracle JDK 也是商业免费（比如 JDK 8），但并不是所有版本都是免费的。\n3. OpenJDK 更新频率更快。Oracle JDK 一般是每 6 个月发布一个新版本，而 OpenJDK 一般是每 3 个月发布一个新版本。（现在你知道为啥 Oracle JDK 更稳定了吧，先在 OpenJDK 试试水，把大部分问题都解决掉了才在 Oracle JDK 上发布）\n基于以上这些原因，OpenJDK 还是有存在的必要的！\n# Java 和 C++ 的区别?\n我知道很多人没学过 C++，但是面试官就是没事喜欢拿咱们 Java 和 C++ 比呀！没办法！！！就算没学过 C++，也要记下来。\n虽然，Java 和 C++ 都是面向对象的语言，都支持封装、继承和多态，但是，它们还是有挺多不相同的地方：\n- Java 不提供指针来直接访问内存，程序内存更加安全\n- Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。\n- Java 有自动内存管理垃圾回收机制(GC)，不需要程序员手动释放无用内存。\n- C ++同时支持方法重载和操作符重载，但是 Java 只支持方法重载（操作符重载增加了复杂性，这与 Java 最初的设计思想不符）。\n- ......","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E4%BA%92%E6%96%A5%E9%94%81%E8%A7%A3%E5%86%B3%E5%8E%9F%E5%AD%90%E6%80%A7%E9%97%AE%E9%A2%98":{"title":"互斥锁：解决原子性问题","content":"一个或者多个操作在 CPU 执行的过程中不被中断的特性，称为“原子性”。\n\n# 如何解决原子性问题\n\n  \n\n原子性问题的源头是**线程切换**，如果能够禁用线程切换那不就能解决这个问题了吗？而操作系统做线程切换是依赖 CPU 中断的，所以禁止 CPU 发生中断就能够禁止线程切换。------早期单核可行，现在不可行\n\n  \n\n- 在单核 CPU 场景下，同一时刻只有一个线程执行，禁止 CPU 中断，意味着操作系统不会重新调度线程，也就是禁止了线程切换，获得 CPU 使用权的线程就可以不间断地执行，所以两次写操作一定是：要么都被执行，要么都没有被执行，具有原子性\n    \n- 在多核场景下，同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1 上，一个线程执行在 CPU-2 上，此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行\n    \n\n  \n\n**“同一时刻只有一个线程执行”这个条件非常重要，我们称之为互斥**。如果我们能够保证对共享变量的修改是互斥的，那么，无论是单核 CPU 还是多核 CPU，就都能保证原子性了 ,可以通过加锁实现互斥\n\n  \n\n# 简易锁模型\n\n![](statistic/asynccode-75.png)\n\n我们把一段需要互斥执行的代码称为临界区。线程在进入临界区之前，首先尝试加锁 lock()，如果成功，则进入临界区，此时我们称这个线程持有锁；否则呢就等待，直到持有锁的线程解锁；持有锁的线程执行完临界区的代码后，执行解锁 unlock()。\n\n存在的问题\n\n却很容易让我们忽视两个非常非常重要的点：我们锁的是什么？我们保护的又是什么？\n\n# 改进后的锁模型\n\n我们知道在现实世界里，锁和锁要保护的资源是有对应关系的，锁和资源也应该有这个关系，但这个关系在我们上面的模型中是没有体现的，所以我们需要完善一下我们的模型\n\n![](statistic/asynccode-74.png)\n\n- 首先，我们要把临界区要保护的资源标注出来，如图中临界区里增加了一个元素：受保护的资源 R；\n    \n- 其次，我们要保护资源 R 就得为它创建一把锁 LR；\n    \n- 最后，针对这把锁 LR，我们还需在进出临界区时添上加锁操作和解锁操作。另外，在锁 LR 和受保护资源之间，我特地用一条线做了关联，这个关联关系非常重要。很多并发 Bug 的出现都是因为把它忽略了，\n    \n- 然后就出现了类似锁自家门来保护他家资产的事情，这样的 Bug 非常不好诊断，因为潜意识里我们认为已经正确加锁了。\n    \n\n# synchronized\n\nsynchronized 关键字可以用来修饰方法，也可以用来修饰代码块\n\nJava 编译器会在 synchronized 修饰的方法或代码块前后自动加上加锁 lock() 和解锁 unlock()，这样做的好处就是加锁 lock() 和解锁 unlock() 一定是成对出现的\n\n那 synchronized 里的加锁 lock() 和解锁 unlock() 锁定的对象在哪里呢\n\n- 修饰代码块的时候，锁定指定的 对象\n    \n- 当修饰静态方法的时候，锁定的是当前类的 Class 对象，在上面的例子中就是 Class X；\n    \n- 当修饰非静态方法的时候，锁定的是当前实例对象 this。\n    \n\n```Go\n\nclass SafeCalc {\n  long value = 0L;\n  long get() {\n    return value;\n  }\n  synchronized void addOne() {\n    value += 1;\n  }\n}\n```\n\n保证了原子性。\n\n至于可见性。\n\n管程，就是我们这里的 synchronized，我们知道 synchronized 修饰的临界区是互斥的，也就是说同一时刻只有一个线程执行临界区的代码；而所谓“对一个锁解锁 Happens-Before 后续对这个锁的加锁”，指的是前一个线程的解锁操作对后一个线程的加锁操作可见，综合 Happens-Before 的传递性原则，我们就能得出前一个线程在临界区修改的共享变量（该操作在解锁之前），对后续进入临界区（该操作在加锁之后）的线程是可见的。\n\n# 锁和受保护资源的关系\n\n- 受保护资源和锁之间的关联关系是 N:1 的关系\n    \n\n们可以用多把锁来保护同一个资源，但在并发领域是不行的，并发领域的锁和现实世界的锁不是完全匹配的。不过倒是可以用同一把锁来保护多个资源，这个对应到现实世界就是我们所谓的“包场”了\n\n- 不能用多把锁来保护一个资源\n    \n\n  \n\n**关联关系**如果用更具体、更专业的语言来描述的话，其实是一种“原子性”特征，在前面的文章中，我们提到的原子性，主要是面向 CPU 指令的，转账操作的原子性则是属于是面向高级语言的，不过它们本质上是一样的\n\n**“原子性”的本质是什么？**其实不是不可分割，不可分割只是外在表现，其本质是多个资源间有一致性的要求，操作的中间状态对外不可见\n\n## 保护没有关联关系的多个资源\n\n银行业务中有针对账户余额（余额是一种资源）的取款操作，也有针对账户密码（密码也是一种资源）的更改操作，我们可以为账户余额和账户密码**分配不同的锁**来解决并发问题，这个还是很简单的\n\n```Go\n\nclass Account {\n  // 锁：保护账户余额\n  private final Object balLock\n    = new Object();\n  // 账户余额  \n  private Integer balance;\n  // 锁：保护账户密码\n  private final Object pwLock\n    = new Object();\n  // 账户密码\n  private String password;\n\n  // 取款\n  void withdraw(Integer amt) {\n    synchronized(balLock) {\n      if (this.balance \u003e amt){\n        this.balance -= amt;\n      }\n    }\n  } \n  // 查看余额\n  Integer getBalance() {\n    synchronized(balLock) {\n      return balance;\n    }\n  }\n\n  // 更改密码\n  void updatePassword(String pw){\n    synchronized(pwLock) {\n      this.password = pw;\n    }\n  } \n  // 查看密码\n  String getPassword() {\n    synchronized(pwLock) {\n      return password;\n    }\n  }\n}\n```\n\n我们也可以用**一把互斥锁**来保护多个资源，例如我们可以用 this 这一把锁来管理账户类里所有的资源\n\n- 使用多把锁\n    \n    - 用一把锁有个问题，就是性能太差，会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的\n        \n    - 用不同的锁对受保护资源进行精细化管理，能够提升性能。这种锁还有个名字，叫细粒度锁\n        \n\n## 保护有关联关系的多个资源\n\n  \n\n  \n\n银行业务里面的转账操作，账户 A 减少 100 元，账户 B 增加 100 元。这两个账户就是有关联关系的\n\n```Go\n\nclass Account {\n  private int balance;\n  // 转账\n  void transfer(\n      Account target, int amt){\n    if (this.balance \u003e amt) {\n      this.balance -= amt;\n      target.balance += amt;\n    }\n  } \n}\n```\n\n在这段代码中，临界区内有两个资源，分别是转出账户的余额 this.balance 和转入账户的余额 target.balance，并且用的是一把锁 this。-----有问题\n\n问题就出在 this 这把锁上，this 这把锁可以保护自己的余额 this.balance，却保护不了别人的余额 target.balance，就像你不能用自家的锁来保护别人家的资产。\n\n  \n\n  \n\n假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作：账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元，最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元。\n\n我们假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？我们期望是，但实际上并不是。因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是 300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖），可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200\n\n![](statistic/asynccode-76.png)\n\n  \n\n  \n\n## 正确使用锁\n\n- 要求锁能覆盖所有受保护资源\n    \n\n在上面的例子中，this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？\n\n  \n\n  \n\n- 可以让所有对象都持有一个唯一性的对象，这个对象在创建 Account 时传入。\n    \n\n它要求在创建 Account 对象的时候必须传入同一个对象，如果创建 Account 对象时，传入的 lock 不是同一个对象，那可就惨了，会出现锁自家门来保护他家资产的荒唐事\n\n```Go\n\nclass Account {\n  private Object lock；\n  private int balance;\n  private Account();\n  // 创建Account时传入同一个lock对象\n  public Account(Object lock) {\n    this.lock = lock;\n  } \n  // 转账\n  void transfer(Account target, int amt){\n    // 此处检查所有对象共享的锁\n    synchronized(lock) {\n      if (this.balance \u003e amt) {\n        this.balance -= amt;\n        target.balance += amt;\n      }\n    }\n  }\n}\n```\n\n- 就是用 Account.class 作为共享的锁。\n    \n\n```Go\n\nclass Account {\n  private int balance;\n  // 转账\n  void transfer(Account target, int amt){\n    synchronized(Account.class) {\n      if (this.balance \u003e amt) {\n        this.balance -= amt;\n        target.balance += amt;\n      }\n    }\n  } \n}\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%8F%AF%E8%A7%81%E6%80%A7%E5%8E%9F%E5%AD%90%E6%80%A7%E5%92%8C%E6%9C%89%E5%BA%8F%E6%80%A7%E9%97%AE%E9%A2%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8BBug%E7%9A%84%E6%BA%90%E5%A4%B4":{"title":"可见性、原子性和有序性问题：并发编程Bug的源头","content":"\n我们的 **CPU、内存、I/O 设备**都在不断迭代，不断朝着更快的方向努力。但是，在这个快速发展的过程中，有一个**核心矛盾一直存在，就是这三者的速度差异**\n\n为了**合理利用 CPU 的高性能**，平衡这三者的速度差异，**计算机体系结构、操作系统、编译程序**都做出了贡献，主要体现为：\n\n1. cpu增加缓存，以均衡与内存的速度差异\n    \n2. 操作系统增加了进程、线程，以分时复用CPU,进而均衡 CPU 与 I/O 设备的速度差异\n    \n3. 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。\n    \n\n这些优化导致了一些问题。\n\n# 缓存导致的可见性问题\n\n在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据一致性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的。\n\n![](statistic/asynccode-65.png)\n\n一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为可见性。\n\n多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了\n\n  \n\n  \n\n![](statistic/asynccode-66.png)\n\n# 线程切换带来的原子性问题\n\n我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性。\n\n  \n\nJava 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，\n\n  \n\n# 编译优化带来的有序性问题\n\n  \n\n序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80":{"title":"多线程基础","content":"\n\n# 并行、并发的区别\n\n从操作系统的角度来看，线程是CPU分配的最小单位\n\n- **并发**：两个及两个以上的作业在同一 **时间段** 内执行。\n    \n- **并行**：两个及两个以上的作业在同一 **时刻** 执行。\n    \n\n![](statistic/asynccode-55.png)\n\n# 同步和异步的区别\n\n- **同步** ： 发出一个调用之后，**在没有得到结果之前， 该调用就不可以返回**，一直等待。\n    \n- **异步** ：调用在发出之后，不用等待返回结果，该调用直接返回\n    \n\n  \n\n  \n\n  \n\n# 进程和线程\n\n## 何为进程?\n\n**进程是程序的一次执行过程，是系统运行程序的基本单位**，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。\n\n在 Java 中，当**我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程**。\n\n## 何为线程?\n\n线程与进程相似，但线程是一个比进程更小的执行单位。**一个进程在其执行的过程中可以产生多个线程**。与进程不同的是同类的多个线程共享**进程的堆和方法区资源**，但每个线程有自己的**程序计数器**、**虚拟机栈**和**本地方法栈**，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。\n\n**一个 Java 程序的运行是 main 线程和多个其他线程同时运行**。\n\n# 线程和进程的关系\n\n下图是 Java 内存区域，通过下图我们从 JVM 的角度来说一下线程和进程之间的关系。\n\n![](statistic/asynccode-62.png)\n\n- **一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈**。\n    \n- **总结：** **线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。**\n    \n\n  \n\n## 程序计数器为啥是私有的\n\n- 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。\n    \n- 在多线程的情况下，程序计数器**用于记录当前线程执行的位置**，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。\n    \n- 程序计数器私有主要是为了**线程切换后能恢复到正确的执行位置**\n    \n\n  \n\n## 虚拟机栈和本地方法栈为什么是私有的\n\n- **虚拟机栈：** 每个 Java 方法在执行的同时会创建**一个栈帧用于存储局部变量表、操作数栈、常量池引用**等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。\n    \n- **本地方法栈：** 和虚拟机栈所发挥的作用非常相似，区别是： **虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。** 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。\n    \n\n所以，为了**保证线程中的局部变量不被别的线程访问到**，虚拟机栈和本地方法栈是线程私有的\n\n## 堆和方法区 为啥共用\n\n堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (几乎所有对象都在这里分配内存)，**方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。**\n\n  \n\n# 为什么要使用多线程?\n\n先从总体上来说：\n\n- **从计算机底层来说：** 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。\n    \n- **从当代互联网发展趋势来说：** 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。\n    \n\n再深入到计算机底层来探讨：\n\n- **单核时代**： 在单核时代多线程主要是为了提高单进程利用 CPU 和 IO 系统的效率。 假设只运行了一个 Java 进程的情况，当我们请求 IO 的时候，如果 Java 进程中只有一个线程，此线程被 IO 阻塞则整个进程被阻塞。CPU 和 IO 设备只有一个在运行，那么可以简单地说系统整体效率只有 50%。当使用多线程的时候，一个线程被 IO 阻塞，其他线程还可以继续使用 CPU。从而提高了 Java 进程利用系统资源的整体效率。\n    \n- **多核时代**: 多核时代多线程主要是为了提高进程利用多核 CPU 的能力。举个例子：假如我们要计算一个复杂的任务，我们只用一个线程的话，不论系统有几个 CPU 核心，都只会有一个 CPU 核心被利用到。而创建多个线程，这些线程可以被映射到底层多个 CPU 上执行，在任务中的多个线程没有资源竞争的情况下，任务执行的效率会有显著性的提高，约等于（单核时执行时间/CPU 核心数）。\n    \n\n# 使用多线程可能带来什么问题?\n\n并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：**内存泄漏、死锁、线程不安全**等等。\n\n# 说说线程的生命周期和状态?\n\nJava 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态：\n\n- NEW: 初始状态，线程被创建出来但没有被调用 `start()` 。\n    \n- RUNNABLE: 运行状态，线程被调用了 `start()`等待运行的状态。\n    \n- BLOCKED ：阻塞状态，需要等待锁释放。\n    \n- WAITING：等待状态，表示该线程需要等待其他线程做出一些特定动作（通知或中断）。\n    \n- TIME_WAITING：超时等待状态，可以在指定的时间后自行返回而不是像 WAITING 那样一直等待。\n    \n- TERMINATED：终止状态，表示该线程已经运行完毕。\n    \n\n线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。\n\nJava 线程状态变迁图(图源：[挑错 |《Java 并发编程的艺术》中关于线程状态的三处错误open in new window](https://mp.weixin.qq.com/s/UOrXql_LhOD8dhTq_EPI0w))：\n\n![](statistic/asynccode-54.png)\n\n由上图可以看出：线程创建之后它将处于 **NEW（新建）** 状态，调用 `start()` 方法后开始运行，线程这时候处于 **READY（可运行）** 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 **RUNNING（运行）** 状态。\n\n\u003e 在操作系统层面，线程有 READY 和 RUNNING 状态；而在 JVM 层面，只能看到 RUNNABLE 状态（图源：[HowToDoInJavaopen in new window](https://howtodoinJava.com/)：[Java Thread Life Cycle and Thread Statesopen in new window](https://howtodoinJava.com/Java/multi-threading/Java-thread-life-cycle-and-thread-states/)），所以 Java 系统一般将这两个状态统称为 **RUNNABLE（运行中）** 状态 。\n\u003e \n\u003e **为什么 JVM 没有区分这两种状态呢？** （摘自：[Java 线程运行怎么有第六种状态？ - Dawell 的回答open in new window](https://www.zhihu.com/question/56494969/answer/154053599) ） 现在的时分（time-sharing）多任务（multi-task）操作系统架构通常都是用所谓的“时间分片（time quantum or time slice）”方式进行抢占式（preemptive）轮转调度（round-robin 式）。这个时间分片通常是很小的，一个线程一次最多只能在 CPU 上运行比如 10-20ms 的时间（此时处于 running 状态），也即大概只有 0.01 秒这一量级，**时间片用后就要被切换下来放入调度队列的末尾等待再次调度。（也即回到 ready 状态）。线程切换的如此之快，区分这两种状态就没什么意义了**。\n\n![](statistic/asynccode-58.png)\n\n- 当线程执行 `wait()`方法之后，线程进入 **WAITING（等待）** 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态。\n    \n- **TIMED_WAITING(超时等待)** 状态相当于在等待状态的基础上增加了超时限制，比如通过 `sleep（long millis）`方法或 `wait（long millis）`方法可以将线程置于 TIMED_WAITING 状态。当超时时间结束后，线程将会返回到 RUNNABLE 状态。\n    \n- 当线程进入 `synchronized` 方法/块或者调用 `wait` 后（被 `notify`）重新进入 `synchronized` 方法/块，但是锁被其它线程占有，这个时候线程就会进入 **BLOCKED（阻塞）** 状态。\n    \n- 线程在执行完了 `run()`方法之后将会进入到 **TERMINATED（终止）** 状态。\n    \n\n相关阅读：[线程的几种状态你真的了解么？open in new window](https://mp.weixin.qq.com/s/R5MrTsWvk9McFSQ7bS0W2w)\n\n  \n\n# 什么是上下文切换\n\n**线程在执行过程中会有自己的运行条件和状态（也称上下文）**，比如上文所说到过的程序计数器，虚拟机栈，本地方法栈等。当出现如下情况的时候，线程会从占用 CPU 状态中退出。\n\n1. **主动让出 CPU**，比如调用了 `sleep()`, `wait()` 等。\n    \n2. **时间片用完**，因为操作系统要防止一个线程或者进程长时间占用 CPU 导致其他线程或者进程饿死。\n    \n3. 调用了阻塞类型的系统中断，**比如请求 IO，线程被阻塞**。\n    \n4. 被终止或结束运行\n    \n\n上下文切换是现代操作系统的基本功能，**因其每次需要保存信息恢复信息，这将会占用 CPU，内存等系统资源进行处理**，也就意味着效率会有一定损耗，如果频繁切换就会造成整体效率低下\n\n  \n\n  \n\n# 什么是线程死锁?如何避免死锁\n\n线程死锁描述的是这样一种情况：**多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止**。\n\n![](statistic/asynccode-59.png)\n\n  \n\n## 产生死锁的四个必要条件：\n\n1. 互斥条件：该资源任意一个时刻只由一个线程占用。\n    \n2. 请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放。\n    \n3. 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。\n    \n4. 循环等待条件:若干线程之间形成一种头尾相接的循环等待资源关系。\n    \n\n  \n\n## 如何预防死锁？\n\n1. **破坏请求与保持条件** ：一次性申请所有的资源。\n    \n2. **破坏不剥夺条件** ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。\n    \n3. **破坏循环等待条件** ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。\n    \n\n  \n\n## 如何避免死锁\n\n避免死锁就是在资源分配时，借助于算法（比如银行家算法）对资源分配进行计算评估，使其进入安全状态。\n\n\u003e **安全状态** 指的是系统能够按照某种线程推进顺序（P1、P2、P3.....Pn）来为每个线程分配所需资源，直到满足每个线程对资源的最大需求，使每个线程都可顺利完成。称 `\u003cP1、P2、P3.....Pn\u003e` 序列为安全序列。\n\n  \n\n## 死锁问题如何排查\n\n  \n\n可以使用jdk自带的命令行工具排查：\n\n1. 使用jps查找运行的Java进程：jps -l\n    \n2. 使用jstack查看线程堆栈信息：jstack -l 进程id\n    \n\n基本就可以看到死锁的信息。\n\n还可以利用图形化工具，比如JConsole。出现线程死锁以后，点击JConsole线程面板 的检测到死锁 按钮，将会看到线程的死锁信息\n\n![](statistic/asynccode-61.png)\n\n# sleep() 方法和 wait() 方法对比\n\n共同点：两者都可以暂停线程的执行\n\n不同点：\n\n- **`sleep()`** **方法没有释放锁，而** **`wait()`** **方法释放了锁** 。\n    \n- `wait()` 通常被用于线程间交互/通信，`sleep()`通常被用于暂停执行。\n    \n- `wait()` 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 `notify()`或者 `notifyAll()` 方法。`sleep()`方法执行完成后，线程会自动苏醒，或者也可以使用 `wait(long timeout)` 超时后线程会自动苏醒。\n    \n- `sleep()` 是 `Thread` 类的静态本地方法，`wait()` 则是 `Object` 类的本地方法。\n    \n\n  \n\n# 为什么 wait() 方法不定义在 Thread 中\n\n`wait()` 是让获得对象锁的线程实现等待，会自动释放当前线程占有的对象锁。**每个对象（****`Object`****）都拥有对象锁**，既然要释放当前线程**占有的对象锁**并让其进入 WAITING 状态，自然是要操作对应的对象（`Object`）而非当前的线程（`Thread`）。\n\n## 为什么 `sleep()` 方法定义在 `Thread` 中？\n\n因为 `sleep()` 是让当前线程暂停执行，不涉及到对象类，也不需要获得对象锁。\n\n# 可以直接调用 Thread 类的 run 方法吗？\n\n调用 `start()` 方法方可启动线程并使线程进入就绪状态，直接执行 `run()` 方法的话不会以多线程的方式执行。\n\n  \n\n# 线程的创建方法\n\n![](statistic/asynccode-56.png)\n\n  \n\n- 继承Thread类\n    \n\n![](statistic/asynccode-64.png)\n\n- 实现Runnable 接口\n    \n\n![](statistic/asynccode-57.png)\n\n- 实现Callable 接口\n    \n\n![](statistic/asynccode-63.png)\n\n  \n\n# 线程有哪些常用的调度方法？\n\n![](statistic/asynccode-60.png)\n\n1. 等待与通知\n    \n\n- 等待\n    \n    - wait()：当一个线程A调用一个共享变量的 wait()方法时， 线程A会被阻塞挂起， 发生下面几种情况才会返回\n        \n        - 调用了共享对象 notify()或者 notifyAll()方法\n            \n        - 其他线程调用了线程A的 interrupt() 方法，线程A抛出InterruptedException异常返回。\n            \n    - wait(long timeout) ：这个方法相比 wait() 方法多了一个超时参数，它的不同之处在于，如果线程A调用共享对象的wait(long timeout)方法后，没有在指定的 timeout ms时间内被其它线程唤醒，那**么这个方法还是会因为超时而返回。**\n        \n    - wait(long timeout, int nanos)，其内部调用的是 wait(long timout）函数。\n        \n- 唤醒\n    \n    - notify() : 一个线程A调用共享对象的 notify() 方法后，**会唤醒一个在这个共享变量上调用wait系列方法后被挂起的线程**。 一个共享变量上可能会有多个线程在等待，**具体唤醒哪个等待的线程是随机的**。\n        \n    - notifyAll() ：不同于在共享变量上调用 notify() 函数会唤醒被阻塞到该共享变量上的一个线程，notifyAll()方法则会唤醒所有在该共享变量上由于调用 wait 系列方法而被挂起的线程。\n        \n- Thread类也提供了一个方法用于等待的方法：\n    \n    - join()：如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待 thread线程终止之后才从thread.join()返回\n        \n\n2. 线程休眠\n    \n\n- sleep(long millis) :Thread类中的静态方法，当一个执行中的线程A调用了Thread 的sleep方法后，线程A会暂时让出指定时间的执行权，但是线程A所拥有的监视器资源，比如**锁还是持有不让出的**。\n    \n\n3. 让出优先权\n    \n\n- yield() ：Thread类中的静态方法，当一个线程调用 yield 方法时，实际就是在暗示线程调度器**当前线程请求让出自己的CPU** ，但是线程调度器**可以无条件忽略这 个暗示**。\n    \n\n4. 线程中断\n    \n\n- 线程中断是一种线程间的协作模式，**通过设置线程的中断标志并不能直接终止该线程的执行，而是被中断的线程根据中断状态自行处理。**\n    \n    - void interrupt() ：中断线程，例如，当线程A运行时，**线程B可以调用钱程 interrupt() 方法来设置线程的中断标志为true 并立即返回**。设置标志仅仅是设置标志, 线程A实际并没有被中断， 会继续往下执行。\n        \n    - boolean isInterrupted() 方法： 检测当前线程是否被中断。\n        \n    - boolean interrupted() 方法： 检测当前线程是否被中断，与 isInterrupted 不同的 是，该**方法如果发现当前线程被中断，则会清除中断标志。**\n        \n\n  \n\n# 守护线程\n\nJava中的线程分为两类，分别为 daemon 线程（守护线程）和 user 线程（用户线程）\n\n那么守护线程和用户线程有什么区别呢？\n\n- 区别之一是当**最后一个非守护线程束时**， JVM会正常退出，而不管当前是否存在守护线程，\n    \n- 也就是说守护线程是否结束并不影响 JVM退出\n    \n- 只要有一个用户线程还没结束，正常情况下JVM就不会退出。\n    \n\n  \n\n# 线程间的通信方式\n\n![](statistic/asynccode-57.png)\n\n- volatile和synchronized关键字\n    \n    - volatile可以用来修饰字段（成员变量），就是**告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存**，它能保证所有线程对变量访问的可见性\n        \n    - synchronized可以修饰方法或者以同步块的形式来进行使用，**它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中**，它保证了线程对变量访问的可见性和排他性\n        \n- 等待通知机制\n    \n    - 等待/通知机制（wait()/notify()）实现一个线程修改一个对象的值，而另一个线程感知到了变化，然后进行相应的操作\n        \n- 管道输入/输出流\n    \n    - 管道输入/输出流和普通的文件输入/输出流或者网络输入/输出流不同之处在于，**它主要用于线程之间的数据传输，而传输的媒介为内存**\n        \n- Thread.join()\n    \n    - 如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止 之后才从thread.join()返回。\n        \n- threadLocal\n    \n    - ThreadLocal，即线程变量，是一个以**ThreadLocal对象为键、任意对象为值的存储结构**。这个结构被附带在线程上，也就是说**一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值**。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%AE%89%E5%85%A8%E6%80%A7%E6%B4%BB%E8%B7%83%E6%80%A7%E4%BB%A5%E5%8F%8A%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98":{"title":"安全性、活跃性以及性能问题","content":"存**在共享数据并且该数据会发生变化，通俗地讲就是有多个线程会同时读写同一数据**,需要分析安全性、活跃性以及性能问题。\n\n那如果能够做到不共享数据或者数据状态不发生变化，不就能够保证线程的安全性了嘛。有不少技术方案都是基于这个理论的，例如线程本地存储（Thread Local Storage，TLS）、不变模式等等，后面我会详细介绍相关的技术方案是如何在 Java 语言中实现的。\n\n# 安全性问题\n\n当多个线程同时访问同一数据，并且至少有一个线程会写这个数据的时候，如果我们不采取防护措施，那么就会导致并发 Bug，对此还有一个专业的术语，叫做**数据竞争**（Data Race）\n\n程序的执行结果依赖线程执行的顺序，叫做**竟态条件（**Race Condition**）**\n\n那面对数据竞争和竞态条件问题，又该如何保证线程的安全性呢？其实这两类问题，都**可以用互斥这个技术方案，而实现互斥的方案有很多，CPU 提供了相关的互斥指令，操作系统、编程语言也会提供相关的 API。从逻辑上来看，我们可以统一归为：锁**。\n\n# 活跃性问题\n\n指的是某个操作无法执行下去。我们常见的**“死锁”**就是一种典型的活跃性问题，当然除了死锁外，还有两种情况，**分别是“活锁”和“饥饿**\n\n- 活跃性问题\n    \n    - 死锁\n        \n        - 发生“死锁”后线程会互相等待，而且会一直等待下去，在技术上的表现形式是线程永久地“阻塞”\n            \n    - 活锁\n        \n        - 有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况，这就是所谓的“活锁\n            \n    - 饥饿\n        \n        - 指的是线程因无法访问所需资源而无法执行下去的情况。\n            \n\n# 性能问题\n\n有个阿姆达尔（Amdahl）定律，代表了处理器并行运算之后效率提升的能力，它正好可以解决这个问题，具体公式如下：\n\n![](statistic/asynccode-87.png)\n\n公式里的 n 可以理解为 CPU 的核数，p 可以理解为并行百分比，那（1-p）就是串行百分比了，也就是我们假设的 5%。我们再假设 CPU 的核数（也就是 n）无穷大，那加速比 S 的极限就是 20。也就是说，如果我们的串行率是 5%，那么我们无论采用什么技术，最高也就只能提高 20 倍的性能。\n\n  \n\n如何提高性能：\n\n1. 既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。在这方面有很多相关的技术，例如线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等；Java 并发包里面的原子类也是一种无锁的数据结构；Disruptor 则是一个无锁的内存队列，\n    \n2. 既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。在这方面有很多相关的技术，例如线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等；Java 并发包里面的原子类也是一种无锁的数据结构；Disruptor 则是一个无锁的内存队列，","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/%E4%BF%A1%E5%8F%B7%E9%87%8FSemaphore":{"title":"信号量（Semaphore）","content":"# 信号量模型\n\n号量模型还是很简单的，可以简单概括为：**一个计数器，一个等待队列，三个方法，**\n\n信号量模型里，计数器和等待队列对外是透明的，所以只能通过信号量模型提供的三个方法来访问它们，这三个方法分别是：init()、down() 和 up()。你可以结合下图来形象化地理解。\n\n![](statistic/asynccode-143.png)\n\n- init()：设置计数器的初始值。\n    \n- down()：计数器的值减 1；如果此时计数器的值**小于 0**，则当前线程将被阻塞，否则当前线程可以继续执行。\n    \n- up()：计数器的值加 1；如果此时计数器的值**小于或者等于 0**，则唤醒等待队列中的一个线程，并将其从等待队列中移除。\n    \n\n  \n\n  \n\n对应的代码解释\n\n```Go\n\nclass Semaphore{\n  // 计数器\n  int count;\n  // 等待队列\n  Queue queue;\n  // 初始化操作\n  Semaphore(int c){\n    this.count=c;\n  }\n  // \n  void down(){\n    this.count--;\n    if(this.count\u003c0){\n      //将当前线程插入等待队列\n      //阻塞当前线程\n    }\n  }\n  void up(){\n    this.count++;\n    if(this.count\u003c=0) {\n      //移除等待队列中的某个线程T\n      //唤醒线程T\n    }\n  }\n}\n```\n\n**init()、down() 和 up() 三个方法都是原子性的**，并且这个原子性是由**信号量模型的实现方**保证的\n\n  \n\n  \n\nJava SDK 里面，信号量模型是由 java.util.concurrent.Semaphore 实现的，Semaphore 这个类能够保证这三个方法都是原子操作\n\n# 如何使用信号量\n\n  \n\n信号量的使用也是类似的。这里我们还是用累加器的例子来说明信号量的使用吧。在累加器的例子里面，count+=1 操作是个临界区，只允许一个线程执行，也就是说要保证互斥。\n\n只需要在**进入临界区之前执行一下 down() 操作，退出临界区之前执行一下 up() 操作就可以了**。下面是 Java 代码的示例，**acquire() 就是信号量里的 down() 操作，release() 就是信号量里的 up() 操作。**\n\n```Go\n\nstatic int count;\n//初始化信号量\nstatic final Semaphore s \n    = new Semaphore(1);\n//用信号量保证互斥    \nstatic void addOne() {\n  s.acquire();\n  try {\n    count+=1;\n  } finally {\n    s.release();\n  }\n}\n```\n\n下面我们再来分析一下，信号量是如何保证互斥的。假设两个线程 T1 和 T2 同时访问 addOne() 方法，当它们同时调用 acquire() 的时候，由于 acquire() 是一个原子操作，所以只能有一个线程（假设 T1）把信号量里的计数器减为 0，另外一个线程（T2）则是将计数器减为 -1。对于线程 T1，信号量里面的计数器的值是 0，大于等于 0，所以线程 T1 会继续执行；对于线程 T2，信号量里面的计数器的值是 -1，小于 0，按照信号量模型里对 down() 操作的描述，线程 T2 将被阻塞。所以此时只有线程 T1 会进入临界区执行count+=1；。\n\n当线程 T1 执行 release() 操作，也就是 up() 操作的时候，信号量里计数器的值是 -1，加 1 之后的值是 0，小于等于 0，按照信号量模型里对 up() 操作的描述，此时等待队列中的 T2 将会被唤醒。于是 T2 在 T1 执行完临界区代码之后才获得了进入临界区执行的机会，从而保证了互斥性。\n\n  \n\n  \n\n# 实现一个限流器\n\n**Semaphore 可以允许多个线程访问一个临界区。**比较常见的需求就是我们工作中遇到的各种池化资源，例如连接池、对象池、线程池等等\n\n  \n\n所谓对象池呢，指的是一次性创建出 N 个对象，之后所有的线程重复利用这 N 个对象，当然对象在被释放前，也是不允许其他线程使用的。\n\n```Go\n\nclass ObjPool\u003cT, R\u003e {\n  final List\u003cT\u003e pool;\n  // 用信号量实现限流器\n  final Semaphore sem;\n  // 构造函数\n  ObjPool(int size, T t){\n    pool = new Vector\u003cT\u003e(){};\n    for(int i=0; i\u003csize; i++){\n      pool.add(t);\n    }\n    sem = new Semaphore(size);\n  }\n  // 利用对象池的对象，调用func\n  R exec(Function\u003cT,R\u003e func) {\n    T t = null;\n    sem.acquire();\n    try {\n      t = pool.remove(0);\n      return func.apply(t);\n    } finally {\n      pool.add(t);\n      sem.release();\n    }\n  }\n}\n// 创建对象池\nObjPool\u003cLong, String\u003e pool = \n  new ObjPool\u003cLong, String\u003e(10, 2);\n// 通过对象池获取t，之后执行  \npool.exec(t -\u003e {\n    System.out.println(t);\n    return t.toString();\n});\n```\n\n关键的代码是 ObjPool 里面的 exec() 方法，这个方法里面实现了限流的功能。在这个方法里面，我们首先调用 acquire() 方法（与之匹配的是在 finally 里面调用 release() 方法），假设对象池的大小是 10，信号量的计数器初始化为 10，那么前 10 个线程调用 acquire() 方法，都能继续执行，相当于通过了信号灯，而其他线程则会阻塞在 acquire() 方法上。对于通过信号灯的线程，我们为每个线程分配了一个对象 t（这个分配工作是通过 pool.remove(0) 实现的），分配完之后会执行一个回调函数 func，而函数的参数正是前面分配的对象 t ；执行完回调函数之后，它们就会释放对象（这个释放工作是通过 pool.add(t) 实现的），同时调用 release() 方法来更新信号量的计数器。如果此时信号量里计数器的值小于等于 0，那么说明有线程在等待，此时会自动唤醒等待的线程。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/%E5%8D%B0%E7%AB%A0%E9%94%81StampedLock%E6%AF%94%E8%AF%BB%E5%86%99%E9%94%81%E6%9B%B4%E5%BF%AB%E7%9A%84%E9%94%81":{"title":"印章锁（StampedLock）,比读写锁更快的锁","content":"# StampedLock 支持的三种锁模式\n\nReadWriteLock 支持两种模式：一种是读锁，一种是写锁。\n\nStampedLock 支持三种模式，分别是：写锁、悲观读锁和乐观读。\n\n- 其中，写锁、悲观读锁的语义和 ReadWriteLock 的写锁、读锁的语义非常类似，允许多个线程同时获取悲观读锁，但是只允许一个线程获取写锁，写锁和悲观读锁是互斥的。\n    \n- StampedLock 里的写锁和悲观读锁加锁成功之后，都会返回一个 stamp；然后解锁的时候，需要传入这个 stamp\n    \n\n```Go\n\nfinal StampedLock sl = \n  new StampedLock();\n  \n// 获取/释放悲观读锁示意代码\nlong stamp = sl.readLock();\ntry {\n  //省略业务相关代码\n} finally {\n  sl.unlockRead(stamp);\n}\n\n// 获取/释放写锁示意代码\nlong stamp = sl.writeLock();\ntry {\n  //省略业务相关代码\n} finally {\n  sl.unlockWrite(stamp);\n}\n```\n\n- ReadWriteLock 支持多个线程同时读，但是当多个线程同时读的时候，所有的写操作会被阻塞；而 StampedLock 提供的乐观读，是允许一个线程获取写锁的，也就是说不是所有的写操作都被阻塞\n    \n\n乐观读这个操作是无锁的，所以相比较 ReadWriteLock 的读锁，乐观读的性能更好一些 在 distanceFromOrigin() 这个方法中，首先通过调用 tryOptimisticRead() 获取了一个 stamp，这里的 tryOptimisticRead() 就是我们前面提到的乐观读。之后将共享变量 x 和 y 读入方法的局部变量中，不过需要注意的是，由于 tryOptimisticRead() 是无锁的，所以共享变量 x 和 y 读入方法局部变量时，x 和 y 有可能被其他线程修改了。因此最后读完之后，还需要再次验证一下是否存在写操作，这个验证操作是通过调用 validate(stamp) 来实现的\n\n```Go\n\nclass Point {\n  private int x, y;\n  final StampedLock sl = \n    new StampedLock();\n  //计算到原点的距离  \n  int distanceFromOrigin() {\n    // 乐观读\n    long stamp = \n      sl.tryOptimisticRead();\n    // 读入局部变量，\n    // 读的过程数据可能被修改\n    int curX = x, curY = y;\n    //判断执行读操作期间，\n    //是否存在写操作，如果存在，\n    //则sl.validate返回false\n    if (!sl.validate(stamp)){\n      // 升级为悲观读锁\n      stamp = sl.readLock();\n      try {\n        curX = x;\n        curY = y;\n      } finally {\n        //释放悲观读锁\n        sl.unlockRead(stamp);\n      }\n    }\n    return Math.sqrt(\n      curX * curX + curY * curY);\n  }\n}\n```\n\n**如果执行乐观读操作的期间，存在写操作，会把乐观读升级为悲观读锁**。这个做法挺合理的，否则你就需要在一个循环里反复执行乐观读，直到执行乐观读操作的期间没有写操作（只有这样才能保证 x 和 y 的正确性和一致性），而循环读会浪费大量的 CPU。升级为悲观读锁，代码简练且不易出错\n\n  \n\n  \n\n# 理解乐观锁\n\n读的时候，会验证版本，数据库里的乐观锁，查询的时候需要把 version 字段查出来，更新的时候要利用 version 字段做验证。这个 version 字段就类似于 StampedLock 里面的 stamp\n\n  \n\n  \n\n# StampedLock 使用注意事项\n\n  \n\nStampedLock 的功能仅仅是 ReadWriteLock 的子集\n\n  \n\n- StampedLock 在命名上并没有增加 Reentrant，StampedLock 不支持重入\n    \n- StampedLock 的悲观读锁、写锁都不支持条件变量\n    \n- 如果线程阻塞在 StampedLock 的 readLock() 或者 writeLock() 上时，此时调用该阻塞线程的 interrupt() 方法，会导致 CPU 飙升\n    \n    - 使用 StampedLock 一定不要调用中断操作，如果需要支持中断功能，一定使用可中断的悲观读锁 readLockInterruptibly() 和写锁 writeLockInterruptibly()\n        \n- StampedLock 支持锁的降级（通过 tryConvertToReadLock() 方法实现）和升级（通过 tryConvertToWriteLock() 方法实现），但是建议你要慎重使用\n    \n\n```Go\n\nprivate double x, y;\nfinal StampedLock sl = new StampedLock();\n// 存在问题的方法\nvoid moveIfAtOrigin(double newX, double newY){\n long stamp = sl.readLock();\n try {\n  while(x == 0.0 \u0026\u0026 y == 0.0){\n    long ws = sl.tryConvertToWriteLock(stamp);\n    if (ws != 0L) {\n      x = newX;\n      y = newY;\n      break;\n    } else {\n      sl.unlockRead(stamp);\n      stamp = sl.writeLock();\n    }\n  }\n } finally {\n  sl.unlock(stamp);\n}\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/%E5%8E%9F%E5%AD%90%E7%B1%BB%E6%97%A0%E9%94%81%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E5%85%B8%E8%8C%83":{"title":"原子类：无锁工具类的典范","content":"```Go\n\npublic class Test {\n  AtomicLong count = \n    new AtomicLong(0);\n  void add10K() {\n    int idx = 0;\n    while(idx++ \u003c 10000) {\n      count.getAndIncrement();\n    }\n  }\n}\n```\n\n- 互斥锁方案为了保证互斥性，需要执行加锁、解锁操作，而加锁、解锁操作本身就消耗性能；同时拿不到锁的线程还会进入阻塞状态，进而触发线程切换，线程切换对性能的消耗也很大。\n    \n- 无锁方案则完全没有加锁、解锁的性能消耗，同时还能保证互斥性，既解决了问题，又没有带来新的问题，可谓绝佳方案\n    \n\n# 无锁方案的实现原理\n\n  \n\n## CAS\n\n其实原子类性能高的秘密很简单，**硬件支持**而已 。CPU 为了解决并发问题，提供了 **CAS 指令**（CAS，全称是 **Compare And Swap，即“比较并交换”**）\n\n  \n\nCAS的3个参数：\n\n- 共享变量的内存地址 A\n    \n- 用于比较的值 B\n    \n- 共享变量的新值 C\n    \n\n  \n\n**并且只有当内存中地址 A 处的值等于 B 时，才能将内存中地址 A 处的值更新为新值 C。**作为一条 CPU 指令，CAS 指令本身是**能够保证原子性**的。\n\n  \n\n  \n\nCAS 指令的模拟代码来理解 CAS 的工作原理、\n\n```Go\n\nclass SimulatedCAS{\n  int count；\n  synchronized int cas(\n    int expect, int newValue){\n    // 读目前count的值\n    int curValue = count;\n    // 比较目前count值是否==期望值\n    if(curValue == expect){\n      // 如果是，则更新count的值\n      count = newValue;\n    }\n    // 返回写入前的值\n    return curValue;\n  }\n}\n```\n\n只有当目前 count 的值和期望值 expect 相等时，才会将 count 更新为 newValue.\n\n## **自旋**\n\n**使用 CAS 来解决并发问题，一般都会伴随着自旋，而所谓自旋，其实就是循环尝试,**\n\n**如果操作的值发生了改变，需要重新加载计算**\n\n```Go\n\nclass SimulatedCAS{\n  volatile int count;\n  // 实现count+=1\n  addOne(){\n    do {\n      newValue = count+1; //①\n    }while(count !=\n      cas(count,newValue) //②\n  }\n  // 模拟实现CAS，仅用来帮助理解\n  synchronized int cas(\n    int expect, int newValue){\n    // 读目前count的值\n    int curValue = count;\n    // 比较目前count值是否==期望值\n    if(curValue == expect){\n      // 如果是，则更新count的值\n      count= newValue;\n    }\n    // 返回写入前的值\n    return curValue;\n  }\n}\n```\n\n首先计算 newValue = count+1，如果 cas(count,newValue) 返回的值不等于 count，则意味着线程在执行完代码①处之后，执行代码②处之前，count 的值被其他线程更新过。那此时该怎么处理呢？可以采用自旋方案，就像下面代码中展示的，可以**重新读 count 最新的值来计算 newValue 并尝试再次更新**，直到成功\n\n## ABA问题\n\n如果 cas(count,newValue) 返回的值不等于count，意味着线程在执行完代码①处之后，执行代码②处之前，count 的值被其他线程更新过”，**那如果 cas(count,newValue) 返回的值等于count**，是否就能够认为 count 的值没有被其他线程更新过呢？显然不是的\n\n  \n\n假设 count 原本是 A，线程 T1 在执行完代码①处之后，执行代码②处之前，有可能 count 被线程 T2 更新成了 B，之后又被 T3 更新回了 A，这样线程 T1 虽然看到的一直是 A，但是其实已经被其他线程更新过了，**这就是 ABA 问题**\n\n  \n\n  \n\n**可能大多数情况下我们并不关心 ABA 问题**\n\n  \n\n# 看 Java 如何实现原子化的 count += 1\n\n  \n\n我们使用原子类 AtomicLong 的 getAndIncrement() 方法替代了count += 1，从而实现了线程安全。\n\n  \n\n在 Java 1.8 版本中，getAndIncrement() 方法会转调 unsafe.getAndAddLong() 方法。**这里 this 和 valueOffset 两个参数可以唯一确定共享变量的内存地址**\n\n  \n\n```Go\n\nfinal long getAndIncrement() {\n  return unsafe.getAndAddLong(\n    this, valueOffset, 1L);\n}\n```\n\nunsafe.getAndAddLong() 方法的源码\n\n```Go\n\npublic final long getAndAddLong(\n  Object o, long offset, long delta){\n  long v;\n  do {\n    // 读取内存中的值\n    v = getLongVolatile(o, offset);\n  } while (!compareAndSwapLong(\n      o, offset, v, v + delta));\n  return v;\n}\n//原子性地将变量更新为x\n//条件是内存中的值等于expected\n//更新成功则返回true\nnative boolean compareAndSwapLong(\n  Object o, long offset, \n  long expected,\n  long x);\n```\n\n该方法首先会在内存中读取共享变量的值，之后循环调用 compareAndSwapLong() 方法来尝试设置共享变量的值，直到成功为止。compareAndSwapLong() 是一个 native 方法，只有当内存中共享变量的值等于 expected 时，才会将共享变量的值更新为 x，并且返回 true；否则返回 fasle。compareAndSwapLong 的语义和 CAS 指令的语义的差别仅仅是返回值不同而已\n\n# 原子类概览\n\n  \n\n我们可以将它们分为五个类别：**原子化的基本数据类型、原子化的对象引用类型、原子化数组、原子化对象属性更新器和原子化的累加器**\n\n- 原子类\n    \n    - 基本数据类型\n        \n        - AtomicBoolean\n            \n        - AtomicInteger\n            \n        - AtomicLong\n            \n    - 引用类型\n        \n        - AtomicReference\n            \n        - AtomicStampedReference\n            \n        - AtomicMarkableReference\n            \n    - 数组\n        \n        - AtomicIntegerArray\n            \n        - AtomicLongArray\n            \n        - AromicReferenceArray\n            \n    - 对象属性更新器\n        \n        - AtomicIntegerFeildUpdater\n            \n        - AtomicLongFeildUpdater\n            \n        - AtomicReferenceFeildUpdater\n            \n    - 累加器\n        \n        - DoubleAccumlator\n            \n        - DoubleAdder\n            \n        - LongAccumulator\n            \n        - LongAdder","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8":{"title":"并发容器","content":"# 同步容器及其注意事项\n\n  \n\n**都是基于 synchronized 这个同步关键字实现的，所以也被称为同步容器**。\n\n  \n\nJava 中的容器主要可以分为四个大类，分别是 List、Map、Set 和 Queue，但并不是所有的 Java 容器都是线程安全的。\n\n  \n\n如何将非线程安全的容器变成线程安全的容器？\n\n**只要把非线程安全的容器封装在对象内部，然后控制好访问路径就可以了**\n\n  \n\n以 ArrayList 为例，看看如何将它变成线程安全的在下面的代码中，SafeArrayList 内部持有一个 ArrayList 的实例 c，所有访问 c 的方法我们都增加了 synchronized 关键字，需要注意的是我们还增加了一个 addIfNotExist() 方法，这个方法也是用 synchronized 来保证原子性的。\n\n```Go\n\nSafeArrayList\u003cT\u003e{\n  //封装ArrayList\n  List\u003cT\u003e c = new ArrayList\u003c\u003e();\n  //控制访问路径\n  synchronized\n  T get(int idx){\n    return c.get(idx);\n  }\n\n  synchronized\n  void add(int idx, T t) {\n    c.add(idx, t);\n  }\n\n  synchronized\n  boolean addIfNotExist(T t){\n    if(!c.contains(t)) {\n      c.add(t);\n      return true;\n    }\n    return false;\n  }\n}\n```\n\nJava SDK 的开发人员也想到了，所以他们在 Collections 这个类中还提供了一套完备的包装类，比如下面的示例代码中，分别把 ArrayList、HashSet 和 HashMap 包装成了线程安全的 List、Set 和 Map。\n\n```Go\n\nList list = Collections.synchronizedList(new ArrayList());\nSet set = Collections.synchronizedSet(new HashSet());\nMap map = Collections.synchronizedMap(new HashMap());\n```\n\n**组合操作需要注意竞态条件问题**，例如上面提到的 addIfNotExist() 方法就包含组合操作。**组合操作往往隐藏着竞态条件问题，即便每个操作都能保证原子性，也并不能保证组合操作的原子性**，这个一定要注意。\n\n  \n\n  \n\n在容器领域一**个容易被忽视的“坑”是用迭代器遍历容器**，例如在下面的代码中，通过迭代器遍历容器 list，对每个元素调用 foo() 方法，这就存在并发问题，这些组合的操作不具备原子性。\n\n```Go\n\nList list = Collections.synchronizedList(new ArrayList());\nIterator i = list.iterator(); \nwhile (i.hasNext())\n  foo(i.next());\n```\n\n而正确做法是下面这样，锁住 list 之后再执行遍历操作\n\n```Go\n\nList list = Collections.synchronizedList(new ArrayList());\nsynchronized (list) {  \n  Iterator i = list.iterator(); \n  while (i.hasNext())\n    foo(i.next());\n}    \n```\n\n**都是基于 synchronized 这个同步关键字实现的，所以也被称为同步容器**。\n\nJava 提供的同步容器还有 **Vector、Stack 和 Hashtable**，这三个容器不是基于包装类实现的，但同样是基于 synchronized 实现的，对这三个容器的遍历，同样要加锁保证互斥。\n\n  \n\n  \n\n# 并发容器及其注意事项\n\nJava 在 1.5 版本之前所谓的线程安全的容器，主要指的就是同步容器,\n\n不过同步容器有个最大的问题，那就是性能差，所有方法都用 synchronized 来保证互斥，串行度太高了.\n\n因此 Java 在 1.5 及之后版本提供了性能更高的容器，我们一般称为并发容器。\n\n**并发容器虽然数量非常多，但依然是前面我们提到的四大类：List、Map、Set 和 Queue，下面的并发容器关系图，基本上把我们经常用的容器都覆盖到了**\n\n- 并发容器\n    \n    - List\n        \n        - CopyOnWriteArrayList\n            \n    - Map\n        \n        - ConcurrentHashMap\n            \n        - ConcurrentSkipListMap\n            \n    - Set\n        \n        - ConcurrentSkipListSet\n            \n        - CopyOnWriteArraySet\n            \n    - Queue\n        \n        - BlockingDeque\n            \n            - ListedBlockingDeque\n                \n        - BlockingQueue\n            \n            - ArrayBlockingQueue\n                \n            - LinkedBlockingQueue\n                \n            - SynchronousQueuq\n                \n            - LinkedTransferQueue\n                \n            - ProiorityBlockingQueue\n                \n            - DelayQueue\n                \n        - ConcurrentListedQueue\n            \n        - ConcurrentListedDeque\n            \n\n## List\n\n  \n\n**只有一个实现类就是 CopyOnWriteArrayList**\n\n  \n\nCopyOnWrite，顾名思义就是写的时候会将共享变量新复制一份出来，这样做的好处是读操作完全无锁。\n\nCopyOnWriteArrayList 内部维护了一个数组，成员变量 array 就指向这个内部数组，所有的读操作都是基于 array 进行的，如下图所示，迭代器 Iterator 遍历的就是 array 数组。\n\n  \n\n![](statistic/asynccode-152.png)\n\n还有一个写操作，例如增加元素，CopyOnWriteArrayList 是如何处理的呢？CopyOnWriteArrayList 会将 array 复制一份，然后在新复制处理的数组上执行增加元素的操作，执行完之后再将 array 指向这个新的数组。\n\n**读写是可以并行的，遍历操作一直都是基于原 array 执行，而写操作则是基于新 array 进行**\n\n![](statistic/asynccode-151.png)\n\n  \n\nCopyOnWriteArrayList 需要注意的“坑”:\n\n- CopyOnWriteArrayList 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致。\n    \n- CopyOnWriteArrayList 迭代器是只读的，不支持增删改。因为迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的。\n    \n\n  \n\n## Map\n\nMap 接口的两个实现是 ConcurrentHashMap 和 ConcurrentSkipListMap,\n\n主要区别：\n\n- ConcurrentHashMap 的 key 是无序的\n    \n- ConcurrentSkipListMap 的 key 是有, 跳表插入、删除、查询操作平均的时间复杂度是 O(log n)，理论上和并发线程数没有关系。\n    \n\n  \n\n使用 ConcurrentHashMap 和 ConcurrentSkipListMap 需要注意的地方是，它们的 key 和 value 都不能为空，否则会抛出NullPointerException这个运行时异常\n\n  \n\n  \n\n对Map 相关的实现类对于 key 和 value 的要求\n\n![](statistic/asynccode-153.png)\n\n## Set\n\nCopyOnWriteArraySet 和 ConcurrentSkipListSet，使用场景可以参考前面讲述的 CopyOnWriteArrayList 和 ConcurrentSkipListMap\n\n  \n\n## Queue\n\nJava 并发包里面 Queue 这类并发容器是最复杂的，你可以从以下两个维度来分类\n\n- **阻塞与非阻塞**，所谓阻塞指的是当队列已满时，入队操作阻塞；当队列已空时，出队操作阻塞，**阻塞队列都用 Blocking 关键字标识**\n    \n- 单端与双端，单端指的是只能队尾入队，队首出队；而双端指的是队首队尾皆可入队出队，单端队列使用 Queue 标识，双端队列使用 Deque 标识。\n    \n\n  \n\n然后组合\n\n- Queue\n    \n    - 单端阻塞队列\n        \n        - BlockingQueue\n            \n            - ArrayBlockingQueue\n                \n                - 支持有界\n                    \n            - LinkedBlockingQueue\n                \n                - 支持有界\n                    \n            - SynchronousQueuq\n                \n            - LinkedTransferQueue\n                \n            - ProiorityBlockingQueue\n                \n            - DelayQueue\n                \n    - 双端阻塞队列\n        \n        - ListedBlockingDeque\n            \n    - 单端非阻塞队列\n        \n        - ConcurrentLinkedQueue\n            \n    - 双端非阻塞队列\n        \n        - ConcurrentLinkedDeque\n            \n\n### 单端阻塞队列\n\nArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、LinkedTransferQueue、PriorityBlockingQueue 和 DelayQueue\n\n  \n\n- 这个队列可以是数组（其实现是 ArrayBlockingQueue）也可以是链表（其实现是 LinkedBlockingQueue）；\n    \n- 甚至还可以不持有队列（其实现是 SynchronousQueue），此时生产者线程的入队操作必须等待消费者线程的出队操作。\n    \n- 而 LinkedTransferQueue 融合 LinkedBlockingQueue 和 SynchronousQueue 的功能，性能比 LinkedBlockingQueue 更好；\n    \n- PriorityBlockingQueue 支持按照优先级出队；\n    \n- DelayQueue 支持延时出队\n    \n\n  \n\n### 双端阻塞队列\n\nLinkedBlockingDeque\n\n  \n\n### 单端非阻塞队列\n\nConcurrentLinkedQueue\n\n### 双端非阻塞队列\n\nConcurrentLinkedDeque\n\n  \n\n  \n\n  \n\n需要格外注意队列是否**支持有界（所谓有界指的是内部的队列是否有容量限制**）。实际工作中，一般都不建议使用无界的队列，因为数据量大了之后很容易导致 OOM。\n\n上面我们提到的这些 Queue 中，**只有 ArrayBlockingQueue 和 LinkedBlockingQueue 是支持有界的**","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/%E8%AF%BB%E5%86%99%E9%94%81ReadWriteLock":{"title":"读写锁（ReadWriteLock）","content":"一种非常普遍的并发场景：读多写少场景。实际工作中，为了优化性能，我们经常会使用缓存，例如缓存元数据、缓存基础数据等，这就是一种典型的读多写少应用场景。缓存之所以能提升性能，一个重要的条件就是缓存的数据一定是读多写少的，例如元数据和基础数据基本上不会发生变化（写少），但是使用它们的地方却很多（读多）。针对读多写少这种并发场景，Java SDK 并发包提供了读写锁——ReadWriteLock，非常容易使用，并且性能很好。 \n\n# **什么是读写锁** \n\n而是一个广为使用的通用技术，所有的读写锁都遵守以下三条基本原则： 1允许多个线程同时读共享变量； 2只允许一个线程写共享变量； 3如果一个写线程正在执行写操作，此时禁止读线程读共享变量。读写锁与互斥锁的一个重要区别就是**读写锁允许多个线程同时读共享变量，而互斥锁是不允许的**，这是读写锁在读多写少场景下性能优于互斥锁的关键。但读写锁的写操作是互斥的 \n\n\n\n# **实现一个缓存** \n\n在下面的代码中，我们声明了一个 Cache 类，其中类型参数 K 代表缓存里 key 的类型，V 代表缓存里 value 的类型，缓存的数据保存在 Cache 类内部的 HashMap 里面，HashMap 不是线程安全的，这里我们使用读写锁 ReadWriteLock 来保证其线程安全。ReadWriteLock 是一个接口，它的实现类是 ReentrantReadWriteLock，通过名字你应该就能判断出来，它是支持可重入的。下面我们通过 rwl 创建了一把读锁和一把写锁\n\n```Go\n\nclass Cache\u003cK,V\u003e {\n  final Map\u003cK, V\u003e m =\n    new HashMap\u003c\u003e();\n  final ReadWriteLock rwl =\n    new ReentrantReadWriteLock();\n  // 读锁\n  final Lock r = rwl.readLock();\n  // 写锁\n  final Lock w = rwl.writeLock();\n  // 读缓存\n  V get(K key) {\n    r.lock();\n    try { return m.get(key); }\n    finally { r.unlock(); }\n  }\n  // 写缓存\n  V put(K key, V value) {\n    w.lock();\n    try { return m.put(key, v); }\n    finally { w.unlock(); }\n  }\n}\n```\n\n# 实现缓存的按需加载\n\n如果源头数据量非常大，那么就需要按需加载了，按需加载也叫懒加载，指的是只有当应用查询缓存，并且数据不在缓存里的时候，才触发加载源头相关数据进缓存的操作\n\n![](statistic/asynccode-142.png)\n\n  \n\n  \n\n```Go\n\nclass Cache\u003cK,V\u003e {\n  final Map\u003cK, V\u003e m =\n    new HashMap\u003c\u003e();\n  final ReadWriteLock rwl = \n    new ReentrantReadWriteLock();\n  final Lock r = rwl.readLock();\n  final Lock w = rwl.writeLock();\n \n  V get(K key) {\n    V v = null;\n    //读缓存\n    r.lock();         ①\n    try {\n      v = m.get(key); ②\n    } finally{\n      r.unlock();     ③\n    }\n    //缓存中存在，返回\n    if(v != null) {   ④\n      return v;\n    }  \n    //缓存中不存在，查询数据库\n    w.lock();         ⑤\n    try {\n      //再次验证\n      //其他线程可能已经查询过数据库\n      v = m.get(key); ⑥\n      if(v == null){  ⑦\n        //查询数据库\n        v=省略代码无数\n        m.put(key, v);\n      }\n    } finally{\n      w.unlock();\n    }\n    return v; \n  }\n}\n```\n\n# 读写锁的升级和降级\n\n```Go\n\n//读缓存\nr.lock();         ①\ntry {\n  v = m.get(key); ②\n  if (v == null) {\n    w.lock();\n    try {\n      //再次验证并更新缓存\n      //省略详细代码\n    } finally{\n      w.unlock();\n    }\n  }\n} finally{\n  r.unlock();     ③\n}\n```\n\n在①处获取读锁，在③处释放读锁，那是否可以在②处的下面增加验证缓存并更新缓存的逻辑,\n\n这样看上去好像是没有问题的，先是获取读锁，然后再升级为写锁，对此还有个专业的名字，叫**锁的升级**。可惜 **ReadWriteLock 并不支持这种升级**。在上面的代码示例中，**读锁还没有释放，此时获取写锁，会导致写锁永久等待，最终导致相关线程都被阻塞**，永远也没有机会被唤醒。锁的升级是不允许的，这个你一定要注意.\n\n  \n\n不过，虽然锁的升级是不允许的，但是锁的降级却是允许的\n\n```Go\n\nclass CachedData {\n  Object data;\n  volatile boolean cacheValid;\n  final ReadWriteLock rwl =\n    new ReentrantReadWriteLock();\n  // 读锁  \n  final Lock r = rwl.readLock();\n  //写锁\n  final Lock w = rwl.writeLock();\n  \n  void processCachedData() {\n    // 获取读锁\n    r.lock();\n    if (!cacheValid) {\n      // 释放读锁，因为不允许读锁的升级\n      r.unlock();\n      // 获取写锁\n      w.lock();\n      try {\n        // 再次检查状态  \n        if (!cacheValid) {\n          data = ...\n          cacheValid = true;\n        }\n        // 释放写锁前，降级为读锁\n        // 降级是可以的\n        r.lock(); ①\n      } finally {\n        // 释放写锁\n        w.unlock(); \n      }\n    }\n    // 此处仍然持有读锁\n    try {use(data);} \n    finally {r.unlock();}\n  }\n}\n```\n\n读写锁类似于 ReentrantLock，也支持公平模式和非公平模式。读锁和写锁都实现了 java.util.concurrent.locks.Lock 接口，所以除了支持 lock() 方法外，tryLock()、lockInterruptibly() 等方法也都是支持的。但是有一点需要注意，那就是只有写锁支持条件变量，读锁是不支持条件变量的，读锁调用 newCondition() 会抛出 UnsupportedOperationException 异常。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/CompletableFuture%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7%E7%B1%BB":{"title":"CompletableFuture：异步编程工具类","content":"前面我们不止一次提到，用多线程优化性能，其实不过就是将串行操作变成并行操作\n\n  \n\n异步化，是并行方案得以实施的基础，更深入地讲其实就是**：利用多线程优化性能这个核心方案得以实施的基础**\n\n# CompletableFuture 的核心优势\n\n  \n\n这里我们用 CompletableFuture 重新实现前面曾提及的烧水泡茶程序\n\n![](statistic/asynccode-148.png)\n\n我们分了 3 个任务：\n\n- 任务 1 负责洗水壶、烧开水，\n    \n- 任务 2 负责洗茶壶、洗茶杯和拿茶叶，\n    \n- 任务 3 负责泡茶。其中任务 3 要等待任务 1 和任务 2 都完成后才能开始\n    \n\n```Go\n\n//任务1：洗水壶-\u003e烧开水\nCompletableFuture\u003cVoid\u003e f1 = \n  CompletableFuture.runAsync(()-\u003e{\n  System.out.println(\"T1:洗水壶...\");\n  sleep(1, TimeUnit.SECONDS);\n\n  System.out.println(\"T1:烧开水...\");\n  sleep(15, TimeUnit.SECONDS);\n});\n//任务2：洗茶壶-\u003e洗茶杯-\u003e拿茶叶\nCompletableFuture\u003cString\u003e f2 = \n  CompletableFuture.supplyAsync(()-\u003e{\n  System.out.println(\"T2:洗茶壶...\");\n  sleep(1, TimeUnit.SECONDS);\n\n  System.out.println(\"T2:洗茶杯...\");\n  sleep(2, TimeUnit.SECONDS);\n\n  System.out.println(\"T2:拿茶叶...\");\n  sleep(1, TimeUnit.SECONDS);\n  return \"龙井\";\n});\n//任务3：任务1和任务2完成后执行：泡茶\nCompletableFuture\u003cString\u003e f3 = \n  f1.thenCombine(f2, (__, tf)-\u003e{\n    System.out.println(\"T1:拿到茶叶:\" + tf);\n    System.out.println(\"T1:泡茶...\");\n    return \"上茶:\" + tf;\n  });\n//等待任务3执行结果\nSystem.out.println(f3.join());\n\nvoid sleep(int t, TimeUnit u) {\n  try {\n    u.sleep(t);\n  }catch(InterruptedException e){}\n}\n// 一次执行结果：\nT1:洗水壶...\nT2:洗茶壶...\nT1:烧开水...\nT2:洗茶杯...\nT2:拿茶叶...\nT1:拿到茶叶:龙井\nT1:泡茶...\n上茶:龙井\n```\n\n优势：\n\n1. 无需手工维护线程，没有繁琐的手工维护线程的工作，给任务分配线程的工作也不需要我们关注；\n    \n2. 语义更清晰，例如 f3 = f1.thenCombine(f2, ()-\u003e{}) 能够清晰地表述“任务 3 要等待任务 1 和任务 2 都完成后才能开始”；\n    \n3. 代码更简练并且专注于业务逻辑，几乎所有代码都是业务逻辑相关的\n    \n\n# 创建CompletableFuture对象\n\n创建CompletableFuture对象主要靠这4个静态方法\n\n```Go\n\n//使用默认线程池\nstatic CompletableFuture\u003cVoid\u003e runAsync(Runnable runnable)\nstatic \u003cU\u003e CompletableFuture\u003cU\u003e supplyAsync(Supplier\u003cU\u003e supplier)\n//可以指定线程池  \nstatic CompletableFuture\u003cVoid\u003e runAsync(Runnable runnable, Executor executor)\nstatic \u003cU\u003e CompletableFuture\u003cU\u003e supplyAsync(Supplier\u003cU\u003e supplier, Executor executor)  \n```\n\n它们之间的区别是**：**\n\n- **Runnable 接口的 run() 方法没有返回值，而 Supplier 接口的 get() 方法是有返回值的**\n    \n- **以及能否指点默认的线程池**\n    \n    - 默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数\n        \n    - 如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿。所以，强烈建议你**要根据不同的业务类型创建不同的线程池**，以避免互相干扰\n        \n\n  \n\n  \n\n创建完 CompletableFuture 对象之后，**会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法**\n\n  \n\n你需要关注两个问题：一个是异步操作什么时候结束，另一个是如何获取异步操作的执行结果。\n\n- 因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决\n    \n\n  \n\n  \n\n# 如何理解 CompletionStage 接口\n\n  \n\n你可以站在分工的角度类比一下工作流。任务是有时序关系的，比如有串行关系、并行关系、汇聚关系\n\n- 串行关系\n    \n\n![](statistic/asynccode-146.png)\n\n- 并行关系\n    \n\n![](statistic/asynccode-149.png)\n\n- 汇聚关系\n    \n\n![](statistic/asynccode-147.png)\n\n  \n\nCompletionStage 接口可以清晰地描述任务之间的这种时序关系\n\n水泡茶程序中的汇聚关系是一种 AND 聚合关系，\n\n- 这里的 AND 指的是**所有依赖的任务（烧开水和拿茶叶）都完成后才开始执行当前任务（泡茶）**。\n    \n- 既然有 AND 聚合关系，那就一定还有 OR 聚合关系，所谓 OR 指的是**依赖的任务只要有一个完成就可以执行当前任务**\n    \n\nCompletionStage 接口也可以方便地描述异常处理。\n\n  \n\n下面我们就来一一介绍，CompletionStage 接口如何描述串行关系、AND 聚合关系、OR 聚合关系以及异常处理\n\n  \n\n## 描述串行关系\n\n  \n\n主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口\n\n```Go\n\nCompletionStage\u003cR\u003e thenApply(fn);\nCompletionStage\u003cR\u003e thenApplyAsync(fn);\nCompletionStage\u003cVoid\u003e thenAccept(consumer);\nCompletionStage\u003cVoid\u003e thenAcceptAsync(consumer);\nCompletionStage\u003cVoid\u003e thenRun(action);\nCompletionStage\u003cVoid\u003e thenRunAsync(action);\nCompletionStage\u003cR\u003e thenCompose(fn);\nCompletionStage\u003cR\u003e thenComposeAsync(fn);\n```\n\nAsync 代表的是异步执行 fn、consumer 或者 action\n\n\n```\n- thenApply\n    \n    - 函数里参数 fn 的类型是接口 Function\u003cT, R\u003e，这个接口里与 CompletionStage 相关的方法是 R apply(T t)，这个方法既能接收参数也支持返回值，所以 thenApply 系列方法返回的是CompletionStage\u003cT, R\u003e\n        \n- thenAccept\n    \n    - 参数 consumer 的类型是接口Consumer\u003cT\u003e，这个接口里与 CompletionStage 相关的方法是 void accept(T t)，这个方法虽然支持参数，但却不支持回值，所以 thenAccept 系列方法返回的是CompletionStage\u003cVoid\u003e\n        \n- thenRun\n    \n    - thenRun 系列方法里 action 的参数是 Runnable，所以 action 既不能接收参数也不支持返回值，所以 thenRun 系列方法返回的也是CompletionStage\u003cVoid\u003e\n        \n- thenCompose\n    \n    - thenCompose系列方法，这个系列的方法会新创建出一个子流程，最终结果和 thenApply 系列是相同的\n```\n\n\n\n        \n\n```Go\n\nCompletableFuture\u003cString\u003e f0 = \n  CompletableFuture.supplyAsync(\n    () -\u003e \"Hello World\")      //①\n  .thenApply(s -\u003e s + \" QQ\")  //②\n  .thenApply(String::toUpperCase);//③\n\nSystem.out.println(f0.join());\n//输出结果\nHELLO WORLD QQ\n```\n\n首先通过 supplyAsync() 启动一个异步流程，之后是两个串行操作，整体看起来还是挺简单的。不过，虽然这是一个异步流程，但任务①②③却是串行执行的，②依赖①的执行结果，③依赖②的执行结果。\n\n  \n\n## 描述 AND 汇聚关系\n\n主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口.\n\n```Go\n\nCompletionStage\u003cR\u003e thenCombine(other, fn);\nCompletionStage\u003cR\u003e thenCombineAsync(other, fn);\nCompletionStage\u003cVoid\u003e thenAcceptBoth(other, consumer);\nCompletionStage\u003cVoid\u003e thenAcceptBothAsync(other, consumer);\nCompletionStage\u003cVoid\u003e runAfterBoth(other, action);\nCompletionStage\u003cVoid\u003e runAfterBothAsync(other, action);\n```\n\n## 描述 OR 汇聚关系\n\n主要是 applyToEither、acceptEither 和 runAfterEither 系列的接口\n\n```Go\n\nCompletionStage applyToEither(other, fn);\nCompletionStage applyToEitherAsync(other, fn);\nCompletionStage acceptEither(other, consumer);\nCompletionStage acceptEitherAsync(other, consumer);\nCompletionStage runAfterEither(other, action);\nCompletionStage runAfterEitherAsync(other, action);\n```\n\n示例代码展示了如何使用 applyToEither() 方法来描述一个 OR 汇聚关系\n\n```Go\n\nCompletableFuture\u003cString\u003e f1 = \n  CompletableFuture.supplyAsync(()-\u003e{\n    int t = getRandom(5, 10);\n    sleep(t, TimeUnit.SECONDS);\n    return String.valueOf(t);\n});\n\nCompletableFuture\u003cString\u003e f2 = \n  CompletableFuture.supplyAsync(()-\u003e{\n    int t = getRandom(5, 10);\n    sleep(t, TimeUnit.SECONDS);\n    return String.valueOf(t);\n});\n\nCompletableFuture\u003cString\u003e f3 = \n  f1.applyToEither(f2,s -\u003e s);\n\nSystem.out.println(f3.join());\n```\n\n  \n\n## 异常处理\n\n虽然上面我们提到的 fn、consumer、action 它们的核心方法都**不允许抛出可检查异常**，但是却**无法限制它们抛出运行时异常**\n\n```Go\n\nCompletableFuture\u003cInteger\u003e \n  f0 = CompletableFuture.\n    .supplyAsync(()-\u003e(7/0))\n    .thenApply(r-\u003er*10);\nSystem.out.println(f0.join());\n```\n\n执行 7/0 就会出现除零错误这个运行时异常。非异步编程里面，我们可以使用 try{}catch{}来捕获并处理异常，那在异步编程里面，异常该如何处理呢？ 下面是相关的方法，使用这些方法进行异常处理和串行操作是一样的，都支持链式编程方式()\n\n```Go\n\nCompletionStage exceptionally(fn);\nCompletionStage\u003cR\u003e whenComplete(consumer);\nCompletionStage\u003cR\u003e whenCompleteAsync(consumer);\nCompletionStage\u003cR\u003e handle(fn);\nCompletionStage\u003cR\u003e handleAsync(fn);\n```\n\n- exceptionally() 的使用非常类似于 try{}catch{}中的 **catch{}**\n    \n- whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 **finally{}**，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn,whenComplete() 和 handle() 的区别在于 **whenComplete() 不支持返回结果，而 handle() 是支持返回结果的\\**\n    \n\n```Go\n\nCompletableFuture\u003cInteger\u003e \n  f0 = CompletableFuture\n    .supplyAsync(()-\u003e(7/0))\n    .thenApply(r-\u003er*10)\n    .exceptionally(e-\u003e0);\nSystem.out.println(f0.join());\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/CompletionService%E6%89%B9%E9%87%8F%E6%89%A7%E8%A1%8C%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1":{"title":"CompletionService：批量执行异步任务","content":"有一个询价应用，这个应用需要从三个电商询价，然后保存在自己的数据库里。核心示例代码如下所示，由于是串行的，所以性能很慢，你来试着优化一下吧\n\n```Go\n\n// 向电商S1询价，并保存\nr1 = getPriceByS1();\nsave(r1);\n// 向电商S2询价，并保存\nr2 = getPriceByS2();\nsave(r2);\n// 向电商S3询价，并保存\nr3 = getPriceByS3();\nsave(r3);\n```\n\n使用“ThreadPoolExecutor+Future”的方案，化结果很可能是下面示例代码这样：用三个线程异步执行询价，通过三次调用 Future 的 get() 方法获取询价结果，之后将询价结果保存在数据库中\n\n```Go\n\n// 创建线程池\nExecutorService executor =Executors.newFixedThreadPool(3);\n// 异步向电商S1询价\nFuture\u003cInteger\u003e f1 = executor.submit(()-\u003egetPriceByS1());\n// 异步向电商S2询价\nFuture\u003cInteger\u003e f2 = executor.submit(()-\u003egetPriceByS2());\n// 异步向电商S3询价\nFuture\u003cInteger\u003e f3 = executor.submit(()-\u003egetPriceByS3());\n    \n// 获取电商S1报价并保存\nr=f1.get();\nexecutor.execute(()-\u003esave(r));\n  \n// 获取电商S2报价并保存\nr=f2.get();\nexecutor.execute(()-\u003esave(r));\n  \n// 获取电商S3报价并保存  \nr=f3.get();\nexecutor.execute(()-\u003esave(r));\n```\n\n但是有个地方的处理需要你注意，那就是如果获取电商 S1 报价的耗时很长，那么即便获取电商 S2 报价的耗时很短，也无法让保存 S2 报价的操作先执行，因为这个主线程都阻塞在了 f1.get() 操作上.\n\n  \n\n增加一个阻塞队列，获取到 S1、S2、S3 的报价都进入阻塞队列，然后在主线程中消费阻塞队列，这样就能保证先获取到的报价先保存到数据库了。下面的示例代码展示了如何利用阻塞队列实现先获取到的报价先保存到数据库。\n\n```Go\n\n// 创建阻塞队列\nBlockingQueue\u003cInteger\u003e bq =new LinkedBlockingQueue\u003c\u003e();\n//电商S1报价异步进入阻塞队列  \nexecutor.execute(()-\u003ebq.put(f1.get()));\n//电商S2报价异步进入阻塞队列  \nexecutor.execute(()-\u003ebq.put(f2.get()));\n//电商S3报价异步进入阻塞队列  \nexecutor.execute(()-\u003ebq.put(f3.get()));\n//异步保存所有报价  \nfor (int i=0; i\u003c3; i++) {\n  Integer r = bq.take();\n  executor.execute(()-\u003esave(r));\n}  \n```\n\n# 利用 CompletionService 实现询价系统\n\nCompletionService 的实现原理也是内部维护了一个阻塞队列，当任务执行结束就把任务的执行结果加入到阻塞队列中，**不同的是 CompletionService 是把任务执行结果的 Future 对象加入到阻塞队列中**，而上面的示例代码是把任务最终的执行结果放入了阻塞队列中\n\n## 创建 CompletionService\n\nCompletionService 接口的实现类是 ExecutorCompletionService，这个实现类的构造方法有两个:\n\n- ExecutorCompletionService(Executor executor)\n    \n- ExecutorCompletionService(Executor executor, BlockingQueue\u003e completionQueue)\n    \n\n这两个构造方法都需要传入一个线程池，如果不指定 completionQueue，那么默认会使用无界的 LinkedBlockingQueue。任务执行结果的 Future 对象就是加入到 **completionQueue**\n\n```Go\n\n// 创建线程池\nExecutorService executor = Executors.newFixedThreadPool(3);\n// 创建CompletionService\nCompletionService\u003cInteger\u003e cs = new ExecutorCompletionService\u003c\u003e(executor);\n\ncs.submit(()-\u003egetPriceByS1());\ncs.submit(()-\u003egetPriceByS2());\ncs.submit(()-\u003egetPriceByS3());\n// 将询价结果异步保存到数据库\nfor (int i=0; i\u003c3; i++) {\n  Integer r = cs.take().get();\n  executor.execute(()-\u003esave(r));\n}\n```\n\n# CompletionService 接口说明\n\nCompletionService 接口提供的方法有 5 个，这 5 个方法的方法签名如下所示\n\n```Go\n\nFuture\u003cV\u003e submit(Callable\u003cV\u003e task);\nFuture\u003cV\u003e submit(Runnable task, V result);\nFuture\u003cV\u003e take() throws InterruptedException;\nFuture\u003cV\u003e poll();\nFuture\u003cV\u003e poll(long timeout, TimeUnit unit) throws InterruptedException;\n```\n\n\n\n```\n\n- submit();\n    \n    - 与ThreadPoolExecutor 的 submit相似\n        \n- Future\u003cV\u003e take() throws InterruptedException;\n    \n    - 从阻塞队列中获取并移除一个元素，那么调用 take() 方法的线程会被阻塞\n        \n- Future\u003cV\u003e poll();\n    \n    - 从阻塞队列中获取并移除一个元素，poll() 方法会返回 null 值\n        \n- Future\u003cV\u003e poll(long timeout, TimeUnit unit) throws InterruptedException;\n    \n    - 支持以超时的方式获取并移除阻塞队列头部的一个元素，如果等待了 timeout unit 时间，阻塞队列还是空的，那么该方法会返回 null 值\n```\n\n        \n\n# 利用 CompletionService 实现 Dubbo 中的 Forking Cluster\n\n  \n\n**Dubbo 中有一种叫做 Forking 的集群模式**，这种集群模式下，**支持并行地调用多个查询服务，只要有一个成功返回结果，整个服务就可以返回了。**\n\n例如你需要提供一个地址转坐标的服务，为了保证该服务的高可用和性能，你可以并行地调用 3 个地图服务商的 API，然后只要有 1 个正确返回了结果 r，那么地址转坐标这个服务就可以直接返回 r\n\n```Go\n\ngeocoder(addr) {\n  //并行执行以下3个查询服务， \n  r1=geocoderByS1(addr);\n  r2=geocoderByS2(addr);\n  r3=geocoderByS3(addr);\n  //只要r1,r2,r3有一个返回\n  //则返回\n  return r1|r2|r3;\n}\n```\n\n利用 CompletionService 可以快速实现 Forking 这种集群模式\n\n```Go\n\n// 创建线程池\nExecutorService executor = Executors.newFixedThreadPool(3);\n// 创建CompletionService\nCompletionService\u003cInteger\u003e cs = new ExecutorCompletionService\u003c\u003e(executor);\n// 用于保存Future对象\nList\u003cFuture\u003cInteger\u003e\u003e futures =new ArrayList\u003c\u003e(3);\n//提交异步任务，并保存future到futures \nfutures.add(cs.submit(()-\u003egeocoderByS1()));\nfutures.add(cs.submit(()-\u003egeocoderByS2()));\nfutures.add(cs.submit(()-\u003egeocoderByS3()));\n// 获取最快返回的任务执行结果\nInteger r = 0;\ntry {\n  // 只要有一个成功返回，则break\n  for (int i = 0; i \u003c 3; ++i) {\n    r = cs.take().get();\n    //简单地通过判空来检查是否成功返回\n    if (r != null) {\n      break;\n    }\n  }\n} finally {\n  //取消所有任务\n  for(Future\u003cInteger\u003e f : futures)\n    f.cancel(true);\n}\n// 返回结果\nreturn r;\n```\n\n当需要批量提交异步任务的时候建议你使用 CompletionService。CompletionService 将线程池 Executor 和阻塞队列 BlockingQueue 的功能融合在了一起,\n\nCompletionService 能够让异步任务的执行结果有序化，先执行完的先进入阻塞队列，利用这个特性，你可以轻松实现后续处理的有序性","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/CountDownLatch%E5%92%8CCyclicBarrier%E8%AE%A9%E7%BA%BF%E7%A8%8B%E6%AD%A5%E8%B0%83%E4%B8%80%E8%87%B4":{"title":"CountDownLatch和CyclicBarrier让线程步调一致","content":"目前对账系统的处理逻辑是首先查询订单，然后查询派送单，之后对比订单和派送单，将差异写入差异库。\n\n![](statistic/asynccode-140.png)\n\n  \n\n对账系统的代码抽象之后，也很简单，核心代码如下，就是在一个单线程里面循环查询订单、派送单，然后执行对账，最后将写入差异库。\n\n```Go\n\nwhile(存在未对账订单){\n  // 查询未对账订单\n  pos = getPOrders();\n  // 查询派送单\n  dos = getDOrders();\n  // 执行对账操作\n  diff = check(pos, dos);\n  // 差异写入差异库\n  save(diff);\n} \n```\n\n# 利用并行优化对账系统\n\n目前对账系统是单线程执行的，图形化后是下图这个样子。\n\n![](statistic/asynccode-137.png)\n\n查询未对账订单 getPOrders() 和查询派送单 getDOrders() 是否可以并行处理呢？显然是可以的，因为这两个操作并没有先后顺序的依赖。这两个最耗时的操作并行之后，执行过程如下图所示\n\n![](statistic/asynccode-138.png)\n\n代码为：\n\n```Go\n\nwhile(存在未对账订单){\n  // 查询未对账订单\n  Thread T1 = new Thread(()-\u003e{\n    pos = getPOrders();\n  });\n  T1.start();\n  // 查询派送单\n  Thread T2 = new Thread(()-\u003e{\n    dos = getDOrders();\n  });\n  T2.start();\n  // 等待T1、T2结束\n  T1.join();\n  T2.join();\n  // 执行对账操作\n  diff = check(pos, dos);\n  // 差异写入差异库\n  save(diff);\n} \n```\n\n主线程需要等待线程 T1 和 T2 执行完才能执行 check() 和 save() 这两个操作，为此我们通过调用 T1.join() 和 T2.join() 来实现等待，**当 T1 和 T2 线程退出时，调用 T1.join() 和 T2.join() 的主线程就会从阻塞态被唤醒**，从而执行之后的 check() 和 save()。\n\n  \n\n# 用 CountDownLatch 实现线程等待\n\n  \n\n问题：\n\nwhile 循环里面每次都会创建新的线程，而创建线程可是个耗时的操作。所以最好是创建出来的线程能够循环利用，估计这时你已经想到线程池了，是的，线程池就能解决这个问题。\n\n我们首先创建了一个固定大小为 2 的线程池，之后在 while 循环里重复利用。**前面主线程通过调用线程 T1 和 T2 的 join() 方法来等待线程 T1 和 T2 退出，但是在线程池的方案里，线程根本就不会退出，所以 join() 方法已经失效了**。\n\n```Go\n\n// 创建2个线程的线程池\nExecutor executor = \n  Executors.newFixedThreadPool(2);\nwhile(存在未对账订单){\n  // 查询未对账订单\n  executor.execute(()-\u003e {\n    pos = getPOrders();\n  });\n  // 查询派送单\n  executor.execute(()-\u003e {\n    dos = getDOrders();\n  });\n  \n  /* ？？如何实现等待？？*/\n  \n  // 执行对账操作\n  diff = check(pos, dos);\n  // 差异写入差异库\n  save(diff);\n}   \n```\n\n那如何解决这个问题呢?\n\n最直接的办法是弄一个计数器，初始值设置成 2，当执行完pos = getPOrders();这个操作之后将计数器减 1，执行完dos = getDOrders();之后也将计数器减 1.在主线程里，等待计数器等于 0；当计数器等于 0 时，说明这两个查询操作执行完了。等待计数器等于 0 其实就是一个条件变量.\n\n因为 Java 并发包里已经提供了实现类似功能的工具类：**CountDownLatch**\n\n```Go\n\n// 创建2个线程的线程池\nExecutor executor = \n  Executors.newFixedThreadPool(2);\nwhile(存在未对账订单){\n  // 计数器初始化为2\n  CountDownLatch latch = \n    new CountDownLatch(2);\n  // 查询未对账订单\n  executor.execute(()-\u003e {\n    pos = getPOrders();\n    latch.countDown();\n  });\n  // 查询派送单\n  executor.execute(()-\u003e {\n    dos = getDOrders();\n    latch.countDown();\n  });\n  \n  // 等待两个查询操作结束\n  latch.await();\n  \n  // 执行对账操作\n  diff = check(pos, dos);\n  // 差异写入差异库\n  save(diff);\n}\n```\n\n# 进一步优化性能\n\n前面我们将 getPOrders() 和 getDOrders() 这两个查询操作并行了，但这两个查询操作和对账操作 check()、save() 之间还是串行的。很显然，这两个查询操作和对账操作也是可以并行的，也就是说，在执行对账操作的时候，可以同时去执行下一轮的查询操作，这个过程可以形象化地表述为下面这幅示意图。\n\n![](statistic/asynccode-136.png)\n\n  \n\n两次查询操作能够和对账操作并行，对账操作还依赖查询操作的结果，这明显有点生产者 - 消费者的意思，两次查询操作是生产者，对账操作是消费者。既然是生产者 - 消费者模型，那就需要有个队列，来保存生产者生产的数据，而消费者则从这个队列消费数据\n\n  \n\n具体如下图所示，订单查询操作将订单查询结果插入订单队列，派送单查询操作将派送单插入派送单队列，这两个队列的元素之间是有一一对应的关系的。两个队列的好处是，对账操作可以每次从订单队列出一个元素，从派送单队列出一个元素，然后对这两个元素执行对账操作，这样数据一定不会乱掉。\n\n  \n\n![](statistic/asynccode-139.png)\n\n一个线程 T1 执行订单的查询工作，一个线程 T2 执行派送单的查询工作，当线程 T1 和 T2 都各自生产完 1 条数据的时候，通知线程 T3 执行对账操作**.其实还隐藏着一个条件，那就是线程 T1 和线程 T2 的工作要步调一致，不能一个跑得太快，一个跑得太慢，只有这样才能做到各自生产完 1 条数据的时候，通知线程 T3**\n\n  \n\n线程 T1 和线程 T2 只有都生产完 1 条数据的时候，才能一起向下执行，也就是说，线程 T1 和线程 T2 要互相等待，步调要一致；同时当线程 T1 和 T2 都生产完一条数据的时候，还要能够通知线程 T3 执行对账操作\n\n![](statistic/asynccode-141.png)\n\n# 用 CyclicBarrier 实现线程同步\n\n这个方案的难点有两个：一个是线程 T1 和 T2 要做到步调一致，另一个是要能够通知到线程 T3。\n\n你依然可以利用一个计数器来解决这两个难点，计数器初始化为 2，线程 T1 和 T2 生产完一条数据都将计数器减 1，如果计数器大于 0 则线程 T1 或者 T2 等待。如果计数器等于 0，则通知线程 T3，并唤醒等待的线程 T1 或者 T2，与此同时，将计数器重置为 2.","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/Executor-%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%B1%A0":{"title":"Executor 和线程池","content":"线程是一个重量级的对象，应该避免频繁创建和销毁，可以使用线程池\n\n  \n\n一般意义上的池化资源，都是下面这样，当你需要资源的时候就调用 acquire() 方法来申请资源，用完之后就调用 release() 释放资源。\n\n```Go\n\nclass XXXPool{\n  // 获取池化资源\n  XXX acquire() {\n  }\n  // 释放池化资源\n  void release(XXX x){\n  }\n}  \n```\n\n若你带着这个固有模型来看并发包里线程池相关的工具类时，会很遗憾地发现它们完全匹配不上，Java 提供的线程池里面压根就没有申请线程和释放线程的方法.\n\n  \n\n**为什么线程池没有采用一般意义上池化资源的设计方法呢**\n\n  \n\n# 线程池是一种生产者 - 消费者模式\n\n  \n\n你可以来思考一下，假设我们获取到一个空闲线程 T1，然后该如何使用 T1 呢？你期望的可能是这样：通过调用 T1 的 execute() 方法，传入一个 Runnable 对象来执行具体业务逻辑，就像通过构造函数 Thread(Runnable target) 创建线程一样。可惜的是，你翻遍 Thread 对象的所有方法，**都不存在类似 execute(Runnable target) 这样的公共方法**\n\n```Go\n\n//采用一般意义上池化资源的设计方法\nclass ThreadPool{\n  // 获取空闲线程\n  Thread acquire() {\n  }\n  // 释放线程\n  void release(Thread t){\n  }\n} \n//期望的使用\nThreadPool pool；\nThread T1=pool.acquire();\n//传入Runnable对象\nT1.execute(()-\u003e{\n  //具体业务逻辑\n  ......\n});\n```\n\n那线程池该如何设计呢？目前业界线程池的设计，普遍采用的都是**生产者 - 消费者模式,**\n\n**线程池的使用方是生产者，线程池本身是消费者**\n\n```Go\n\n//简化的线程池，仅用来说明工作原理\nclass MyThreadPool{\n  //利用阻塞队列实现生产者-消费者模式\n  BlockingQueue\u003cRunnable\u003e workQueue;\n  //保存内部工作线程\n  List\u003cWorkerThread\u003e threads \n    = new ArrayList\u003c\u003e();\n  // 构造方法\n  MyThreadPool(int poolSize, \n    BlockingQueue\u003cRunnable\u003e workQueue){\n    this.workQueue = workQueue;\n    // 创建工作线程\n    for(int idx=0; idx\u003cpoolSize; idx++){\n      WorkerThread work = new WorkerThread();\n      work.start();\n      threads.add(work);\n    }\n  }\n  // 提交任务\n  void execute(Runnable command){\n    workQueue.put(command);\n  }\n  // 工作线程负责消费任务，并执行任务\n  class WorkerThread extends Thread{\n    public void run() {\n      //循环取任务并执行\n      while(true){ ①\n        Runnable task = workQueue.take();\n        task.run();\n      } \n    }\n  }  \n}\n\n/** 下面是使用示例 **/\n// 创建有界阻塞队列\nBlockingQueue\u003cRunnable\u003e workQueue = \n  new LinkedBlockingQueue\u003c\u003e(2);\n// 创建线程池  \nMyThreadPool pool = new MyThreadPool(\n  10, workQueue);\n// 提交任务  \npool.execute(()-\u003e{\n    System.out.println(\"hello\");\n});\n```\n\n# 如何使用Java中的线程池\n\nJava 并发包里提供的线程池，远比我们上面的示例代码强大得多，当然也复杂得多。Java 提供的线程池相关的工具类中，最核心的是 ThreadPoolExecutor，通过名字你也能看出来，它强调的是 Executor，而不是一般意义上的池化资源\n\n```Go\n\nThreadPoolExecutor(\n  int corePoolSize,\n  int maximumPoolSize,\n  long keepAliveTime,\n  TimeUnit unit,\n  BlockingQueue\u003cRunnable\u003e workQueue,\n  ThreadFactory threadFactory,\n  RejectedExecutionHandler handler) \n```\n\n把线程池类比为一个项目组，而线程就是项目组的成员\n\n  \n\n- corePoolSize：表示线程池保有的最小线程数。有些项目很闲，但是也不能把人都撤了，至少要留 corePoolSize 个人坚守阵地。\n    \n- maximumPoolSize：表示线程池创建的最大线程数。当项目很忙时，就需要加人，但是也不能无限制地加，最多就加到 maximumPoolSize 个人。当项目闲下来时，就要撤人了，最多能撤到 corePoolSize 个人。\n    \n- keepAliveTime \u0026 unit：上面提到项目根据忙闲来增减人员，那在编程世界里，如何定义忙和闲呢？很简单，一个线程如果在一段时间内，都没有执行任务，说明很闲，keepAliveTime 和 unit 就是用来定义这个“一段时间”的参数。也就是说，如果一个线程空闲了keepAliveTime \u0026 unit这么久，而且线程池的线程数大于 corePoolSize ，那么这个空闲的线程就要被回收了。\n    \n- workQueue：工作队列，和上面示例代码的工作队列同义。\n    \n- threadFactory：通过这个参数你可以自定义如何创建线程，例如你可以给线程指定一个有意义的名字。\n    \n- handler：通过这个参数你可以自定义任务的拒绝策略。如果线程池中所有的线程都在忙碌，并且工作队列也满了（前提是工作队列是有界队列），那么此时提交任务，线程池就会拒绝接收。至于拒绝的策略，你可以通过 handler 这个参数来指定。\n    \n    - ThreadPoolExecutor 已经提供了以下 4 种策略。\n        \n        - CallerRunsPolicy：提交任务的线程自己去执行该任务。\n            \n        - AbortPolicy：默认的拒绝策略，会 throws RejectedExecutionException。\n            \n        - DiscardPolicy：直接丢弃任务，没有任何异常抛出。\n            \n        - DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列。\n            \n\n# 使用线程池需要注意的\n\n考虑到 ThreadPoolExecutor 的构造函数实在是有些复杂，所以 Java 并发包里提供了一个线程池的静态工厂类 Executors，利用 Executors 你可以快速创建线程池。\n\n- 不过目前**大厂的编码规范中基本上都不建议使用 Executors** 了\n    \n    - **Executors 提供的很多方法默认使用的都是无界的 LinkedBlockingQueue，高负载情境下，无界队列很容易导致 OOM，而 OOM 会导致所有请求都无法处理，这是致命问题**\n        \n- 默认拒绝策略要慎重使用\n    \n    - 线程池默认的拒绝策略会 throw RejectedExecutionException 这是个**运行时异常**，对于运行时异常编译器并不强制 catch 它，所以开发人员很容易忽略。\n        \n        - 如果线程池处理的任务非常重要，建议自定义自己的拒绝策略；并且在实际工作中，自定义的拒绝策略往往和降级策略配合使用。\n            \n            - 例如将任务信息插入数据库或者消息队列，启用一个专门用作补偿的线程池去进行补偿。所谓降级就是在服务无法正常提供功能的情况下，采取的补救措施\n                \n- 使用线程池，还要注意异常处理的问题，\n    \n    - 例如通过 ThreadPoolExecutor 对象的 execute() 方法提交任务时，如果任务在执行的过程中**出现运行时异常，会导致执行任务的线程终止。**\n        \n    - 不过，最致命的是任务虽然异常了，但是你却获取不到任何通知，这会让你误以为任务都执行得很正常。\n        \n    - 虽然线程池提供了很多用于异常处理的方法，但是**最稳妥和简单的方案还是捕获所有异常并按需处理**，你可以参考下面的示例代码\n        \n    \n    ```Go\n    \n    try {\n      //业务逻辑\n    } catch (RuntimeException x) {\n      //按需处理\n    } catch (Throwable x) {\n      //按需处理\n    } \n    ```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/Fork-Join%E5%8D%95%E6%9C%BA%E7%89%88%E7%9A%84MapReduce":{"title":"Fork Join单机版的MapReduce","content":"**对于简单的并行任务，你可以通过“线程池 +Future”的方案来解决；如果任务之间有聚合关系，无论是 AND 聚合还是 OR 聚合，都可以通过 CompletableFuture 来解决；而批量的并行任务，则可以通过 CompletionService 来解决。**\n\n  \n\n  \n\n我们一直讲，并发编程可以分为三个层面的问题，分别是分工、协作和互斥当你关注于任务的时候，你会发现你的视角已经从并发编程的细节中跳出来了，你应用的更多的是现实世界的思维模式，类比的往往是现实世界里的分工，所以我把线程池、Future、CompletableFuture 和 CompletionService 都列到了分工里面\n\n  \n\n  \n\n上面提到的简单并行、聚合、批量并行这三种任务模型，基本上能够覆盖日常工作中的并发场景了，但还是不够全面，因为还有一种“分治”的任务模型没有覆盖到.\n\n  \n\n  \n\n分治，顾名思义，即分而治之，是一种解决复杂问题的思维方法和模式；具体来讲，**指的是把一个复杂的问题分解成多个相似的子问题，然后再把子问题分解成更小的子问题，直到子问题简单到可以直接求解**。\n\n  \n\n  \n\n# 分治任务模型\n\n  \n\n分治任务模型可分为两个阶段：\n\n- 一个阶段是**任务分解**，也就是将任务迭代地分解为子任务，直至子任务可以直接计算出结果；\n    \n- 另一个阶段是**结果合并**，即逐层合并子任务的执行结果，直至获得最终结果\n    \n\n  \n\n![](statistic/asynccode-144.png)\n\n在这个分治任务模型里，**任务和分解后的子任务具有相似性**，这种相似性往往体现在**任务和子任务的算法是相同的**，但是计算的数据规模是不同的。具备这种相似性的问题，我们往往都采用**递归算法**。\n\n  \n\n# Fork/Join 的使用\n\nFork/Join 是一个并行计算的框架，主要就是用来支持分治任务模型的，这个计算框架里的 **Fork 对应的是分治任务模型里的任务分解**，**Join 对应的是结果合并**\n\n  \n\n  \n\nFork/Join 计算框架主要包含两部分，\n\n- 一部分是分治任务的线程池 ForkJoinPool，\n    \n- 另一部分是分治任务 ForkJoinTask\n    \n\n  \n\n## ForkJoinTask\n\nForkJoinTask 是一个抽象类，它的方法有很多，最核心的是 fork() 方法和 join() 方法，其中 **fork() 方法会异步地执行一个子任务**，而 **join() 方法则会阻塞当前线程来等待子任务的执行结果**\n\n  \n\nForkJoinTask 有两个子类——RecursiveAction 和 RecursiveTask，这两个子类都定义了抽象方法 compute()\n\n- RecursiveAction 定义的 compute() 没有返回值\n    \n- RecursiveTask 定义的 compute() 方法是有返回值的\n    \n\n  \n\n这两个子类也是抽象类，在使用的时候，需要你定义子类去扩展\n\n  \n\n## 用 Fork/Join 这个并行计算框架计算斐波那契数列\n\n看看如何用 Fork/Join 这个并行计算框架计算斐波那契数列（下面的代码源自 Java 官方示例）。首先我们需要创建一个分治任务线程池以及计算斐波那契数列的分治任务，之后通过调用分治任务线程池的 invoke() 方法来启动分治任务。由于计算斐波那契数列需要有返回值，所以 Fibonacci 继承自 RecursiveTask。分治任务 Fibonacci 需要实现 compute() 方法，这个方法里面的逻辑和普通计算斐波那契数列非常类似，区别之处在于计算 Fibonacci(n - 1) 使用了异步子任务，这是通过 f1.fork() 这条语句实现的。\n\n```Go\n\nstatic void main(String[] args){\n  //创建分治任务线程池  \n  ForkJoinPool fjp = \n    new ForkJoinPool(4);\n  //创建分治任务\n  Fibonacci fib = \n    new Fibonacci(30);   \n  //启动分治任务  \n  Integer result = \n    fjp.invoke(fib);\n  //输出结果  \n  System.out.println(result);\n}\n//递归任务\nstatic class Fibonacci extends \n    RecursiveTask\u003cInteger\u003e{\n  final int n;\n  Fibonacci(int n){this.n = n;}\n  protected Integer compute(){\n    if (n \u003c= 1)\n      return n;\n    Fibonacci f1 = new Fibonacci(n - 1);\n    //创建子任务  \n    f1.fork();\n    Fibonacci f2 = new Fibonacci(n - 2);\n    //等待子任务结果，并合并结果  \n    return f2.compute() + f1.join();\n  }\n}\n```\n\n# ForkJoinPool 工作原理\n\n  \n\n应该已经知道 ThreadPoolExecutor 本质上是一个生产者 - 消费者模式的实现，内部有一个任务队列，这个任务队列是生产者和消费者通信的媒介,ThreadPoolExecutor 可以有多个工作线程，但是这些工作线程都共享一个任务队列.\n\n  \n\nForkJoinPool 本质上也是一个生产者 - 消费者的实现.\n\n- **ThreadPoolExecutor 内部只有一个任务队列，而 ForkJoinPool 内部有多个任务队列**，\n    \n- 当我们通过 ForkJoinPool 的 invoke() 或者 submit() 方法提交任务时，**ForkJoinPool 根据一定的路由规则把任务提交到一个任务队列中**，\n    \n    - **如果任务在执行过程中会创建出子任务，那么子任务会提交到工作线程对应的任务队列中**\n        \n- 如果工作线程对应的任务队列空了，是不是就没活儿干了呢？不是的，ForkJoinPool 支持一种叫做“**任务窃取**”的机制\n    \n    - 如果工作线程空闲了，那它可以“窃取”其他工作任务队列里的任务\n        \n    - 例如下图 线程 T2 对应的任务队列已经空了，它可以“窃取”线程 T1 对应的任务队列的任务。如此一来，所有的工作线程都不会闲下来了\n        \n    \n    ![](statistic/asynccode-145.png)\n    \n- ForkJoinPool 中的任务队列采用的是双端队列，工作线程正常获取任务和“窃取任务”分别是从任务队列不同的端消费\n    \n\n  \n\n  \n\n# 模拟 MapReduce 统计单词数量\n\n学习 MapReduce 有一个入门程序，统计一个文件里面每个单词的数量\n\n可以先用二分法递归地将一个文件拆分成更小的文件，直到文件里只有一行数据，然后统计这一行数据里单词的数量，最后再逐级汇总结果，你可以对照前面的简版分治任务模型图来理解这个过程\n\n  \n\n下面的示例程序用一个字符串数组 String[] fc 来模拟文件内容，fc 里面的元素与文件里面的行数据一一对应\n\n```Go\n\nstatic void main(String[] args){\n  String[] fc = {\"hello world\",\n          \"hello me\",\n          \"hello fork\",\n          \"hello join\",\n          \"fork join in world\"};\n  //创建ForkJoin线程池    \n  ForkJoinPool fjp = \n      new ForkJoinPool(3);\n  //创建任务    \n  MR mr = new MR(\n      fc, 0, fc.length);  \n  //启动任务    \n  Map\u003cString, Long\u003e result = \n      fjp.invoke(mr);\n  //输出结果    \n  result.forEach((k, v)-\u003e\n    System.out.println(k+\":\"+v));\n}\n//MR模拟类\nstatic class MR extends \n  RecursiveTask\u003cMap\u003cString, Long\u003e\u003e {\n  private String[] fc;\n  private int start, end;\n  //构造函数\n  MR(String[] fc, int fr, int to){\n    this.fc = fc;\n    this.start = fr;\n    this.end = to;\n  }\n  @Override protected \n  Map\u003cString, Long\u003e compute(){\n    if (end - start == 1) {\n      return calc(fc[start]);\n    } else {\n      int mid = (start+end)/2;\n      MR mr1 = new MR(\n          fc, start, mid);\n      mr1.fork();\n      MR mr2 = new MR(\n          fc, mid, end);\n      //计算子任务，并返回合并的结果    \n      return merge(mr2.compute(),\n          mr1.join());\n    }\n  }\n  //合并结果\n  private Map\u003cString, Long\u003e merge(\n      Map\u003cString, Long\u003e r1, \n      Map\u003cString, Long\u003e r2) {\n    Map\u003cString, Long\u003e result = \n        new HashMap\u003c\u003e();\n    result.putAll(r1);\n    //合并结果\n    r2.forEach((k, v) -\u003e {\n      Long c = result.get(k);\n      if (c != null)\n        result.put(k, c+v);\n      else \n        result.put(k, v);\n    });\n    return result;\n  }\n  //统计单词数量\n  private Map\u003cString, Long\u003e \n      calc(String line) {\n    Map\u003cString, Long\u003e result =\n        new HashMap\u003c\u003e();\n    //分割单词    \n    String [] words = \n        line.split(\"\\\\s+\");\n    //统计单词数量    \n    for (String w : words) {\n      Long v = result.get(w);\n      if (v != null) \n        result.put(w, v+1);\n      else\n        result.put(w, 1L);\n    }\n    return result;\n  }\n}\n```\n\nFork/Join 并行计算框架的核心组件是 ForkJoinPool,Java 1.8 提供的 Stream API 里面并行流也是以 ForkJoinPool 为基础的。不过需要你注意的是，默认情况下所有的并行流计算都共享一个 ForkJoinPool.\n\n**如果所有的并行流计算都是 CPU 密集型计算的话，完全没有问题，但是如果存在 I/O 密集型的并行流计算，那么很可能会因为一个很慢的 I/O 计算而拖慢整个系统的性能。所以建议用不同的 ForkJoinPool 执行不同类型的计算任务**。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/Future":{"title":"Future","content":"使用 ThreadPoolExecutor 的时候，如何获取任务执行结果\n\n# 如何获取任务执行结果\n\nJava 通过 ThreadPoolExecutor 提供的 3 个 submit() 方法和 1 个 FutureTask 工具类来支持获得任务执行结果的需求\n\n  \n\n## submit()\n\n```Go\n\n// 提交Runnable任务\nFuture\u003c?\u003e  submit(Runnable task);\n// 提交Callable任务\n\u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task);\n// 提交Runnable任务及结果引用  \n\u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result);\n```\n\n会发现它们的返回值都是 Future 接口，Future 接口有 5 个方法，我都列在下面了\n\n```Go\n\n// 取消任务\nboolean cancel(boolean mayInterruptIfRunning);\n// 判断任务是否已取消  \nboolean isCancelled();\n// 判断任务是否已结束\nboolean isDone();\n// 获得任务执行结果\nget();\n// 获得任务执行结果，支持超时\nget(long timeout, TimeUnit unit);\n```\n\n3个submit()方法的区别\n\n\n\n```\n- Future\u003c?\u003e submit(Runnable task);\n    \n    - 这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 submit(Runnable task) 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()\n        \n- \u003cT\u003e Future\u003cT\u003e submit(Callable\u003cT\u003e task);\n    \n    - 这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果\n        \n- \u003cT\u003e Future\u003cT\u003e submit(Runnable task, T result);\n    \n    - 假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result\n        \n        - 用法： Runnable 接口的实现类 Task 声明了一个有参构造函数 Task(Result r) ，创建 Task 对象的时候传入了 result 对象，这样就能在类 Task 的 run() 方法中对 result 进行各种操作了。result 相当于主线程和子线程之间的桥梁，通过它主子线程可以共享数据。\n```\n\n\n\n            \n\n\n```Go\n\nExecutorService executor = Executors.newFixedThreadPool(1);\n// 创建Result对象r\nResult r = new Result();\nr.setAAA(a);\n// 提交任务\nFuture\u003cResult\u003e future = executor.submit(new Task(r), r);  \nResult fr = future.get();\n// 下面等式成立\nfr === r;\nfr.getAAA() === a;\nfr.getXXX() === x\n\nclass Task implements Runnable{\n  Result r;\n  //通过构造函数传入result\n  Task(Result r){\n    this.r = r;\n  }\n  void run() {\n    //可以操作result\n    a = r.getAAA();\n    r.setXXX(x);\n  }\n}\n```\n\n## FutureTask\n\nFutureTask 是一个实实在在的工具类，这个工具类有两个构造函数\n\n```Go\n\nFutureTask(Callable\u003cV\u003e callable);\nFutureTask(Runnable runnable, V result);\n```\n\nFutureTask 实现了 Runnable 和 Future 接口，\n\n- 由于实现了 Runnable 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行\n    \n- 又因为实现了 Future 接口，所以也能用来获得任务的执行结果\n    \n\n```Go\n\n// 创建FutureTask\nFutureTask\u003cInteger\u003e futureTask = new FutureTask\u003c\u003e(()-\u003e 1+2);\n// 创建线程池\nExecutorService es = Executors.newCachedThreadPool();\n// 提交FutureTask \nes.submit(futureTask);\n// 获取计算结果\nInteger result = futureTask.get();\n```\n\n# 实现最优的烧水泡茶程序\n\n以前初中语文课文里有一篇著名数学家华罗庚先生的文章《统筹方法》，这篇文章里介绍了一个烧水泡茶的例子\n\n![](statistic/asynccode-150.png)\n\n并发编程可以总结为三个核心问题：分工、同步和互斥\n\n  \n\n对于烧水泡茶这个程序，一种最优的分工方案可以是下图所示的这样：\n\n- 用两个线程 T1 和 T2 来完成烧水泡茶程序，\n    \n- T1 负责洗水壶、烧开水、泡茶这三道工序，\n    \n- T2 负责洗茶壶、洗茶杯、拿茶叶三道工序，\n    \n- 其中 T1 在执行泡茶这道工序时需要等待 T2 完成拿茶叶的工序\n    \n\n  \n\n  \n\n可以使用Thread.join()、CountDownLatch，甚至阻塞队列都可以解决，**但是如何使用 Future 特性来实现**？\n\n  \n\n- 首先，我们创建了两个 FutureTask——ft1 和 ft2，\n    \n    - ft1 完成洗水壶、烧开水、泡茶的任务，\n        \n    - ft2 完成洗茶壶、洗茶杯、拿茶叶的任务；\n        \n- 这里需要注意的是 ft1 这个任务在执行泡茶任务前，需要等待 ft2 把茶叶拿来，\n    \n    - 所以 ft1 内部需要引用 ft2，并在执行泡茶之前，调用 ft2 的 get() 方法实现等待\n        \n\n```Go\n\n// 创建任务T2的FutureTask\nFutureTask\u003cString\u003e ft2 = new FutureTask\u003c\u003e(new T2Task());\n// 创建任务T1的FutureTask\nFutureTask\u003cString\u003e ft1 = new FutureTask\u003c\u003e(new T1Task(ft2));\n// 线程T1执行任务ft1\nThread T1 = new Thread(ft1);\nT1.start();\n// 线程T2执行任务ft2\nThread T2 = new Thread(ft2);\nT2.start();\n// 等待线程T1执行结果\nSystem.out.println(ft1.get());\n\n// T1Task需要执行的任务：\n// 洗水壶、烧开水、泡茶\nclass T1Task implements Callable\u003cString\u003e{\n  FutureTask\u003cString\u003e ft2;\n  // T1任务需要T2任务的FutureTask\n  T1Task(FutureTask\u003cString\u003e ft2){\n    this.ft2 = ft2;\n  }\n  @Override\n  String call() throws Exception {\n    System.out.println(\"T1:洗水壶...\");\n    TimeUnit.SECONDS.sleep(1);\n    \n    System.out.println(\"T1:烧开水...\");\n    TimeUnit.SECONDS.sleep(15);\n    // 获取T2线程的茶叶  \n    String tf = ft2.get();\n    System.out.println(\"T1:拿到茶叶:\"+tf);\n\n    System.out.println(\"T1:泡茶...\");\n    return \"上茶:\" + tf;\n  }\n}\n// T2Task需要执行的任务:\n// 洗茶壶、洗茶杯、拿茶叶\nclass T2Task implements Callable\u003cString\u003e {\n  @Override\n  String call() throws Exception {\n    System.out.println(\"T2:洗茶壶...\");\n    TimeUnit.SECONDS.sleep(1);\n\n    System.out.println(\"T2:洗茶杯...\");\n    TimeUnit.SECONDS.sleep(2);\n\n    System.out.println(\"T2:拿茶叶...\");\n    TimeUnit.SECONDS.sleep(1);\n    return \"龙井\";\n  }\n}\n// 一次执行结果：\nT1:洗水壶...\nT2:洗茶壶...\nT1:烧开水...\nT2:洗茶杯...\nT2:拿茶叶...\nT1:拿到茶叶:龙井\nT1:泡茶...\n上茶:龙井\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E5%B8%B8%E7%94%A8%E7%B1%BB/Lock%E5%92%8CCondition":{"title":"Lock和Condition","content":"ava SDK 并发包通过 Lock 和 Condition 两个接口来实现管程，其中 Lock 用于解决互斥问题，Condition 用于解决同步问题。\n\n  \n\n  \n\n![](statistic/asynccode-2.jpg)\n\nJava 语言本身提供的 synchronized 也是管程的一种实现，既然 Java 从语言层面已经实现了管程了，那为什么还要在 SDK 里提供另外一种实现呢？\n\n# 再造管程的理由\n\n例如在 Java 的 1.5 版本中，synchronized 性能不如 SDK 里面的 Lock，但 1.6 版本之后，synchronized 做了很多优化，将性能追了上来，所以 1.6 之后的版本又有人推荐使用 synchronized 了。\n\n那性能是否可以成为“重复造轮子”的理由呢？显然不能。因为性能问题优化一下就可以了，完全没必要“重复造轮子”\n\n  \n\n  \n\n在介绍死锁问题的时候，提出了一个破坏不可抢占条件方案，但是这个方案 synchronized 没有办法解决。\n\n原因是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源\n\n对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。\n\n  \n\n重新设计锁的思路\n\n- **能够响应中断**。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。\n    \n- **支持超时**。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。\n    \n- **非阻塞地获取锁。**如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。\n    \n\n这三个方案就是“重复造轮子”的主要原因，体现在 API 上，就是 Lock 接口的三个方法.\n\n```Go\npackage java.util.concurrent.locks;\nimport java.util.concurrent.TimeUnit;\n\n\npublic interface Lock {\n\n   \n    void lock();\n\n                // 支持中断的API\n    void lockInterruptibly() throws InterruptedException;\n                // 支持中断的API\n    boolean tryLock();\n                // 支持超时的API\n    boolean tryLock(long time, TimeUnit unit) throws InterruptedException;\n\n    void unlock();\n\n    Condition newCondition();\n}\n```\n\n# 如何保证可见性\n\n你已经知道 Java 里多线程的可见性是通过 Happens-Before 规则保证的，而 synchronized 之所以能够保证可见性，也是因为有一条 synchronized 相关的规则：synchronized 的解锁 Happens-Before 于后续对这个锁的加锁。\n\n```Go\n\nclass X {\n  private final Lock rtl =\n  new ReentrantLock();\n  int value;\n  public void addOne() {\n    // 获取锁\n    rtl.lock();  \n    try {\n      value+=1;\n    } finally {\n      // 保证锁能释放\n      rtl.unlock();\n    }\n  }\n}\n```\n\nJava SDK 里面锁的实现非常复杂，这里我就不展开细说了，但是原理还是需要简单介绍一下：**它是利用了 volatile 相关的 Happens-Before 规则**，Java SDK 里面的 ReentrantLock，内部持有一个 volatile 的成员变量 state，获取锁的时候，会读写 state 的值；解锁的时候，也会读写 state 的值（简化后的代码如下面所示）。也就是说，在执行 value+=1 之前，程序先读写了一次 volatile 变量 state，在执行 value+=1 之后，又读写了一次 volatile 变量 state。根据相关的 Happens-Before 规则：\n\n1. 顺序性规则：对于线程 T1，value+=1 Happens-Before 释放锁的操作 unlock()；\n    \n2. volatile 变量规则：由于 state = 1 会先读取 state，所以线程 T1 的 unlock() 操作 Happens-Before 线程 T2 的 lock() 操作；\n    \n3. 传递性规则：线程 T1 的 value+=1 Happens-Before 线程 T2 的 lock() 操作。\n    \n\n# 可重入锁（ReentrantLock）\n\n所谓可重入锁，顾名思义，指的是线程可以重复获取同一把锁。\n\n```Go\n\nclass X {\n  private final Lock rtl =\n  new ReentrantLock();\n  int value;\n  public int get() {\n    // 获取锁\n    rtl.lock();         ②\n    try {\n      return value;\n    } finally {\n      // 保证锁能释放\n      rtl.unlock();\n    }\n  }\n  public void addOne() {\n    // 获取锁\n    rtl.lock();  \n    try {\n      value = 1 + get(); ①\n    } finally {\n      // 保证锁能释放\n      rtl.unlock();\n    }\n  }\n}\n```\n\n当线程 T1 执行到 ① 处时，已经获取到了锁 rtl ，当在 ① 处调用 get() 方法时，会在 ② 再次对锁 rtl 执行加锁操作。此时，如果锁 rtl 是可重入的，那么线程 T1 可以再次加锁成功；如果锁 rtl 是不可重入的，那么线程 T1 此时会被阻塞。\n\n  \n\n  \n\n除了可重入锁，可能你还听说过可重入函数，可重入函数怎么理解呢？指的是线程可以重复调用？显然不是，所谓可重入函数，指的是多个线程可以同时调用该函数，每个线程都能得到正确结果；同时在一个线程内支持线程切换，无论被切换多少次，结果都是正确的。多线程可以同时执行，还支持线程切换，这意味着什么呢？线程安全啊。所以，可重入函数是线程安全的。\n\n  \n\n# 公平锁与非公平锁\n\n  \n\n  \n\n入口等待队列，锁都对应着一个等待队列，如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。\n\n- 如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；\n    \n- 如果是非公平锁，则不提供这个公平保证\n    \n\n会发现 ReentrantLock 这个类有两个构造函数，一个是无参构造函数，一个是传入 fair 参数的构造函数。fair 参数代表的是锁的公平策略，如果传入 true 就表示需要构造一个公平锁，反之则表示要构造一个非公平锁\n\n```Go\n\n//无参构造函数：默认非公平锁\npublic ReentrantLock() {\n    sync = new NonfairSync();\n}\n//根据公平策略参数创建锁\npublic ReentrantLock(boolean fair){\n    sync = fair ? new FairSync() \n                : new NonfairSync();\n}\n```\n\n# 锁的最佳实践\n\n1. 永远只在更新对象的成员变量时加锁\n    \n2. 永远只在访问可变的成员变量时加锁\n    \n3. 永远不在调用其他对象的方法时加锁\n    \n\n# Condition\n\nCondition 实现了管程模型里面的条件变量，\n\n我们提到过 Java 语言内置的管程里只有一个条件变量，而 **Lock\u0026Condition 实现的管程是支持多个条件变量的**，这是二者的一个重要区别。\n\n# 如何利用两个条件变量快速实现阻塞队列\n\n  \n\n一个阻塞队列，需要两个条件变量，一个是队列不空（空队列不允许出队），另一个是队列不满（队列已满不允许入队）\n\n```Go\n\npublic class BlockedQueue\u003cT\u003e{\n  final Lock lock =\n    new ReentrantLock();\n  // 条件变量：队列不满  \n  final Condition notFull =\n    lock.newCondition();\n  // 条件变量：队列不空  \n  final Condition notEmpty =\n    lock.newCondition();\n\n  // 入队\n  void enq(T x) {\n    lock.lock();\n    try {\n      while (队列已满){\n        // 等待队列不满\n        notFull.await();\n      }  \n      // 省略入队操作...\n      //入队后,通知可出队\n      notEmpty.signal();\n    }finally {\n      lock.unlock();\n    }\n  }\n  // 出队\n  void deq(){\n    lock.lock();\n    try {\n      while (队列已空){\n        // 等待队列不空\n        notEmpty.await();\n      }  \n      // 省略出队操作...\n      //出队后，通知可入队\n      notFull.signal();\n    }finally {\n      lock.unlock();\n    }  \n  }\n}\n```\n\nlock 和 Conditin 实现的管程，**线程等待和通知需要调用await()、signal() 、sgnalAll()，他们的语义和wait()、notaify()、notifyAll()相同**\n\nLock\u0026Condition 实现的管程里只能使用前面的 await()、signal()、signalAll()，而后面的 wait()、notify()、notifyAll() 只有在 synchronized 实现的管程里才能使用\n\n  \n\n# 同步和异步\n\n那同步和异步的区别到底是什么呢？**通俗点来讲就是调用方是否需要等待结果，如果需要等待结果，就是同步；**\n\n**如果不需要等待结果，就是异步。**\n\n  \n\n同步，是Java代码默认的处理方法。如果你想让你的程序支持异步，可以通过下面两张方式实现\n\n1. 调用方创建一个子线程，在子线程中执行方法调用，这种调用我们称为异步调用\n    \n2. 方法实现的时候，创建一个新的线程执行主要逻辑，主线程直接 return，这种方法我们一般称为异步方法。\n    \n\n  \n\n# Dobbo 分析\n\n其实在编程领域，异步的场景还是挺多的，比如 TCP 协议本身就是异步的，我们工作中经常用到的 RPC 调用，在 TCP 协议层面，发送完 RPC 请求后，线程是不会等待 RPC 的响应结果的。\n\n**知名的 RPC 框架 Dubbo 就给我们做了异步转同步的事情**\n\n```Go\n\nDemoService service = 初始化部分省略\nString message = \n  service.sayHello(\"dubbo\");\nSystem.out.println(message);\n```\n\n对于下面一个简单的 RPC 调用，默认情况下 sayHello() 方法，是个同步方法，也就是说，执行 service.sayHello(“dubbo”) 的时候，线程会停下来等结果。\n\n  \n\n通过调用栈，你能看到线程是阻塞在 DefaultFuture.get() 方法上，所以可以推断：Dubbo 异步转同步的功能应该是通过 DefaultFuture 这个类实现的.\n\n不过为了理清前后关系，还是有必要分析一下调用 DefaultFuture.get() 之前发生了什么.DubboInvoker 的 108 行调用了 DefaultFuture.get()，这一行很关键，我稍微修改了一下列在了下面。这一行先调用了 request(inv, timeout) 方法，这个方法其实就是发送 RPC 请求，之后通过调用 get() 方法等待 RPC 返回结果\n\n```Go\n\npublic class DubboInvoker{\n  Result doInvoke(Invocation inv){\n    // 下面这行就是源码中108行\n    // 为了便于展示，做了修改\n    return currentClient \n      .request(inv, timeout)\n      .get();\n  }\n}\n```\n\nDefaultFuture 这个类是很关键，我把相关的代码精简之后，列到了下面\n\n```Go\n\n// 创建锁与条件变量\nprivate final Lock lock \n    = new ReentrantLock();\nprivate final Condition done \n    = lock.newCondition();\n\n// 调用方通过该方法等待结果\nObject get(int timeout){\n  long start = System.nanoTime();\n  lock.lock();\n  try {\n  while (!isDone()) {\n    done.await(timeout);\n      long cur=System.nanoTime();\n    if (isDone() || \n          cur-start \u003e timeout){\n      break;\n    }\n  }\n  } finally {\n  lock.unlock();\n  }\n  if (!isDone()) {\n  throw new TimeoutException();\n  }\n  return returnFromResponse();\n}\n// RPC结果是否已经返回\nboolean isDone() {\n  return response != null;\n}\n// RPC结果返回时调用该方法   \nprivate void doReceived(Response res) {\n  lock.lock();\n  try {\n    response = res;\n    if (done != null) {\n      done.signal();\n    }\n  } finally {\n    lock.unlock();\n  }\n}\n```\n\n当 RPC 返回结果之前，阻塞调用线程，让调用线程等待；当 RPC 返回结果后，唤醒调用线程，让调用线程重新执行。不知道你有没有似曾相识的感觉，这不就是经典的等待 - 通知机制吗？这个时候想必你的脑海里应该能够浮现出管程的解决方案了\n\n  \n\n  \n\n调用线程通过调用 get() 方法等待 RPC 返回结果，这个方法里面，你看到的都是熟悉的“面孔”：调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁；获取锁后，通过经典的在循环中调用 await() 方法来实现等待。当 RPC 结果返回时，会调用 doReceived() 方法，这个方法里面，调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁，获取锁后通过调用 signal() 来通知调用线程，结果已经返回，不用继续等待了。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E6%AD%BB%E9%94%81":{"title":"死锁","content":"在上一篇文章中，我们用 Account.class 作为互斥锁，来解决银行业务里面的转账问题，虽然这个方案不存在并发问题，但是所有账户的转账操作都是串行的。\n\n  \n\n账户 A 转账户 B、账户 C 转账户 D 这两个转账操作现实世界里是可以并行的；\n\n具体操作为：\n\n1. 文件架上恰好有转出账本和转入账本，那就同时拿走；\n    \n2. 如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来；\n    \n3. 转出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。\n    \n\n![](statistic/asynccode-77.png)\n\n```Go\n\nclass Account {\n  private int balance;\n  // 转账\n  void transfer(Account target, int amt){\n    // 锁定转出账户\n    synchronized(this) {              \n      // 锁定转入账户\n      synchronized(target) {           \n        if (this.balance \u003e amt) {\n          this.balance -= amt;\n          target.balance += amt;\n        }\n      }\n    }\n  } \n}\n```\n\n# 死锁\n\n使用细粒度锁可以提高并行度，是性能优化的一个重要手段\n\n使用细粒度锁是有代价的，这个代价就是可能会导致死锁。\n\n使用细粒度锁是有代价的，这个代价就是可能会导致死锁。\n\n```Go\n\nclass Account {\n  private int balance;\n  // 转账\n  void transfer(Account target, int amt){\n    // 锁定转出账户\n    synchronized(this){     ①\n      // 锁定转入账户\n      synchronized(target){ ②\n        if (this.balance \u003e amt) {\n          this.balance -= amt;\n          target.balance += amt;\n        }\n      }\n    }\n  } \n}\n```\n\n关于这种现象，我们还可以借助资源分配图来可视化锁的占用情况（资源分配图是个有向图，它可以描述资源和线程的状态）。其中，资源用方形节点表示，线程用圆形节点表示；资源中的点指向线程的边表示线程已经获得该资源，线程指向资源的边则表示线程请求资源，但尚未得到\n\n![](statistic/asynccode-78.png)\n\n# 预防死锁\n\n产生死锁的条件：\n\n1. 互斥，共享资源 X 和 Y 只能被一个线程占用；\n    \n2. 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；\n    \n3. 不可抢占，其他线程不能强行抢占线程 T1 占有的资源；\n    \n4. 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。\n    \n\n  \n\n如何预防：\n\n只要我们破坏其中一个，就可以成功避免死锁的发生。\n\n互斥这个条件我们没有办法破坏，因为我们用锁为的就是互斥\n\n  \n\n1. 对于“占用且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了。\n    \n2. 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。\n    \n3. 对于“循环等待”这个条件，可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了\n    \n\n  \n\n## 破坏占用且等待条件\n\n对应到编程领域，“同时申请”这个操作是一个临界区，我们也需要一个角色（Java 里面的类）来管理这个临界区，我们就把这个角色定为 Allocator。它有两个重要功能，分别是：同时申请资源 apply() 和同时释放资源 free()。账户 Account 类里面持有一个 Allocator 的单例（必须是单例，只能由一个人来分配资源）\n\n```Go\n\nclass Allocator {\n  private List\u003cObject\u003e als =\n    new ArrayList\u003c\u003e();\n  // 一次性申请所有资源\n  synchronized boolean apply(\n    Object from, Object to){\n    if(als.contains(from) ||\n         als.contains(to)){\n      return false;  \n    } else {\n      als.add(from);\n      als.add(to);  \n    }\n    return true;\n  }\n  // 归还资源\n  synchronized void free(\n    Object from, Object to){\n    als.remove(from);\n    als.remove(to);\n  }\n}\n\nclass Account {\n  // actr应该为单例\n  private Allocator actr;\n  private int balance;\n  // 转账\n  void transfer(Account target, int amt){\n    // 一次性申请转出账户和转入账户，直到成功\n    while(!actr.apply(this, target))\n      ；\n    try{\n      // 锁定转出账户\n      synchronized(this){              \n        // 锁定转入账户\n        synchronized(target){           \n          if (this.balance \u003e amt){\n            this.balance -= amt;\n            target.balance += amt;\n          }\n        }\n      }\n    } finally {\n      actr.free(this, target)\n    }\n  } \n}\n```\n\n## 破坏不可抢占条件\n\n破坏不可抢占条件看上去很简单，核心是要能够主动释放它占有的资源，\n\nsynchronized 是做不到的\n\njava.util.concurrent 这个包下面提供的 Lock 是可以轻松解决这个问题的\n\n## 破坏循环等待条件\n\n破坏这个条件，需要对资源进行排序，然后按序申请资源。这个实现非常简单，我们假设每个账户都有不同的属性 id，这个 id 可以作为排序字段，申请的时候，我们可以按照从小到大的顺序来申请。比如下面代码中，①~⑥处的代码对转出账户（this）和转入账户（target）排序，然后按照序号从小到大的顺序锁定账户。这样就不存在“循环”等待了。\n\n```Go\n\nclass Account {\n  private int id;\n  private int balance;\n  // 转账\n  void transfer(Account target, int amt){\n    Account left = this        ①\n    Account right = target;    ②\n    if (this.id \u003e target.id) { ③\n      left = target;           ④\n      right = this;            ⑤\n    }                          ⑥\n    // 锁定序号小的账户\n    synchronized(left){\n      // 锁定序号大的账户\n      synchronized(right){ \n        if (this.balance \u003e amt){\n          this.balance -= amt;\n          target.balance += amt;\n        }\n      }\n    }\n  } \n}\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%AD%89%E5%BE%85%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6%E4%BC%98%E5%8C%96%E5%BE%AA%E7%8E%AF%E7%AD%89%E5%BE%85":{"title":"等待通知机制优化循环等待","content":"等待通知机制优化循环等待\n\n在破坏占用且等待条件的时候，如果转出账本和转入账本不满足同时在文件架上这个条件，就用死循环的方式来循环等待\n\n```Go\n\n// 一次性申请转出账户和转入账户，直到成功\nwhile(!actr.apply(this, target))\n  ；\n```\n\n- 如果 apply() 操作耗时长，或者并发冲突量大的时候，循环等待这种方案就不适用了，因为在这种场景下，可能要循环上万次才能获取到锁，太消耗 CPU 了。\n    \n- 最好的方案应该是：如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，**则线程阻塞自己**，进入**等待状**态；当线程要求的条件（转出账本和转入账本同在文件架上）满足后，**通知等待的线程重新执行**\n    \n\n# 就医过程中的等待通知\n\n就医过程\n\n1. 患者先去挂号，然后到就诊门口分诊，等待叫号；\n    \n2. 当叫到自己的号时，患者就可以找大夫就诊了；\n    \n3. 就诊过程中，大夫可能会让患者去做检查，同时叫下一位患者；\n    \n4. 当患者做完检查后，拿检测报告重新分诊，等待叫号；\n    \n5. 当大夫再次叫到自己的号时，患者再去找大夫就诊\n    \n\n  \n\n细节：\n\n1. 患者到就诊门口分诊，类似于线程要去获取互斥锁；\n    \n2. 当患者被叫到时，类似线程已经获取到锁了。大夫让患者去做检查（缺乏检测报告不能诊断病因），类似于线程要求的条件没有满足。患者去做检查，类似于线程进入等待状态；\n    \n3. 然后大夫叫下一个患者，这个步骤我们在前面的等待 - **通知机制中忽视了，这个步骤对应到程序里，本质是线程释放持有的互斥锁**。\n    \n4. 患者做完检查，类似于线程要求的条件已经满足；**患者拿检测报告重新分诊，类似于线程需要重新获取互斥锁，这个步骤我们在前面的等待 - 通知机制中也忽视了**\n    \n\n  \n\n**一个完整的等待 - 通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁。**\n\n  \n\n  \n\n# 用 synchronized 实现等待 - 通知机制\n\n，比如 Java 语言内置的 synchronized 配合 wait()、notify()、notifyAll() 这三个方法就能轻松实现\n\n![](statistic/asynccode-79.png)\n\n在并发程序中，当一个线程进入临界区后，由于某些条件不满足，需要进入等待状态，Java 对象的 wait() 方法就能够满足这种需求。如上图所示，当调用 wait() 方法后，当前线程就会被阻塞，并且进入到右边的等待队列中，**这个等待队列也是互斥锁的等待队列。这个等待队列和互斥锁是一对一的关系，每个互斥锁都有自己独立的等待队列**。 线程在进入等待队列的同时，会释放持有的互斥锁，线程释放锁后，其他线程就有机会获得锁，并进入临界区了。\n\n![](statistic/asynccode-80.png)\n\n当条件满足时调用 notify()，会通知等待队列（互斥锁的等待队列）中的线程，**告诉它条件曾经满足过**。为什么说是曾经满足过呢？**因为 notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队**）\n\n强调：\n\n1. 一直强调 wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列，所以如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()、this.notifyAll()；如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 。\n    \n2. 而且 wait()、notify()、notifyAll() 这三个方法能够被调用的前提是已经获取了相应的互斥锁，所以我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的。如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时常：java.lang.IllegalMonitorStateException\n    \n\n  \n\n# 一个更好的资源管理器\n\n等待 - 通知机制中，我们需要考虑以下四个要素：\n\n1. 互斥锁：上一篇文章我们提到 Allocator 需要是单例的，所以我们可以用 this 作为互斥锁\n    \n2. 线程要求的条件：转出账户和转入账户都没有被分配过。\n    \n3. 何时等待：线程要求的条件不满足就等待。\n    \n4. 何时通知：当有线程释放账户时就通知\n    \n\n```Go\n\n  while(条件不满足) {\n    wait();\n  }\n```\n\n利用这种范式可以解决上面提到的条件**曾经满足过**这个问题。因为当 wait() 返回时，有可能条件已经发生变化了，曾经条件满足，但是现在已经不满足了，所以要重新检验条件是否满足\n\n```Go\n\nclass Allocator {\n  private List\u003cObject\u003e als;\n  // 一次性申请所有资源\n  synchronized void apply(\n    Object from, Object to){\n    // 经典写法\n    while(als.contains(from) ||\n         als.contains(to)){\n      try{\n        wait();\n      }catch(Exception e){\n      }   \n    } \n    als.add(from);\n    als.add(to);  \n  }\n  // 归还资源\n  synchronized void free(\n    Object from, Object to){\n    als.remove(from);\n    als.remove(to);\n    notifyAll();\n  }\n}\n```\n\n# 使用notifyAll()\n\n这二者是有区别的，notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等待队列中的所有线程。\n\n- 感觉上来讲，应该是 notify() 更好一些，因为即便通知所有线程，也只有一个线程能够进入临界区。\n    \n- 实际上使用 notify() 也很有风险，它的风险在于可能导致某些线程永远不会被通知到","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%AE%A1%E7%A8%8B":{"title":"管程","content":"# 什么是管程\n\n**Java 采用的是管程技术**，**synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分**。而管程和信号量是等价的，所谓等价指的**是用管程能够实现信号量，也能用信号量实现管程**\n\n所**谓管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发**\n\n  \n\n# MESA 模型\n\n管程的发展历史上有三种管程模型\n\n- 管程模型\n    \n    - Hasen 模型\n        \n    - Hoare 模型\n        \n    - MESA 模型\n        \n        - 广泛应用的是 MESA 模型\n            \n        - Java 管程的实现参考的也是 MESA 模型\n            \n\n并发编程的两个核心问题：\n\n- 互斥：同一时刻只允许一个线程访问共享资源\n    \n- 同步：线程之间如何通信、协作\n    \n\n## 管程是如何管理互斥的？\n\n**将共享变量及其对共享变量的操作统一封装起来**\n\n  \n\n假如我们要实现一个线程安全的阻塞队列，一个最直观的想法就是：**将线程不安全的队列封装起来，对外提供线程安全的操作方法**，\n\n管程 X 将共享变量 queue 这个线程不安全的队列和相关的操作入队操作 enq()、出队操作 deq() 都封装起来了；线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现；enq()、deq() 保证互斥性，只允许一个线程进入管程。\n\n![](statistic/asynccode-86.png)\n\n  \n\n## 管程如何管理同步\n\n在管程模型里，共享变量和对共享变量的操作是被封装起来的，图中最外层的框就代表封装的意思。\n\n- 框的上面只有一个入口，并且在入口旁边还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。\n    \n\n![](statistic/asynccode-85.png)\n\n- 管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列\n    \n\n**那条件变量和条件变量等待队列的作用是什么呢？其实就是解决线程同步问题**\n\n  \n\n1. 假设有个线程 T1 执行阻塞队列的出队操作，执行出队操作，需要注意有个前提条件，就是阻塞队列不能是空的（空队列只能出 Null 值，是不允许的），**阻塞队列不空这个前提条件对应的就是管程里的条件变量**.如果线程 T1 进入管程后恰好发现阻塞队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。\n    \n2. 线程 T1 进入条件变量的等待队列后，是**允许其他线程进入管程的**。\n    \n3. 再假设之后另外一个线程 T2 执行阻塞队列的入队操作，入队操作执行成功之后**，“阻塞队列不空”这个条件对于线程 T1 来说已经满足了**，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，**会从等待队列里面出来**，但是出来之后不是马上执行，而是**重新进入到入口等待队列里面**\n    \n\n  \n\n  \n\n  \n\n#### wait()、notify()、notifyAll() 这三个操作\n\n  \n\n1. 前面提到线程 T1 发现“阻塞队列不空”这个条件不满足，需要进到对应的等待队列里等待。这个过程就是通过调用 wait() 来实现的\n    \n2. 同理当“则塞队列不空”这个条件满足时，线程 T2 需要调用 A.notify() 来通知 A 等待队列中的一个线程，此时这个等待队列里面只有线程 T1。至于 notifyAll() 这个方法，它可以通知等待队列中的所有线程\n    \n\n  \n\n  \n\n# wait()的正确姿势\n\n有一个编程范式，就是**需要在一个 while 循环里面调用 wait()**。这个是 MESA 管程特有的。\n\n```Go\n\n\nwhile(条件不满足) {\n  wait();\n}\n```\n\nHasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？\n\n1. Hasen 模型里面，要求 notify() 放在代码的最后，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。\n    \n2. Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。\n    \n3. MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，**可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量**。\n    \n\n# notify() 何时可以使用\n\n除非经过深思熟虑，否则尽量使用 notifyAll()\n\n那什么时候可以使用 notify() 呢？\n\n- 所有等待线程拥有相同的等待条件；\n    \n- 所有等待线程被唤醒后，执行相同的操作；\n    \n- 只需要唤醒一个线程。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%BA%BF%E7%A8%8B%E6%B1%A0":{"title":"线程池","content":"线程池\n\n# 什么是线程池?\n\n顾名思义，线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，处理完之后线程并不会立即被销毁，而是等待下一个任务。\n\n# 为什么要用线程池？\n\n池化技术想必大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。\n\n**线程池**提供了一种限制和管理资源（包括执行一个任务）的方式。 每个**线程池**还维护一些基本统计信息，例如已完成任务的数量。\n\n这里借用《Java 并发编程的艺术》提到的来说一下**使用线程池的好处**：\n\n- **降低资源消耗**。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。\n    \n- **提高响应速度**。当任务到达时，任务可以不需要等到线程创建就能立即执行。\n    \n- **提高线程的可管理性**。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。\n    \n\n# 如何创建线程池？\n\n**方式一：通过****`ThreadPoolExecutor`****构造函数来创建（推荐）。**\n\n**方式二：通过** **`Executor`** **框架的工具类** **`Executors`** **来创建。**\n\n我们可以创建多种类型的 `ThreadPoolExecutor`：\n\n- **`FixedThreadPool`** ： **该方法返回一个固定线程数量的线程池**。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。\n    \n- **`SingleThreadExecutor`****：** 该方法返回一个**只有一个线程的线程池**。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。\n    \n- **`CachedThreadPool`****：** 该方法返回一个**可根据实际情况调整线程数量的线程池**。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。\n    \n- **`ScheduledThreadPool`** ：该**返回一个用来在给定的延迟后运行任务或者定期执行任务的线程池**。\n    \n\n对应 `Executors` 工具类中的方法如图所示：\n\n![](statistic/asynccode-128.png)\n\n# 为什么不推荐使用内置线程池？\n\n**为什么呢？**\n\n\u003e 使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。\n\n另外，《阿里巴巴 Java 开发手册》中强制线程池不允许使用 `Executors` 去创建，而是通过 `ThreadPoolExecutor` 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险\n\n`Executors` 返回线程池对象的弊端如下(后文会详细介绍到)：\n\n- **`FixedThreadPool`** **和** **`SingleThreadExecutor`** ： 使用的是无界的 `LinkedBlockingQueue`，任务队列最大长度为 `Integer.MAX_VALUE`,可能堆积大量的请求，从而导致 OOM。\n    \n- **`CachedThreadPool`** ：使用的是同步队列 `SynchronousQueue`, 允许创建的线程数量为 `Integer.MAX_VALUE` ，可能会创建大量线程，从而导致 OOM。\n    \n- **`ScheduledThreadPool`** **和** **`SingleThreadScheduledExecutor`** : 使用的无界的延迟阻塞队列`DelayedWorkQueue`，任务队列最大长度为 `Integer.MAX_VALUE`,可能堆积大量的请求，从而导致 OOM。\n    \n\n# 线程池常见参数有哪些？如何解释？\n\n![](statistic/asynccode-126.png)\n\n**`ThreadPoolExecutor`** **3 个最重要的参数：**\n\n- **`corePoolSize`** **:** 任务队列未达到队列容量时，最大可以同时运行的线程数量。\n    \n- **`maximumPoolSize`** **:** 任务队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。\n    \n- **`workQueue`****:** 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。\n    \n\n`ThreadPoolExecutor`其他常见参数 :\n\n- **`keepAliveTime`**:线程池中的线程数量大于 `corePoolSize` 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 `keepAliveTime`才会被回收销毁；\n    \n- **`unit`** : `keepAliveTime` 参数的时间单位。\n    \n- **`threadFactory`** :executor 创建新线程的时候会用到。\n    \n- **`handler`** :饱和策略。关于饱和策略下面单独介绍一下。\n    \n\n  \n\n![](statistic/asynccode-134.png)\n\n# 线程池的饱和策略有哪些？\n\n如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，`ThreadPoolTaskExecutor` 定义一些策略:\n\n- **`ThreadPoolExecutor.AbortPolicy`****：** 抛出 `RejectedExecutionException`来拒绝新任务的处理。\n    \n- **`ThreadPoolExecutor.CallerRunsPolicy`****：** 调用执行自己的线程运行任务，也就是直接在调用`execute`方法的线程中运行(`run`)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。\n    \n- **`ThreadPoolExecutor.DiscardPolicy`****：** 不处理新任务，直接丢弃掉。\n    \n- **`ThreadPoolExecutor.DiscardOldestPolicy`****：** 此策略将丢弃最早的未处理的任务请求。\n    \n\n# 线程池常用的阻塞队列有哪些？\n\n新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。\n\n不同的线程池会选用不同的阻塞队列，我们可以结合内置线程池来分析。\n\n- **ArrayBlockingQueue**：ArrayBlockingQueue（有界队列）是一个用数组实现的有 界阻塞队列，按FIFO排序量。\n    \n- **`LinkedBlockingQueue`**：容量为 `Integer.MAX_VALUE` 的 `LinkedBlockingQueue`（无界队列）：`FixedThreadPool` 和 `SingleThreadExector` 。由于队列永远不会被放满，因此`FixedThreadPool`最多只能创建核心线程数的线程。\n    \n- **`DelayedWorkQueue`****（延迟阻塞队列）**：`ScheduledThreadPool` 和 `SingleThreadScheduledExecutor` 。`DelayedWorkQueue` 的内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构，可以保证每次出队的任务都是当前队列中执行时间最靠前的。`DelayedWorkQueue` 添加元素满了之后会自动扩容原来容量的 1/2，即永远不会阻塞，最大扩容可达 `Integer.MAX_VALUE`，所以最多只能创建核心线程数的线程。\n    \n- **`SynchronousQueue`****（同步队列**） ：`CachedThreadPool` 。`SynchronousQueue` 没有容量，不存储元素，目的是保证对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务。也就是说，`CachedThreadPool` 的最大线程数是 `Integer.MAX_VALUE` ，可以理解为线程数是可以无限扩展的，可能会创建大量线程，从而导致 OOM。\n    \n- **PriorityBlockingQueue**：PriorityBlockingQueue（优先级队列）是具有优先级的无界阻塞队列\n    \n- **SynchronousQueue**：SynchronousQueue（同步队列）是一个不存储元素的阻塞队 列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQuene，newCachedThreadPool线程池使用了这个队列。\n    \n\n# 线程池提交execute和submit的区别\n\n- execute 用于提交不需要返回值的任务\n    \n\n![](statistic/asynccode-132.png)\n\n- submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个 future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值\n    \n\n![](statistic/asynccode-133.png)\n\n# 线程池处理任务的流程了解吗？\n\n![](statistic/asynccode-125.png)\n\n1. 如果当前运行的线程数小于核心线程数，那么就会新建一个线程来执行任务。\n    \n2. 如果当前运行的线程数等于或大于核心线程数，但是小于最大线程数，那么就把该任务放入到任务队列里等待执行。\n    \n3. 如果向任务队列投放任务失败（任务队列已经满了），但是当前运行的线程数是小于最大线程数的，就新建一个线程来执行任务。\n    \n4. 如果当前运行的线程数已经等同于最大线程数了，新建线程将会使当前运行的线程超出最大线程数，那么当前任务会被拒绝，饱和策略会调用`RejectedExecutionHandler.rejectedExecution()`方法。\n    \n\n# 如何给线程池命名？\n\n初始化线程池的时候需要显示命名（设置线程池名称前缀），有利于定位问题。\n\n默认情况下创建的线程名字类似 `pool-1-thread-n` 这样的，没有业务含义，不利于我们定位问题。\n\n给线程池里的线程命名通常有下面两种方式：\n\n**1、利用 guava 的** **`ThreadFactoryBuilder`**\n\n```Java\nThreadFactory threadFactory = new ThreadFactoryBuilder()\n                            .setNameFormat(threadNamePrefix + \"-%d\")\n                            .setDaemon(true).build();\nExecutorService threadPool = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.MINUTES, workQueue, threadFactory)\n```\n\n**2、自己实现** **`ThreadFactor`****。**\n\n```Java\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ThreadFactory;\nimport java.util.concurrent.atomic.AtomicInteger;\n/**\n * 线程工厂，它设置线程名称，有利于我们定位问题。\n */\n public final class NamingThreadFactory implements ThreadFactory {\n     private final AtomicInteger threadNum = new AtomicInteger();\n     private final ThreadFactory delegate;private final String name;\n     \n     /**\n     * 创建一个带名字的线程池生产工厂\n     */\n     public NamingThreadFactory(ThreadFactory delegate, String name) {\n         this.delegate = delegate;\n         this.name = name; // TODO consider uniquifying this\n     }\n     \n    @Override\n    public Thread newThread(Runnable r) {\n        Thread t = delegate.newThread(r);\n        t.setName(name + \" [#\" + threadNum.incrementAndGet() + \"]\");\n        return t;\n    }\n}\n```\n\n# 如何设定线程池的大小？\n\n**并不是人多就能把事情做好，增加了沟通交流成本。你本来一件事情只需要 3 个人做，你硬是拉来了 6 个人，会提升做事效率嘛？我想并不会。** 线程数量过多的影响也是和我们分配多少人做事情一样，对于多线程这个场景来说主要是增加了**上下文切换**成本。不清楚什么是上下文切换的话，可以看我下面的介绍。\n\n\u003e 上下文切换：\n\u003e \n\u003e 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为**每个线程分配时间片并轮转的形式**。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：**当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换**。\n\u003e \n\u003e 上下文切换通常是**计算密集型**的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\n\u003e \n\u003e Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n\n类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。\n\n- **如果我们设置的线程池数量太小的话**，如果同一时间有大量任务/请求需要处理，可能**会导致大量的请求/任务在任务队列中排队等待执行**，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者~~大量任务堆积在任务队列导致 OOM~~。这样很明显是有问题的，CPU 根本没有得到充分利用。\n    \n- 如果我们设置线程数量太大，**大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换**，从而增加线程的执行时间，影响了整体执行效率。\n    \n\n有一个简单并且适用面比较广的公式：\n\n- **CPU 密集型任务(N+1)：** 这种任务消耗的主要是 CPU 资源，**可以将线程数设置为 N（CPU 核心数）+1**。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。\n    \n- **I/O 密集型任务(2N)：** 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，**具体的计算方法是 2N**。\n    \n\n**如何判断是 CPU 密集任务还是 IO 密集任务？**\n\nCPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。\n\n\u003e 🌈 拓展一下（参见：[issue#1737open in new window](https://github.com/Snailclimb/JavaGuide/issues/1737)）：\n\u003e \n\u003e 线程数更严谨的计算的方法应该是：`最佳线程数 = N（CPU 核心数）∗（1+WT（线程等待时间）/ST（线程计算时间））`，其中 `WT（线程等待时间）=线程运行总时间 - ST（线程计算时间）`。\n\u003e \n\u003e 线程等待时间所占比例越高，需要越多线程。线程计算时间所占比例越高，需要越少线程。\n\u003e \n\u003e 我们可以通过 JDK 自带的工具 VisualVM 来查看 `WT/ST` 比例。\n\u003e \n\u003e CPU 密集型任务的 `WT/ST` 接近或者等于 0，因此， 线程数可以设置为 N（CPU 核心数）∗（1+0）= N，和我们上面说的 N（CPU 核心数）+1 差不多。\n\u003e \n\u003e IO 密集型任务下，几乎全是线程等待时间，从理论上来说，你就可以将线程数设置为 2N（按道理来说，WT/ST 的结果应该比较大，这里选择 2N 的原因应该是为了避免创建过多线程吧）。\n\n公示也只是参考，具体还是要根据项目实际线上运行情况来动态调整。我在后面介绍的美团的线程池参数动态配置这种方案就非常不错，很实用！\n\n# 如何动态修改线程池的参数？\n\n美团技术团队在[《Java 线程池实现原理及其在美团业务中的实践》open in new window](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)这篇文章中介绍到对线程池参数实现可自定义配置的思路和方法。\n\n美团技术团队的思路是主要对线程池的核心参数实现自定义可配置。这三个核心参数是：\n\n- **`corePoolSize`** **:** 核心线程数线程数定义了最小可以同时运行的线程数量。\n    \n- **`maximumPoolSize`** **:** 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。\n    \n- **`workQueue`****:** 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。\n    \n\n**为什么是这三个参数？**\n\n我在[Java 线程池详解open in new window](https://javaguide.cn/java/concurrent/java-thread-pool-summary.html) 这篇文章中就说过这三个参数是 `ThreadPoolExecutor` 最重要的参数，它们基本决定了线程池对于任务的处理策略。\n\n**如何支持参数动态配置？** 且看 `ThreadPoolExecutor` 提供的下面这些方法。\n\n![](statistic/asynccode-130.png)\n\n格外需要注意的是`corePoolSize`， 程序运行期间的时候，我们调用 `setCorePoolSize（）`这个方法的话，线程池会首先判断当前工作线程数是否大于`corePoolSize`，如果大于的话就会回收工作线程。\n\n另外，你也看到了上面并没有动态指定队列长度的方法，美团的方式是自定义了一个叫做 `ResizableCapacityLinkedBlockIngQueue` 的队列（主要就是把`LinkedBlockingQueue`的 capacity 字段的 final 关键字修饰给去掉了，让它变为可变的）。\n\n最终实现的可动态修改线程池参数效果如下。👏👏👏\n\n![](statistic/asynccode-127.png)\n\n还没看够？推荐 why 神的[如何设置线程池参数？美团给出了一个让面试官虎躯一震的回答。open in new window](https://mp.weixin.qq.com/s/9HLuPcoWmTqAeFKa1kj-_A)这篇文章，深度剖析，很不错哦！\n\n如果我们的项目也想要实现这种效果的话，可以借助现成的开源项目：\n\n- **[Hippo-4 （open in new windo](https://github.com/opengoofy/hippo4j)****）** ：一款强大的动态线程池框架，解决了传统线程池使用存在的一些痛点比如线程池参数没办法动态修改、不支持运行时变量的传递、无法执行优雅关闭。除了支持动态修改线程池参数、线程池任务传递上下文，还支持通知报警、运行监控等开箱即用的功能。\n    \n- **[Dynamic TP （open in new window](https://github.com/dromara/dynamic-tp)****）** ：轻量级动态线程池，内置监控告警功能，集成三方中间件线程池管理，基于主流配置中心（已支持Nacos、Apollo，Zookeeper、Consul、Etcd，可通过SPI自定义实现）。\n    \n\n  \n\n# 线程池如何关闭？\n\n可以通过调用线程池的 **shutdown 或 shutdownNow** 方法来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的**interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止**。\n\n  \n\n- **shutdown()** 将线程池状态置为**shutdown,**并不会立即停止\n    \n    - 停止接收外部submit的任务\n        \n    - 内部正在跑的任务和队列里等待的任务，会执行完\n        \n    - 等到第二步完成后，才真正停止\n        \n- **shutdownNow()** 将线程池状态置为**stop**。一般会立即停止，事实上不一定：\n    \n    - 和shutdown()一样，先停止接收外部提交的任务\n        \n    - 忽略队列里等待的任务\n        \n    - 尝试将正在跑的任务interrupt中断\n        \n    - 返回未执行的任务列表\n        \n\nshutdown 和shutdownnow简单来说区别如下：\n\n- shutdownNow()能立即停止线程池，正在跑的和正在等待的任务都停下了。这样 做立即生效，但是风险也比较大。\n    \n- shutdown()只是关闭了提交通道，用submit()是无效的；而内部的任务该怎么跑还是怎么跑，跑完再彻底停止线程池\n    \n\n  \n\n# 线程池异常如何处理？\n\n![](statistic/asynccode-131.png)\n\n  \n\n# 线程池的几种状态\n\n线程池有这几个状态：RUNNING,SHUTDOWN,STOP,TIDYING,TERMINATED。\n\n![](statistic/asynccode-131.png)\n\n![](statistic/asynccode-131.png)\n\n- **RUNNING**\n    \n    - 该状态的线程池会接收新任务，并处理阻塞队列中的任务;\n        \n    - 调用线程池的shutdown()方法，可以切换到SHUTDOWN状态;\n        \n    - 调用线程池的shutdownNow()方法，可以切换到STOP状态;\n        \n- **SHUTDOWN**\n    \n    - 该状态的线程池不会接收新任务，但会处理阻塞队列中的任务；\n        \n    - 队列为空，并且线程池中执行的任务也为空,进入TIDYING状态;\n        \n- **STOP**\n    \n    - 该状态的线程不会接收新任务，也不会处理阻塞队列中的任务，而且会中断正在运行的任务；\n        \n    - 线程池中执行的任务为空,进入TIDYING状态;\n        \n- **TIDYING**\n    \n    - 该状态表明所有的任务已经运行终止，记录的任务数量为0。\n        \n    - terminated()执行完毕，进入TERMINATED状态\n        \n- **TERMINATED**\n    \n    - 该状态表示线程池彻底终止\n        \n\n  \n\n# 线程池调优了解吗？\n\n上线之前也要进行充分的测试，上线之后要建立完善的线程池监控机制。事中结合监控告警机制，分析线程池的问题，或者可优化点，结合线程池动态参数\n\n配置机制来调整配置。\n\n事后要注意仔细观察，随时调整\n\n![](statistic/asynccode-135.png)\n\n  \n\n# ForkJoin框架\n\n- 分而治之\n    \n\nFork/Join框架的定义，其实就体现了分治思想：将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同\n\n![](statistic/asynccode-129.png)\n\n![](statistic/asynccode-129.png)","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BA%BF%E7%A8%8B%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8%E6%A8%A1%E5%BC%8F-ThreadLocal":{"title":"线程本地存储模式 ThreadLocal","content":"已经知道通过**局部变量可以做到避免共享**，那还有没有其他方法可以做到呢？\n\n有的，Java 语言提供的**线程本地存储（ThreadLocal）**就能够做到\n\n  \n\n# ThreadLocal 的使用方法\n\n下面这个静态类 ThreadId 会为每个线程分配一个唯一的线程 Id，如果一个线程前后两次调用 ThreadId 的 get() 方法，两次 get() 方法的返回值是相同的。但如果是两个线程分别调用 ThreadId 的 get() 方法，那么两个线程看到的 get() 方法的返回值是不同的。\n\n```Go\n\nstatic class ThreadId {\n  static final AtomicLong nextId=new AtomicLong(0);\n  //定义ThreadLocal变量\n  static final ThreadLocal\u003cLong\u003e tl= ThreadLocal.withInitial(()-\u003enextId.getAndIncrement());\n  //此方法会为每个线程分配一个唯一的Id\n  static long get(){\n    return tl.get();\n  }\n}\n```\n\n你可能知道 SimpleDateFormat 不是线程安全的，那如果需要在并发场景下使用它，你该怎么办呢？\n\n```Go\n\nstatic class SafeDateFormat {\n  //定义ThreadLocal变量\n  static final ThreadLocal\u003cDateFormat\u003e\n  tl=ThreadLocal.withInitial(\n    ()-\u003e new SimpleDateFormat(\n      \"yyyy-MM-dd HH:mm:ss\"));\n      \n  static DateFormat get(){\n    return tl.get();\n  }\n}\n//不同线程执行下面代码\n//返回的df是不同的\nDateFormat df =\n  SafeDateFormat.get()；\n```\n\n同样地，不同线程调用 SafeDateFormat 的 get() 方法将返回不同的 SimpleDateFormat 对象实例，由于不同线程并不共享 SimpleDateFormat，所以就像局部变量一样，是线程安全的\n\n  \n\n# ThreadLocal的工作原理\n\n  \n\n## 猜测实现\n\nThreadLocal 的目标是让不同的线程有不同的变量 V，那最直接的方法就是创建一个 Map，它的 Key 是线程，Value 是每个线程拥有的变量 V，ThreadLocal 内部持有这样的一个 Map 就可以了\n\n![](statistic/asynccode-155.png)\n\n```Go\n\nclass MyThreadLocal\u003cT\u003e {\n  Map\u003cThread, T\u003e locals = \n    new ConcurrentHashMap\u003c\u003e();\n  //获取线程变量  \n  T get() {\n    return locals.get(\n      Thread.currentThread());\n  }\n  //设置线程变量\n  void set(T t) {\n    locals.put(\n      Thread.currentThread(), t);\n  }\n}\n```\n\n## ava实现\n\n  \n\nJava 的实现里面也有一个 Map，叫做 ThreadLocalMap，不过持有 ThreadLocalMap 的不是 ThreadLocal，而是 Thread。Thread 这个类内部有一个私有属性 threadLocals，其类型就是 ThreadLocalMap，ThreadLocalMap 的 Key 是 ThreadLocal。\n\n  \n\n![](statistic/asynccode-154.png)\n\n```Go\n\nclass Thread {\n  //内部持有ThreadLocalMap\n  ThreadLocal.ThreadLocalMap \n    threadLocals;\n}\nclass ThreadLocal\u003cT\u003e{\n  public T get() {\n    //首先获取线程持有的\n    //ThreadLocalMap\n    ThreadLocalMap map =\n      Thread.currentThread()\n        .threadLocals;\n    //在ThreadLocalMap中\n    //查找变量\n    Entry e = \n      map.getEntry(this);\n    return e.value;  \n  }\n  static class ThreadLocalMap{\n    //内部是数组而不是Map\n    Entry[] table;\n    //根据ThreadLocal查找Entry\n    Entry getEntry(ThreadLocal key){\n      //省略查找逻辑\n    }\n    //Entry定义\n    static class Entry extends\n    WeakReference\u003cThreadLocal\u003e{\n      Object value;\n    }\n  }\n}\n```\n\n很显然 Java 的实现更合理一些。在 Java 的实现方案里面，\n\n- ThreadLocal 仅仅是一个代理工具类，内部并不持有任何与线程相关的数据，所有和线程相关的数据都存储在 Thread 里面\n    \n- 还有一个更加深层次的原因，那就是不容易产生内存泄露\n    \n    - 在我们的设计方案中，ThreadLocal 持有的 Map 会持有 Thread 对象的引用，这就意味着，只要 ThreadLocal 对象存在，那么 Map 中的 Thread 对象就永远不会被回收\n        \n    - 而 Java 的实现中 Thread 持有 ThreadLocalMap，而且 ThreadLocalMap 里对 ThreadLocal 的引用还是弱引用（WeakReference），所以只要 Thread 对象可以被回收，那么 ThreadLocalMap 就能被回收\n        \n\n# ThreadLocal与内存泄漏\n\n  \n\n**在线程池中使用 ThreadLocal 为什么可能导致内存泄露呢**？\n\n原因就出在线程池中线程的存活时间太长，往往都是和程序同生共死的，这就意味着 Thread 持有的 ThreadLocalMap 一直都不会被回收，再加上 ThreadLocalMap 中的 Entry 对 ThreadLocal 是弱引用（WeakReference），所以只要 ThreadLocal 结束了自己的生命周期是可以被回收掉的。但是 Entry 中的 Value 却是被 Entry 强引用的，所以即便 Value 的生命周期结束了，Value 也是无法被回收的，从而导致内存泄露\n\n  \n\n那在线程池中，我们该如何正确使用 ThreadLocal 呢？\n\n，既然 JVM 不能做到自动释放对 Value 的强引用，那我们手动释放就可以了。如何能做到手动释放呢？估计你马上想到 try{}finally{}方案了，这个简直就是手动释放资源的利器\n\n```Go\n\nExecutorService es;\nThreadLocal tl;\nes.execute(()-\u003e{\n  //ThreadLocal增加变量\n  tl.set(obj);\n  try {\n    // 省略业务逻辑代码\n  }finally {\n    //手动清理ThreadLocal \n    tl.remove();\n  }\n});\n```\n\n  \n\n# InheritableThreadLocal 与继承性\n\n通过 ThreadLocal 创建的线程变量，其子线程是无法继承的。也就是说你在线程中**通过 ThreadLocal 创建了线程变量 V，而后该线程创建了子线程，你在子线程中是无法通过 ThreadLocal 来访问父线程的线程变量 V 的**.\n\nJava 提供了 **InheritableThreadLocal** 来支持这种特性，InheritableThreadLocal 是 ThreadLocal 子类，所以用法和 ThreadLocal 相同\n\n  \n\n不过，我完全不建议你在线程池中使用 InheritableThreadLocal\n\n- 可能导致内存泄露","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/Copy-on-Write":{"title":"Copy-on-Write","content":"![](statistic/asynccode-156.png)\n\n每次 RPC 调用都需要通过负载均衡器来计算目标服务的 IP 和端口号，而负载均衡器需要通过路由表获取接口的所有路由信息，也就是说，每次 RPC 调用都需要访问路由表，所以访问路由表这个操作的性能要求是很高的**。不过路由表对数据的一致性要求并不高，一个服务提供方从上线到反馈到客户端的路由表里，即便有 5 秒钟，很多时候也都是能接受的**（5 秒钟，对于以纳秒作为时钟周期的 CPU 来说，那何止是一万年，所以路由表对一致性的要求并不高）。\n\n  \n\n  \n\n对读的性能要求很高，读多写少，弱一致性。它们综合在一起，你会想到什么呢？CopyOnWriteArrayList 和 CopyOnWriteArraySet 天生就适用这种场景啊。\n\n  \n\nRouteTable 这个类内部我们通过ConcurrentHashMap\u003e这个数据结构来描述路由表，ConcurrentHashMap 的 Key 是接口名，Value 是路由集合，这个路由集合我们用是 CopyOnWriteArraySet\n\n  \n\n再来思考 Router 该如何设计，服务提供方的每一次上线、下线都会更新路由信息，这时候你有两种选择。一种是通过更新 Router 的一个状态位来标识，如果这样做，那么所有访问该状态位的地方都需要同步访问，这样很影响性能。另外一种就是采用 Immutability 模式，每次上线、下线都创建新的 Router 对象或者删除对应的 Router 对象。由于上线、下线的频率很低，所以后者是最好的选择。\n\nRouter 的实现代码如下所示，是一种典型 Immutability 模式的实现，需要你注意的是我们重写了 equals 方法，这样 CopyOnWriteArraySet 的 add() 和 remove() 方法才能正常工作\n\n```Go\n\n//路由信息\npublic final class Router{\n  private final String  ip;\n  private final Integer port;\n  private final String  iface;\n  //构造函数\n  public Router(String ip, \n      Integer port, String iface){\n    this.ip = ip;\n    this.port = port;\n    this.iface = iface;\n  }\n  //重写equals方法\n  public boolean equals(Object obj){\n    if (obj instanceof Router) {\n      Router r = (Router)obj;\n      return iface.equals(r.iface) \u0026\u0026\n             ip.equals(r.ip) \u0026\u0026\n             port.equals(r.port);\n    }\n    return false;\n  }\n  public int hashCode() {\n    //省略hashCode相关代码\n  }\n}\n//路由表信息\npublic class RouterTable {\n  //Key:接口名\n  //Value:路由集合\n  ConcurrentHashMap\u003cString, CopyOnWriteArraySet\u003cRouter\u003e\u003e \n    rt = new ConcurrentHashMap\u003c\u003e();\n  //根据接口名获取路由表\n  public Set\u003cRouter\u003e get(String iface){\n    return rt.get(iface);\n  }\n  //删除路由\n  public void remove(Router router) {\n    Set\u003cRouter\u003e set=rt.get(router.iface);\n    if (set != null) {\n      set.remove(router);\n    }\n  }\n  //增加路由\n  public void add(Router router) {\n    Set\u003cRouter\u003e set = rt.computeIfAbsent(\n      route.iface, r -\u003e \n        new CopyOnWriteArraySet\u003c\u003e());\n    set.add(router);\n  }\n}\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/Immutability%E6%A8%A1%E5%BC%8F%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E8%A7%A3%E5%86%B3%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98":{"title":"Immutability模式：如何利用不变性解决并发问题","content":"Immutability 模式：如何利用不变性解决并发问题\n\n解决并发问题，其实最简单的办法就是让共享变量只有读操作，而没有写操作\n\n**不变性（Immutability）模式**。所谓不变性，简单来讲，就是对象一旦被创建之后，状态就不再发生变化\n\n  \n\n# 快速实现具备不可变性的类\n\n  \n\n将一个类所有的**属性都设置成 final 的，并且只允许存在只读方法**，那么这个类基本上就具备不可变性了，更严格的做法是**这个类本身也是 final 的**，也就是不允许继承。\n\n  \n\nJava SDK 里很多类都具备不可变性，如经常用到的 String 和 Long、Integer、Double 等基础类型的包装类都具备不可变性，\n\n  \n\n如果你仔细翻看这些类的声明、属性和方法，你会发现它们都严格遵守不可变类的三点要求：**类和属性都是 final 的，所有方法均是只读的**。\n\n  \n\n如果具备不可变性的类，需要提供类似修改的功能，具体该怎么操作呢？做法很简单，那就是创建一个新的不可变对象，这是与可变对象的一个重要区别，可变对象往往是修改自己的属性。\n\n  \n\n  \n\n**你可能会有这种担心：是不是创建的对象太多了，有点太浪费内存呢**？是的，这样做的确有些浪费，那如何解决呢\n\n  \n\n# 利用享元模式避免创建重复对象\n\n  \n\n  \n\n享元模式（Flyweight Pattern）。利用享元模式可以减少创建对象的数量，从而减少内存占用。Java 语言里面 Long、Integer、Short、Byte 等这些基本数据类型的包装类都用到了享元模式。\n\n  \n\n  \n\nLong的享元模式\n\n```Go\n\nLong valueOf(long l) {\n  final int offset = 128;\n  // [-128,127]直接的数字做了缓存\n  if (l \u003e= -128 \u0026\u0026 l \u003c= 127) { \n    return LongCache\n      .cache[(int)l + offset];\n  }\n  return new Long(l);\n}\n//缓存，等价于对象池\n//仅缓存[-128,127]直接的数字\nstatic class LongCache {\n  static final Long cache[] \n    = new Long[-(-128) + 127 + 1];\n\n  static {\n    for(int i=0; i\u003ccache.length; i++)\n      cache[i] = new Long(i-128);\n  }\n}\n```\n\n享元模式本质上其实就是一个对象池，利用享元模式创建对象的逻辑也很简单：创建之前，首先去对象池里看看是不是存在；如果已经存在，就利用对象池里的对象；如果不存在，就会新创建一个对象，并且把这个新创建出来的对象放进对象池里.\n\n  \n\n“Integer 和 String 类型的对象不适合做锁”，其实基本上所有的基础类型的包装类都不适合做锁，因为它们内部用到了享元模式，**这会导致看上去私有的锁，其实是共有的**\n\n  \n\n# 使用Immutability模式的注意事项\n\n1. 对象的所有属性都是 final 的，并不能保证不可变性；\n    \n2. 不可变对象也需要正确发布。\n    \n\n  \n\n**在使用 Immutability 模式的时候一定要确认保持不变性的边界在哪里**，是否要求属性对象也具备不可变性\n\n```Go\n\nclass Foo{\n  int age=0;\n  int name=\"abc\";\n}\nfinal class Bar {\n  final Foo foo;\n  void setAge(int a){\n    foo.age=a;\n  }\n}\n```\n\n**不可变对象虽然是线程安全的，但是并不意味着引用这些不可变对象的对象就是线程安全的**。\n\n```Go\n\n//Foo线程安全\nfinal class Foo{\n  final int age=0;\n  final int name=\"abc\";\n}\n//Bar线程不安全\nclass Bar {\n  Foo foo;\n  void setFoo(Foo f){\n    this.foo=f;\n  }\n}\n```\n\n**具备不变性的对象，只有一种状态，这个状态由对象内部所有的不变属性共同决定**。\n\n其实还有一种更简单的不变性对象，那就是**无状态**。\n\n无状态对象内部没有属性，只有方法。除了无状态的对象，你可能还听说过无状态的服务、无状态的协议等等。无状态有很多好处**，最核心的一点就是性能**。在多线程领域，**无状态对象没有线程安全问题**，无需同步处理，自然性能很好；在分布式领域，无状态意味着可以无限地水平扩展，所以分布式领域里面性能的瓶颈一定不是出在无状态的服务节点上。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E9%94%81":{"title":"锁","content":"# 乐观锁和悲观锁\n\n  \n\n- **悲观锁**：悲观锁总是假设最坏的情况，认为**共享资源每次被访问的时候就会出现问题**(比如共享数据被修改)，所以每次在**获取资源操作的时候都会上锁，这样其他线程想拿到这个资源就会阻塞**直到锁被上一个持有者释放\n    \n    - 共享资源每次**只给一个线程使用，其它线程阻塞**，用完后再把资源转让给其它线程\n        \n    - Java 中`synchronized`和`ReentrantLock`等独占锁就是悲观锁思想的实现\n        \n    - 悲观锁通常多用于写多比较多的情况下（多写场景），避免频繁失败和重试影响性能\n        \n- 乐观锁：总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，**只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了（具体方法可以使用版本号机制或 CAS 算法**）\n    \n    - `java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。\n        \n    - 乐观锁通常多于写比较少的情况下（多读场景），避免频繁加锁影响性能，大大提升了系统的吞吐量\n        \n\n## 如何实现乐观锁\n\n  \n\n乐观锁一般会使用版本号机制或 CAS 算法实现，CAS 算法相对来说更多一些\n\n### cas算法\n\nCAS 的全称是 Compare And Swap（比较与交换） ，用于实现乐观锁，被广泛应用于各大框架中。CAS 的思想很简单，**就是用一个预期值和要更新的变量值进行比较，两值相等才会进行更新**\n\n- CAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。\n    \n\n\u003e CAS 涉及到三个操作数：\n\u003e \n\u003e - V ：要更新的变量值(Var)\n\u003e     \n\u003e - E ：预期值(Expected)\n\u003e     \n\u003e - N ：拟写入的新值(New)\n\u003e     \n\u003e \n\u003e 当且仅当 V 的值等于 E 时，CAS 通过原子方式用新值 N 来更新 V 的值。如果不等，说明已经有其它线程更新了 V，则当前线程放弃更新。\n\n  \n\n`sun.misc`包下的`Unsafe`类提供了`compareAndSwapObject`、`compareAndSwapInt`、`compareAndSwapLong`方法来实现的对`Object`、`int`、`long`类型的 CAS 操作\n\n```Go\n/**\n        *  CAS\n  * @param o         包含要修改field的对象\n  * @param offset    对象中某field的偏移量\n  * @param expected  期望值\n  * @param update    更新值\n  * @return          true | false\n  */\npublic final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);\n\npublic final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);\n\npublic final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);\n```\n\n关于 `Unsafe` 类的详细介绍可以看这篇文章：[Java 魔法类 Unsafe 详解 - JavaGuide - 2022](https://javaguide.cn/java/basis/unsafe.html)\n\n### 版本号机制\n\n一般是在数据表中加上一个数据版本号 `version` 字段，表示数据被修改的次数。\n\n- 当数据被修改时，`version` 值会加一。\n    \n- 当线程 A 要更新数据值时，在读取数据的同时也会读取 `version` 值，\n    \n    - 在提交更新时，若刚才读取到的 version 值为当前数据库中的 `version` 值相等时才更新，\n        \n    - 否则重试更新操作，直到更新成功。\n        \n\n  \n\n## 乐观锁存在的问题\n\n  \n\n### ABA问题\n\n- 如果一个变量 V 初次读取的时候是 A 值，**在这段时间它的值可能被改为其他值，然后又改回 A**，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 **\"ABA\"问题。**\n    \n- 解决思路 ：变量前面追加上版本号或者时间戳\n    \n    - `AtomicStampedReference` 类就是用来解决 ABA 问题的\n        \n    \n    ```Go\n    public boolean compareAndSet(V   expectedReference,\n                                 V   newReference,\n                                 int expectedStamp,\n                                 int newStamp) {\n        Pair\u003cV\u003e current = pair;\n        return\n            expectedReference == current.reference \u0026\u0026\n            expectedStamp == current.stamp \u0026\u0026\n            ((newReference == current.reference \u0026\u0026\n              newStamp == current.stamp) ||\n             casPair(current, Pair.of(newReference, newStamp)));\n    }\n    ```\n    \n\n  \n\n  \n\n### 循环时间开销大\n\nCAS 经常会用到自旋操作来进行重试，也就是不成功就一直循环执行直到成功。如果长时间不成功，会给 CPU 带来非常大的执行开销\n\n如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用：\n\n1. 可以延迟流水线执行指令，使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。\n    \n2. 可以避免在退出循环的时候因内存顺序冲而引起 CPU 流水线被清空，从而提高 CPU 的执行效率\n    \n\n  \n\n#### 只能保证一个共享变量的原子操作\n\n**CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效**。\n\n- `AtomicReference`类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用`AtomicReference`类把多个共享变量合并成一个共享变量来操作。\n    \n\n  \n\n## Java 有那些保证原子性的方法。如何保证多线程下i++结果正确？\n\n![](statistic/asynccode-91.png)\n\n  \n\n  \n\n## 原子操作类了解多少\n\n在Atomic包里一共提供了13个类，属于4种类型的原子更新方式，分别是**原子更新基本类型、原子更新数组、原子更新引用和原子更新属性（字段）**\n\n![](statistic/asynccode-103.png)\n\nAtomic包里的类基本都是使用Unsafe实现的包装类。\n\n- 更新基本类型，\n    \n    - AtomicBoolean：原子更新布尔类型。\n        \n    - AtomicInteger：原子更新整型。\n        \n    - AtomicLong：原子更新长整型。\n        \n- 通过原子的方式更新数组里的某个元素，Atomic包提供了以下4个类：\n    \n    - AtomicIntegerArray：原子更新整型数组里的元素。\n        \n    - AtomicLongArray：原子更新长整型数组里的元素。\n        \n    - AtomicReferenceArray：原子更新引用类型数组里的元素。\n        \n    - AtomicIntegerArray：类主要是提供原子的方式更新数组里的整型\n        \n- 原子更新引用类型提供的类。\n    \n    - AtomicReference：原子更新引用类型。AtomicReferenceFieldUpdater：原子更新引用类型里的字段。\n        \n    - AtomicMarkableReference：原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型。构造方法是AtomicMarkableReference（V initialRef，boolean initialMark）。\n        \n- 原子更新字段类，：\n    \n    - AtomicIntegerFieldUpdater：原子更新整型的字段的更新器。\n        \n    - AtomicLongFieldUpdater：原子更新长整型字段的更新器。\n        \n    - AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的 ABA问题。\n        \n\n## **AtomicInteger** 的原理\n\n使用CAS\n\n以AtomicInteger的添加方法为例：\n\n![](statistic/asynccode-100.png)\n\n通过 Unsafe 类的实例来进行添加操作，来看看具体的CAS操作：\n\n![](statistic/asynccode-89.png)\n\ncompareAndSwapInt 是一个native方法，基于CAS来操作int类型变量。其它的原子操作类基本都是大同小异。\n\n  \n\n# synchronized\n\n## synchronized是啥？有啥用？\n\nSynchronized 是Java的一个关键字，翻译成中文是**同步的意思**，主要解决的是多个线程之间访问资源的同步性，可以保证被**它修饰的方法或者代码块在任意时刻只能有一个线程执行**。\n\n  \n\n- 早期版本中，`synchronized` 属于**重量级锁，**这是因为**监视器锁（monitor）是依赖于底层的操作系统的** **`Mutex Lock`** **来实现的**，Java 的线程是映射到操作系统的原生线程之上的。而操作系统实现线程之间的切换时需要从**用户态转换到内核态**，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。\n    \n- java6 以后，引入了大量的优化如**自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销**，这些优化让 `synchronized` 锁的效率提升了很多\n    \n\n  \n\n## 如何使用 synchronized？\n\n`synchronized` 关键字的使用方式主要有下面 3 种：\n\n1. 修饰实例方法\n    \n2. 修饰静态方法\n    \n3. 修饰代码块\n    \n\n**1、修饰实例方法** （**锁当前对象实例**）\n\n给当前对象实例加锁，进入同步代码前要获得 **当前对象实例的锁** 。\n\n```Java\nsynchronized void method() {\n    //业务代码\n}\n```\n\n**2、修饰静态方法** （**锁当前类**）\n\n给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 **当前 class 的锁**。\n\n这是因为静态成员不属于任何一个实例对象，归整个类所有，不依赖于类的特定实例，被类的所有实例共享。\n\n```Java\nsynchronized static void method() {\n    //业务代码\n}\n```\n\n静态 `synchronized` 方法和非静态 `synchronized` 方法之间的调用互斥么？\n\n不互斥！因为他们的锁不一致\n\n**3、修饰代码块** （锁指定对象/类）\n\n对括号里指定的对象/类加锁：\n\n- `synchronized(object)` 表示进入同步代码库前要获得 **给定对象的锁**。\n    \n- `synchronized(类.class)` 表示进入同步代码前要获得 **给定 Class 的锁**\n    \n\n```Java\nsynchronized(this) {\n    //业务代码\n}\n```\n\n**总结：**\n\n- `synchronized` 关键字加到 `static` 静态方法和 `synchronized(class)` 代码块上都是是给 Class 类上锁；\n    \n- `synchronized` 关键字加到实例方法上是给对象实例上锁；\n    \n- 尽量不要使用 `synchronized(String a)` 因为 JVM 中，字符串常量池具有缓存功能。\n    \n\n## 构造方法可以用 synchronized 修饰么？\n\n先说结论：**构造方法不能使用 synchronized 关键字修饰。**\n\n构造方法本身就属于线程安全的，不存在同步的构造方法一说。\n\n## synchronized 底层原理了解吗？\n\nsynchronized 关键字底层原理属于 JVM 层面的东西。\n\n### synchronized 同步语句块的情况\n\n- synchronized修饰代码块时，JVM采用 monitorenter 、 monitorexit 两个指令来实现同步， monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指向同步代码块的结束位置。\n    \n\n```Java\npublic class SynchronizedDemo {\n    public void method() {\n        synchronized (this) {\n            System.out.println(\"synchronized 代码块\");\n        }\n    }\n}\n\n```\n\n  \n\n![](statistic/asynccode-94.png)\n\n当执行 `monitorenter` 指令时，线程试图获取锁也就是获取 **对象监视器** **`monitor`** **的持有权**。\n\n\u003e 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由[ObjectMonitoropen in new window](https://github.com/openjdk-mirror/jdk7u-hotspot/blob/50bdefc3afe944ca74c3093e7448d6b889cd20d1/src/share/vm/runtime/objectMonitor.cpp)实现的。每个对象中都内置了一个 `ObjectMonitor`对象。\n\u003e \n\u003e 另外，`wait/notify`等方法也依赖于`monitor`对象，这就是为什么只有在同步的块或者方法中才能调用`wait/notify`等方法，否则会抛出`java.lang.IllegalMonitorStateException`的异常的原因。\n\n  \n\n### synchronized 修饰方法的的情况\n\n**JVM采用 ACC_SYNCHRONIZED 标记符来实现同 步，这个标识指明了该方法是一个同步方法。**\n\n```Java\npublic class SynchronizedDemo2 {\n    public synchronized void method() {\n        System.out.println(\"synchronized 方法\");\n    }\n}\n\n```\n\n![](statistic/asynccode-88.png)\n\n### Synchronized锁住的是啥？\n\nmonitorenter、monitorexit或者ACC_SYNCHRONIZED都是基于**Monitor**实现的。\n\n- 实例对象结构里有对象头，对象头里面有一块结构叫Mark Word，Mark Word指针指 向了**monitor**。\n    \n- 所谓的Monitor其实是一种同步工具，也可以说是一种同步机制。在Java虚拟机 HotSpot）中，Monitor是由**ObjectMonitor**实现的，可以叫做内部锁，或者Monitor 锁\n    \n\n#### ObjectMonitor的工作原理\n\n- ObjectMonitor有两个队列： WaitSet、 EntryList，用来保存ObjectWaiter 对象列表\n    \n- _owner，获取 Monitor 对象的线程进入 _owner 区时， _count + 1。如果线程调用了 wait() 方法，此时会释放 Monitor 对象， _owner 恢复为空， _count - 1。同时该等待线程进入 _WaitSet 中，等待被唤醒。\n    \n\n![](statistic/asynccode-96.png)\n\n![](statistic/asynccode-101.png)\n\n同步对象锁住的是啥了\n\n- monitorenter，在判断拥有同步标识 ACC_SYNCHRONIZED 抢先进入此方法的线 程会优先拥有 Monitor 的 owner ，此时计数器 +1。\n    \n- monitorexit，当执行完退出后，计数器 -1，归 0 后被其他进入的线程获得。\n    \n\n在执行`monitorenter`时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。\n\n![](statistic/asynccode-104.png)\n\n对象锁的的拥有者线程才可以执行 `monitorexit` 指令来释放锁。**在执行** **`monitorexit`** **指令后，将锁计数器设为 0，**表明锁被释放，其他线程可以尝试获取锁。\n\n![](statistic/asynccode-90.png)\n\n如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。\n\n\u003e 相关推荐：[Java 锁与线程的那些事 - 有赞技术团队open in new window](https://tech.youzan.com/javasuo-yu-xian-cheng-de-na-xie-shi/) 。\n\n  \n\n## **synchronized**可见性，有序性，可重入性怎么实现？\n\n### synchronized 怎么保证可见性？\n\n- 线程加锁前，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值。\n    \n- 线程加锁后，其它线程无法获取主内存中的共享变量。\n    \n- 线程解锁前，必须把共享变量的最新值刷新到主内存中\n    \n\n  \n\n### synchronized 怎么保证有序性\n\n- synchronized同步的代码块，具有排他性，一次只能被一个线程拥有，所以 synchronized保证同一时刻，代码是单线程执行的\n    \n- as-if-serial语义的存在，单线程的程序能保证最终结果是有序的，但是不保证不会指令重排。\n    \n\nsynchronized保证的有序是**执行结果的有序性，而不是防止指令重排的有序性**\n\n  \n\n### synchronized怎么实现可重入的？\n\nsynchronized 是可重入锁，也就是说，**允许一个线程二次请求自己持有对象锁的临界资源**，这种情况称为可重入锁。\n\n之所以，是可重入的。是因为 synchronized 锁对象有个计数器，会随着**线程获取锁后 +1 计数，当线程执行完毕后 -1，直到清零释放锁**。\n\n  \n\n## JDK1.6 之后的 synchronized 底层做了哪些优化？\n\n### 做了那些优化？\n\nJDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。\n\n- 偏向锁：在**无竞争的情况下**，只是在**Mark Word里存储当前线程指针**，CAS操作都不做。\n    \n- 轻量级锁：在**没有多线程竞争时**，相对重量级锁，**减少操作系统互斥量**带来的性能消耗。但是，如果存在锁竞争，除了互斥量本身开销，还**额外有CAS操作的开销**。\n    \n- 自旋锁：减少不必要的CPU上下文切换。**在轻量级锁升级为重量级锁时，就使用了自旋加锁的方式**\n    \n- 锁粗化：**将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁**。\n    \n- 锁消除：虚拟机即时编译器在运行时，对一些代码上要求同步，但是被**检测到不可能存在共享数据竞争的锁进行消除**。\n    \n\n### 锁升级\n\n- 锁主要存在四种状态，依次是：**无锁状态、偏向锁状态、轻量级锁状态、重量级锁**\n    \n- **锁可以升级不可降级**，这种策略是为了提高获得锁和释放锁的效率。\n    \n\n关于这几种优化的详细信息可以查看下面这篇文章：[Java6 及以上版本对 synchronized 的优化open in new window](https://www.cnblogs.com/wuqinglong/p/9945618.html) 。\n\n  \n\n#### 各个锁的说明\n\n- 偏向锁：**针对于一个线程而言的**，线程**获得锁之后就不会再有解锁等操作了**，这样可以省略很多开销。假如有**两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁**了\n    \n- 轻量级锁：之所以是轻量级，是因为它**仅仅使用 CAS 进行操作**，实现获取锁。\n    \n- 重量级锁：重量级锁（`heavy weight lock`），是使用**操作系统互斥量（****`mutex`****）来实现的传统锁**。 当所有对锁的优化都失效时，将退回到重量级锁。它与轻量级锁不同竞争的线程不再通过自旋来竞争线程， **而是直接进入堵塞状态，此时不消耗CPU，然后等拥有锁的线程释放锁后，唤醒堵塞的线程**， 然后线程再次竞争锁\n    \n\n  \n\n#### MarkWord 的内容\n\n##### **1、无锁状态**\n\n|   |   |   |   |\n|---|---|---|---|\n|25bit|4bit|1bit(是否是偏向锁)|2bit(锁标志位)|\n|对象的hashCode|对象分代年龄|0|1|\n\n这里的 hashCode 是 Object#hashCode 或者 System#identityHashCode 计算出来的值，不是用户覆盖产生的 hashCode。\n\n##### **2、偏向锁状态**\n\n|   |   |   |   |   |\n|---|---|---|---|---|\n|23bit|2bit|4bit|1bit|2bit|\n|线程ID|epoch|对象分代年龄|1|1|\n\n这里 **线程ID** 和 epoch 占用了 hashCode 的位置，所以，如果对象如果计算过 identityHashCode 后，便无法进入偏向锁状态，反过来，如果对象处于偏向锁状态，并且需要计算其 identityHashCode 的话，则偏向锁会被撤销，升级为重量级锁。\n\nepoch：\n\n对于偏向锁，如果 线程ID = 0 表示未加锁。\n\n##### **3、轻量级锁状态**\n\n|   |   |\n|---|---|\n|30bit|2bit|\n|指向线程栈锁记录的指针|0|\n\n这里指向**栈帧中的 Lock Record 记录**，里面当然可以记录对象的 identityHashCode。\n\n##### **4、重量级锁状态**\n\n|   |   |\n|---|---|\n|30bit|2bit|\n|指向锁监视器的指针|10|\n\n这里**指向了内存中对象的 ObjectMonitor 对象**，而 ObectMontitor 对象可以存储对象的 identityHashCode 的值\n\n  \n\n#### **1、偏向锁**\n\n偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了。\n\n\u003e _为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。_\n\n如果支持偏向锁（没有计算 hashCode），那么在分配对象时，分配一个可偏向而未偏向的对象（MarkWord的最后 3 位为 101，并且 Thread Id 字段的值为 0）。\n\n##### **a、偏向锁的加锁**\n\n1. 偏向锁标志是未偏向状态，**使用 CAS 将 MarkWord 中的线程ID设置为自己的线程ID**，\n    \n    1. 如果成功，则获取偏向锁成功。\n        \n    2. 如果失败，则进行锁升级。\n        \n2. 偏向锁标志是已偏向状态\n    \n    1. MarkWord 中的线程 ID 是自己的线程 ID，成功获取锁\n        \n    2. MarkWord 中的线程 ID 不是自己的线程 ID，需要进行锁升级\n        \n\n偏向锁的锁升级需要进行偏向锁的撤销。\n\n##### **b、偏向锁的撤销**\n\n1. **偏向锁不会主动释放(撤销)**，只有遇到其他线程竞争时才会执行撤销，由于撤销需要知道当前持有该偏向锁的可偏向状态。因此要等到safepoint时执行。\n    \n2. 判断是否是可偏向状态（**MarkWord中锁标志是否为‘01’，是否偏向锁是否为‘1’** ）\n    \n3. 对象是不可偏向状态\n    \n    1. 不需要撤销\n        \n4. 对象是可偏向状态\n    \n    1. MarkWord 中指向的线程不存活\n        \n        1. 允许重偏向：退回到可偏向但未偏向的状态\n            \n        2. 不允许重偏向：变为无锁状态\n            \n    2. MarkWord 中的线程存活\n        \n        1. 线程ID指向的线程仍然拥有锁\n            \n            1. 升级为轻量级锁，将 mark word 复制到线程栈中\n                \n        2. 不再拥有锁\n            \n            1. 允许重偏向：退回到可偏向但未偏向的状态\n                \n            2. 不允许重偏向：变为无锁状态\n                \n\n  \n\n偏向锁的撤销流程\n\n![](statistic/asynccode-98.png)\n\n## **2、轻量级锁**\n\n之所以是轻量级，是因为它仅仅使用 CAS 进行操作，实现获取锁。\n\n#### **a、加锁流程**\n\n1. 进行加锁操作时，jvm会判断是否已经是**重量级锁**，如果不是，则会在当前线程栈帧中划出一块空间，作为该锁的锁记录，并且将锁对象MarkWord复制到该锁记录中\n    \n2. 复制成功之后，jvm使用CAS操作将对象头MarkWord更新为**指向锁记录的指针，**并将锁记录里的owner指针指向对象头的MarkWord。\n    \n    1. 更新成功，则当前线程持有该对象锁，并且对象MarkWord锁标志设置为‘00’，即表示此对象处于轻量级锁状态\n        \n    2. 更新失败，jvm先检查对象MarkWord是否指向当前线程栈帧中的锁记录，如果是则执行‘5’，否则执行‘4’\n        \n        1. 如果是表示锁重入；然后当前线程栈帧中增加一个锁记录第一部分（Displaced Mark Word）为null，并指向Mark Word的锁对象，起到一个重入计数器的作用。\n            \n        2. 如果不是，表示该锁对象已经被其他线程抢占，则进行 自旋等待 （默认10次），等待次数达到阈值仍未获取到锁，则升级为重量级锁\n            \n\n##### **b、撤销流程**\n\n轻量级锁解锁时，如果对象的Mark Word仍然指向着线程的锁记录，会使用CAS操作， 将Dispalced Mark Word替换到对象头，如果成功，则表示没有竞争发生。如果失败， 表示当前锁存在锁竞争，锁就会膨胀为重量级锁。\n\n## **3、重量级锁**\n\n重量级锁（`heavy weight lock`），是使用操作系统互斥量（`mutex`）来实现的传统锁。 当所有对锁的优化都失效时，将退回到重量级锁。它与轻量级锁不同竞争的线程不再通过自旋来竞争线程， 而是直接进入堵塞状态，此时不消耗CPU，然后等拥有锁的线程释放锁后，唤醒堵塞的线程， 然后线程再次竞争锁。但是注意，当锁膨胀（`inflate`）为重量锁时，就不能再退回到轻量级锁。\n\n  \n\n大体的升级流程\n\n![](statistic/asynccode-92.png)\n\n完整的升级流程\n\n![](statistic/asynccode-99.png)\n\n## synchronized 和 volatile 有什么区别？\n\n`synchronized` 关键字和 `volatile` 关键字是两个互补的存在，而不是对立的存在！\n\n- `volatile` 关键字是线程同步的轻量级实现，所以 `volatile`性能肯定比`synchronized`关键字要好 。但是 `volatile` 关键字只能用于变量而 `synchronized` 关键字可以修饰方法以及代码块 。\n    \n- `volatile` 关键字能保证数据的可见性，但不能保证数据的原子性。`synchronized` 关键字两者都能保证。\n    \n- `volatile`关键字主要用于解决变量在多个线程之间的可见性，而 `synchronized` 关键字解决的是多个线程之间访问资源的同步性。\n    \n\n# ReentrantLock\n\n## ReentrantLock 是什么？\n\n`ReentrantLock` 实现了 `Lock` 接口，是一个**可重入且独占式的锁**，和 `synchronized` 关键字类似。不过，`ReentrantLock` 更灵活、更强大，增加了轮询、超时、中断、公平锁和非公平锁等高级功能。\n\n```Java\npublic class ReentrantLock implements Lock, java.io.Serializable {}\n```\n\n`ReentrantLock` 里面有一个**内部类** **`Sync`**，`Sync` 继承 AQS（`AbstractQueuedSynchronizer`），添加锁和释放锁的大部分操作实际上都是在 `Sync` 中实现的。`Sync` 有公平锁 `FairSync` 和非公平锁 `NonfairSync` 两个子类。\n\n![](statistic/asynccode-95.png)\n\n`ReentrantLock` 默认使用非公平锁，也可以通过构造器来显示的指定使用公平锁。\n\n```Java\n// 传入一个 boolean 值，true 时为公平锁，false 时为非公平锁\npublic ReentrantLock(boolean fair) {\n    sync = fair ? new FairSync() : new NonfairSync();\n}\n```\n\n从上面的内容可以看出， `ReentrantLock` 的底层就是由 AQS 来实现的。关于 AQS 的相关内容推荐阅读 [AQS 详解open in new window](https://javaguide.cn/java/concurrent/aqs.html) 这篇文章。\n\n## AQS 介绍\n\n[AQS 详解](https://javaguide.cn/java/concurrent/aqs.html)\n\n![](statistic/asynccode-97.png)\n\nAQS 的全称为 `AbstractQueuedSynchronizer` ，翻译过来的意思就是抽象队列同步器。这个类在 `java.util.concurrent.locks` 包下面。\n\n![](statistic/asynccode-101.png)\n\nAQS 就是一个抽象类，主要用来构建锁和同步器。\n\n```Java\npublic abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable {\n}\n```\n\nAQS 为构建锁和同步器提供了一些通用功能的是实现，因此，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 `ReentrantLock`，`Semaphore`，其他的诸如 `ReentrantReadWriteLock`，`SynchronousQueue`等等皆是基于 AQS 的。\n\n### AQS 原理\n\n在面试中被问到并发知识的时候，大多都会被问到“请你说一下自己对于 AQS 原理的理解”。下面给大家一个示例供大家参考，面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来。\n\n### AQS 核心思想\n\nAQS 核心思想是，\n\n- **如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态**。\n    \n- 如果被请求的共享资源被占用，那么就需要**一套线程阻塞等待以及被唤醒时锁分配的机制**，这个机制 AQS 是基于 **CLH 锁** （Craig, Landin, and Hagersten locks） 实现的。\n    \n    - CLH 锁是对自旋锁的一种改进，**是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系），暂时获取不到锁的线程将被加入到该队列中。AQS 将每条请求共享资源的线程封装成一个 CLH 队列锁的一个结点（Node）来实现锁的分配**。在 CLH 队列锁中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）。\n        \n\nCLH 队列锁结构如下图所示：\n\n![](statistic/asynccode-102.png)\n\n关于AQS 核心数据结构-CLH 锁的详细解读，强烈推荐阅读 [Java AQS 核心数据结构-CLH 锁 - Qunar技术沙龙open in new window](https://mp.weixin.qq.com/s/jEx-4XhNGOFdCo4Nou5tqg) 这篇文章。\n\nAQS(`AbstractQueuedSynchronizer`)的核心原理图（图源[Java 并发之 AQS 详解open in new window](https://www.cnblogs.com/waterystone/p/4920797.html)）如下：\n\n![](statistic/asynccode-93.png)\n\nAQS 使用 **int 成员变量** **`state`** **表示同步状态**，通过内置的 **线程等待队列** 来完成获取资源线程的排队工作。\n\n`state` 变量由 `volatile` 修饰，用于展示当前临界资源的获锁情况。\n\n```Java\n// 共享变量，使用volatile修饰保证线程可见性\nprivate volatile int state;\n```\n\n另外，状态信息 `state` 可以通过 `protected` 类型的`getState()`、`setState()`和`compareAndSetState()` 进行操作。并且，这几个方法都是 `final` 修饰的，在子类中无法被重写。\n\n```Java\n//返回同步状态的当前值\nprotected final int getState() {\n     return state;\n}\n // 设置同步状态的值\nprotected final void setState(int newState) {\n     state = newState;\n}\n//原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）\nprotected final boolean compareAndSetState(int expect, int update) {\n      return unsafe.compareAndSwapInt(this, stateOffset, expect, update);\n}\n```\n\n以 `ReentrantLock` 为例，**`state`** **初始值为 0，表示未锁定状态**。\n\n- A 线程 `lock()` 时，会调用 `tryAcquire()` 独占该锁并将 `state+1` 。此后，其他线程再 `tryAcquire()` 时就会失败，\n    \n- 直到 A 线程 `unlock()` 到 `state=`0（即释放锁）为止，其它线程才有机会获取该锁。**当然，释放锁之前，A 线程自己是可以重复获取此锁的（****`state`** **会累加），这就是可重入的概念**。但要注意，获取多少次就要释放多少次，这样才能保证 state 是能回到零态的。相关阅读：[从ReentrantLock的实现看AQS的原理及应用](https://javaguide.cn/java/concurrent/reentrantlock.html)\n    \n\n再以 `CountDownLatch` 以例，\n\n- 任务分为 N 个子线程去执行，`state` 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，\n    \n- 每个子线程执行完后`countDown()` 一次，state 会 CAS(Compare and Swap) 减 1。\n    \n- 等到所有子线程都执行完后(即 `state=0` )，会 `unpark()` 主调用线程，然后主调用线程就会从 `await()` 函数返回，继续后余动作。\n    \n\n### AQS 资源共享方式\n\nAQS 定义两种资源共享方式：`Exclusive`（独占，只有一个线程能执行，如`ReentrantLock`）和`Share`（共享，多个线程可同时执行，如`Semaphore`/`CountDownLatch`）。\n\n一般来说，自定义同步器的共享方式要么是独占，要么是共享，他们也只需实现`tryAcquire-tryRelease`、`tryAcquireShared-tryReleaseShared`中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如`ReentrantReadWriteLock`。\n\n### 自定义同步器\n\n同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：\n\n1. 使用者继承 `AbstractQueuedSynchronizer` 并重写指定的方法。\n    \n2. 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。\n    \n\n这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用。\n\n**AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的钩子方法：**\n\n```Java\n//独占方式。尝试获取资源，成功则返回true，失败则返回false。\nprotected boolean tryAcquire(int)\n//独占方式。尝试释放资源，成功则返回true，失败则返回false。\nprotected boolean tryRelease(int)\n//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。\nprotected int tryAcquireShared(int)\n//共享方式。尝试释放资源，成功则返回true，失败则返回false。\nprotected boolean tryReleaseShared(int)\n//该线程是否正在独占资源。只有用到condition才需要去实现它。\nprotected boolean isHeldExclusively()\n```\n\n**什么是钩子方法呢？** 钩子方法是一种被声明在抽象类中的方法，一般使用 `protected` 关键字修饰，它可以是空方法（由子类实现），也可以是默认实现的方法。模板设计模式通过钩子方法控制固定步骤的实现。\n\n篇幅问题，这里就不详细介绍模板方法模式了，不太了解的小伙伴可以看看这篇文章：[用 Java8 改造后的模板方法模式真的是 yyds!open in new window](https://mp.weixin.qq.com/s/zpScSCktFpnSWHWIQem2jg)。\n\n除了上面提到的钩子方法之外，AQS 类中的其他方法都是 `final` ，所以无法被其他类重写。\n\n  \n\n  \n\n## 公平锁和非公平锁有什么区别？\n\n- **公平锁** : 锁被释放之后，**先申请的线程先得到锁**。性能较差一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。\n    \n- **非公平锁** ：锁被释放之后，**后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的**。性能更好，但可能会导致某些线程永远无法获取到锁。\n    \n\n## synchronized 和 ReentrantLock 有什么区别？\n\n### 两者都是可重入锁\n\n**可重入锁** 也叫递归锁，指的是**线程可以再次获取自己的内部锁**。\n\n在下面的代码中，`method1()` 和 `method2()`都被 `synchronized` 关键字修饰，`method1()`调用了`method2()`。\n\n```Java\npublic class ReentrantLockDemo {\n    public synchronized void method1() {\n        System.out.println(\"方法1\");\n        method2();\n    }\n\n    public synchronized void method2() {\n        System.out.println(\"方法2\");\n    }\n}\n```\n\n由于 `synchronized`锁是可重入的，同一个线程在调用`method1()` 时可以直接获得当前对象的锁，执行 `method2()` 的时候可以再次获取这个对象的锁，**不会产生死锁问题**。\n\n### synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API\n\n`synchronized` 是依赖于 JVM 实现的\n\n`ReentrantLock` 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成）\n\n### ReentrantLock 比 synchronized 增加了一些高级功能\n\n相比`synchronized`，`ReentrantLock`增加了一些高级功能。主要来说主要有三点：\n\n- **等待可中断** : `ReentrantLock`提供了一种能够中断等待锁的线程的机制，通过 `lock.lockInterruptibly()` 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。\n    \n- **可实现公平锁** : `ReentrantLock`可以指定是公平锁还是非公平锁。而`synchronized`只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。`ReentrantLock`默认情况是非公平的，可以通过 `ReentrantLock`类的`ReentrantLock(boolean fair)`构造方法来制定是否是公平的。\n    \n- **可实现选择性通知（锁可以绑定多个条件）**: `synchronized`关键字与`wait()`和`notify()`/`notifyAll()`方法相结合可以实现等待/通知机制。`ReentrantLock`类当然也可以实现，但是需要借助于`Condition`接口与`newCondition()`方法。\n    \n\n如果你想使用上述功能，那么选择 `ReentrantLock` 是一个不错的选择。\n\n关于 `Condition`接口的补充：\n\n\u003e `Condition`是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个`Lock`对象中可以创建多个`Condition`实例（即对象监视器），**线程对象可以注册在指定的****`Condition`****中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用****`notify()/notifyAll()`****方法进行通知时，被通知的线程是由 JVM 选择的，用****`ReentrantLock`****类结合****`Condition`****实例可以实现“选择性通知”** ，这个功能非常重要，而且是 `Condition` 接口默认提供的。而`synchronized`关键字就相当于整个 `Lock` 对象中只有一个`Condition`实例，所有的线程都注册在它一个身上。如果执行`notifyAll()`方法的话就会通知所有处于等待状态的线程，这样会造成很大的效率问题。而`Condition`实例的`signalAll()`方法，只会唤醒注册在该`Condition`实例中的所有等待线程。\n\n### 可中断锁和不可中断锁有什么区别？\n\n- **可中断锁** ：获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理。`ReentrantLock` 就属于是可中断锁。\n    \n- **不可中断锁** ：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 `synchronized` 就属于是不可中断锁。\n    \n\n# ReentrantReadWriteLock\n\n`ReentrantReadWriteLock` 在实际项目中使用的并不多，面试中也问的比较少，简单了解即可。JDK 1.8 引入了性能更好的读写锁 `StampedLock` 。\n\n## ReentrantReadWriteLock 是什么？\n\n`ReentrantReadWriteLock` 实现了 `ReadWriteLock` ，是一个可重入的读写锁，既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全。\n\n```Java\npublic class ReentrantReadWriteLock\n        implements ReadWriteLock, java.io.Serializable{\n}\npublic interface ReadWriteLock {\n    Lock readLock();\n    Lock writeLock();\n}\n```\n\n- 一般锁进行并发控制的规则：读读互斥、读写互斥、写写互斥。\n    \n- 读写锁进行并发控制的规则：读读不互斥、读写互斥、写写互斥（只有读读不互斥）。\n    \n\n**`ReentrantReadWriteLock`** **其实是两把锁，一把是** **`WriteLock`** **(写锁)，一把是** **`ReadLock`****（读锁） 。读锁是共享锁，写锁是独占锁。**读锁可以被同时读，可以同时被多个线程持有，而写锁最多只能同时被一个线程持有。\n\n和 `ReentrantLock` 一样，`ReentrantReadWriteLock` 底层也是基于 AQS 实现的。\n\n![](statistic/asynccode-103.png)\n\n`ReentrantReadWriteLock` 也支持公平锁和非公平锁，默认使用非公平锁，可以通过构造器来显示的指定。\n\n```Java\n// 传入一个 boolean 值，true 时为公平锁，false 时为非公平锁public ReentrantReadWriteLock(boolean fair) {\n    sync = fair ? new FairSync() : new NonfairSync();\n    readerLock = new ReadLock(this);\n    writerLock = new WriteLock(this);}\n```\n\n## ReentrantReadWriteLock 适合什么场景？\n\n由于 `ReentrantReadWriteLock` 既可以保证多个线程同时读的效率，同时又可以保证有写入操作时的线程安全。因此，在读多写少的情况下，使用 `ReentrantReadWriteLock` 能够明显提升系统性能。\n\n## 共享锁和独占锁有什么区别？\n\n- **共享锁** ：一把锁可以被多个线程同时获得。\n    \n- **独占锁** ：一把锁只能被一个线程获得。\n    \n\n## 线程持有读锁还能获取写锁吗？\n\n- 在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。\n    \n- 在线程持有写锁的情况下，**该线程可以继续获取读锁**（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。\n    \n\n读写锁的源码分析，推荐阅读 [聊聊 Java 的几把 JVM 级锁 - 阿里巴巴中间件 open in new window](https://mp.weixin.qq.com/s/h3VIUyH9L0v14MrQJiiDbw) 这篇文章，写的很不错。\n\n## 读锁为什么不能升级为写锁？\n\n**写锁可以降级为读锁，但是读锁却不能升级为写锁。这是因为读锁升级为写锁会引起线程的争夺**，毕竟写锁属于是独占锁，这样的话，会影响性能。\n\n另外，还可能会有死锁问题发生。举个例子：假设两个线程的读锁都想升级写锁，则需要对方都释放自己锁，而双方都不释放，就会产生死锁。\n\n# StampedLock\n\n`StampedLock` 面试中问的比较少，不是很重要，简单了解即可。\n\n## StampedLock 是什么？\n\n`StampedLock` 是 JDK 1.8 引入的性能更好的读写锁，不可重入且不支持条件变量 `Conditon`。\n\n不同于一般的 `Lock` 类，`StampedLock` 并不是直接实现 `Lock`或 `ReadWriteLock`接口，而是基于 **CLH 锁** 独立实现的（AQS 也是基于这玩意）。\n\n```Java\npublic class StampedLock implements java.io.Serializable {}\n```\n\n`StampedLock` 提供了三种模式的读写控制模式：读锁、写锁和乐观读。\n\n- **写锁**：独占锁，一把锁只能被一个线程获得。当一个线程获取写锁后，其他请求读锁和写锁的线程必须等待。类似于 `ReentrantReadWriteLock` 的写锁，不过这里的写锁是不可重入的。\n    \n- **读锁** （悲观读）：共享锁，没有线程获取写锁的情况下，多个线程可以同时持有读锁。如果己经有线程持有写锁，则其他线程请求获取该读锁会被阻塞。类似于 `ReentrantReadWriteLock` 的读锁，不过这里的读锁是不可重入的。\n    \n- **乐观读** ：允许多个线程获取乐观读以及读锁。同时允许一个写线程获取写锁。\n    \n\n另外，`StampedLock` 还支持这三种锁在一定条件下进行相互转换 。\n\n```Java\nlong tryConvertToWriteLock(long stamp){}\nlong tryConvertToReadLock(long stamp){}\nlong tryConvertToOptimisticRead(long stamp){}\n```\n\n`StampedLock` 在获取锁的时候会返回一个 long 型的数据戳，该数据戳用于稍后的锁释放参数，如果返回的数据戳为 0 则表示锁获取失败。当前线程持有了锁再次获取锁还是会返回一个新的数据戳，这也是`StampedLock`不可重入的原因。\n\n```Java\n// 写锁public long writeLock() {long s, next;  // bypass acquireWrite in fully unlocked case onlyreturn ((((s = state) \u0026 ABITS) == 0L \u0026\u0026U.compareAndSwapLong(this, STATE, s, next = s + WBIT)) ?\n            next : acquireWrite(false, 0L));}// 读锁public long readLock() {long s = state, next;  // bypass acquireRead on common uncontended casereturn ((whead == wtail \u0026\u0026 (s \u0026 ABITS) \u003c RFULL \u0026\u0026U.compareAndSwapLong(this, STATE, s, next = s + RUNIT)) ?\n            next : acquireRead(false, 0L));}// 乐观读public long tryOptimisticRead() {long s;return (((s = state) \u0026 WBIT) == 0L) ? (s \u0026 SBITS) : 0L;}\n```\n\n## StampedLock 的性能为什么更好？\n\n相比于传统读写锁多出来的乐观读是`StampedLock`比 `ReadWriteLock` 性能更好的关键原因。`StampedLock` 的乐观读允许一个写线程获取写锁，所以不会导致所有写线程阻塞，也就是当读多写少的时候，写线程有机会获取写锁，减少了线程饥饿的问题，吞吐量大大提高。\n\n## StampedLock 适合什么场景？\n\n和 `ReentrantReadWriteLock` 一样，`StampedLock` 同样适合读多写少的业务场景，可以作为 `ReentrantReadWriteLock`的替代品，性能更好。\n\n不过，需要注意的是`StampedLock`不可重入，不支持条件变量 `Conditon`，对中断操作支持也不友好（使用不当容易导致 CPU 飙升）。如果你需要用到 `ReentrantLock` 的一些高级性能，就不太建议使用 `StampedLock` 了。\n\n另外，`StampedLock` 性能虽好，但使用起来相对比较麻烦，一旦使用不当，就会出现生产问题。强烈建议你在使用`StampedLock` 之前，看看 [StampedLock 官方文档中的案例open in new window](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/StampedLock.html)。\n\n## StampedLock 的底层原理了解吗？\n\n`StampedLock` 不是直接实现 `Lock`或 `ReadWriteLock`接口，而是基于 **CLH 锁** 实现的（AQS 也是基于这玩意），CLH 锁是对自旋锁的一种改良，是一种隐式的链表队列。`StampedLock` 通过 CLH 队列进行线程的管理，通过同步状态值 `state` 来表示锁的状态和类型。\n\n`StampedLock` 的原理和 AQS 原理比较类似，这里就不详细介绍了，感兴趣的可以看看下面这两篇文章：\n\n- [AQS 详解open in new window](https://javaguide.cn/java/concurrent/aqs.html)\n    \n- [StampedLock 底层原理分析open in new window](https://segmentfault.com/a/1190000015808032)\n    \n\n如果你只是准备面试的话，建议多花点精力搞懂 AQS 原理即可，`StampedLock` 底层原理在面试中遇到的概率非常小","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E9%94%81%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B7%A5%E5%85%B7%E7%B1%BB":{"title":"锁相关的工具类","content":"### Semaphore\n\n## Semaphore的作用\n\n`synchronized` 和 `ReentrantLock` 都是一次只允许一个线程访问某个资源，而`Semaphore`(信号量)可以用来控制同时访问特定资源的线程数量。\n\n`Semaphore` 有两种模式：。\n\n- 公平模式： 调用 `acquire()` 方法的顺序就是获取许可证的顺序，遵循 FIFO；\n    \n- 非公平模式： 抢占式的。\n    \n\n  \n\n```Go\npublic Semaphore(int permits) {\n    sync = new NonfairSync(permits);\n}\n\npublic Semaphore(int permits, boolean fair) {\n    sync = fair ? new FairSync(permits) : new NonfairSync(permits);\n}\n```\n\n## Semaphore 的原理是啥\n\n`Semaphore` 是共享锁的一种实现，**它默认构造 AQS 的** **`state`** **值为** **`permits`****，你可以将** **`permits`** **的值理解为许可证的数量，只有拿到许可证的线程才能执行**。\n\n  \n\n- 调用`semaphore.acquire()` ，线程尝试获取许可证\n    \n    - 如果 `state \u003e= 0` 的话，则表示可以获取成功。如果获取成功的话，使用 CAS 操作去修改 `state` 的值 `state=state-1`。\n        \n    - 如果 `state\u003c0` 的话，则表示许可证数量不足。此时会创建一个 Node 节点加入阻塞队列，挂起当前线程。\n        \n- 调用`semaphore.release();` ，线程尝试释放许可证，并使用 CAS 操作去修改 `state` 的值 `state=state+1`。\n    \n- 释放许可证成功之后，同时会唤醒同步队列中的一个线程。被唤醒的线程会重新尝试去修改 `state` 的值 `state=state-1` ，如果 `state\u003e=0` 则获取令牌成功，否则重新进入阻塞队列，挂起线程。\n    \n\n![](statistic/asynccode-105.png)\n\n  \n\n# CountDownLatch\n\n## CountDownLatch有啥作用\n\n`CountDownLatch` **允许** **`count`** **个线程阻塞在一个地方，直至所有线程的任务都执行完毕。**\n\n`CountDownLatch` 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 `CountDownLatch` 使用完毕后，它不能再次被使用\n\n## CountDownLatch的原理是啥？\n\n`CountDownLatch` 是**共享锁的一种实现**,它默认构造 AQS 的 **`state`** **值为** **`count`**。\n\n- 当线程使用 `countDown()` 方法时,其实使用了`tryReleaseShared`方法以 CAS 的操作来减少 `state`,直至 `state` 为 0 。\n    \n- 当调用 `await()` 方法的时候，如果 `state` 不为 0，那就证明任务还没有执行完毕，`await()` 方法就会一直阻塞，也就是说 `await()` 方法之后的语句不会被执行。**然后，****`CountDownLatch`** **会自旋 CAS 判断** **`state == 0`****，如果** **`state == 0`** **的话**，就会释放所有等待的线程，`await()` 方法之后的语句得到执行。\n    \n\n  \n\n## CountDownLatch的使用场景\n\n我们要读取处理 6 个文件，这 **6 个任务都是没有执行顺序依赖的任务，但是我们需要返回给用户的时候将这几个文件的处理的结果进行统计整理**。\n\n为此我们定义了一个线程池和 count 为 6 的`CountDownLatch`对象 。使用线程池处理读取任务，每一个线程处理完之后就将 count-1，调用`CountDownLatch`对象的 `await()`方法，直到所有文件读取完之后，才会接着执行后面的逻辑。\n\n  \n\n```Go\npublic class CountDownLatchExample1 {\n    // 处理文件的数量 \n    private static final int threadCount = 6;\n    \n    public static void main(String[] args) throws InterruptedException {\n        // 创建一个具有固定线程数量的线程池对象（推荐使用构造方法创建）\n        ExecutorService threadPool = Executors.newFixedThreadPool(10);\n        final CountDownLatch countDownLatch = new CountDownLatch(threadCount);\n        for (int i = 0; i \u003c threadCount; i++) {\n            final int threadnum = i;             \n            threadPool.execute(() -\u003e {\n                try {\n                    //处理文件的业务操作\n                    //......\n                } catch (InterruptedException e) {\n                     e.printStackTrace();\n                 } finally {\n                     //表示一个文件已经被完成                     \n                     countDownLatch.countDown();\n                 }\n             });\n        }         \n        countDownLatch.await();         \n        threadPool.shutdown();\n    System.out.println(\"finish\");\n    }\n}\n```\n\n  \n\n可以使用ComletableFuture来优化\n\n```Go\n//文件夹位置\nList\u003cString\u003e filePaths = Arrays.asList(...)\n// 异步处理所有文件\nList\u003cCompletableFuture\u003cString\u003e\u003e fileFutures = filePaths.stream()\n    .map(filePath -\u003e doSomeThing(filePath))\n    .collect(Collectors.toList());\n// 将他们合并起来\nCompletableFuture\u003cVoid\u003e allFutures = CompletableFuture.allOf(     \n    fileFutures.toArray(new CompletableFuture[fileFutures.size()])\n);\ntry {     \n    headerFuture.join();\n} catch (Exception ex) {\n    //......\n}\nSystem.out.println(\"all done. \");\n```\n\n# CyclicBarrier\n\n## CyclicBarrier有啥用\n\n`CyclicBarrier` 和 `CountDownLatch` 非常类似，它也可以实现线程间的技术等待，但是它的功能比 `CountDownLatch` 更加复杂和强大。主要应用场景和 `CountDownLatch` 类似。\n\n\u003e `CountDownLatch` 的实现是基于 AQS 的，而 `CycliBarrier` 是基于 `ReentrantLock`(`ReentrantLock` 也属于 AQS 同步器)和 `Condition` 的。\n\n`CyclicBarrier` 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：**让一组线程到达一个屏障（也可以叫同步点）时被阻塞**，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活\n\n![](statistic/asynccode-107.png)\n\n### CountDownLatch 的原理是什么？\n\n`CyclicBarrier` 内部通过一个 `count` 变量作为计数器，`count` 的初始值为 `parties` 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减 1。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务。\n\n- `CyclicBarrier` 默认的构造方法是 `CyclicBarrier(int parties)`，其参数表示屏障拦截的线程数量，每个线程调用 `await()` 方法告诉 `CyclicBarrier` 我已经到达了屏障，然后当前线程被阻塞。*\n    \n- 当调用 `CyclicBarrier` 对象调用 `await()` 方法时，实际上调用的是 `dowait(false, 0L)`方法。 `await()` 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 `parties` 的值时，栅栏才会打开，线程才得以通过执行。\n    \n\n## **CyclicBarrier**和**CountDownLatch**有什么区别\n\n- CountDownLatch是一次性的，而CyclicBarrier则可以多次设置屏障，实现重复利 用；\n    \n- CountDownLatch中的各个子线程**不可以等待其他线程**，只能完成自己的任务； 而CyclicBarrier中的各个线程可以等待其他线程\n    \n\n![](statistic/asynccode-108.png)\n\n  \n\n# Exchanger\n\nExchanger（交换者）是一个用于线程间协作的工具类。Exchanger用于进行线程间的数据交换**。它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据**。\n\n  \n\n- 这两个线程通过 exchange方法交换数据，如果第一个线程先执行exchange()方法，它 会一直等待第二个线程也执行exchange方法，当两个线程都到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据\n    \n\n  \n\n比如我们需要将纸制银行流水通过人工的方式录入成电子银行流水，为了避免错误，采用AB岗两人进行录入，录入到Excel之后，系统需要加载这两个Excel，并对两个Excel数据进行校对，看看是否录入一致。\n\n![](statistic/asynccode-109.png)\n\n![](statistic/asynccode-106.png)","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E8%A7%A3%E5%86%B3%E5%8F%AF%E8%A7%81%E6%80%A7%E6%9C%89%E5%BA%8F%E6%80%A7%E9%97%AE%E9%A2%98":{"title":"Java内存模型：解决可见性、有序性问题","content":"# Java 内存模型\n\n导致可见性的原因是缓存，导致有序性的原因是编译优化，那解决可见性、有序性**最直接的办法就是禁用缓存和编译优化，但是这样性能堪忧，应该按需禁用缓存以及编译优化。**\n\n  \n\n如何做到“按需禁用”呢？对于并发程序，何时禁用缓存以及编译优化只有程序员知道，那所谓“按需禁用”其实就是指按照程序员的要求来禁用。\n\n本质上可以理解为，**Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法**。具体来说，这些方法包括 **volatile、synchronized 和 final 三个关键字**，以及**六项 Happens-Before 规则**。\n\n  \n\n# volatile\n\nvolatile 关键字并不是 Java 语言的特产，古老的 C 语言里也有，它**最原始的意义就是禁用 CPU 缓存**。\n\n例如，我们声明一个 volatile 变量 volatile int x = 0，它表达的是：**告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。**\n\n  \n\n1.5版本以后对volatile语义进行了增强，其中就包括Happens-Before\n\n# Happens-Before 规则\n\n它真正要表达的是：**前面一个操作的结果对后续操作是可见的**\n\nHappens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则。\n\n- Happens-Before 规则\n    \n    - 程序的顺序性规则\n        \n        - 前面的操作 Happens-Before 于后续的任意操作。\n            \n    - volatile 变量规则\n        \n        - 对一个 volatile 变量的写操作相对于后续对这个 volatile 变量的读操作可见\n            \n    - 传递性\n        \n        - A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。\n            \n    - 管程中锁的规则\n        \n        - 一个锁的解锁 Happens-Before 于后续对这个锁的加锁\n            \n    - 线程 start() 规则\n        \n        - 是指主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作\n            \n    - 线程 join() 规则\n        \n        - 它是指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作\n            \n\n## 程序的顺序性规则\n\n- 前面的操作 Happens-Before 于后续的任意操作。\n    \n\n## volatile 变量规则\n\n- 这条规则是指对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。\n    \n- 对一个 volatile 变量的写操作相对于后续对这个 volatile 变量的读操作可见\n    \n\n  \n\n## 传递性\n\n- 指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。\n    \n\n  \n\n## 管程中锁的规则\n\n- 这条规则是指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。\n    \n- 管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现\n    \n\n## 线程 start() 规则\n\n- 是指主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作\n    \n\n  \n\n## 线程 join() 规则\n\n- 它是指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作\n    \n\n# final\n\nvolatile 为的是禁用缓存以及编译优化\n\nfinal 告诉编译器优化得更好\n\n  \n\nfinal 修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。Java 编译器在 1.5 以前的版本的确优化得很努力，以至于都优化错了。\n\n在 1.5 以后 Java 内存模型对 final 类型变量的重排进行了约束。现在只要我们提供正确构造函数没有“逸出”，就不会出问题了","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java%E7%BA%BF%E7%A8%8B":{"title":"Java线程","content":"# Java 线程的生命周期\n\n对于有生命周期的事物，要学好它，思路非常简单，只要能搞懂**生命周期中各个节点的状态转换机制**就可以了\n\n## 通用的生命周期\n\n| ---        | ---                                                                                                                                                                                                                                                      |\n| ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 初始状态   | 初始状态，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层面，真正的线程还没有创建。                                                                        |\n| 可运行状态 | 可运行状态，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。                                                                                                                                       |\n| 运行状态   | 当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了运行状态。                                                                                                                                           |\n| 休眠状态   | 运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量），那么线程的状态就会转换到休眠状态，同时释放 CPU 使用权，休眠状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。 |\n| 终止状态   | 线程执行完或者出现异常就会进入终止状态，终止状态的线程不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。                                                                                                                               |\n\n## java 中的生命周期\n\n  \n\n1. NEW（初始化状态）\n    \n2. RUNNABLE（可运行 / 运行状态）\n    \n3. BLOCKED（阻塞状态）\n    \n4. WAITING（无时限等待）\n    \n5. TIMED_WAITING（有时限等待）\n    \n6. TERMINATED（终止状态）\n    \n\n  \n\n**在操作系统层面，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也就是说只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权。**\n\n![](statistic/asynccode-81.png)\n\n  \n\n**BLOCKED、WAITING、TIMED_WAITING 可以理解为线程导致休眠状态的三种原因。那具体是哪些情形会导致线程从 RUNNABLE 状态转换到这三种状态呢**\n\n  \n\n### RUNNABLE 与 BLOCKED 的状态转换\n\n只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁。\n\n- synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。\n    \n- 而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态\n    \n\n  \n\n线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？\n\n1. 在操作系统层面，线程是会转换到休眠状态的，\n    \n2. 但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。\n    \n\n  \n\nJVM 层面并不关心操作系统调度相关的状态，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态\n\n而我们平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态。\n\n  \n\n### RUNNABLE 与 WAITING 的状态转换\n\n1. 获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法\n    \n2. 第二种场景，调用无参数的 Thread.join() 方法。\n    \n    1. 其中的 join() 是一种线程同步方法，例如有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完，而等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。\n        \n    2. 当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE\n        \n3. 第三种场景，调用 LockSupport.park() 方法\n    \n    1. 调用 LockSupport.park() 方法，当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING。\n        \n    2. 调用 LockSupport.unpark(Thread thread) 可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE。\n        \n\n### RUNNABLE 与 TIMED_WAITING 的状态转换\n\n1. 调用带超时参数的 Thread.sleep(long millis) 方法；\n    \n2. 获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法；\n    \n3. 调用带超时参数的 Thread.join(long millis) 方法；\n    \n4. 调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法\n    \n5. 调用带超时参数的 LockSupport.parkUntil(long deadline) 方法。\n    \n\n**TIMED_WAITING 和 WAITING 状态的区别，仅仅是触发条件多了超时参数。**\n\n  \n\n  \n\n  \n\n  \n\n其他的状态转换\n\n  \n\n### 从 NEW 到 RUNNABLE 状态\n\nJava 刚创建出来的 Thread 对象就是 NEW 状态.。NEW 状态的线程，不会被操作系统调度，因此不会执行。**Java 线程要执行，就必须转换到 RUNNABLE 状态。从 NEW 状态转换到 RUNNABLE 状态很简单，只要调用线程对象的 start()** 方法就可以了\n\n  \n\n  \n\n### 从 RUNNABLE 到 TERMINATED 状态\n\n线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止。\n\n有时候我们需要强制中断 run() 方法的执行，例如 run() 方法访问一个很慢的网络，我们等不下去了，想终止怎么办呢？Java 的 Thread 类里面倒是有个 stop() 方法，不过已经标记为 @Deprecated，所以不建议使用了。正确的姿势其实是调用 interrupt() 方法\n\n那 stop() 和 interrupt() 方法的主要区别是什么呢？\n\n- 如果线程持有 ReentrantLock 锁，被 stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁\n    \n- 而 interrupt() 方法就温柔多了，interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。一种是异常，另一种是主动检测。\n    \n    - 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，**会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常**\n        \n    - 当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。\n        \n\n  \n\n  \n\n# 创建线程的数量\n\n  \n\n## 为啥要使用多线程\n\n使用多线程，本质上就是提升程序性能。\n\n度量性能的指标有很多，但是有两个指标是最核心的，它们就是延迟和吞吐量\n\n- 思维导图\n    \n    - 延迟\n        \n        - 是发出请求到收到响应这个过程的时间\n            \n    - 吞吐量\n        \n        - 单位时间内能处理请求的数量\n            \n\n我们所谓提升性能，从度量的角度，主要是降低延迟，提高吞吐量\n\n## 多线程的应用场景\n\n- 应用场景\n    \n    - 优化算法\n        \n    - 将硬件的性能发挥到极致\n        \n\n在并发编程领域，提升性能本质上就是提升硬件的利用率，再具体点来说，就是提升 I/O 的利用率和 CPU 的利用率。\n\n## 创建多少线程合适？\n\n- 我们的程序一般都是 CPU 计算和 I/O 操作交叉执行的，由于 I/O 设备的速度相对于 CPU 来说都很慢，所以大部分情况下，I/O 操作执行的时间相对于 CPU 计算来说都非常长，这种场景我们一般都称为 **I/O 密集型计算**\n    \n- 和 I/O 密集型计算相对的就是 CPU 密集型计算了，CPU 密集型计算大部分场景下都是纯 CPU 计算\n    \n\n  \n\n  \n\n### 对于 CPU 密集型计算\n\n- 对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的。\n    \n- 不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，\n    \n\n这样的话，当**线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上**，从而保证 CPU 的利用率。\n\n### 对于 I/O 密集型计算场景\n\n对于 I/O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，我们可以总结出这样一个公式，\n\n最佳线程数 =1 +（I/O 耗时 / CPU 耗时）\n\n面这个公式是针对单核 CPU 的，至于多核 CPU，也很简单，只需要等比扩大就可以了，计算公式如下\n\n最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]\n\n  \n\n  \n\n# 局部变量是线程安全的\n\n  \n\n## 方法是如何被执行的\n\n  \n\n“CPU 去哪里找到调用方法的参数和返回地址？”如果你熟悉 CPU 的工作原理，你应该会立刻想到：**通过 CPU 的堆栈寄存器**。CPU 支持一种栈结构，先入后出。因为这个**栈是和方法调用相关的，因此经常被称为调用栈**。\n\n  \n\n有三个方法 A、B、C，他们的调用关系是 A-\u003eB-\u003eC（A 调用 B，B 调用 C），在运行时，会构建出下面这样的调用栈。每个方法在调用栈里都有自己的独立空间，**称为栈帧**，**每个栈帧里都有对应方法需要的参数和返回地址**。当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。也就是说，栈帧和方法是同生共死的。\n\n![](statistic/asynccode-83.png)\n\n## 局部变量的存储位置\n\n**局部变量的作用域是方法内部，局部变量就是放到了调用栈里，局部变量是和方法同生共死的，一个变量如果想跨越方法的边界，就必须创建在堆里。**\n\n  \n\n![](statistic/asynccode-82.png)\n\n  \n\n## 调用栈与线程\n\n两个线程可以同时用不同的参数调用相同的方法，那调用栈和线程之间是什么关系呢？答案是：**每个线程都有自己独立的调用栈。**\n\nJava 方法里面的局部变量是否存在并发问题？现在你应该很清楚了，一点问题都没有。**因为每个线程都有自己的调用栈，局部变量保存在线程各自的调用栈里面，不会共享**\n\n  \n\n  \n\n方法里的局部变量，因为不会和其他线程共享，所以没有并发问题，这个思路很好，已经成为解决并发问题的一个重要技术，同时还有个响当当的名字叫做**线程封闭**，比较官方的解释是：**仅在单线程内访问数据**","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B":{"title":"Java 内存模型","content":"要想理解透彻 JMM（Java 内存模型），我们先要从 **CPU 缓存模型和指令重排序** 说起！\n\n# 从 CPU 缓存模型说起\n\n## CPU Cache\n\n**CPU Cache 缓存的是内存数据用于解决 CPU 处理速度和内存不匹配的问题，内存缓存的是硬盘数据用于解决硬盘访问速度过慢的问题。**\n\n为了更好地理解，我画了一个简单的 CPU Cache 示意图如下所示。\n\n\u003e **🐛 修正（参见：** **[issue#1848open in new window](https://github.com/Snailclimb/JavaGuide/issues/1848)****）**：对 CPU 缓存模型绘图不严谨的地方进行完善。\n\n![](statistic/asynccode-67.png)\n\n  \n\n## **CPU Cache 的工作方式**\n\n先复制一份数据到 CPU Cache 中，当 CPU 需要用到的时候就可以直接从 CPU Cache 中读取数据，当运算完成后，再将运算得到的数据写回 Main Memory 中。但是，这样存在 **内存缓存不一致性的问题** ！\n\n## **缓存一致协议**\n\n**CPU 为了解决内存缓存不一致性问题可以通过制定缓存一致协议（比如** **[MESI 协议open in new window](https://zh.wikipedia.org/wiki/MESI%E5%8D%8F%E8%AE%AE)****）或者其他手段来解决。** 这个缓存一致性协议指的是在 CPU 高速缓存与主内存交互的时候需要遵守的原则和规范。不同的 CPU 中，使用的缓存一致性协议通常也会有所不同。\n\n![](statistic/asynccode-69.png)\n\n操作系统通过 **内存模型（Memory Model）** 定义一系列规范来解决这个问题。\n\n# 指令重排序\n\n为了提升执行速度/性能，计算机在执行程序代码的时候，会对指令进行重排序。\n\n## **什么是指令重排序？**\n\n简单来说就是系统在执行代码的时候并不一定是按照你写的代码的顺序依次执行。\n\n## 常见的指令重排序有下面 2 种情况：\n\n- **编译器优化重排** ：编译器（包括 JVM、JIT 编译器等）在**不改变单线程程序语义的前提下**，重新安排语句的执行顺序。\n    \n- **指令并行重排** ：现代处理器采用**了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行**。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。\n    \n- **内存系统“重排序”**，在 JMM 里表现为主存和本地内存的内容可能不一致，进而导致程序在多线程下执行可能出现问题。\n    \n\nJava 源代码会经历 **编译器优化重排 —\u003e 指令并行重排 —\u003e 内存系统重排** 的过程，最终才变成操作系统可执行的指令序列。\n\n**指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致** ，所以在多线程下，指令重排序可能会导致一些问题。\n\n## 编译器和处理器的指令重排序的处理方式不一样。\n\n- 对于**编译器，通过禁止特定类型的编译器重排序的方式来禁止重排序**。\n    \n- 对于处理器，通过插入内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）的方式来禁止特定类型的处理器重排序。指令并行重排和内存系统重排都属于是处理器级别的指令重排序。\n    \n\n\u003e **内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种 CPU 指令**，用来禁止处理器指令发生重排序（像屏障一样），从而保障指令执行的有序性。另外，为了达到屏障的效果，它也会使处理器写入、读取值之前，将主内存的值写入高速缓存，清空无效队列，从而保障变量的可见性。\n\n# JMM(Java Memory Model)\n\n## 什么是 JMM？为什么需要 JMM？\n\nJava 是最早尝试提供内存模型的编程语言。由于早期内存模型存在一些缺陷（比如非常容易削弱编译器的优化能力），从 Java5 开始，Java 开始使用新的内存模型 [《JSR-133：Java Memory Model and Thread Specification》open in new window](http://www.cs.umd.edu/~pugh/java/memoryModel/CommunityReview.pdf) 。\n\n## JMM的存在原因\n\n**一般来说，编程语言也可以直接复用操作系统层面的内存模型**。**Java 语言是跨平台的，它需要自己提供一套内存模型以屏蔽系统差异。**\n\n## JMM 是啥\n\n你可以把 JMM 看作是 Java 定义的并发编程相关的一组规范\n\n- 抽象了线程和主内存之间的关系之外，\n    \n- 规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范\n    \n\n## **为什么要遵守这些并发相关的原则和规范呢？**\n\n- 这是因为并发编程下，像 CPU 多级缓存和指令重排这类设计可能会导致程序运行出现一些问题。\n    \n- 为此，JMM 抽象了 happens-before 原则（后文会详细介绍到）来解决这个指令重排序问题。\n    \n- 对于 Java 开发者说，你不需要了解底层原理，直接使用并发相关的一些关键字和类（比如 `volatile`、`synchronized`、各种 `Lock`）即可开发出并发安全的程序。\n    \n\n## JMM 是如何抽象线程和主内存之间的关系？\n\n**Java 内存模型（JMM）** **抽象了线程和主内存之间的关系**，。\n\nJava 内存模型下，线程可以把变量保存 **本地内存** （比如机器的寄存器）中，而不是直接在**主存**中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。\n\n这和我们上面讲到的 **CPU 缓存模型非常相似。**\n\n### **什么是主内存？什么是本地内存？**\n\n- **主内存** ：所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)\n    \n- **本地内存** ：每个线程都有一个私有的本地内存来存储共享变量的副本，并且，每个线程只能访问自己的本地内存，无法访问其他线程的本地内存。本地内存是 JMM 抽象出来的一个概念，存储了主内存中的共享变量副本。\n    \n\nJava 内存模型的抽象示意图如下：\n\n![](statistic/asynccode-68.png)\n\n### 8种同步操作\n\n关于主内存与工作内存直接的具体交互协议，即一个变量如何从主内存拷贝到工作内存，如何从工作内存同步到主内存之间的实现细节，**Java 内存模型定义**来以下**八种同步操作**（了解即可，无需死记硬背）：\n\n- **锁定（lock）**: 作用于主内存中的变量，将他**标记为一个线程独享变量**。\n    \n- **解锁（unlock）**: 作用于主内存中的变量，**解除变量的锁定状态**，被解除锁定状态的变量才能被其他线程锁定。\n    \n- **read（读取）**：作用于主内存的变量，它把**一个变量的值从主内存传输到线程的工作内存**中，以便随后的 load 动作使用。\n    \n- **load(载入)**：把 read 操作**从主内存中得到的变量值放入工作内存的变量的副本**中。\n    \n- **use(使用)**：把**工作内存中的一个变量的值传给执行引擎，**每当虚拟机遇到一个使用到变量的指令时都会使用该指令。\n    \n- **assign（赋值）**：作用于工作内存的变量，它把一个**从执行引擎接收到的值赋给工作内存的变量**，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。\n    \n- **store（存储）**：作用于工作内存的变量，它把**工作内存中一个变量的值传送到主内存**中，以便随后的 write 操作使用。\n    \n- **write（写入）**：作用于主内存的变量，它**把 store 操作从工作内存中得到的变量的值放入主内存的变量**中。\n    \n\n### 同步规则\n\n除了这 8 种同步操作之外，还规定了下面这些同步规则来保证这些同步操作的正确执行（了解即可，无需死记硬背）：\n\n- 不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从线程的工作内存同步回主内存中。\n    \n- 一个新的变量只能在主内存中 “诞生”，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量，换句话说就是对一个变量实施 use 和 store 操作之前，必须先执行过了 assign 和 load 操作。\n    \n- 一个变量在同一个时刻只允许一条线程对其进行 lock 操作，但 lock 操作可以被同一条线程重复执行多次，多次执行 lock 后，只有执行相同次数的 unlock 操作，变量才会被解锁。\n    \n- 如果对一个变量执行 lock 操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行 load 或 assign 操作初始化变量的值。\n    \n- 如果一个变量事先没有被 lock 操作锁定，则不允许对它执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定住的变量。\n    \n- ......\n    \n\n## Java 内存区域和 JMM 有何区别？\n\n这是一个比较常见的问题，很多初学者非常容易搞混。 **Java 内存区域和内存模型是完全不一样的两个东西** ：\n\n- **JVM 内存结构和 Java 虚拟机的运行时区域**相关，定义了 JVM 在运行时如何分区存储程序数据，就比如说堆主要用于存放对象实例。\n    \n- J**ava 内存模型和 Java 的并发编程相关，抽象了线程和主内存之间的关系**就比如说线程之间的共享变量必须存储在主内存中，规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范，其主要目的是为了简化多线程编程，增强程序可移植性的。\n    \n\n## happens-before 原则是什么？\n\n### **为什么需要 happens-before 原则？**\n\nhappens-before 原则的诞生是为了程序员和编译器、处理器之间的平衡。程序员追求的是易于理解和编程的强内存模型，遵守既定规则编码即可。编译器和处理器追求的是较少约束的弱内存模型，让它们尽己所能地去优化性能，让性能最大化。\n\n### happens-before 原则的设计思想\n\n- 为了对编译器和处理器的约束尽可能少，**只要不改变程序的执行结果（单线程程序和正确执行的多线程程序），编译器和处理器怎么进行重排序优化都行**。\n    \n- 对**于会改变程序执行结果的重排序，JMM 要求编译器和处理器必须禁止这种重排序**。\n    \n\n下面这张是 《Java 并发编程的艺术》这本书中的一张 JMM 设计思想的示意图，非常清晰。\n\n![](statistic/asynccode-71.png)\n\n### JSR-133 对 happens-before 原则的定义：\n\n- 如果一个操作 happens-before 另一个操作，那么**第一个操作的执行结果将对第二个操作可见**，**并且第一个操作的执行顺序排在第二个操作之前**。\n    \n- 两个操作之间存在 happens-before 关系，并**不意味着 Java 平台的具体实现必须要按照 happens-before 关系指定的顺序来执行**。如果重排序之后的执行结果，与按 happens-before 关系来执行的结果一致，那么 JMM 也允许这样的重排序。\n    \n\n我们看下面这段代码：\n\n```Java\nint userNum = getUserNum();         // 1\nint teacherNum = getTeacherNum();         // 2\nint totalNum = userNum + teacherNum;        // 3\n```\n\n- 1 happens-before 2\n    \n- 2 happens-before 3\n    \n- 1 happens-before 3\n    \n\n虽然 1 happens-before 2，但对 1 和 2 进行重排序不会影响代码的执行结果，所以 JMM 是允许编译器和处理器执行这种重排序的。但 1 和 2 必须是在 3 执行之前，也就是说 1,2 happens-before 3 。\n\n  \n\n### **happens-before 原则表达的意义**\n\n其实并不是一个操作发生在另外一个操作的前面，虽然这从程序员的角度上来说也并无大碍。更准确地来说，**它更想表达的意义是前一个操作的结果对于后一个操作是可见的**，无论这两个操作是否在同一个线程里。\n\n  \n\n### **as-if-serial**又是什么？\n\n单线程的程序一定是顺序的吗？as-if-serial语义的意思是：**不管怎么重排序（编译器和处理器为了提高并行度），单线程程序的执行结果不能被改变。**\n\n  \n\n## happens-before 常见规则有哪些？谈谈你的理解？\n\nhappens-before 的规则就 8 条，说多不多，重点了解下面列举的 5 条即可。全记是不可能的，很快就忘记了，意义不大，随时查阅即可。\n\n1. **程序顺序规则** ：一个线程内，按照代码顺序，书写在前面的操作 happens-before 于书写在后面的操作；\n    \n2. **解锁规则** ：解锁 happens-before 于加锁；\n    \n3. **volatile 变量规则** ：对一个 volatile 变量的写操作 happens-before 于后面对这个 volatile 变量的读操作。说白了就是对 volatile 变量的写操作的结果对于发生于其后的任何操作都是可见的。\n    \n4. **传递规则** ：如果 A happens-before B，且 B happens-before C，那么 A happens-before C；\n    \n5. **线程启动规则** ：Thread 对象的 `start()`方法 happens-before 于此线程的每一个动作。\n    \n\n如果两个操作不满足上述任意一个 happens-before 规则，那么这两个操作就没有顺序的保障，JVM 可以对这两个操作进行重排序。\n\n## happens-before 和 JMM 什么关系？\n\nhappens-before 与 JMM 的关系用《Java 并发编程的艺术》这本书中的一张图就可以非常好的解释清楚。\n\n![](statistic/asynccode-70.png)\n\n# 再看并发编程三个重要特性\n\n## 原子性\n\n一次操作或者多次操作，**要么所有的操作全部都得到执行并且不会受到任何因素的干扰而中断，要么都不执行**。\n\n- 可以借助`synchronized` 、各种 `Lock` 以及各种原子类实现原子性。\n    \n- `synchronized` 和各种 `Lock` 可以保证任一时刻只有一个线程访问该代码块，因此可以保障原子性。\n    \n- 各种原子类是利用 CAS (compare and swap) 操作（可能也会用到 `volatile`或者`final`关键字）来保证原子操作。\n    \n\n## 可见性\n\n当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。\n\n- 可以借助`synchronized` 、`volatile` 以及各种 `Lock` 实现可见性。\n    \n- 如果我们将变量声明为 `volatile` ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n    \n\n## 有序性\n\n由于指令重排序问题，代码的执行顺序未必就是编写代码时候的顺序。\n\n我们上面讲重排序的时候也提到过：\n\n\u003e **指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致** ，所以在多线程下，指令重排序可能会导致一些问题。\n\n*在 Java 中，`volatile` 关键字可以禁止指令进行重排序优化。\n\n  \n\n# volatile\n\n## Volatile 保证变量的可见性的\n\n将变量声明为 `volatile` ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取\n\n![](statistic/asynccode-73.png)\n\n`volatile` 关键字能保证数据的可见性，但不能保证数据的原子性。`synchronized` 关键字两者都能保证\n\n  \n\n## 如何禁止指令从排序的？有序性\n\n**在 Java 中，****`volatile`** **关键字除了可以保证变量的可见性，还有一个重要的作用就是防止 JVM 的指令重排序。** 如果我们将变量声明为 **`volatile`** ，在对这个变量进行读写操作的时候，会通过插入特定的 **内存屏障** 的方式来禁止指令重排序。\n\n![](statistic/asynccode-72.png)\n\n为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入**内存屏障**来禁止特定类型的处理器重排序\n\n1. 在每个volatile写操作的前面插入一个 StoreStore 屏障\n    \n2. 在每个volatile写操作的后面插入一个 StoreLoad 屏障\n    \n3. 在每个volatile读操作的后面插入一个 LoadLoad 屏障\n    \n4. 在每个volatile读操作的后面插入一个 LoadStore 屏障\n    \n\n  \n\n![](statistic/asynccode-71.png)\n\n![](statistic/asynccode-70.png)\n\n  \n\n双重校验锁实现对象单例（线程安全）\n\n```Go\npublic class Singleton {\n\n    private volatile static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public  static Singleton getUniqueInstance() {\n       //先判断对象是否已经实例过，没有实例化过才进入加锁代码\n        if (uniqueInstance == null) {\n            //类对象加锁\n            synchronized (Singleton.class) {\n                if (uniqueInstance == null) {\n                    uniqueInstance = new Singleton();\n                }\n            }\n        }\n        return uniqueInstance;\n    }\n}\n```\n\n`uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要的， `uniqueInstance = new Singleton();` 这段代码其实是分为三步执行：\n\n1. 为 `uniqueInstance` 分配内存空间\n    \n2. 初始化 `uniqueInstance`\n    \n3. 将 `uniqueInstance` 指向分配的内存地址\n    \n\n但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-\u003e3-\u003e2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。\n\n## volatile 可以保证原子性么？\n\n`volatile` 关键字能保证变量的**可见性**，但不能保证对变量的操作是**原子性的**。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/ThreadLocal":{"title":"ThreadLocal","content":"# ThreadLocal 有什么用？\n\n**`ThreadLocal`****类主要解决的就是让每个线程绑定自己的值，可以将****`ThreadLocal`****类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。**\n\n如果你创建了一个`ThreadLocal`变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是`ThreadLocal`变量名的由来。他们可以使用 `get()` 和 `set()` 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。\n\n  \n\n# 如何使用 ThreadLocal？\n\n相信看了上面的解释，大家已经搞懂 `ThreadLocal` 类是个什么东西了。下面简单演示一下如何在项目中实际使用 `ThreadLocal` 。\n\n```Java\nimport java.text.SimpleDateFormat;\nimport java.util.Random;\n\npublic class ThreadLocalExample implements Runnable{\n\n     // SimpleDateFormat 不是线程安全的，所以每个线程都要有自己独立的副本\n    private static final ThreadLocal\u003cSimpleDateFormat\u003e formatter = ThreadLocal.withInitial(() -\u003e new SimpleDateFormat(\"yyyyMMdd HHmm\"));\n\n    public static void main(String[] args) throws InterruptedException {\n        ThreadLocalExample obj = new ThreadLocalExample();\n        for(int i=0 ; i\u003c10; i++){\n            Thread t = new Thread(obj, \"\"+i);\n            Thread.sleep(new Random().nextInt(1000));\n            t.start();\n        }\n    }\n\n    @Override\n    public void run() {\n        System.out.println(\"Thread Name= \"+Thread.currentThread().getName()+\" default Formatter = \"+formatter.get().toPattern());\n        try {\n            Thread.sleep(new Random().nextInt(1000));\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        //formatter pattern is changed here by thread, but it won't reflect to other threads\n        formatter.set(new SimpleDateFormat());\n\n        System.out.println(\"Thread Name= \"+Thread.currentThread().getName()+\" formatter = \"+formatter.get().toPattern());\n    }\n\n}\n\n```\n\n输出结果 :\n\n```Plain\nThread Name= 0 default Formatter = yyyyMMdd HHmm\nThread Name= 0 formatter = yy-M-d ah:mm\nThread Name= 1 default Formatter = yyyyMMdd HHmm\nThread Name= 2 default Formatter = yyyyMMdd HHmm\nThread Name= 1 formatter = yy-M-d ah:mm\nThread Name= 3 default Formatter = yyyyMMdd HHmm\nThread Name= 2 formatter = yy-M-d ah:mm\nThread Name= 4 default Formatter = yyyyMMdd HHmm\nThread Name= 3 formatter = yy-M-d ah:mm\nThread Name= 4 formatter = yy-M-d ah:mm\nThread Name= 5 default Formatter = yyyyMMdd HHmm\nThread Name= 5 formatter = yy-M-d ah:mm\nThread Name= 6 default Formatter = yyyyMMdd HHmm\nThread Name= 6 formatter = yy-M-d ah:mm\nThread Name= 7 default Formatter = yyyyMMdd HHmm\nThread Name= 7 formatter = yy-M-d ah:mm\nThread Name= 8 default Formatter = yyyyMMdd HHmm\nThread Name= 9 default Formatter = yyyyMMdd HHmm\nThread Name= 8 formatter = yy-M-d ah:mm\nThread Name= 9 formatter = yy-M-d ah:mm\n```\n\n从输出中可以看出，虽然 `Thread-0` 已经改变了 `formatter` 的值，但 `Thread-1` 默认格式化值与初始化值相同，其他线程也一样。\n\n上面有一段代码用到了创建 `ThreadLocal` 变量的那段代码用到了 Java8 的知识，它等于下面这段代码，如果你写了下面这段代码的话，IDEA 会提示你转换为 Java8 的格式(IDEA 真的不错！)。因为 ThreadLocal 类在 Java 8 中扩展，使用一个新的方法`withInitial()`，将 Supplier 功能接口作为参数。\n\n```Java\nprivate static final ThreadLocal\u003cSimpleDateFormat\u003e formatter = new ThreadLocal\u003cSimpleDateFormat\u003e(){@Overrideprotected SimpleDateFormat initialValue(){return new SimpleDateFormat(\"yyyyMMdd HHmm\");}};\n```\n\n# ThreadLocal 原理了解吗？\n\n  \n\n## ThreadLocal 的数据结构\n\n  \n\n![](statistic/asynccode-114.png)\n\n```Java\npublic class Thread implements Runnable {\n    //......\n    //与此线程有关的ThreadLocal值。由ThreadLocal类维护\n    ThreadLocal.ThreadLocalMap threadLocals = null;\n\n    //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护\n    ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;\n    //......\n}\n```\n\n- 从上面`Thread`类 源代码可以看出`Thread` 类中有一个 `threadLocals` 和 一个 `inheritableThreadLocals` 变量，它们都是 `ThreadLocalMap` 类型的变量,默认情况下这两个变量都是 null，只有当前线程调用 `ThreadLocal` 类的 `set`或`get`方法时才创建它们。\n    \n    - 每个线程在往`ThreadLocal`里放值的时候，**都会往自己的****`ThreadLocalMap`****里存**，读也是以`ThreadLocal`作为引用，在自己的`map`里找对应的`key`，从而实现了线程隔离。\n        \n    - `ThreadLocalMap`有点类似`HashMap`的结构，**只是****`HashMap`****是由数组+链表实现的，而****`ThreadLocalMap`****中并没有链表结构**。\n        \n\n`ThreadLocal`类的`set()`方法\n\n![](statistic/asynccode-115.png)\n\n  \n\n```Java\npublic void set(T value) {\n    //获取当前请求的线程\n    Thread t = Thread.currentThread();\n    //取出 Thread 类内部的 threadLocals 变量(哈希表结构)\n    ThreadLocalMap map = getMap(t);\n    if (map != null)\n        // 将需要存储的值放入到这个哈希表中\n        map.set(this, value);\n    else\n        createMap(t, value);\n}\nThreadLocalMap getMap(Thread t) {\n    return t.threadLocals;\n}\n```\n\n- **最终的变量是放在了当前线程的** **`ThreadLocalMap`** **中，并不是存在** **`ThreadLocal`** **上，****`ThreadLocal`** **可以理解为只是****`ThreadLocalMap`****的封装，传递了变量值。** `ThrealLocal` 类中可以通过`Thread.currentThread()`获取到当前线程对象后，直接通过`getMap(Thread t)`可以访问到该线程的`ThreadLocalMap`对象。\n    \n- **每个****`Thread`****中都具备一个****`ThreadLocalMap`****，而****`ThreadLocalMap`****可以存储以****`ThreadLocal`****为 key ，Object 对象为 value 的键值对。**\n    \n\n```Java\nThreadLocalMap(ThreadLocal\u003c?\u003e firstKey, Object firstValue) {\n    //......\n}\n```\n\n比如我们在同一个线程中声明了两个 `ThreadLocal` 对象的话， `Thread`内部都是使用仅有的那个`ThreadLocalMap` 存放数据的，`ThreadLocalMap`的 key 就是 `ThreadLocal`对象，value 就是 `ThreadLocal` 对象调用`set`方法设置的值。\n\n`ThreadLocal` 数据结构如下图所示：\n\n![](statistic/asynccode-110.png)\n\n`ThreadLocalMap`是`ThreadLocal`的静态内部类。\n\n![](statistic/asynccode-111.png)\n\n# ThreadLocal 内存泄露问题是怎么导致的？\n\n- `ThreadLocalMap` 中使用的 key 为 `ThreadLocal` 的**弱引用**，而 value 是强引用。\n    \n- 如果 `ThreadLocal` 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。\n    \n- `ThreadLocalMap` 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。**`ThreadLocalMap`** **实现中已经考虑了这种情况**，在调用 `set()`、`get()`、`remove()` 方法的时候，会清理掉 key 为 null 的记录。\n    \n- 使用完 `ThreadLocal`方法后 最好手动调用`remove()`方法\n    \n\n```Java\nstatic class Entry extends WeakReference\u003cThreadLocal\u003c?\u003e\u003e {/** The value associated with this ThreadLocal. */Object value;Entry(ThreadLocal\u003c?\u003e k, Object v) {super(k);\n        value = v;}}\n```\n\n\u003e ## Java 的四种应用类型\n\n\u003e 1. **强引用**：我们常常 new 出来的对象就是强引用类型，只要强引用存在，**垃圾回收器将永远不会回收被引用的对象，哪怕内存不足的时候**\n\u003e     \n\u003e 2. **软引用**：使用 SoftReference 修饰的对象被称为软引用，**软引用指向的对象在内存要溢出的时候被回收**\n\u003e     \n\u003e 3. **弱引用**：使用 WeakReference 修饰的对象被称为弱引用，**只要发生垃圾回收，若这个对象只被弱引用指向，那么就会被回收**\n\u003e     \n\u003e 4. **虚引用**：虚引用是最弱的引用，在 Java 中使用 PhantomReference 进行定义。**虚引用中唯一的作用就是用队列接收对象即将死亡的通知**\n\u003e     \n\n  \n\n## Key 为啥设计陈弱应用\n\nkey被设计成强引用，如果ThreadLocal Reference被销毁，此时**它指向ThreadLocal的强引用就没有了**，但是此时**key还强引用指向ThreadLocal**，就会导致ThreadLocal不能被回收，这时候就发生了内存泄漏的问题。\n\n# 父子线程怎么共享数据\n\nInheritableThreadLocal：在主线程的InheritableThreadLocal实例设置值，在子线程中就可以 拿到了\n\n![](statistic/asynccode-113.png)\n\n## 原理是啥？\n\n在Thread类里还有另外一个变量：\n\n![](statistic/asynccode-111.png)\n\n在Thread.init的时候，**如果父线程的 inheritableThreadLocals 不为空，就把它赋给当前线程（子线程）的 inheritableThreadLocals 。**\n\n![](statistic/asynccode-122.png)\n\n# ThreadLocalMap\n\nThreadLocalMap虽然被叫做Map，其实它是**没有实现Map接口的，但是结构还是和 HashMap比较类似的**，主要关注的是两个要素： 元素数组 和 散列方法 。\n\n![](statistic/asynccode-116.png)\n\n- 元素数组:一个table数组，存储Entry类型的元素，Entry是ThreaLocal弱引用作为key，Object作为value的结构\n    \n- 散列方法：散列方法就是怎么把对应的key映射到table数组的相应下标，ThreadLocalMap用 的是哈希取余法，取出key的threadLocalHashCode，然后和table数组长度减一\u0026 运算（**相当于取余**）。\n    \n\n![](statistic/asynccode-112.png)\n\nthreadLocalHashCode计算有点东西，每创建一个ThreadLocal对象，它就会新 增 **0x61c88647** ，这个值很特殊，它是斐波那契数 也叫 黄金分割数。 hash 增量为 这个数字，带来的好处就是 hash 分布非常均匀。\n\n  \n\n## ThreadLocalMap的Hash算法？\n\nThreadLocalMap没有使用链表，自然也不是用链地址法来解决冲突了，它用的是另外一种方式——**开放定址法**。\n\n- 简单来说，就是这个坑被 人占了，那就接着去找空着的坑。\n    \n- `ThreadLocalMap`中`hash`算法很简单，这里`i`就是当前 key 在散列表中对应的数组下标位置。\n    \n    ![](statistic/asynccode-113.png)\n    \n    - 这里最关键的就是`threadLocalHashCode`值的计算，`ThreadLocal`中有一个属性为`HASH_INCREMENT = 0x61c88647`\n        \n    - 创建一个`ThreadLocal`对象，这个`ThreadLocal.nextHashCode` 这个值就会增长 `0x61c88647` 这个值很特殊，它是**斐波那契数** 也叫 **黄金分割数**。`hash`增量为 这个数字，带来的好处就是 `hash` **分布非常均匀**。\n        \n\n  \n\n![](statistic/asynccode-113.png)\n\n## ThreadLocalMap的Hash冲突\n\n\u003e **注明：** 下面所有示例图中，**绿色块**`Entry`代表**正常数据**，**灰色块**代表`Entry`的`key`值为`null`，**已被垃圾回收**。**白色块**表示`Entry`为`null`。\n\n虽然`ThreadLocalMap`中使用了**黄金分割数**来作为`hash`计算因子，大大减少了`Hash`冲突的概率，但是仍然会存在冲突。\n\n`HashMap`中解决冲突的方法是在数组上构造一个**链表**结构，冲突的数据挂载到链表上，如果链表长度超过一定数量则会转化成**红黑树**。\n\n`ThreadLocalMap` 中并没有链表结构，所以这里不能使用 `HashMap` 解决冲突的方式了。\n\n![](statistic/asynccode-116.png)\n\n如上图所示，如果我们插入一个`value=27`的数据，通过 `hash` 计算后应该落入槽位 4 中，而槽位 4 已经有了 `Entry` 数据。\n\n此时就会线性向后查找，一直找到 `Entry` 为 `null` 的槽位才会停止查找，将当前元素放入此槽位中。当然迭代过程中还有其他的情况，**比如遇到了** **`Entry`** **不为** **`null`** **且** **`key`** **值相等的情况，还有** **`Entry`** **中的** **`key`** **值为** **`null`** **的情况等等都会有不同的处理**\n\n这里还画了一个`Entry`中的`key`为`null`的数据（**Entry=2 的灰色块数据**），因为`key`值是**弱引用**类型，所以会有这种数据存在。在`set`过程中，**如果遇到了****`key`****过期的****`Entry`****数据，**实际上是会进行一轮**探测式清理**操作的，具体操作方式后面会讲到。\n\n  \n\n## ThreadLocalMap.set详解\n\n看完了`ThreadLocal` hash 算法后，我们再来看`set`是如何实现的，有好几种情况\n\n- 第一种情况： 通过`hash`计算后的槽位对应的`Entry`数据为空，这里直接将数据放到该槽位即可。\n    \n\n![](statistic/asynccode-115.png)\n\n  \n\n- 第二种情况： 槽位数据不为空，`key`值与当前`ThreadLocal`通过`hash`计算获取的`key`值一致：这里直接更新该槽位的数据。\n    \n\n![](statistic/asynccode-114.png)\n\n- 第三种情况： 槽位数据不为空，往后遍历过程中，在找到`Entry`为`null`的槽位之前，没有遇到`key`过期的`Entry`，遍历散列数组，线性往后查找，如果找到`Entry`为`null`的槽位，则将数据放入该槽位中，或者往后遍历过程中，遇到了key 值相等的数据，直接更新即可。\n    \n\n![](statistic/asynccode-119.png)\n\n- 第四种情况： 槽位数据不为空，往后遍历过程中，在找到`Entry`为`null`的槽位之前，**遇到****`key`****过期的****`Entry`**，如下图，往后遍历过程中，遇到了`index=7`的槽位数据`Entry`的`key=null`：散列数组下标为 7 位置对应的`Entry`数据`key`为`null`，表明此数据`key`值已经被垃圾回收掉了，此时就会执行`replaceStaleEntry()`方法，该方法含义是**替换过期数据的逻辑**，以**index=7**位起点开始遍历，**进行探测式数据清理工作**。\n    \n    ![](statistic/asynccode-117.png)\n    \n      \n    \n    - 初始化探测式清理过期数据扫描的开始位置：`slotToExpunge = staleSlot = 7`，\n        \n    - 以当前`staleSlot`开始 向前迭代查找，找其他过期的数据，然后更新过期数据起始扫描下标`slotToExpunge`。`for`循环迭代，直到碰到`Entry`为`null`结束。\n        \n    - 如果找到了过期的数据，继续向前迭代，直到遇到`Entry=null`的槽位才停止迭代，如下图所示，**slotToExpunge 被更新为 0**：\n        \n    - 上面向前迭代的操作是为了更新探测清理过期数据的起始下标`slotToExpunge`的值，这个值在后面会讲解，它是**用来判断当前过期槽位****`staleSlot`****之前是否还有过期元素**。\n        \n    \n    ![](statistic/asynccode-123.png)\n    \n    - 接着开始以`staleSlot`位置(`index=7`)向后迭代，如果找到了相同 key 值的 Entry 数据\n        \n    \n    ![](statistic/asynccode-120.png)\n    \n\n  \n\n从当前节点`staleSlot`向后查找`key`值相等的`Entry`元素，找到后更新`Entry`的值并交换`staleSlot`元素的位置(`staleSlot`位置为过期元素)，更新`Entry`数据，然后开始进行过期`Entry`的清理工作，如下图所示：\n\n![](statistic/asynccode-119.png)\n\n  \n\n向后遍历过程中，如果没有找到相同 key 值的 Entry 数据：\n\n![](statistic/asynccode-119.png)\n\n  \n\n从当前节点`staleSlot`向后查找`key`值相等的`Entry`元素，直到`Entry`为`null`则停止寻找。通过上图可知，此时`table`中没有`key`值相同的`Entry`。\n\n创建新的`Entry`，替换`table[stableSlot]`位置：\n\n![](statistic/asynccode-120.png)\n\n替换完成后也是进行过期元素清理工作，清理工作主要是有两个方法：`expungeStaleEntry()`和`cleanSomeSlots()`，具体细节后面会讲到，请继续往后看\n\n  \n\n  \n\n## ThreadLocalMap扩容机制\n\n在ThreadLocalMap.set()方法的最后，\n\n- 如果**执行完启发式清理工作后**，未清理到任何数据，\n    \n- 且当前散列数组中 Entry 的数量已经达到了列表的**扩容阈值 (len*2/3)** ，\n    \n\n就开始执行 rehash() 逻辑：\n\n![](statistic/asynccode-116.png)\n\n  \n\n看rehash()具体实现：\n\n- **这里会先进行探测式清理工作，**\n    \n- **然后还要根据条件判断也就是 size \u003e= threshold* 3/4 来决定是否需要扩容**\n    \n- 扩容后的 **newTab 的大小为老数组的两倍**，然后遍历老的table数组，散列方法重新计算位置，开放地址解决冲突，然后放到新的newTab ，遍历完成之后， oldTab 中所有的 entry 数据都已经放入到 newTab 中了，然后table引用指向 newTab\n    \n\n![](statistic/asynccode-120.png)\n\n  \n\n  \n\n## ThreadLocalMap.get详解\n\n- 第一种情况： 通过查找`key`值计算出散列表中`slot`位置，然后该`slot`位置中的`Entry.key`和查找的`key`一致，则直接返回：\n    \n\n![](statistic/asynccode-118.png)\n\n- 第二种情况： `slot`位置中的`Entry.key`和要查找的`key`不一致：\n    \n    - 继续往后迭代查找\n        \n    - 迭代到`index=5`的数据时，此时`Entry.key=null`，触发一次探测式数据回收操作\n        \n\n![](statistic/asynccode-121.png)\n\n- 执行`expungeStaleEntry()`方法，执行完后，`index 5,8`的数据都会被回收，而`index 6,7`的数据都会前移。`index 6,7`前移之后，继续从 `index=5` 往后迭代，于是就在 `index=5` 找到了`key`值相等的`Entry`数据，如下图所示\n    \n\n![](statistic/asynccode-121.png)\n\n  \n\n## ThreadLocalMap 过期Key的启发式清理过程\n\n  \n\n多次提及到`ThreadLocalMap`过期key的两种清理方式：**探测式清理(expungeStaleEntry())**、**启发式清理(cleanSomeSlots())**\n\n探测式清理是以当前`Entry` 往后清理，遇到值为`null`则结束清理，属于**线性探测清理**。\n\n![](statistic/asynccode-124.png)","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E9%9B%86%E5%90%88/%E9%9B%86%E5%90%88%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"集合面试题、","content":"![[Java集合框架常见面试题.pdf]]\n\n\n\n\n\n\n\n\n\n\n![[集合框架.pdf]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E9%9B%86%E5%90%88/Java-%E9%9B%86%E5%90%88-overview":{"title":"Java 集合 overview","content":"\n# 常见的集合\n\nJava 集合， 也叫作容器，主要是由两大接口派生而来：一个是 `Collection`接口，主要用于存放单一元素；另一个是 `Map` 接口，主要用于存放键值对。对于`Collection` 接口，下面又有三个主要的子接口：`List`、`Set` 和 `Queue`。\n\nJava 集合框架如下图所示\n\n![](statistic/asynccode-40.png)\n\n  \n\n其中 Collection 是集合 List 、 Set 、Queue的父接口，它主要有两个子接口：\n\n- `List`(对付顺序的好帮手): 存储的元素是有序的、可重复的。\n    \n- `Set`(注重独一无二的性质): 存储的元素是无序的、不可重复的。\n    \n- `Queue`(实现排队功能的叫号机): 按特定的排队规则来确定先后顺序，存储的元素是有序的、可重复的。\n    \n\nMap 是另外的接口，是键值对映射结构的集合\n\n# 集合框架底层数据结构总结\n\n## List\n\n- `ArrayList`： `Object[]` 数组\n    \n- `Vector`：`Object[]` 数组\n    \n- `LinkedList`： 双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)\n    \n\n## Set\n\n- `HashSet`(无序，唯一): 基于 `HashMap` 实现的，底层采用 `HashMap` 来保存元素\n    \n- `LinkedHashSet`: `LinkedHashSet` 是 `HashSet` 的子类，并且其内部是通过 `LinkedHashMap` 来实现的。有点类似于我们之前说的 `LinkedHashMap` 其内部是基于 `HashMap` 实现一样，不过还是有一点点区别的\n    \n- `TreeSet`(有序，唯一): 红黑树(自平衡的排序二叉树)\n    \n\n## Queue\n\n- `PriorityQueue`: `Object[]` 数组来实现二叉堆\n    \n- `ArrayQueue`: `Object[]` 数组 + 双指针\n    \n\n再来看看 `Map` 接口下面的集合。\n\n## Map\n\n- `HashMap`： JDK1.8 之前 `HashMap` 由**数组+链表**组成的，数组是 `HashMap` 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当**链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）**时，将链表转化为红黑树，以减少搜索时间\n    \n- `LinkedHashMap`： `LinkedHashMap` 继承自 `HashMap`，所以它的**底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成**。另外，`LinkedHashMap` 在上面结构的基础上，**增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序**。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：[《LinkedHashMap 源码详细分析（JDK1.8）》open in new window](https://www.imooc.com/article/22931)\n    \n- `Hashtable`： 数组+链表组成的，数组是 `Hashtable` 的主体，链表则是主要为了解决哈希冲突而存在的\n    \n- `TreeMap`： 红黑树（自平衡的排序二叉树）","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":["Overview/Java/集合"]},"/Java/%E9%9B%86%E5%90%88/List":{"title":"List","content":"\n# ArrayList 和 Vector 的区别\n\n- `ArrayList` 是 `List` 的主要实现类，底层使用 `Object[]`存储，适用于频繁的查找工作，线程不安全 ；\n    \n- `Vector` 是 `List` 的古老实现类，底层使用`Object[]` 存储，线程安全的。\n    \n\n  \n\n  \n\n# ArrayList和LinkedList有什么区别？\n\n- 是否线程安全\n    \n    - `ArrayList` 和 `LinkedList` 都是不同步的，也就是不保证线程安全\n        \n- 数据结构不同\n    \n    - ArrayList基于数组实现\n        \n    - LinkedList基于双向链表实现\n        \n\n![](statistic/asynccode-43.png)\n\n- 多数情况ArrayList更利于查找，LinkedList更利于增删\n    \n    - 查找：ArrayList基于数组实现，get(int index)可以直接通过数组下标获取，时间复杂度 是O(1)；LinkedList基于链表实现，get(int index)需要遍历链表，时间复杂度是 O(n)；当然，get(E element)这种查找，两种集合都需要遍历，时间复杂度都是 O(n)。\n        \n    - 增删：ArrayList增删如果是数组末尾的位置，直接插入或者删除就可以了，但是如果插 入中间的位置，就需要把插入位置后的元素都向前或者向后移动，甚至还有可能 触发扩容；双向链表的插入和删除只需要改变前驱节点、后继节点和插入节点的 指向就行了，不需要移动元素\n        \n\n![](statistic/asynccode-42.png)\n\n- 是否支持随机访问\n    \n    - ArrayList基于数组，所以它可以根据下标查找，**支持随机访问**，当然，它也实现 了**RandmoAccess** 接口，这个接口只是用来标识是否支持随机访问。\n        \n    - LinkedList基于链表，所以它没法根据序号直接获取元素，它没有实现 RandmoAccess 接口，标记**不支持随机访问**\n        \n- 内存占用，ArrayList基于数组，是**一块连续的内存空间**，LinkedList基于链 表，**内存空间不连续**，它们在空间占用上都有一些额外的消耗：\n    \n    - ArrayList是预先定义好的数组，可能会有空的内存空间，存在一定空间浪费\n        \n    - LinkedList每个节点，需要存储前驱和后继，所以每个节点会占用更多的空间\n        \n\n  \n\n# ArrayList的扩容机制了解吗\n\nArrayList是基于数组的集合，数组的容量是在定义的时候确定的，如果数组满了，再插入，就会数组溢出。所以在插入时候，会先检查是否需要扩容，如果当前容量+1超过数组长度，就会进行扩容\n\nArrayList的扩容是创建一个**1.5倍的新数组**，然后把原数组的值拷贝过去。\n\n![](statistic/asynccode-45.png)\n\n具体的过程：[ArrayList源码\u0026扩容机制分析](https://javaguide.cn/java/collection/arraylist-source-code.html#%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90-arraylist-%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6)\n\n# ArrayList怎么序列化的知道吗？ 为什么用transient修饰数组\n\nArrayList的序列化不太一样，它**使用 transient 修饰存储元素的 elementData 的** **数组**， **transient 关键字的作用是让被修饰的成员属性不被序列化**。\n\n## 为什么最ArrayList不直接序列化元素数组呢？\n\n出于效率的考虑，数组可能长度100，但实际只用了50，剩下的50不用其实不用序列 化，这样可以提高序列化和反序列化的效率，还可以节省内存空间\n\n## 那ArrayList怎么序列化呢？\n\nArrayList通过两个方法readObject、writeObject自定义序列化和反序列化策略，实 际直接使用两个流 ObjectOutputStream 和 ObjectInputStream 来进行序列化和 反序列化。\n\n![](statistic/asynccode-41.png)\n\n  \n\n# 快速失败(fail-fast)和安全失败(fail-safe)了解吗\n\n## 快速失败（fail—fast）\n\n**快速失败（fail—fast）：快速失败是Java集合的一种错误检测机制**\n\n- 在用迭代器遍历一个集合对象时，代集合的过程中该集合在结构上发生改变的时候，则会抛出Concurrent Modification Exception。\n    \n- 原理：迭代器在遍历时直接访问集合中的内容，并**且在遍历过程中使用一个** **modCount 变量**。集合在被遍历期间如果内容发生变化，就会改变 modCount 的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测 modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常， 终止遍历。\n    \n- 注意：这里异常的抛出条件是检测到 **modCount！=expectedmodCount** 这个条 件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值， 则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程， 这个异常只建议用于检测并发修改的bug。\n    \n- 场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改 （迭代过程中被修改），比如ArrayList 类。\n    \n\n## 安全失败（fail—safe）\n\n采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先 复制原有集合内容，在拷贝的集合上进行遍历\n\n- 采用安全失败机制的集合容器，在遍历时**不是直接在集合内容上访问的，而是先** **复制原有集合内容，在拷贝的集合上进行遍历**。\n    \n- 原理：由于迭代时是**对原集合的拷贝进行遍历**，所以在遍历过程中对原集合所作 的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。\n    \n- 缺点：基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样 地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿 到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。\n    \n- 场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用， 并发修改，比如CopyOnWriteArrayList类\n    \n\n  \n\n# 有哪几种实现ArrayList线程安全的方法\n\nfail-fast是一种可能触发的机制，实际上，ArrayList的线程安全仍然没有保证，一 般，保证ArrayList的线程安全可以通过这些方案： * 使用 Vector 代替 ArrayList。（不推荐，Vector是一个历史遗留类）\n\n- 使用 Collections.synchronizedList 包装 ArrayList，然后操作包装后的 list。\n    \n- 使用 CopyOnWriteArrayList 代替 ArrayList。\n    \n- 在使用 ArrayList 时，应用程序通过同步机制去控制 ArrayList 的读写。\n    \n\n# CopyOnWriteArrayList了解多少\n\nCopyOnWriteArrayList就是线程安全版本的ArrayList。 它的名字叫 CopyOnWrite ——**写时复制，已经明示了它的原理。** CopyOnWriteArrayList采用了一种读写分离的并发策略。**CopyOnWriteArrayList容器** **允许并发读，读操作是无锁的，性能较高。至于写操作，比如向容器中添加一个元** **素，则首先将当前容器复制一份，然后在新副本上执行写操作，结束之后再将原容** **器的引用指向新容器**\n\n![](statistic/asynccode-44.png)\n\n# **HashMap** 内部节点是有序的吗？\n\nHashMap是无序的，根据 hash 值随机插入。如果想使用有序的Map，可以使用\n\nLinkedHashMap 或者 TreeMap。\n\n  \n\n# **LinkedHashMap** 怎么实现有序的\n\nLinkedHashMap维护了一个双向链表，有头尾节点，同时 LinkedHashMap 节点 Entry\n\n内部除了继承 HashMap 的 Node 属性，还有 before 和 after 用于标识前置节点和后\n\n置节点\n\n  \n\n# TreeMap怎么实现有序\n\n\n","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":["Java","集合","List"]},"/Java/%E9%9B%86%E5%90%88/Map":{"title":"Map","content":"\n# HashMap 和 HashTable 的区别\n\n- **线程是否安全：** **`HashMap`** **是非线程安全的，****`Hashtable`** **是线程安全的**,因为 `Hashtable` 内部的方法基本都经过`synchronized` 修饰。（如果你要保证线程安全的话就使用 `ConcurrentHashMap` 吧！）；\n    \n- **效率：** 因为线程安全的问题，**`HashMap`** **要比** **`Hashtable`** **效率高一点**。另外，`Hashtable` 基本被淘汰，不要在代码中使用它；\n    \n- **对 Null key 和 Null value 的支持：** `H`**`ashMap`** **可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；Hashtable 不允许有 null 键和 null 值**，否则会抛出 `NullPointerException`。\n    \n- **初始容量大小和每次扩充容量大小的不同 ：**\n    \n    - ① 创建时如果不指定容量初始值，**`Hashtable`** **默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1**。`HashMap` 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。\n        \n    - ② 创建时如果给定了容量初始值，那**么** **`Hashtable`** **会直接使用你给定的大小**，而 `HashMap` **会将其扩充为 2 的幂次方大小（****`HashMap`** **中的****`tableSizeFor()`****方法保证，下面给出了源代码）。也就是说** **`HashMap`** 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。\n        \n- **底层数据结构：** JDK1.8 以后的 `HashMap` 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间（后文中我会结合源码对这一过程进行分析）。`Hashtable` 没有这样的机制。\n    \n\n  \n\n# HashMap 和 HashSet 区别\n\n`HashSet` 源码的话就应该知道：`HashSet` 底层就是基于 `HashMap` 实现的。（`HashSet` 的源码非常非常少，因为除了 `clone()`、`writeObject()`、`readObject()`是 `HashSet` 自己不得不实现之外，其他方法都是直接调用 `HashMap` 中的方法。\n\n  \n\n| HashMap                            | HashSet                                                                                                          |\n| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n| 实现了 Map 接口                    | 实现 Set 接口                                                                                                    |\n| 存储键值对                         | 仅存储对象                                                                                                       |\n| 调用 put()向 map 中添加元素        | 调用 add()方法向 Set 中添加元素                                                                                  |\n| HashMap 使用键（Key）计算 hashcode | HashSet 使用成员对象来计算 hashcode 值，对于两个对象来说 hashcode 可能相同，所以equals()方法用来判断对象的相等性 |\n\n# HashMap 和 TreeMap 区别\n\n- `TreeMap` 和`HashMap` 都继承自`AbstractMap` ，但是需要注意的是`TreeMap`它还实现了`NavigableMap`接口和`SortedMap` 接口。\n    \n    - 实现 `NavigableMap` 接口让 `TreeMap` 有了对集合内元素的搜索的能力\n        \n    - 实现 `SortedMap` 接口让 `TreeMap` 有了对集合内元素的搜索的能力\n        \n\n  \n\n# HashSet 如何实现检查重复\n\n- 在 JDK1.8 中，`HashSet`的`add()`方法只是简单的调用了`HashMap`的`put()`方法，并且判断了一下返回值以确保是否有重复元素。直接看一下`HashSet`中的源码：\n    \n\n```Go\n// Returns: true if this set did not already contain the specified element\n// 返回值：当 set 中没有包含 add 的元素时返回真\npublic boolean add(E e) {\n        return map.put(e, PRESENT)==null;\n}\n```\n\n实际上无论`HashSet`中是否已经存在了某元素，`HashSet`都会直接插入，只是会在`add()`方法的返回值处告诉我们插入前是否存在相同元素\n\n# 能说一下HashMap的数据结构吗\n\nJDK1.7的数据结构是 数组 + 链表\n\nJDK1.8的数据结构是 数组 + 链表 + 红黑树 。\n\n![](statistic/asynccode-51.png)\n\n桶数组是用来存储数据元素，链表是用来解决冲突，红黑树是为了提高查询 的效率。\n\n- 数据元素通过映射关系，也就是散列函数，映射到桶数组对应索引的位置\n    \n- 如果发生冲突，从冲突的位置拉一个链表，插入冲突的元素\n    \n- 如果链表长度\u003e8\u0026数组大小\u003e=64，链表转为红黑树\n    \n- 如果红黑树节点个数\u003c6 ，转为链表\n    \n\n  \n\n# 对红黑树了解多少？为什么不用二叉树平衡树呢？\n\n红黑树本质上是一种二叉查找树，为了保持平衡，它又在二叉查找树的基础上增加\n\n了一些规则：\n\n1. 每个节点要么是红色，要么是黑色；\n    \n2. 根节点永远是黑色的；\n    \n3. 所有的叶子节点都是是黑色的（注意这里说叶子节点其实是图中的 NULL 节点）；\n    \n4. 每个红色节点的两个子节点一定都是黑色；\n    \n5. 从任一节点到其子树中每个叶子节点的路径都包含相同数量的黑色节点；\n    \n\n![](statistic/asynccode-47.png)\n\n## 为啥不用二叉树\n\n红黑树是一种平衡的二叉树，插入、删除、查找的最坏时间复杂度都为 O(logn)，避 免了二叉树最坏情况下的O(n)时间复杂度。\n\n## 为啥不用平衡二叉树\n\n平衡二叉树是比红黑树更严格的平衡树，为了保持保持平衡，需要旋转的次数更 多，也就是说平衡二叉树保持平衡的效率更低，所以平衡二叉树插入和删除的效率 比红黑树要低\n\n  \n\n# 红黑树怎么保持平衡的知道吗\n\n红黑树有两种方式保持平衡： 旋转 和 染色 。\n\n- 旋转：旋转分为两种，左旋和右旋\n    \n\n![](statistic/asynccode-46.png)\n\n![](statistic/asynccode-47.png)\n\n- 染色\n    \n\n![](statistic/asynccode-50.png)\n\n  \n\n# **HashMap**的**put**流程知道吗？\n\n![](statistic/asynccode-48.png)\n\n1. 首先进行哈希值的扰动，获取一个新的哈希值。\n    \n\n```Go\n  (key == null) ? 0 : (h = key.hashCode()) ^ (h \u003e\u003e\u003e 16); \n```\n\n2. 判断tab是否位空或者长度为0，如果是则进行扩容操作。\n    \n3. 根据哈希值计算下标，如果对应小标正好没有存放数据，则直接插入即可否则需要覆盖。\n    \n\n```Go\ntab[i = (n - 1) \u0026 hash] \n```\n\n4. 判断tab[i]是否为树节点，否则向链表中插入数据，是则向树中插入节点。\n    \n5. 如果链表中插入节点的时候，链表长度大于等于8，则需要把链表转换为红黑树。\n    \n\n```Go\ntreeifyBin(tab, hash); \n```\n\n6. 最后所有元素处理完成后，判断是否超过阈值； threshold ，超过则扩容。\n    \n\n# **HashMap**怎么查找元素的呢？\n\n![](statistic/asynccode-48.png)\n\n  \n\nHashMap的查找就简单很多：\n\n1. 使用扰动函数，获取新的哈希值\n    \n2. 计算数组下标，获取节点\n    \n3. 当前节点和key匹配，直接返回\n    \n4. 否则，当前节点是否为树节点，查找红黑树\n    \n5. 否则，遍历链表查找\n    \n\n  \n\n  \n\n# **HashMap**的哈希扰动函数是怎么设计的?\n\nHashMap的哈希函数是先拿到 key 的hashcode，**是一个32位的int类型的数值**，然后**让 hashcode的高16位和低16位进行异或操作**。\n\n![](statistic/asynccode-51.png)\n\n# 为什么哈希扰动函数能降**hash**碰撞\n\n因为 key.hashCode() 函数调用的是 key 键值类型自带的哈希函数，返回 int 型散列 值。int 值范围为 **-2147483648~2147483647**，加起来大概 40 亿的映射空间\n\n  \n\n源码中模运算就是把散列值和数组长度 - 1 做一个 \" 与\u0026 \" 操作，**位运算比取余 % 运 算要快**。\n\n## 正好解释了为什么 HashMap 的数组长度要取 2 的整数幂。\n\n- 方便取余：因为这样（数组长度 - 1）正好相当于一个 “低位掩码”。 与操作的结果就是散列值的**高位全部归零**，只保留低位值，用来做数组下标访问。\n    \n- 在扩容时，利用扩容后的大小也是2的倍数，将已经产生hash碰撞 的元素完美的转移到新的table中去\n    \n\n  \n\n## 低位有规律，冲突依然会很大，如何解决的\n\n新的问题来了，就算散列值分布再松散，要是只取最后几位的话，碰撞也会很严重。如果散列本身做得不好，分布上成等差数列的漏洞，如果正好让最后几个低位呈现规律性重复,\n\n自己的高半区和低半区做异或，就是为了**混合原始哈希码的高位和低位，以此来加大低位的随机性**\n\n  \n\n# 如果初始化**HashMap**，传一个**17**的值 **new HashMap\u003c\u003e** ，它会 怎么处理\n\nHashMap会向上寻找 **离得最近的2的倍数** ，所以传入17，但HashMap的实际容量是32\n\n  \n\n  \n\n# Hash函数的构造方法\n\nHashMap里哈希构造函数的方法叫\n\n- 除留取余法 ：H（key)=key%p（p\u003c=N）,关键字除以一个不大于哈希表长度的正 整数p，所得余数为地址，当然HashMap里进行了优化改造，效率更高，散列也 更均衡。\n    \n\n其他的hash函数\n\n- 直接定址法 ：直接根据 key 来映射到对应的数组位置，例如1232放到下标1232的位置。\n    \n- 数字分析法 ：取 key 的某些数字（例如十位和百位）作为映射的位置\n    \n- 平方取中法 ：取 key 平方的中间几位作为映射的位置\n    \n- 折叠法 ：将 key 分割成位数相同的几段，然后把它们的叠加和作为映射的位置\n    \n\n  \n\n# 解决hash冲突的方法\n\nHashMap使用**链表的原因**为了处理哈希冲突，这种方法就是 所谓的：\n\n- 链地址法 ：在冲突的位置拉一个链表，把冲突的元素放进去\n    \n\n其他方法：\n\n- 开放定址法：开放定址法就是从冲突的位置再接着往下找，给冲突元素找个空位。\n    \n    - 线行探查法: 从冲突的位置开始，依次判断下一个位置是否空闲，直至找到 空闲位置\n        \n    - 平方探查法: 从冲突的位置x开始，第一次增加 1^2 个位置，第二次增加 2^2 …，直至找到空闲的位置\n        \n    - 再哈希法 ：换种哈希函数，重新计算冲突元素的地址\n        \n    - 建立公共溢出区 ：再建一个数组，把冲突的元素放进去。\n        \n\n  \n\n# **HashMap**链表转红黑树的阈值为**8**呢\n\n和统计学有关。理想情况下，使用**随机哈希码，链表里的节点符合泊松分布**，出现节点个数的概率是递减的，节点个数为8的情况，发生概率仅为 0.00000006\n\n## 红黑树转回链表的阈值为什么是6，而不是8？\n\n是因为如果这个阈值也设置成8， 假如发生碰撞，**节点增减刚好在8附近，会发生链表和红黑树的不断转换**，导致资源浪费\n\n  \n\n  \n\n# 扩容在什么时候呢？为什么扩容因子是**0.75**？\n\n当前HashMap的元素个数**达到一个临界值的时候，就会触发扩容，把所有元素rehash**之后再放在扩容后的容器中，这是一个相当耗时的操作\n\n个 临界值**threshold 就是由加载因子和当前容器的容量大小来确定的**，假如采用默认的构造方法：\n\n\u003e 临界值（threshold ）= 默认容量（DEFAULT_INITIAL_CAPACITY） * 默认扩容因子（DEFAULT_LOAD_FACTOR）\n\n那就是大于 16x0.75=12 时，就会触发扩容操作\n\n## 为什么选择了0.75作为HashMap的默认加载因子\n\n简单来说，这是对 空间 成本和 时间 成本平衡的考虑\n\n- HashMap的散列构造方式是Hash取余，负载因子决定元素个数达到多少时候扩容\n    \n    - 多了哈希冲突变多\n        \n    - 少了需要更多的空间\n        \n    \n      \n    \n\n  \n\n# 扩容机制了解\n\n- HashMap是基于数组+链表和红黑树实现的，但用于存放key值的桶数组的长度是固定的，由初始化参数确定。\n    \n- 扩容中有一个非常重要的点，就是jdk1.8中的优化操作，可以不需要再重新 计算每一个元素的哈希值。\n    \n- 因为HashMap的初始容量是2的次幂，扩容之后的长度是原来的二倍，**新的容量也是2的次幂**，所以，元素，要么在原位置，要么在原位置再移动2的次幂\n    \n- 只需要看原来的hash值新增的那一位是0还是1就行了，是0的话索引 没变，是1的化变成 原索引+oldCap\n    \n\n![](statistic/asynccode-49.png)\n\n# **jdk1.8**对**HashMap**主要做了哪些优化\n\n1. 数据结构：数组 + 链表改成了数组 + 链表或红黑树\n    \n    1. 原因 ：发生 hash 冲突，元素会存入链表，链表过长转为红黑树，将时间复杂度 由 O(n) 降为 O(logn)\n        \n2. 链表插入方式：链表的插入方式从头插法改成了尾插法\n    \n    1. 原因 ：因为 1.7 头插法扩容时，头插法会使链表发生反转，多线程环境下会产生环。\n        \n3. 扩容**rehash**：扩容的时候 1.7 需要对原数组中的元素进行重新 hash 定位在新数组的位置，1.8 采用更简单的判断逻辑，不需要重新通过哈希函数计算位置，新的位置不变或索引 + 旧的数组容量大小。\n    \n    1. 原因： 提高扩容的效率，更快地扩容。\n        \n4. 扩容时机：在插入时，1.7 先判断是否需要扩容，再插入，1.8 先进行插入，插入完成再判断是否需要扩容；\n    \n5. 散列函数：1.7 做了四次移位和四次异或，jdk1.8只做一次。\n    \n    1. 原因 ：做 4 次的话，边际效用也不大，改为一次，提升效率\n        \n\n  \n\n  \n\n# **HashMap** 是线程安全的吗？多线程下会有什么问题？\n\nHashMap**不是线程安全的**，可能会发生这些问题：\n\n- 多线程下扩容死循环。\n    \n    - 1.7 头插法，多线程会出现环\n        \n    - 1.8 使用尾插法，不会出现\n        \n- 多线程的 put 可能导致元素的丢失。\n    \n- 多线程的 put 可能导致元素的丢失。\n    \n\n  \n\n# 有什么办法能解决**HashMap**线程不安全的问题呢\n\nHashTable、Collections.synchronizedMap、以及 ConcurrentHashMap 可以实现线程安全的 Map\n\n- HashTable ：是直接在操作方法上加 synchronized 关键字，锁住整个table数组，粒 度比较大；\n    \n- Collections.synchronizedMap 是使用 Collections 集合工具的内部类，通过传入Map 封装出一个 SynchronizedMap 对象，内部定义了一个对象锁，方法内通过对象锁实现；\n    \n- ConcurrentHashMap 在jdk1.7中使用分段锁，在jdk1.8中使用CAS+synchronized。\n    \n\n  \n\n# 能具体说一下**ConcurrentHashmap**的实现\n\n- jdk1.7版本是基于 分段锁 实现，在jdk1.8是基于 CAS+synchronized 实现。\n    \n\n  \n\n## 1.7 分段锁\n\n包含一个Segment数组，Segment继承于ReentrantLock，Segment则包含HashEntry的数组，HashEntry本\n\n身就是一个链表的结构，具有保存key、value的能力能指向下一个节点的指针。\n\n实际上就是**相当于每个Segment都是一个HashMap，默认的Segment长度是16，也就**\n\n**是支持16个线程的并发写**，Segment之间相互不会受到影响。\n\n![](statistic/asynccode-48.png)\n\n**put**流程\n\n整个流程和HashMap非常类似，只不过是先定位到具体的Segment，然后通过\n\nReentrantLock去操作而已，后面的流程，就和HashMap基本上是一样的。\n\n1. 计算hash，定位到segment，segment如果是空就先初始化\n    \n2. 使用ReentrantLock加锁，如果获取锁失败则尝试自旋，自旋超过次数就阻塞获\n    \n\n取，保证一定获取锁成功\n\n3. 遍历HashEntry，就是和HashMap一样，数组中key和hash一样就直接替换，不存\n    \n\n在就再插入链表，链表同样操作\n\n  \n\nget也很简单，key通过hash定位到segment，再遍历链表定位到具体的元素上，需要\n\n注意的是value是volatile的，所以get是不需要加锁的\n\n## **1.8 CAS+synchronized**\n\n它的数据结构和HashMap是一样的，数组+链表+红黑树。它实现线程安全的关键点在于put流程。\n\n**Put 查询**\n\n- 首先计算hash，遍历node数组，如果node是空的话，就通过CAS+自旋的方式初始化\n    \n- 如果当前数组位置是空则直接通过CAS自旋写入数据\n    \n- 如果hash==MOVED，说明需要扩容，执行扩容\n    \n- 如果都不满足，就使用synchronized写入数据，写入数据同样判断链表、红黑 树，链表写入和HashMap的方式一样，key hash一样就覆盖，反之就尾插法，链表长度超过8就转换成红黑树\n    \n\n![](statistic/asynccode-53.png)\n\n# ConcurrentHashMap 和 Hashtable 的区别\n\n- **底层数据结构：** JDK1.7 的 `ConcurrentHashMap` 底层采用 **分段的数组+链表** 实现，JDK1.8 采用的数据结构跟 `HashMap1.8` 的结构一样**，数组+链表/红黑二叉树**。`Hashtable` 和 JDK1.8 之前的 `HashMap` 的底层数据结构类似都是采用 **数组+链表** 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的；\n    \n- **实现线程安全的方式（重要）：**\n    \n    - 在 JDK1.7 的时候，`ConcurrentHashMap` 对整个桶数组进行了分割分段(`Segment`，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图），多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。\n        \n    - 到了 JDK1.8 的时候，`ConcurrentHashMap` 已经摒弃了 `Segment` 的概念，而是直接用 `Node` 数组+链表+红黑树的数据结构来实现，并发控制使用 `synchronized` 和 CAS 来操作。（JDK1.6 以后 `synchronized` 锁做了很多优化） 整个看起来就像是优化过且线程安全的 `HashMap`，虽然在 JDK1.8 中还能看到 `Segment` 的数据结构，但是已经简化了属性，只是为了兼容旧版本；\n        \n    - **`Hashtable`****(同一把锁)** :使用 `synchronized` 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。\n        \n\n下面，我们再来看看两者底层数据结构的对比图。\n\n## HashTable\n\n![](statistic/asynccode-52.png)\n\n## 1.7 的ConcurrentHashMap\n\n![](statistic/asynccode-49.png)\n\n  \n\n`ConcurrentHashMap` 是由 `Segment` 数组结构和 `HashEntry` 数组结构组成。\n\n`Segment` 数组中的每个元素包含一个 `HashEntry` 数组，每个 `HashEntry` 数组属于链表结构。\n\n## **1.8 的ConcurrentHashMap**\n\n**Node 数组 + 链表 / 红黑树**。不过，Node 只能用于链表的情况，红黑树的情况需要使用 **`TreeNode`**。当冲突链表达到一定长度时，链表会转换成红黑树。\n\n`TreeNode`是存储红黑树节点，被`TreeBin`包装。`TreeBin`通过`root`属性维护红黑树的根结点，因为红黑树在旋转的时候，根结点可能会被它原来的子节点替换掉，在这个时间点**，如果有其他线程要写这棵红黑树就会发生线程不安全问题**，所以在 `ConcurrentHashMap` 中`TreeBin`通过`waiter`属性维护当前使用这棵红黑树的线程，来防止其他线程的进入。\n\n```Go\nstatic final class TreeBin\u003cK,V\u003e extends Node\u003cK,V\u003e {\n        TreeNode\u003cK,V\u003e root;\n        volatile TreeNode\u003cK,V\u003e first;\n        volatile Thread waiter;\n        volatile int lockState;\n        // values for lockState\n        static final int WRITER = 1; // set while holding write lock\n        static final int WAITER = 2; // set when waiting for write lock\n        static final int READER = 4; // increment value for setting read lock\n...\n}\n```","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E9%9B%86%E5%90%88/Queue":{"title":"Queue","content":"\n# Queue 与 Deque 的区别\n\n- Queue\n    \n    - `Queue` 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 **先进先出（FIFO）** 规\n        \n    - `Queue` 扩展了 `Collection` 的接口，根据 **因为容量问题而导致操作失败后处理方式的不同** 可以分为两类方法: 一种在操作失败后会抛出异常，另一种则会返回特殊值。\n        \n    \n\n| Queue 接口   | 抛出异常  | 返回特殊值 |\n| ------------ | --------- | ---------- |\n| 插入队尾     | add(E e)  | offer(E e) |\n| 删除队首     | remove()  | poll()     |\n| 查询队首元素 | element() | peek()     |\n    \n- Deque\n    \n    - `Deque` 是双端队列，在队列的两端均可以插入或删除元素。\n        \n    - `Deque` 扩展了 `Queue` 的接口, 增加了在队首和队尾进行插入和删除的方法，同样根据失败后处理方式的不同分为两类：\n        \n    \n| Deque 接口   | 抛出异常      | 返回特殊值      |\n    | ------------ | ------------- | --------------- |\n    | 插入队首     | addFirst(E e) | offerFirst(E e) |\n    | 插入队尾     | addLast(E e)  | offerLast(E e)  |\n    | 删除队首     | removeFirst() | pollFirst()     |\n    | 删除队尾     | removeLast()  | pollLast()      |\n    | 查询队首元素 | getFirst()    | peekFirst()     |\n    | 查询队尾元素 | getLast()     | peekLast()      |\n    \n\n  \n\n  \n\n# ArrayDeque 与 LinkedList 的区别\n\n`ArrayDeque` 和 `LinkedList` 都实现了 `Deque` 接口，两者都具有队列的功能，但两者有什么区别呢？\n\n- `ArrayDeque` 是基于**可变长的数组和双指针**来实现，而 `LinkedList` 则通过链表来实现。\n    \n- `ArrayDeque` 不支持存储 `NULL` 数据，但 `LinkedList` 支持。\n    \n- `ArrayDeque` 是在 JDK1.6 才被引入的，而`LinkedList` 早在 JDK1.2 时就已经存在。\n    \n- `ArrayDeque` 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 `LinkedList` 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。\n    \n\n  \n\n  \n\n# PriorityQueue\n\n这里列举其相关的一些要点：\n\n- `PriorityQueue` 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据\n    \n- `PriorityQueue` 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。\n    \n- `PriorityQueue` 是非线程安全的，且不支持存储 `NULL` 和 `non-comparable` 的对象。\n    \n- `PriorityQueue` 默认是小顶堆，但可以接收一个 `Comparator` 作为构造参数，从而来自定义元素优先级的先后。\n    \n\n---\n\n著作权归所有 原文链接：https://javaguide.cn/java/collection/java-collection-questions-01.html","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E9%9B%86%E5%90%88/Set":{"title":"Set","content":"\n# comparable 和 Comparator 的区别\n\n- `comparable` 接口实际上是出自`java.lang`包 它有一个 `compareTo(Object obj)`方法用来排序\n    \n- `comparator`接口实际上是出自 java.util 包它有一个`compare(Object obj1, Object obj2)`方法用来排序\n    \n\n一般我们需要对一个集合使用自定义排序时，我们就要重写`compareTo()`方法或`compare()`方法，当我们需要对某一个集合实现两种排序方式，比如一个 song 对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写`compareTo()`方法和使用自制的`Comparator`方法或者以两个 Comparator 来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 `Collections.sort()`.\n\n## Comparator 定制排序\n\n```Java\nArrayList\u003cInteger\u003e arrayList = new ArrayList\u003cInteger\u003e();\narrayList.add(-1);\narrayList.add(3);\narrayList.add(3);\narrayList.add(-5);\narrayList.add(7);\narrayList.add(4);\narrayList.add(-9);\narrayList.add(-7);\nSystem.out.println(\"原始数组:\");\nSystem.out.println(arrayList);\n// void reverse(List list)：反转\nCollections.reverse(arrayList);\nSystem.out.println(\"Collections.reverse(arrayList):\");\nSystem.out.println(arrayList);\n\n// void sort(List list),按自然排序的升序排序\nCollections.sort(arrayList);\nSystem.out.println(\"Collections.sort(arrayList):\");\nSystem.out.println(arrayList);\n// 定制排序的用法\nCollections.sort(arrayList, new Comparator\u003cInteger\u003e() {\n\n@Override\npublic int compare(Integer o1, Integer o2) {\n\treturn o2.compareTo(o1);\n}\n});\nSystem.out.println(\"定制排序后：\");\nSystem.out.println(arrayList);\n```\n\nOutput:\n\n```Plain\n原始数组:\n[-1, 3, 3, -5, 7, 4, -9, -7]\nCollections.reverse(arrayList):\n[-7, -9, 4, 7, -5, 3, 3, -1]\nCollections.sort(arrayList):\n[-9, -7, -5, -1, 3, 3, 4, 7]\n定制排序后：\n[7, 4, 3, 3, -1, -5, -7, -9]\n```\n\n## 重写 compareTo 方法实现按年龄来排序\n\n```Java\n// person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使treemap中的数据按顺序排列\n// 前面一个例子的String类已经默认实现了Comparable接口，详细可以查看String类的API文档，另外其他\n// 像Integer类等都已经实现了Comparable接口，所以不需要另外实现了\npublic  class Person implements Comparable\u003cPerson\u003e {\n    private String name;\n    private int age;\n\n    public Person(String name, int age) {\n        super();\n        this.name = name;\n        this.age = age;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    /**\n     * T重写compareTo方法实现按年龄来排序\n     */\n    @Override\n    public int compareTo(Person o) {\n        if (this.age \u003e o.getAge()) {\n            return 1;\n        }\n        if (this.age \u003c o.getAge()) {\n            return -1;\n        }\n        return 0;\n    }\n}\n\n```\n\n```Java\n    public static void main(String[] args) {\n        TreeMap\u003cPerson, String\u003e pdata = new TreeMap\u003cPerson, String\u003e();\n        pdata.put(new Person(\"张三\", 30), \"zhangsan\");\n        pdata.put(new Person(\"李四\", 20), \"lisi\");\n        pdata.put(new Person(\"王五\", 10), \"wangwu\");\n        pdata.put(new Person(\"小红\", 5), \"xiaohong\");\n        // 得到key的值的同时得到key所对应的值\n        Set\u003cPerson\u003e keys = pdata.keySet();\n        for (Person key : keys) {\n            System.out.println(key.getAge() + \"-\" + key.getName());\n\n        }\n    }\n```\n\nOutput：\n\n```Plain\n5-小红\n10-王五\n20-李四\n30-张三\n```\n\n# 无序性和不可重复性的含义是什么\n\n- 无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。\n    \n- 不可重复性是指添加的元素按照 `equals()` 判断时 ，返回 false，需要同时重写 `equals()` 方法和 `hashCode()` 方法。\n    \n\n# 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同\n\n- `HashSet`、`LinkedHashSet` 和 `TreeSet` 都是 `Set` 接口的实现类，都能保证元素唯一，并且都不是线程安全的。\n    \n- `HashSet`、`LinkedHashSet` 和 `TreeSet` 的主要区别在于底层数据结构不同。`HashSet` 的底层数据结构是哈希表（基于 `HashMap` 实现）。`LinkedHashSet` 的底层数据结构是**链表和哈希表**，元素的插入和取出顺序满足 FIFO。`TreeSet` 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。\n    \n- 底层数据结构不同又导致这三者的应用场景不同。`HashSet` 用于不需要保证元素插入和取出顺序的场景，`LinkedHashSet` 用于保证元素的插入和取出顺序满足 FIFO 的场景，`TreeSet` 用于支持对元素自定义排序规则的场景","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"面试题","content":"![[《面渣逆袭手册》V1.1.pdf]]","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/IO/AIO":{"title":"AIO","content":"","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/IO/BIO":{"title":"BIO","content":"# 字节流\n\n## InputStream（字节输入流）\n\n`InputStream`用于从源头（通常是文件）读取数据（字节信息）到内存中，`java.io.InputStream`抽象类是所有字节输入流的父类。\n\n`InputStream` 常用方法 ：\n- `read()` ：返回输入流中下一个字节的数据。返回的值介于 0 到 255 之间。如果未读取任何字节，则代码返回 `-1` ，表示文件结束。\n    \n- `read(byte b[ ])` : 从输入流中读取一些字节存储到数组 `b` 中。如果数组 `b` 的长度为零，则不读取。如果没有可用字节读取，返回 `-1`。如果有可用字节读取，则最多读取的字节数最多等于 `b.length` ， 返回读取的字节数。这个方法等价于 `read(b, 0, b.length)`。\n    \n- `read(byte b[], int off, int len)` ：在`read(byte b[ ])` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字节数）。\n    \n- `skip(long n)` ：忽略输入流中的 n 个字节 ,返回实际忽略的字节数。\n    \n- `available()` ：返回输入流中可以读取的字节数。\n    \n- `close()` ：关闭输入流释放相关的系统资源。\n    \n\n从 Java 9 开始，`InputStream` 新增加了多个实用的方法：\n\n- `readAllBytes()` ：读取输入流中的所有字节，返回字节数组。\n    \n- `readNBytes(byte[] b, int off, int len)` ：阻塞直到读取 `len` 个字节。\n    \n- `transferTo(OutputStream out)` ： 将所有字节从一个输入流传递到一个输出流。\n    \n\n`FileInputStream` 是一个比较常用的字节输入流对象，可直接指定文件路径，可以直接读取单字节数据，也可以读取至字节数组中。\n\n`FileInputStream` 代码示例：\n\n```Java\ntry (InputStream fis = new FileInputStream(\"input.txt\")) {\n    System.out.println(\"Number of remaining bytes:\"\n            + fis.available());\n    int content;\n    long skip = fis.skip(2);\n    System.out.println(\"The actual number of bytes skipped:\" + skip);\n    System.out.print(\"The content read from file:\");\n    while ((content = fis.read()) != -1) {\n        System.out.print((char) content);\n    }\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n`input.txt` 文件内容：\n\n![](statistic/asynccode-35.png)\n\n输出：\n\n```Plain\nNumber of remaining bytes:11\nThe actual number of bytes skipped:2\nThe content read from file:JavaGuide\n```\n\n不过，一般我们是不会直接单独使用 `FileInputStream` ，通常会配合 `BufferedInputStream`（字节缓冲输入流，后文会讲到）来使用。\n\n像下面这段代码在我们的项目中就比较常见，我们通过 `readAllBytes()` 读取输入流所有字节并将其直接赋值给一个 `String` 对象。\n\n```Java\n// 新建一个 BufferedInputStream 对象\nBufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"input.txt\"));\n// 读取文件的内容并复制到 String 对象中\nString result = new String(bufferedInputStream.readAllBytes());\nSystem.out.println(result);\n```\n\n`DataInputStream` 用于读取指定类型数据，不能单独使用，必须结合 `FileInputStream` 。\n\n```Java\nObjectInputStream input = new ObjectInputStream(new FileInputStream(\"object.data\"));\nMyClass object = (MyClass) input.readObject();\ninput.close();\n\n```\n\n**`ObjectInputStream`** **用于从输入流中读取 Java 对象（反序列化），****`ObjectOutputStream`** **用于将对象写入到输出流(序列化)。**\n\n```Java\nObjectInputStream input = new ObjectInputStream(new FileInputStream(\"object.data\"));\nMyClass object = (MyClass) input.readObject();\ninput.close();\n```\n\n另外，用于序列化和反序列化的类必须实现 `Serializable` 接口，对象中如果有属性不想被序列化，使用 `transient` 修饰。\n\n## OutputStream（字节输出流）\n\n`OutputStream`用于将数据（字节信息）写入到目的地（通常是文件），`java.io.OutputStream`抽象类是所有字节输出流的父类。\n\n`OutputStream` 常用方法 ：\n\n- `write(int b)` ：将特定字节写入输出流。\n    \n- `write(byte b[ ])` : 将数组`b` 写入到输出流，等价于 `write(b, 0, b.length)` 。\n    \n- `write(byte[] b, int off, int len)` : 在`write(byte b[ ])` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字节数）。\n    \n- `flush()` ：刷新此输出流并强制写出所有缓冲的输出字节。\n    \n- `close()` ：关闭输出流释放相关的系统资源。\n    \n\n`FileOutputStream` 是最常用的字节输出流对象，可直接指定文件路径，可以直接输出单字节数据，也可以输出指定的字节数组。\n\n`FileOutputStream` 代码示例：\n\n```Java\ntry (FileOutputStream output = new FileOutputStream(\"output.txt\")) {\n    byte[] array = \"JavaGuide\".getBytes();\n    output.write(array);\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n运行结果：\n\n![](statistic/asynccode-35.png)\n\n类似于 `FileInputStream`，`FileOutputStream` 通常也会配合 `BufferedOutputStream`（字节缓冲输出流，后文会讲到）来使用。\n\n```Java\nFileOutputStream fileOutputStream = new FileOutputStream(\"output.txt\");\nBufferedOutputStream bos = new BufferedOutputStream(fileOutputStream)\n```\n\n**`DataOutputStream`** 用于写入指定类型数据，不能单独使用，必须结合 `FileOutputStream`\n\n```Java\n// 输出流\nFileOutputStream fileOutputStream = new FileOutputStream(\"out.txt\");\nDataOutputStream dataOutputStream = new DataOutputStream(fileOutputStream);\n// 输出任意数据类型\ndataOutputStream.writeBoolean(true);\ndataOutputStream.writeByte(1);\n```\n\n`ObjectInputStream` 用于从输入流中读取 Java 对象（`ObjectInputStream`,反序列化），`ObjectOutputStream`将对象写入到输出流(`ObjectOutputStream`，序列化)。\n\n```Java\nObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(\"file.txt\")\nPerson person = new Person(\"Guide哥\", \"JavaGuide作者\");\noutput.writeObject(person);\n```\n\n# 字符流\n\n不管是文件读写还是网络发送接收，信息的最小存储单元都是字节。 **那为什么 I/O 流操作要分为字节流操作和字符流操作呢？**\n\n个人认为主要有两点原因：\n\n- 字符流是由 Java 虚拟机将字节转换得到的**，这个过程还算是比较耗时**。\n    \n- 如果我们不知道编码类型就很容易出现乱码问题。\n    \n\n乱码问题这个很容易就可以复现，我们只需要将上面提到的 `FileInputStream` 代码示例中的 `input.txt` 文件内容改为中文即可，原代码不需要改动。\n\n![](statistic/asynccode-34.png)\n\n输出：\n\n```Java\nNumber of remaining bytes:9\nThe actual number of bytes skipped:2\nThe content read from file:§å®¶å¥½\n```\n\n可以很明显地看到读取出来的内容已经变成了乱码。\n\n因此，I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。\n\n字符流默认采用的是 `Unicode` 编码，我们可以通过构造方法自定义编码。顺便分享一下之前遇到的笔试题：常用字符编码所占字节数？`utf8` :英文占 1 字节，中文占 3 字节，`unicode`：任何字符都占 2 个字节，`gbk`：英文占 1 字节，中文占 2 字节。\n\n## Reader（字符输入流）\n\n`Reader`用于从源头（通常是文件）读取数据（字符信息）到内存中，`java.io.Reader`抽象类是所有字符输入流的父类。\n\n`Reader` 用于读取文本， `InputStream` 用于读取原始字节。\n\n`Reader` 常用方法 ：\n\n- `read()` : 从输入流读取一个字符。\n    \n- `read(char[] cbuf)` : 从输入流中读取一些字符，并将它们存储到字符数组 `cbuf`中，等价于 `read(cbuf, 0, cbuf.length)` 。\n    \n- `read(char[] cbuf, int off, int len)` ：在`read(char[] cbuf)` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字符数）。\n    \n- `skip(long n)` ：忽略输入流中的 n 个字符 ,返回实际忽略的字符数。\n    \n- `close()` : 关闭输入流并释放相关的系统资源。\n    \n\n`InputStreamReader` 是字节流转换为字符流的桥梁，其子类 `FileReader` 是基于该基础上的封装，可以直接操作字符文件。\n\n```Java\n// 字节流转换为字符流的桥梁\npublic class InputStreamReader extends Reader {\n}\n// 用于读取字符文件\npublic class FileReader extends InputStreamReader {\n}\n```\n\n`FileReader` 代码示例：\n\n```Java\ntry (FileReader fileReader = new FileReader(\"input.txt\");) {\n    int content;\n    long skip = fileReader.skip(3);\n    System.out.println(\"The actual number of bytes skipped:\" + skip);\n    System.out.print(\"The content read from file:\");\n    while ((content = fileReader.read()) != -1) {\n        System.out.print((char) content);\n    }\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n`input.txt` 文件内容：\n\n![](statistic/asynccode-34.png)\n\n输出：\n\n```Plain\nThe actual number of bytes skipped:3\nThe content read from file:我是Guide。\n```\n\n## Writer（字符输出流）\n\n`Writer`用于将数据（字符信息）写入到目的地（通常是文件），`java.io.Writer`抽象类是所有字符输出流的父类。\n\n`Writer` 常用方法 ：\n\n- `write(int c)` : 写入单个字符。\n    \n- `write(char[] cbuf)` ：写入字符数组 `cbuf`，等价于`write(cbuf, 0, cbuf.length)`。\n    \n- `write(char[] cbuf, int off, int len)` ：在`write(char[] cbuf)` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字符数）。\n    \n- `write(String str)` ：写入字符串，等价于 `write(str, 0, str.length())` 。\n    \n- `write(String str, int off, int len)` ：在`write(String str)` 方法的基础上增加了 `off` 参数（偏移量）和 `len` 参数（要读取的最大字符数）。\n    \n- `append(CharSequence csq)` ：将指定的字符序列附加到指定的 `Writer` 对象并返回该 `Writer` 对象。\n    \n- `append(char c)` ：将指定的字符附加到指定的 `Writer` 对象并返回该 `Writer` 对象。\n    \n- `flush()` ：刷新此输出流并强制写出所有缓冲的输出字符。\n    \n- `close()`:关闭输出流释放相关的系统资源。\n    \n\n`OutputStreamWriter` 是字符流转换为字节流的桥梁，其子类 `FileWriter` 是基于该基础上的封装，可以直接将字符写入到文件。\n\n```Java\n// 字符流转换为字节流的桥梁\npublic class OutputStreamWriter extends Writer {\n}\n// 用于写入字符到文件\npublic class FileWriter extends OutputStreamWriter {\n}\n```\n\n`FileWriter` 代码示例：\n\n```Java\ntry (Writer output = new FileWriter(\"output.txt\")) {\n    output.write(\"你好，我是Guide。\");\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n输出结果：\n\n![](statistic/asynccode-37.png)\n\n# 字节缓冲流\n\nIO 操作是很消耗性能的，缓冲流将数据加载至缓冲区，一次性读取/写入多个字节，从而避免频繁的 IO 操作，提高流的传输效率。\n\n字节缓冲流这里采用了装饰器模式来增强 `InputStream` 和`OutputStream`子类对象的功能。\n\n举个例子，我们可以通过 `BufferedInputStream`（字节缓冲输入流）来增强 `FileInputStream` 的功能。\n\n```Java\n// 新建一个 BufferedInputStream 对象\nBufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\"input.txt\"));\n```\n\n字节流和字节缓冲流的性能差别主要体现在我们使用两者的时候都是调用 `write(int b)` 和 `read()` 这两个一次只读取一个字节的方法的时候。由于字节缓冲流内部有缓冲区（字节数组），因此，字节缓冲流会先将读取到的字节存放在缓存区，大幅减少 IO 次数，提高读取效率。\n\n我使用 `write(int b)` 和 `read()` 方法，分别通过字节流和字节缓冲流复制一个 `524.9 mb` 的 PDF 文件耗时对比如下：\n\n```Plain\n使用缓冲流复制PDF文件总耗时:15428 毫秒\n使用普通字节流复制PDF文件总耗时:2555062 毫秒\n```\n\n两者耗时差别非常大，缓冲流耗费的时间是字节流的 1/165。\n\n测试代码如下:\n\n```Java\n@Test\nvoid copy_pdf_to_another_pdf_buffer_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"深入理解计算机操作系统.pdf\"));\n         BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\"))) {\n        int content;\n        while ((content = bis.read()) != -1) {\n            bos.write(content);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用缓冲流复制PDF文件总耗时:\" + (end - start) + \" 毫秒\");\n}\n\n@Test\nvoid copy_pdf_to_another_pdf_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (FileInputStream fis = new FileInputStream(\"深入理解计算机操作系统.pdf\");\n         FileOutputStream fos = new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\")) {\n        int content;\n        while ((content = fis.read()) != -1) {\n            fos.write(content);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用普通流复制PDF文件总耗时:\" + (end - start) + \" 毫秒\");\n}\nSystem.currentTimeMillis();System.out.println(\"使用普通流复制PDF文件总耗时:\" + (end - start) + \" 毫秒\");}\n```\n\n如果是调用 `read(byte b[])` 和 `write(byte b[], int off, int len)` 这两个写入一个字节数组的方法的话，只要字节数组的大小合适，两者的性能差距其实不大，基本可以忽略。\n\n这次我们使用 `read(byte b[])` 和 `write(byte b[], int off, int len)` 方法，分别通过字节流和字节缓冲流复制一个 524.9 mb 的 PDF 文件耗时对比如下：\n\n```Plain\n使用缓冲流复制PDF文件总耗时:695 毫秒\n使用普通字节流复制PDF文件总耗时:989 毫秒\n```\n\n两者耗时差别不是很大，缓冲流的性能要略微好一点点。\n\n测试代码如下：\n\n```Java\n@Test\nvoid copy_pdf_to_another_pdf_with_byte_array_buffer_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\"深入理解计算机操作系统.pdf\"));\n         BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\"))) {\n        int len;\n        byte[] bytes = new byte[4 * 1024];\n        while ((len = bis.read(bytes)) != -1) {\n            bos.write(bytes, 0, len);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用缓冲流复制PDF文件总耗时:\" + (end - start) + \" 毫秒\");\n}\n\n@Test\nvoid copy_pdf_to_another_pdf_with_byte_array_stream() {\n    // 记录开始时间\n    long start = System.currentTimeMillis();\n    try (FileInputStream fis = new FileInputStream(\"深入理解计算机操作系统.pdf\");\n         FileOutputStream fos = new FileOutputStream(\"深入理解计算机操作系统-副本.pdf\")) {\n        int len;\n        byte[] bytes = new byte[4 * 1024];\n        while ((len = fis.read(bytes)) != -1) {\n            fos.write(bytes, 0, len);\n        }\n    } catch (IOException e) {\n        e.printStackTrace();\n    }\n    // 记录结束时间\n    long end = System.currentTimeMillis();\n    System.out.println(\"使用普通流复制PDF文件总耗时:\" + (end - start) + \" 毫秒\");\n}\n```\n\n## BufferedInputStream（字节缓冲输入流）\n\n`BufferedInputStream` 从源头（通常是文件）读取数据（字节信息）到内存的过程中不会一个字节一个字节的读取，而是会先将读取到的字节存放在缓存区，并从内部缓冲区中单独读取字节。这样大幅减少了 IO 次数，提高了读取效率。\n\n`BufferedInputStream` 内部维护了一个缓冲区，这个缓冲区实际就是一个字节数组，通过阅读 `BufferedInputStream` 源码即可得到这个结论。\n\n```Java\npublic\nclass BufferedInputStream extends FilterInputStream {\n    // 内部缓冲区数组\n    protected volatile byte buf[];\n    // 缓冲区的默认大小\n    private static int DEFAULT_BUFFER_SIZE = 8192;\n    // 使用默认的缓冲区大小\n    public BufferedInputStream(InputStream in) {\n        this(in, DEFAULT_BUFFER_SIZE);\n    }\n    // 自定义缓冲区大小\n    public BufferedInputStream(InputStream in, int size) {\n        super(in);\n        if (size \u003c= 0) {\n            throw new IllegalArgumentException(\"Buffer size \u003c= 0\");\n        }\n        buf = new byte[size];\n    }\n}\n```\n\n缓冲区的大小默认为 **8192** 字节，当然了，你也可以通过 `BufferedInputStream(InputStream in, int size)` 这个构造方法来指定缓冲区的大小。\n\n## BufferedOutputStream（字节缓冲输出流）\n\n`BufferedOutputStream` 将数据（字节信息）写入到目的地（通常是文件）的过程中不会一个字节一个字节的写入，而是会先将要写入的字节存放在缓存区，并从内部缓冲区中单独写入字节。这样大幅减少了 IO 次数，提高了读取效率\n\n```Java\ntry (BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"output.txt\"))) {\n    byte[] array = \"JavaGuide\".getBytes();\n    bos.write(array);\n} catch (IOException e) {\n    e.printStackTrace();\n}\n```\n\n类似于 `BufferedInputStream` ，`BufferedOutputStream` 内部也维护了一个缓冲区，并且，这个缓存区的大小也是 **8192** 字节。\n\n# 字符缓冲流\n\n`BufferedReader` （字符缓冲输入流）和 `BufferedWriter`（字符缓冲输出流）类似于 `BufferedInputStream`（字节缓冲输入流）和`BufferedOutputStream`（字节缓冲输入流），内部都维护了一个字节数组作为缓冲区。不过，前者主要是用来操作字符信息。\n\n# 打印流\n\n下面这段代码大家经常使用吧？\n\n```Java\nSystem.out.print(\"Hello！\");\nSystem.out.println(\"Hello！\");\n```\n\n`System.out` 实际是用于获取一个 `PrintStream` 对象，`print`方法实际调用的是 `PrintStream` 对象的 `write` 方法。\n\n`PrintStream` 属于字节打印流，与之对应的是 `PrintWriter` （字符打印流）。`PrintStream` 是 `OutputStream` 的子类，`PrintWriter` 是 `Writer` 的子类。\n\n```Java\nSystem.out.print(\"Hello！\");\nSystem.out.println(\"Hello！\");\n```\n\n# 随机访问流\n\n这里要介绍的随机访问流指的是支持随意跳转到文件的任意位置进行读写的 `RandomAccessFile` 。\n\n`RandomAccessFile` 的构造方法如下，我们可以指定 `mode`（读写模式）。\n\n```Java\n// openAndDelete 参数默认为 false 表示打开文件并且这个文件不会被删除\npublic RandomAccessFile(File file, String mode)\n    throws FileNotFoundException {\n    this(file, mode, false);\n}\n// 私有方法\nprivate RandomAccessFile(File file, String mode, boolean openAndDelete)  throws FileNotFoundException{\n  // 省略大部分代码\n}\n```\n\n读写模式主要有下面四种：\n\n- `r` : 只读模式。\n    \n- `rw`: 读写模式\n    \n- `rws`: 相对于 `rw`，`rws` 同步更新对“文件的内容”或“元数据”的修改到外部存储设备。\n    \n- `rwd` : 相对于 `rw`，`rwd` 同步更新对“文件的内容”的修改到外部存储设备。\n    \n\n文件内容指的是文件中实际保存的数据，元数据则是用来描述文件属性比如文件的大小信息、创建和修改时间。\n\n`RandomAccessFile` 中有一个文件指针用来表示下一个将要被写入或者读取的字节所处的位置。我们可以通过 `RandomAccessFile` 的 `seek(long pos)` 方法来设置文件指针的偏移量（距文件开头 `pos` 个字节处）。如果想要获取文件指针当前的位置的话，可以使用 `getFilePointer()` 方法。\n\n`RandomAccessFile` 代码示例：\n\n```Java\nRandomAccessFile randomAccessFile = new RandomAccessFile(new File(\"input.txt\"), \"rw\");\nSystem.out.println(\"读取之前的偏移量：\" + randomAccessFile.getFilePointer() + \",当前读取到的字符\" + (char) randomAccessFile.read() + \"，读取之后的偏移量：\" + randomAccessFile.getFilePointer());\n// 指针当前偏移量为 6\nrandomAccessFile.seek(6);\nSystem.out.println(\"读取之前的偏移量：\" + randomAccessFile.getFilePointer() + \",当前读取到的字符\" + (char) randomAccessFile.read() + \"，读取之后的偏移量：\" + randomAccessFile.getFilePointer());\n// 从偏移量 7 的位置开始往后写入字节数据\nrandomAccessFile.write(new byte[]{'H', 'I', 'J', 'K'});\n// 指针当前偏移量为 0，回到起始位置\nrandomAccessFile.seek(0);\nSystem.out.println(\"读取之前的偏移量：\" + randomAccessFile.getFilePointer() + \",当前读取到的字符\" + (char) randomAccessFile.read() + \"，读取之后的偏移量：\" + randomAccessFile.getFilePointer());\n```\n\n`input.txt` 文件内容：\n\n![](statistic/asynccode-36.png)\n\n输出：\n\n```Plain\n读取之前的偏移量：0,当前读取到的字符A，读取之后的偏移量：1\n读取之前的偏移量：6,当前读取到的字符G，读取之后的偏移量：7\n读取之前的偏移量：0,当前读取到的字符A，读取之后的偏移量：1\n```\n\n`input.txt` 文件内容变为 `ABCDEFGHIJK` 。\n\n`RandomAccessFile` 的 `write` 方法在写入对象的时候如果对应的位置已经有数据的话，会将其覆盖掉。\n\n```Java\nRandomAccessFile randomAccessFile = new RandomAccessFile(new File(\"input.txt\"), \"rw\");\nrandomAccessFile.write(new byte[]{'H', 'I', 'J', 'K'});\n```\n\n假设运行上面这段程序之前 `input.txt` 文件内容变为 `ABCD` ，运行之后则变为 `HIJK` 。\n\n`RandomAccessFile` 比较常见的一个应用就是实现大文件的 **断点续传** 。何谓断点续传？简单来说就是上传文件中途暂停或失败（比如遇到网络问题）之后，不需要重新上传，只需要上传那些未成功上传的文件分片即可。分片（先将文件切分成多个文件分片）上传是断点续传的基础。\n\n`RandomAccessFile` 可以帮助我们合并文件分片，示例代码如下：\n\n![](statistic/asynccode-39.png)\n\n我在[《Java 面试指北》open in new window](https://javaguide.cn/zhuanlan/java-mian-shi-zhi-bei.html)中详细介绍了大文件的上传问题。\n\n![](statistic/asynccode-38.png)\n\n`RandomAccessFile` 的实现依赖于 `FileDescriptor` (文件描述符) 和 `FileChannel` （内存映射文件）","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/IO/IO%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3":{"title":"IO模型详解","content":"\n# 何为 I/O?\n\nI/O（**I**nput/**O**utpu） 即**输入／输出** 。\n\n**我们先从计算机结构的角度来解读一下 I/O。**\n\n根据冯.诺依曼结构，计算机结构分为 5 大部分：运算器、控制器、存储器、输入设备、输出设备。\n\n![](statistic/asynccode-1.jpg)\n\n**从计算机结构的视角来看的话， I/O 描述了计算机系统与外部设备之间通信的过程。**\n\n**我们再先从应用程序的角度来解读一下 I/O。**\n\n- 为了保证操作系统的稳定性和安全性，一个进程的地址空间划分为 **用户空间（User space）** 和 **内核空间（Kernel space ）** 。\n    \n- **平常运行的应用程序都是运行在用户空间**，**只有内核空间才能进行系统态级别的资源有关的操作**，比如**文件管理、进程通信、内存管理**等等。\n    \n- 当**想要执行 IO 操作时，由于没有执行这些操作的权限，只能发起系统调用请求操作系统帮忙完成**。\n    \n\n  \n\n  \n\n我们在平常开发过程中接触最多的就是 **磁盘 IO（读写文件）** 和 **网络 IO（网络请求和响应）**。\n\n**从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。**\n\n当应用程序发起 I/O 调用后，会经历两个步骤：\n\n1. 内核等待 I/O 设备准备好数据\n    \n2. 内核将数据从内核空间拷贝到用户空间。\n    \n\n# 有哪些常见的 IO 模型?\n\nUNIX 系统下， IO 模型一共有 5 种：\n\n|   |   |\n|---|---|\n|IO模型|JAVA实现|\n|**同步阻塞 I/O**|BIO|\n|**同步非阻塞 I/O**||\n|**I/O 多路复用**|NIO|\n|**信号驱动 I/O**||\n|**异步 I/O**|AIO|\n\n# Java 中 3 种常见 IO 模型\n\n## **阻塞 IO 模型-**BIO (Blocking I/O)\n\n**BIO 属于同步阻塞 IO 模型** 。\n\n同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。\n\n![](statistic/asynccode-29.png)\n\n在客户端连接数量不高的情况下，是没问题的。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。\n\n  \n\n## **同步非阻塞 IO 模型。**\n\n![](statistic/asynccode-31.png)\n\n同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待**数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间**。\n\n相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。\n\n但是，这种 IO 模型同样存在问题：**应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。**。\n\n## **I/O 多路复用模型** -NIO (Non-blocking/New I/O)\n\nJava 中的 NIO 于 Java 1.4 中引入，对应 `java.nio` 包，提供了 `Channel` , `Selector`，`Buffer` 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它是支持面向缓冲的，基于通道的 I/O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO 。\n\nJava 中的 NIO 可以看作是 **I/O 多路复用模型**。也有很多人认为，Java 中的 NIO 属于同步非阻塞 IO 模型。\n\n![](statistic/asynccode-28.png)\n\nIO 多路复用模型中，线程首先发起 **select 调用，询问内核数据是否准备就绪，等内核把数据准备好了**，**用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -\u003e 用户空间）还是阻塞的**。\n\n\u003e 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。\n\u003e \n\u003e - **select 调用** ：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。\n\u003e     \n\u003e - **epoll 调用** ：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。\n\u003e     \n\n**IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。**\n\nJava 中的 NIO ，有一个非常重要的**选择器 ( Selector )** 的概念，也可以被称为 **多路复用器**。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。\n\n![](statistic/asynccode-32.png)\n\n## 异步 IO 模型-AIO (Asynchronous I/O)\n\n异步 IO 是基于**事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里**，当后台处理完成，操作系统会通知相应的线程进行后续的操作。\n\n![](statistic/asynccode-30.png)\n\n目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。\n\n最后，来一张图，简单总结一下 Java 中的 BIO、NIO、AIO。\n\n![](statistic/asynccode-33.png)\n\n---","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":["IO","IO模型"]},"/Java/IO/Java-IO-overview":{"title":"Java IO overview","content":"# Java 中的 IO 流分为几种？\n\n流按照不同的特点，有很多种划分方式\n\n- 按照流的流向分，可以分为 输入流 和 输出流 ；\n    \n- 按照操作单元划分，可以划分为 字节流 和 字符流 ；\n    \n- 按照流的角色划分为 节点流 和 处理流\n    \n\nJava IO流的40 多个类都是从以下4个抽象类中派生出来的\n\n- `InputStream`/`Reader`: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。\n    \n- `OutputStream`/`Writer`: 所有输出流的基类，前者是字节输出流，后者是字符输出流。\n    \n\n![](statistic/asynccode-26.png)\n\n## IO流使用了什么设计模式\n\n- 装饰器模式\n    \n\n![](statistic/asynccode-27.png)\n\n# I/O 流为什么要分为字节流和字符流呢?\n\n问题本质想问：**不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？**\n\n个人认为主要有两点原因：\n\n- 字符流是由 Java 虚拟机将字节转换得到的，**这个过程还算是比较耗时**；\n    \n- 如果我们**不知道编码类型的话，使用字节流的过程中很容易出现乱码问题**。\n    \n\n  \n\n  \n\n# Java IO 中的设计模式有哪些？\n\n参考答案：Java IO 设计模式总结\n\n# BIO、NIO 和 AIO 的区别？\n\n![](statistic/asynccode-25.png)\n\n**BIO(blocking I/O）**：就是传统的IO，**同步阻塞**，服务器实现模式为**一个连接一个线 程，即客户端有连接请求时服务器端就需要启动一个线程进行处理**，如果这个 **连接不做任何事情会造成不必要的线程开销**，可以通过连接池机制改善(实现多个客 户连接服务器)。\n\n![](statistic/asynccode-22.png)\n\n**NIO（java non-blocking IO）：**NIO是**同步非阻塞的**，**服务器端用一个线程处理多个连接，客户端发送的连接请求 会注册到多路复用器上，多路复用器轮询到连接有IO请求就进行处理**\n\n![](statistic/asynccode-23.png)\n\nNIO的数据是面向缓冲区**Buffer**的，必须从Buffer中读取或写入\n\n![](statistic/asynccode-24.png)\n\nNIO的运行机制\n\n- 每个Channel对应一个Buffer。\n    \n- Selector对应一个线程，一个线程对应多个Channel。\n    \n- Selector会根据不同的事件，在各个通道上切换。\n    \n- Buffer是内存块，底层是数据\n    \n\n**AIO：（ Asynchronous I/O）**，是**异步不阻塞的 IO**。在进行 I/O 编程中， 常用到两种模式：R**eactor 和 Proactor。**Java 的 NIO 就是 Reactor，当有事件触发 时，**服务器端得到通知，进行相应的处理，完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接时间较长的应用**。","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":["overview/Java/IO"]},"/Java/IO/NIO":{"title":"NIO","content":"\n## NIO 与 IO\n\n\n### 主要区别  \n\n\n ![[Pasted image 20230727001911.png]]\n        \n### - 面向流与面向缓冲  \n- io  \n\t- Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方  \n\t- 它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区  \n- nio  \n\t- NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动  \n### - 阻塞与非阻塞IO  \n- nio  \n\t- 使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取  \n\t- 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）  \n- io  \n\t- 当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了  \n### - 选择器  \n- NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道  \n- 这种选择机制，使得一个单独的线程很容易来管理多个通道。  \n        \n### - NIO和IO如何影响应用程序的设计  \n- 对NIO或IO类的API调用  \n\t- 使用NIO的API调用时看起来与使用IO时有所不同，但这并不意外，因为并不是仅从一个InputStream逐字节读取，而是数据必须先读入缓冲区再处理  \n- 数据处理  \n- 用来处理数据的线程数  \n\t- NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂  \n\t- 如果你有少量的连接使用非常高的带宽，一次发送大量的数据，也许典型的IO服务器实现可能非常契合\n\n## NIO Path\n- java.nio.file.Path  \n- java中的Path表示文件系统的路径。可以指向文件或文件夹  \n- 有相对路径和绝对路径之分  \n    - 绝对路径表示从文件系统的根路径到文件或是文件夹的路径  \n    - 相对路径表示从特定路径下访问指定文件或文件夹的路径  \n- java.nio.file.Path接口和操作系统的path环境变量没有任何关系  \n- java.nio.file.Path接口和java.io.File有相似性，但也有一些细微的差别。在很多情况下，可以用Path来代替File类  \n- 创建Path实例  \n    - 生成绝对路径Path  \n        - windows  \n            - Path path = Paths.get(\"c:\\\\data\\\\myfile.txt\");  \n            - 绝对路径是c:\\data\\myfile.txt。有两个\\字符的原因是第一个\\是转义字符，表示紧跟着它的字符需要被转义。\\\\表示需要向字符串中写入一个\\字符。  \n            - 如果把以/开头path的格式运行在windows系统中，系统会将其解析为相对路径  \n                - /home/jakobjenkov/myfile.txt  \n                - 将会被解析为路径是在C盘。对应的绝对路径是：  \n        - Unix系统  \n            - Path path = Paths.get(\"/home/jakobjenkov/myfile.txt\");  \n    - 创建相对路径Path  \n        - java NIO Path类也能使用相对路径。可以通过Paths.get(basePath, relativePath)创建一个相对路径Path  \n    - Path.normalize()  \n        - Path 的normalize()方法可以标准化路径。标准化的含义是路径中的.和..都被去掉，指向真正的路径目录地址。\n\n## NIO Files\n\n- java NIO Files 类(java.nio.file.Files) 提供了操作文件的相关方法。本篇文章将会覆盖大多数常用的方法。Files 类包含了很多方法，如果你需要的功能在文中没有提及，需要自己查阅 JavaDoc 文档确认，也许 Files 类提供了相应方法(译者注：但本文中没有涉及) java.nio.file.Files 类需要和 java.nio.file.Path 一起使用，在学习 Files 类前，你需要掌握 Path 类的相关用法。  \n- Files.exists()  \n    - Files.exists()方法用来检查文件系统中是否存在某路径\n\n## Channel、Buffer、Selector\n基本上，所有的 IO 在 NIO 中都从一个 Channel 开始。Channel 有点象流。数据可以从 Channel 读到 Buffer 中，也可以从 Buffer 写到 Channel 中  \n    \n### 主要的Channel  \n    \n- FileChannel  \n- DatagramChannel  \n- SocketChannel  \n- ServerSocketChannel  \n        \n### 主要的Buffer  \n\n- ByteBuffer  \n- CharBuffer  \n- DoubleBuffer  \n- FloatBuffer  \n- IntBuffer  \n- LongBuffer  \n- ShortBuffer  \n- MappedByteBuffer  用于表示内存映射文件  \n            \n### Selector  \n    \n- Selector允许单线程处理多个 Channel  \n- 要使用Selector，得向Selector注册Channel，然后调用它的select()方法。 这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回， 线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等\n\n## Channel\n\n### 与流的不同  \n    \n- 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的  \n- 通道可以异步地读写  \n- 通道中的数据总是要先读到一个Buffer，或者总是要从一个Buffer中写入  \n        \n### - 主要的Channel  \n    \n- FileChannel  \n\t- 从文件中读写数据。  \n- DatagramChannel  \n\t- 能通过UDP读写网络中的数据。  \n- SocketChannel  \n\t- 能通过TCP读写网络中的数据。  \n- ServerSocketChannel  \n\t- 可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。\n\n![[Pasted image 20230727000045.png]]\n\n## Buffer\n\nJava NIO中的Buffer用于和NIO通道进行交互据是从通道读入缓冲区，从缓冲区写入到通道中的。 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存 这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存\n\n###  buffer 的用法\n\n\n- 写入数据到 Buffer  \n- 调用flip()方法，翻转  \n- 从Buffer中读取数据  \n- 调用clear()方法或者compact()方法  \n\t- clear()方法会清空整个缓冲区  \n\t- compact()方法只会清除已经读过的数据  \n\t\t- 任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面  \n- 当向 buffer 写入数据时，buffer 会记录下写了多少数据。一旦要读取数据，需要通过 flip ()方法将 Buffer 从写模式切换到读模式。在读模式下，可以读取之前写入到 buffer 的所有数据一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入\n\n### 三个成员变量 \n\n| 名称     | 说明                                                                                                                                                              |\n| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| capacity | A buffer's capacity is the number of elements it contains. The capacity of a buffer is never negative and never changes.                                          |\n| position | A buffer's limit is the index of the first element that should not be read or written. A buffer's limit is never negative and is never greater than its capacity. |\n| limit    | A buffer's position is the index of the next element to be read or written. A buffer's position is never negative and is never greater than its limit.            | \n\n### 主要的 Buffer  \n    \n- ByteBuffer  \n- CharBuffer  \n- DoubleBuffer  \n- FloatBuffer  \n- IntBuffer  \n- LongBuffer  \n- ShortBuffer  \n- MappedByteBuffer  - 用于表示内存映射文件\n\n### - Buffer 的创建  \n- ByteBuffer buf = ByteBuffer.allocate(48);  \n        \n### - 向Buffer中写数据  \n    \n- 从Channel写到Buffer。  \n\t- intbytesRead = inChannel.read(buf); //read into buffer.  \n            \n    \n- 通过put方法写Buffer的例子：  \n\t- buf.put(127);  \n            \n### - flip()方法  \n- flip方法将Buffer从输入模式切换到输出模式。切换完成可进输出调用flip()方法会将position设回0，并将limit设置成之前position的值。  \n        \n### - rewind()方法  \n- Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。  \n        \n### - clear()与compact()方法  \n\n- 如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了  \n\t\n\n- 果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。  \n\t- compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。  \n            \n### - mark()与reset()方法  \n- 通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position\n\n\n## Scatter/Gather（分散/聚集）\n\nscatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体\n\n### 分散（scatter）\n\n从Channel中读取是指在读操作时将读取的数据写入多个buffer中\n\n![[Pasted image 20230727000659.png]]\n\n### 聚集（gather） \n\n写入Channel是指在写操作时将多个buffer的数据写入同一个Channel\n![[Pasted image 20230727000728.png]]\n\n\n## Selector\n\nSelector（选择器）是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。 这样，一个单独的线程可以管理多个channel，从而管理多个网络连接\n\n\n**好处**  ：Selector（选择器）是 Java NIO 中能够检测一到多个 NIO 通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个 channel，从而管理多个网络连接\n\n\n### - Selector 的创建  \n- Selector selector = Selector.open();  \n        \n### - 向Selector注册通道  \n    \n![[Pasted image 20230727000908.png]]\n    \n- 与Selector一起使用时，Channel必须处于非阻塞模式下。  \n        - 这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式  \n            \n    \n- 第二个参数表示感兴趣那种类型的事件  \n\t- 四种类型的事件  \n\t\t- Connect  \n\t\t\t- 某个 channel 成功连接到另一个服务器称为“连接就绪”  \n\t\t\t- SelectionKey.OP_CONNECT  \n\t\t- Accept  \n\t\t\t- 准备好接收新进入的连接称为“接收就绪”  \n\t\t\t- SelectionKey.OP_ACCEPT  \n\t\t- Read  \n\t\t\t- 一个有数据可读的通道可以说是“读就绪”  \n\t\t\t- SelectionKey.OP_READ  \n\t\t- Write  \n\t\t\t- 等待写数据的通道可以说是“写就绪”  \n\t\t\t- SelectionKey.OP_WRITE  \n                        \n### - SelectionKey  \n    \n- 当向Selector注册Channel时，register()方法会返回一个SelectionKey对象  \n- 包含的属性  \n\t- interest  \n\t\t- 你所选择的感兴趣的事件集合。  \n\t- ready集合  \n\t\t- ready 集合是通道已经准备就绪的操作的集合  \n\t- Channel + Selector  \n\t\t- 从SelectionKey访问Channel和Selector很简单  \n\t- 附加的对象  \n\t\t- 可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道  \n\t\t\t- 附加  \n\t\t\t\t- selectionKey.attach(theObject);  \n\t\t\t\t- SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject);  \n\t\t\t- 获取附加的对象  \n\t\t\t\t- Object attachedObj = selectionKey.attachment();  \n                        \n### - 通过Selector选择通道  \n    \n- 一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法  \n\t- 这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道  \n- select 方法返回的int值表示有多少通道已经就绪  \n\t- int select()  \n\t\t- 阻塞到至少有一个通道在你注册的事件上就绪了。  \n\t- int select(long timeout)  \n\t\t- select()一样，除了最长会阻塞timeout毫秒(参数  \n\t- int selectNow()  \n\t\t- 不会阻塞，不管什么通道就绪都立刻返回  \n- selectedKeys()  \n\t- 然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。  \n\t- Set selectedKeys = selector.selectedKeys();  \n\t- 注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。 必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中  \n\t\t![[Pasted image 20230727001143.png]]\n\t\t\t\n\t- close()  \n\t\t- 用完Selector后调用其close()方法会关闭该Selector，且使注册到该Selector上的所有SelectionKey实例无效。通道本身并不会关闭\n\n\n## FileChannel\n- FileChannel 是一个连接到文件的通道  \n- FileChannel无法设置为非阻塞模式，它总是运行在阻塞模式下。  \n- 打开FileChannel  \n- 我们无法直接打开一个FileChannel，需要通过使用一个InputStream、 OutputStream或RandomAccessFile来获取一个FileChannel实例  \n    - RandomAccessFile aFile = new RandomAccessFile(\"data/nio-data.txt\", \"rw\"); FileChannel inChannel = aFile.getChannel();  \n- 从FileChannel读取数据  \n    - ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf);  \n- 向FileChannel写数据  \n     ![[Pasted image 20230727001243.png]]\n        \n- 关闭FileChannel  \n    - channel.close();  \n- position（）  \n    - 可以通过调用position()方法获取FileChannel的当前位置。  \n    - 也可以通过调用position(long pos)方法设置FileChannel的当前位置。\n\n## SocketChannel\n\n- Java NIO 中的 SocketChannel 是一个连接到 TCP 网络套接字的通道  \n- 创建SocketChannel  \n    - 打开一个SocketChannel并连接到互联网上的某台服务器  \n    - 一个新连接到达ServerSocketChannel时，会创建一个SocketChannel。  \n- 打开 SocketChannel  \n    - SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(newInetSocketAddress(\"http://www.baidu.com\", 80));  \n- 写入SocketChannel  \n   ![[Pasted image 20230727001350.png]]\n\n        \n- 非阻塞模式  \n    \n    - 可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了  \n    - connect()  \n        - 如果SocketChannel在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用finishConnect()的方法。  \n        ![[Pasted image 20230727001411.png]]\n    \n    - write()  \n        - 非阻塞模式下，write()方法在尚未写出任何内容时可能就返回了。所以需要在循环中调用write()  \n    - read()  \n        - 非阻塞模式下,read()方法在尚未读取到任何数据时可能就返回了。所以需要关注它的int返回值，它会告诉你读取了多少字节  \n            \n- 非阻塞模式与选择器  \n    - 非阻塞模式与选择器搭配会工作的更好，通过将一或多个SocketChannel注册到Selector，可以询问选择器哪个通道已经准备好了读取，写入等\n## ServerSocketChannel\n\n- ServerSocketChannel 是一个可以监听新进来的 TCP 连接的通道  \n- 示例  \n   ![[Pasted image 20230727001459.png]]\n- 打开 ServerSocketChannel  \n    - ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();  \n- 关闭 ServerSocketChannel  \n    - serverSocketChannel.close();  \n- 监听新进来的连接  \n    - ServerSocketChannel.accept()  \n    - 当 accept()方法返回的时候,它返回一个包含新进来的连接的 SocketChannel  \n    - 因此, accept()方法会一直阻塞到有新连接到达  \n    - 通常不会仅仅只监听一个连接,在while循环中调用 accept()方法.  \n   ![[Pasted image 20230727001541.png]]\n- 非阻塞模式  \n    - ServerSocketChannel可以设置成非阻塞模式。在非阻塞模式下，accept() 方法会立刻返回，如果还没有新进来的连接,返回的将是null。 因此，需要检查返回的SocketChannel是否是null  \n  ![[Pasted image 20230727001520.png]]\n\n## DatagramChannel\n\n- DatagramChannel 是一个能收发 UDP 包的通道。因为 UDP 是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包  \n- 打开 DatagramChannel  \n    ![[Pasted image 20230727001633.png]]\n- 接收数据receive()  \n    - ![[Pasted image 20230727001638.png]]\n    - receive()方法会将接收到的数据包内容复制到指定的Buffer. 如果Buffer容不下收到的数据，多出的数据将被丢弃。  \n- 发送数据send()  \n    - ![[Pasted image 20230727001645.png]]\n- 连接到特定的地址  \n    - 可以将DatagramChannel“连接”到网络中的特定地址的。由于UDP是无连接的，连接到特定地址并不会像TCP通道那样创建一个真正的连接。而是锁住DatagramChannel ，让其只能从特定地址收发数据  \n    - channel.connect(newInetSocketAddress(\"jenkov.com\", 80));\n## Pipe\n- Java NIO 管道是 2 个线程之间的单向数据连接。Pipe 有一个 source 通道和一个 sink 通道。数据会被写到 sink 通道，从 source 通道读取  \n- ![[Pasted image 20230727001736.png]]\n- 创建管道  \n    - Pipe pipe = Pipe.open();  \n- 要向管道写数据  \n    - 需要访问sink通道  \n        - Pipe.SinkChannel sinkChannel = pipe.sink();  \n    - 通过调用SinkChannel的write()方法，将数据写入SinkChannel  \n        ![[Pasted image 20230727001744.png]]\n- 从管道读取数据  \n    - 从读取管道的数据，需要访问source通道  \n        - Pipe.SourceChannel sourceChannel = pipe.source();  \n    - 调用source通道的read()方法来读取数据，read()方法返回的int值会告诉我们多少字节被读进了缓冲区  \n        - ![[Pasted image 20230727001829.png]]\n","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/JVM/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8":{"title":"类加载器","content":"# 回顾下类加载过程\n\n- 加载\n    \n- 连接\n    \n    - 验证\n        \n    - 准备\n        \n    - 解析\n        \n- 初始化\n    \n\n加载是类加载过程的第一步，主要完成下面 3 件事情：\n\n1. 通过全类名获取定义此类的二进制字节流\n    \n2. 将字节流所代表的静态存储结构转换为方法区的运行时数据结构\n    \n3. 在内存中生成一个代表该类的 `Class` 对象，作为方法区这些数据的访问入口\n    \n\n# 类加载器\n\n## 类加载器介绍\n\n从上面的介绍可以看出:\n\n- 类加载器是一个负责加载类的对象，用于实现类加载过程中的加载这一步。\n    \n- 每个 Java 类都有一个引用指向加载它的 `ClassLoader`。\n    \n- 数组类不是通过 `ClassLoader` 创建的（数组类没有对应的二进制字节流），是由 JVM 直接生成的。\n    \n    - 数组类通过`getClassLoader()`方法获取 `ClassLoader` 的时候和该数组的元素类型的 `ClassLoader` 是一致的。\n        \n\n```Java\nclass Class\u003cT\u003e {\n  ...\n  private final ClassLoader classLoader;\n  @CallerSensitive\n  public ClassLoader getClassLoader() {\n     //...\n  }\n  ...\n}\n```\n\n类加载器的作用\n\n- **加载类**\n    \n    - **类加载器的主要作用就是加载 Java 类的字节码（** **`.class`** **文件）到 JVM 中（在内存中生成一个代表该类的** **`Class`** **对象）。** 字节码可以是 Java 源程序（`.java`文件）经过 `javac` 编译得来，也可以是通过工具动态生成或者通过网络下载得来。\n        \n- 加载 Java 应用所需的资源如文本、图像、配置文件、视频等等文件资源。\n    \n\n## 类加载器加载规则\n\n- 并不会一次性加载所有的类，而是根据需要去动态加载。也就是说，大部分类在具体用到的时候才会去加载，这样对内存更加友好。\n    \n- 对于已经加载的类会被放在 `ClassLoader` 中。在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。\n    \n\n```Java\npublic abstract class ClassLoader {\n  ...\n  private final ClassLoader parent;\n  // 由这个类加载器加载的类。\n  private final Vector\u003cClass\u003c?\u003e\u003e classes = new Vector\u003c\u003e();\n  // 由VM调用，用此类加载器记录每个已加载类。\n  void addClass(Class\u003c?\u003e c) {\n        classes.addElement(c);\n   }\n  ...\n}\n```\n\n## 类加载器总结\n\nJVM 中内置了三个重要的 `ClassLoader`：\n\n1. **`BootstrapClassLoader`****(启动类加载器)** ：最顶层的加载类，由 C++实现，通常表示为 null，并且没有父级，主要用来加载 JDK 内部的核心类库（ `%JAVA_HOME%/lib`目录下的 `rt.jar` 、`resources.jar` 、`charsets.jar`等 jar 包和类）以及被 `-Xbootclasspath`参数指定的路径下的所有类。\n    \n2. **`ExtensionClassLoader`****(扩展类加载器)** ：主要负责加载 `%JRE_HOME%/lib/ext` 目录下的 jar 包和类以及被 `java.ext.dirs` 系统变量所指定的路径下的所有类。\n    \n3. **`AppClassLoader`****(应用程序类加载器)** ：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。\n    \n\n\u003e 🌈 拓展一下：\n\u003e \n\u003e - **`rt.jar`** ： rt 代表“RunTime”，`rt.jar`是Java基础类库，包含Java doc里面看到的所有的类的类文件。也就是说，我们常用内置库 `java.xxx.*` 都在里面，比如`java.util.*`、`java.io.*`、`java.nio.*`、`java.lang.*`、`java.sql.*`、`java.math.*`。\n\u003e     \n\u003e - Java 9 引入了模块系统，并且略微更改了上述的类加载器。扩展类加载器被改名为平台类加载器（platform class loader）。Java SE 中除了少数几个关键模块，比如说 `java.base` 是由启动类加载器加载之外，其他的模块均由平台类加载器所加载。\n\u003e     \n\n![](statistic/asynccode-170.png)\n\n除了 `BootstrapClassLoader` 是 JVM 自身的一部分之外，其他所有的类加载器都是在 JVM 外部实现的，并且全都继承自 `ClassLoader`抽象类。这样做的好处是用户可以自定义类加载器，以便让应用程序自己决定如何去获取所需的类。\n\n每个 `ClassLoader` 可以通过`getParent()`获取其父 `ClassLoader`，如果获取到 `ClassLoader` 为`null`的话，那么该类是通过 `BootstrapClassLoader` 加载的。\n\n```Java\npublic abstract class ClassLoader {\n  ...\n  // 父加载器\n  private final ClassLoader parent;\n  @CallerSensitive\n  public final ClassLoader getParent() {\n     //...\n  }\n  ...\n}\n```\n\n### **为什么 获取到** **`ClassLoader`** **为****`null`****就是** **`BootstrapClassLoader`** **加载的呢？**\n\n这是因为`BootstrapClassLoader` 由 C++ 实现，由于这个 C++ 实现的类加载器在 Java 中是没有与之对应的类的，所以拿到的结果是 null。\n\n下面我们来看一个获取 `ClassLoader` 的小案例：\n\n```Java\npublic class PrintClassLoaderTree {\n\n    public static void main(String[] args) {\n\n        ClassLoader classLoader = PrintClassLoaderTree.class.getClassLoader();\n\n        StringBuilder split = new StringBuilder(\"|--\");\n        boolean needContinue = true;\n        while (needContinue){\n            System.out.println(split.toString() + classLoader);\n            if(classLoader == null){\n                needContinue = false;\n            }else{\n                classLoader = classLoader.getParent();\n                split.insert(0, \"\\t\");\n            }\n        }\n    }\n\n}\n```\n\n输出结果(JDK 8 )：\n\n```Plain\n|--sun.misc.Launcher$AppClassLoader@18b4aac2\n    |--sun.misc.Launcher$ExtClassLoader@53bd815b\n        |--null\n```\n\n从输出结果可以看出：\n\n- 我们编写的 Java 类 `PrintClassLoaderTree` 的 `ClassLoader` 是`AppClassLoader`；\n    \n- `AppClassLoader`的父 `ClassLoader` 是`ExtClassLoader`；\n    \n- `ExtClassLoader`的父`ClassLoader`是`Bootstrap ClassLoader`，因此输出结果为 null。\n    \n\n## 自定义类加载器\n\n我们前面也说说了，除了 `BootstrapClassLoader` 其他类加载器均由 Java 实现且全部继承自`java.lang.ClassLoader`。如果我们要自定义自己的类加载器，很明显需要继承 `ClassLoader`抽象类。\n\n**`ClassLoader`** 类有两个关键的方法：\n\n- `protected Class loadClass(String name, boolean resolve)`：加载指定二进制名称的类，实现了双亲委派机制 。`name` 为类的二进制名称，`resove` 如果为 true，在加载时调用 `resolveClass(Class\u003c?\u003e c)` 方法解析该类。\n    \n- `protected Class findClass(String name)`：根据类的二进制名称来查找类，默认实现是空方法。\n    \n\n官方 API 文档中写到：\n\n\u003e Subclasses of `ClassLoader` are encouraged to override `findClass(String name)`, rather than this method.\n\u003e \n\u003e 建议 `ClassLoader`的子类重写 `findClass(String name)`方法而不是`loadClass(String name, boolean resolve)` 方法。\n\n如果我们不想打破双亲委派模型，就重写 `ClassLoader` 类中的 `findClass()` 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 `loadClass()` 方法。\n\n# 双亲委派模型\n\n## 双亲委派模型介绍\n\n类加载器有很多种，当我们想要加载一个类的时候，具体是哪个类加载器加载呢？这就需要提到双亲委派模型了。\n\n根据官网介绍：\n\n\u003e The ClassLoader class uses a delegation model to search for classes and resources. Each instance of ClassLoader has an associated parent class loader. When requested to find a class or resource, a ClassLoader instance will delegate the search for the class or resource to its parent class loader before attempting to find the class or resource itself. The virtual machine's built-in class loader, called the \"bootstrap class loader\", does not itself have a parent but may serve as the parent of a ClassLoader instance.\n\n翻译过来大概的意思是：\n\n\u003e `ClassLoader` 类使用委托模型来搜索类和资源。每个 `ClassLoader` 实例都有一个相关的父类加载器。需要查找类或资源时，`ClassLoader` 实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。 虚拟机中被称为 \"bootstrap class loader\"的内置类加载器本身没有父类加载器，但是可以作为 `ClassLoader` 实例的父类加载器。\n\n从上面的介绍可以看出：\n\n- `ClassLoader` 类使用委托模型来搜索类和资源。\n    \n- 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。\n    \n- `ClassLoader` **实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器**。\n    \n\n下图展示的各种类加载器之间的层次关系被称为类加载器的“**双亲委派模型(Parents Delegation Model)**”。\n\n![](statistic/asynccode-170.png)\n\n注意⚠️：**双亲委派模型并不是一种强制性的约束，只是 JDK 官方推荐的一种方式。如果我们因为某些特殊需求想要打破双亲委派模型**\n\n**类加载器之间的父子关系一般不是以继承的关系来实现的，而是通常使用组合关系来复用父加载器的代码**。\n\n```Java\npublic abstract class ClassLoader {\n  ...\n  // 组合\n  private final ClassLoader parent;\n  protected ClassLoader(ClassLoader parent) {\n       this(checkCreateClassLoader(), parent);\n  }\n  ...\n}\n```\n\n在面向对象编程中，有一条非常经典的设计原则： **组合优于继承，多用组合少用继承。**\n\n## 双亲委派模型的执行流程\n\n双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 `java.lang.ClassLoader` 的 `loadClass()` 中，相关代码如下所示。\n\n```Java\nprotected Class\u003c?\u003e loadClass(String name, boolean resolve)\n    throws ClassNotFoundException\n{\n    synchronized (getClassLoadingLock(name)) {\n        //首先，检查该类是否已经加载过\n        Class c = findLoadedClass(name);\n        if (c == null) {\n            //如果 c 为 null，则说明该类没有被加载过\n            long t0 = System.nanoTime();\n            try {\n                if (parent != null) {\n                    //当父类的加载器不为空，则通过父类的loadClass来加载该类\n                    c = parent.loadClass(name, false);\n                } else {\n                    //当父类的加载器为空，则调用启动类加载器来加载该类\n                    c = findBootstrapClassOrNull(name);\n                }\n            } catch (ClassNotFoundException e) {\n                //非空父类的类加载器无法找到相应的类，则抛出异常\n            }\n\n            if (c == null) {\n                //当父类加载器无法加载时，则调用findClass方法来加载该类\n                //用户可通过覆写该方法，来自定义类加载器\n                long t1 = System.nanoTime();\n                c = findClass(name);\n\n                //用于统计类加载器相关的信息\n                sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);\n                sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);\n                sun.misc.PerfCounter.getFindClasses().increment();\n            }\n        }\n        if (resolve) {\n            //对类进行link操作\n            resolveClass(c);\n        }\n        return c;\n    }\n}\n```\n\n每当一个类加载器接收到加载请求时，**它会先将请求转发给父类加载器**。在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。\n\n结合上面的源码，简单总结一下双亲委派模型的执行流程：\n\n- 在类加载的时候，系统**会首先判断当前类是否被加载过**。已经被加载的类会直接返回，否则才会尝试加载（每个父类加载器都会走一遍这个流程）。\n    \n- 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是**把这个请求委派给父类加载器去完成（调用父加载器** **`loadClass()`****方法来加载类）。这样的话，所有的请求最终都会传送到顶层的启动类加载器** **`BootstrapClassLoader`** 中。\n    \n- 只有当父加载器反馈**自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载（调用自己的** **`findClass()`** **方法来加载类）**。\n    \n\n🌈 拓展一下：\n\n**JVM 判定两个 Java 类是否相同的具体规则** ：\n\n- **JVM 不仅要看类的全名是否相同，**\n    \n- **还要看加载此类的类加载器是否一样。**\n    \n\n**只有两者都相同的情况，才认为两个类是相同的**。\n\n## 双亲委派模型的好处\n\n双亲委派模型保证了 Java 程序的稳定运行，\n\n- 可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。\n    \n- 保证使用到官方的类\n    \n    - 比如我们编写一个称为 `java.lang.Object` 类的话，那么程序运行的时候，系统就会出现两个不同的 `Object` 类。双亲委派模型可以保证加载的是 JRE 里的那个 `Object` 类，而不是你写的 `Object` 类。\n        \n\n## 打破双亲委派模型方法\n\n自定义加载器的话，需要继承 `ClassLoader` 。\n\n- **如果我们不想打破双亲委派模型，就重写** **`ClassLoader`** **类中的** **`findClass()`** **方法即可，无法被父类加载器加载的类最终会通过这个方法被加载**\n    \n- **如果想打破双亲委派模型则需要重写** **`loadClass()`** **方法**。\n    \n\n为什么是重写 `loadClass()` 方法打破双亲委派模型呢？双亲委派模型的执行流程已经解释了：\n\n\u003e 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器 `loadClass()`方法来加载类）。\n\nT**omcat 服务器为了能够优先加载 Web 应用目录下的类，然后再加载其他目录下的类，就自定义了类加载器** **`WebAppClassLoader`** **来打破双亲委托机制**。这也是 Tomcat 下 Web 应用之间的类实现隔离的具体原理。\n\nTomcat 的类加载器的层次结构如下：\n\n![](statistic/asynccode-171.png)\n\n  \n\n## Tomcat的类加载机制\n\n![](statistic/asynccode-172.png)\n\n**Tomcat实际上也是破坏了双亲委派模型的。**\n\n- Tomact是web容器，可能需要部署多个应用程序。不同的应用程序**可能会依赖同一个 第三方类库的不同版本**，但是不同版本的类库中某一个类的全路径名可能是一样 的。\n    \n- 所以，Tomcat破坏了双亲委派原则，提供隔离的机制，为每个web容器单独提供一 个WebAppClassLoader加载器。每一个WebAppClassLoader负责加载本身的目录下的 class文件，加载不到时再交CommonClassLoader加载，这和双亲委派刚好相反。","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/JVM/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B":{"title":"类加载过程","content":"类从被加载到虚拟机内存中开始到卸载出内存为止，它的整个生命周期可以简单概括为 7 个阶段：\n\n- 加载（Loading）\n    \n- 连接\n    \n    - 验证（Verification）\n        \n    - 准备（Preparation）\n        \n    - 解析（Resolution）\n        \n- 初始化（Initialization）\n    \n- 使用（Using）\n    \n- 卸载（Unloading）\n    \n\n  \n\n  \n\n![](statistic/asynccode-177.png)\n\n# 类加载过程\n\n**Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？**\n\n系统加载 Class 类型的文件主要三步：**加载-\u003e连接-\u003e初始化**。连接过程又可分为三步：**验证-\u003e准备-\u003e解析**。\n\n![](statistic/asynccode-173.png)\n\n详见 [Java Virtual Machine Specification - 5.3. Creation and Loadingopen in new window](https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-5.html#jvms-5.3)。\n\n## 加载\n\n类加载过程的第一步，主要完成下面 3 件事情：\n\n1. 通过全类名获取定义此类的**二进制字节流**。\n    \n2. 将字节流所代表的静态存储结构转换为方法区的**运行时数据结构**。\n    \n3. 在内存中生成一个代表该类的 `Class` 对象，作为方法区这些数据的访问入口。\n    \n\n加载这一步主要是通过我们后面要讲到的 **类加载器** 完成的。类加载器有很多种，当我们想要加载一个类的时候，具体是哪个类加载器加载由 **双亲委派模型** 决定（不过，我们也能打破由双亲委派模型）。\n\n\u003e 类加载器、双亲委派模型也是非常重要的知识点，这部分内容在[类加载器详解open in new window](https://javaguide.cn/java/jvm/classloader.html)这篇文章中有详细介绍到。阅读本篇文章的时候，大家知道有这么个东西就可以了。\n\n**每个 Java 类都有一个引用指向加载它的** **`ClassLoader`****。不过，数组类不是通过** **`ClassLoader`** **创建的，而是 JVM 在需要的时候自动创建的，数组类通过****`getClassLoader()`****方法获取** **`ClassLoader`** **的时候和该数组的元素类型的** **`ClassLoader`** **是一致**的。\n\n一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 `loadClass()` 方法）。\n\n**加载阶段与连接阶段的部分动作(如一部分字节码文件格式验证动作)是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。**\n\n## 验证\n\n**验证是连接阶段的第一步，这一阶段的目的是确保 Class 文件的字节流中包含的信息符合《Java 虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。**\n\n验证阶段这一步在整个类加载过程中耗费的资源还是相对较多的，但很有必要，可以有效防止恶意代码的执行。任何时候，程序安全都是第一位。\n\n**在生产环境的实施阶段就可以考虑使用** **`-Xverify:none`** **参数来关闭大部分的类验证措施**，以缩短虚拟机类加载的时间。\n\n验证阶段主要由四个检验阶段组成：\n\n1. 文件格式验证（Class 文件格式检查）\n    \n2. 元数据验证（字节码语义检查）\n    \n3. 字节码验证（程序语义检查）\n    \n4. 符号引用验证（类的正确性检查）\n    \n\n![](statistic/asynccode-174.png)\n\n- 文件格式验证这一阶段是**基于该类的二进制字节流进行的，主要目的是保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个 Java 类型信息的要求**。除了这一阶段之外，其余三个验证阶段都是基于方法区的存储结构上进行的，不会再直接读取、操作字节流了。\n    \n\n\u003e 方法区属于是 JVM 运行时数据区域的一块逻辑区域，是各个线程共享的内存区域。当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。方法区会存储已被虚拟机加载的 **类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据**。\n\u003e \n\u003e 关于方法区的详细介绍，推荐阅读 [Java 内存区域详解open in new window](https://javaguide.cn/java/jvm/memory-area.html) 这篇文章。\n\n符号引用验证发生在类加载过程中的解析阶段，具体点说是 JVM 将符号引用转化为直接引用的时候（解析阶段会介绍符号引用和直接引用）。\n\n- 符号引用验证的主要目的是确保**解析阶段能正常执行，如果无法通过符号引用验证，JVM 会抛出异常**，比如：\n    \n\n  \n\n## 准备\n\n**准备阶段是正式为类变量分配内存并设置类变量初始值的阶段**，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：\n\n1. 这时候进行内存分配的**仅包括类变量（ Class Variables ，即静态变量，被** **`static`** **关键字修饰的变量，只与类相关，因此被称为类变量），而不包括实例变量。**实例变量会在对象实例化时随着对象一块分配在 Java 堆中。\n    \n2. 从概念上讲，**类变量所使用的内存都应当在 方法区 中进行分配**。不过有一点需要注意的是：JDK 7 之前，HotSpot 使用永久代来实现方法区的时候，实现是完全符合这种逻辑概念的。 而在 JDK 7 及之后，HotSpot 已经把原本放在永久代的字符串常量池、静态变量等移动到堆中，这个时候类变量则会随着 Class 对象一起存放在 Java 堆中。相关阅读：[《深入理解 Java 虚拟机（第 3 版）》勘误#75open in new window](https://github.com/fenixsoft/jvm_book/issues/75)\n    \n3. 这里所**设置的初始值\"通常情况\"下是数据类型默认的零值**（如 0、0L、null、false 等）。特殊情况：**比如给 value 变量加上了 final 关键字****`public static final int value=111`** **，那么准备阶段 value 的值就被赋值为 111。**\n    \n\n**基本数据类型的零值** ： (图片来自《深入理解 Java 虚拟机》第 3 版 7.33 )\n\n![](statistic/asynccode-175.png)\n\n## 解析\n\n**解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。** 解析动作主要针**对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行**。\n\n《深入理解 Java 虚拟机》7.34 节第三版对符号引用和直接引用的解释如下：\n\n![](statistic/asynccode-176.png)\n\n**解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。**\n\n## 初始化\n\n**初始化阶段是执行初始化方法** **`\u003cclinit\u003e ()`****方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码)。**\n\n\u003e 说明： `\u003cclinit\u003e ()`方法是编译之后自动生成的。\n\n对于`\u003cclinit\u003e ()` 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 `\u003cclinit\u003e ()` 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起多个线程阻塞，并且这种阻塞很难被发现。\n\n对于初始化阶段，虚拟机严格规范了有且只有 5 种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)：\n\n1. 当遇到 `new` 、 `getstatic`、`putstatic` 或 `invokestatic` 这 4 条直接码指令时，比如 `new` 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。\n    \n    1. 当 jvm 执行 `new` 指令时会初始化类。即当程序创建一个类的实例对象。\n        \n    2. 当 jvm 执行 `getstatic` 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。\n        \n    3. 当 jvm 执行 `putstatic` 指令时会初始化类。即程序给类的静态变量赋值。\n        \n    4. 当 jvm 执行 `invokestatic` 指令时会初始化类。即程序调用类的静态方法。\n        \n2. 使用 `java.lang.reflect` 包的方法对类进行反射调用时如 `Class.forname(\"...\")`, `newInstance()` 等等。如果类没初始化，需要触发其初始化。\n    \n3. 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。\n    \n4. 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 `main` 方法的那个类)，虚拟机会先初始化这个类。\n    \n5. `MethodHandle` 和 `VarHandle` 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用， 就必须先使用 `findStaticVarHandle` 来初始化要调用的类。\n    \n6. **「补充，来自****[issue745open in new window](https://github.com/Snailclimb/JavaGuide/issues/745)****」** 当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。\n    \n\n## 类卸载\n\n\u003e 卸载这部分内容来自 [issue#662open in new window](https://github.com/Snailclimb/JavaGuide/issues/662)由 **[guang19open in new window](https://github.com/guang19)** 补充完善。\n\n**卸载类即该类的 Class 对象被 GC。**\n\n卸载类需要满足 3 个要求:\n\n1. 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。\n    \n2. 该类没有在其他任何地方被引用\n    \n3. 该类的类加载器的实例已被 GC\n    \n\n所以，在 JVM 生命周期内，由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。\n\n只要想通一点就好了，JDK 自带的 `BootstrapClassLoader`, `ExtClassLoader`, `AppClassLoader` 负责加载 JDK 提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/JVM/%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84":{"title":"类文件结构","content":"# 回顾一下字节码\n\n在 Java 中，JVM 可以理解的代码就叫做`字节码`（即扩展名为 `.class` 的文件），**只面向虚拟机**\n\n- Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，\n    \n- **同时又保留了解释型语言可移植的特点**。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，\n    \n\n下图展示了不同的语言被不同的编译器编译成`.class`文件最终运行在 Java 虚拟机之上。`.class`文件的二进制格式可以使用 [WinHexo](https://www.x-ways.net/winhex/)查看。\n\n![](statistic/asynccode-178.png)\n\n# Class 文件结构总结\n\n根据 Java 虚拟机规范，Class 文件通过 `ClassFile` 定义，有点类似 C 语言的结构体。\n\n`ClassFile` 的结构如下：\n\n```Java\nClassFile {\n    u4             magic; //Class 文件的标志\n    u2             minor_version;//Class 的小版本号\n    u2             major_version;//Class 的大版本号\n    u2             constant_pool_count;//常量池的数量\n    cp_info        constant_pool[constant_pool_count-1];//常量池\n    u2             access_flags;//Class 的访问标记\n    u2             this_class;//当前类\n    u2             super_class;//父类\n    u2             interfaces_count;//接口\n    u2             interfaces[interfaces_count];//一个类可以实现多个接口\n    u2             fields_count;//Class 文件的字段属性\n    field_info     fields[fields_count];//一个类可以有多个字段\n    u2             methods_count;//Class 文件的方法数量\n    method_info    methods[methods_count];//一个类可以有个多个方法\n    u2             attributes_count;//此类的属性表中的属性数\n    attribute_info attributes[attributes_count];//属性表集合}\n```\n\n通过分析 `ClassFile` 的内容，我们便可以知道 class 文件的组成。\n\n![](statistic/asynccode-3.jpg)\n\n下面这张图是通过 IDEA 插件 `jclasslib` 查看的，你可以更直观看到 Class 文件结构。\n\n![](statistic/asynccode-181.png)\n\n使用 `jclasslib` 不光可以直观地查看某个类对应的字节码文件，还可以查看类的基本信息、常量池、接口、属性、函数等信息。\n\n下面详细介绍一下 Class 文件结构涉及到的一些组件。\n\n## 魔数（Magic Number）\n\n```Java\n    u4             magic; //Class 文件的标志\n```\n\n每个 Class 文件的头 4 个字节称为魔数（Magic Number）,它的唯一作用是**确定这个文件是否为一个能被虚拟机接收的 Class 文件**。\n\n## Class 文件版本号（Minor\u0026Major Version）\n\n```Java\n    u2             minor_version;//Class 的小版本号\n    u2             major_version;//Class 的大版本号\n```\n\n紧接着魔数的四个字节存储的是 Class 文件的版本号：第 5 和第 6 位是**次版本号**，第 7 和第 8 位是**主版本号**。\n\n每当 Java 发布大版本（比如 Java 8，Java9）的时候，主版本号都会加 1。你可以使用 `javap -v` 命令来快速查看 Class 文件的版本号信息。\n\n**高版本的 Java 虚拟机可以执行低版本编译器生成的 Class 文件，但是低版本的 Java 虚拟机不能执行高版本编译器生成的 Class 文件**。所以，我们在实际开发的时候要确保开发的的 JDK 版本和生产环境的 JDK 版本保持一致。\n\n## 常量池（Constant Pool）\n\n```Java\n    u2             constant_pool_count;//常量池的数量\n    cp_info        constant_pool[constant_pool_count-1];//常量池\n```\n\n紧接着主次版本号之后的是常量池，常量池的数量是 `constant_pool_count-1`（**常量池计数器是从 1 开始计数的，将第 0 项常量空出来是有特殊考虑的，索引值为 0 代表“不引用任何一个常量池项”**）。\n\n常量池主要存放两大常量：**字面量和符号引用**。\n\n- 字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。\n    \n- 符号引用则属于编译原理方面的概念。包括下面三类常量：\n    \n    - 类和接口的全限定名\n        \n    - 字段的名称和描述符\n        \n    - 方法的名称和描述符\n        \n\n常量池中每一项常量都是一个表，这 14 种表有一个共同的特点：**开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型．**\n\n|   |   |   |\n|---|---|---|\n|类型|标志（tag）|描述|\n|CONSTANT_utf8_info|1|UTF-8 编码的字符串|\n|CONSTANT_Integer_info|3|整形字面量|\n|CONSTANT_Float_info|4|浮点型字面量|\n|CONSTANT_Long_info|５|长整型字面量|\n|CONSTANT_Double_info|６|双精度浮点型字面量|\n|CONSTANT_Class_info|７|类或接口的符号引用|\n|CONSTANT_String_info|８|字符串类型字面量|\n|CONSTANT_Fieldref_info|９|字段的符号引用|\n|CONSTANT_Methodref_info|10|类中方法的符号引用|\n|CONSTANT_InterfaceMethodref_info|11|接口中方法的符号引用|\n|CONSTANT_NameAndType_info|12|字段或方法的符号引用|\n|CONSTANT_MothodType_info|16|标志方法类型|\n|CONSTANT_MethodHandle_info|15|表示方法句柄|\n|CONSTANT_InvokeDynamic_info|18|表示一个动态方法调用点|\n\n`.class` 文件可以通过`javap -v class类名` 指令来看一下其常量池中的信息(`javap -v class类名-\u003e temp.txt` ：将结果输出到 temp.txt 文件)。\n\n## 访问标志(Access Flags)\n\n```Java\n    u2             access_flags;//Class 的访问标记\n```\n\n在常量池结束之后，紧接着的两个字节代表访问标志，**这个标志用于识别一些类或者接口层次的访问信息，**包括：这个 Class 是类还是接口，是否为 `public` 或者 `abstract` 类型，如果是类的话是否声明为 `final` 等等。\n\n类访问和属性修饰符:\n\n![](statistic/asynccode-179.png)\n\n我们定义了一个 `Employee` 类\n\n```Java\npackage top.snailclimb.bean;\npublic class Employee {\n   ...\n}\n```\n\n通过`javap -v class类名` 指令来看一下类的访问标志。\n\n![](statistic/asynccode-180.png)\n\n## 当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合\n\n```Java\n    u2             this_class;//当前类\n    u2             super_class;//父类\n    u2             interfaces_count;//接口\n    u2             interfaces[interfaces_count];//一个类可以实现多个接口\n```\n\nJava 类的继承关系由**类索引、父类索引和接口索引集合**三项确定。类索引、父类索引和接口索引集合按照顺序排在访问标志之后，\n\n- 类索引用于确定这个类的全限定名，\n    \n- 父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 `java.lang.Object` 之外，所有的 Java 类都有父类，因此除了 `java.lang.Object` 外，所有 Java 类的父类索引都不为 0。\n    \n- 接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按 `implements` (如果这个类本身是接口的话则是`extends`) 后的接口顺序从左到右排列在接口索引集合中。\n    \n\n## 字段表集合（Fields）\n\n```Java\n    u2             fields_count;//Class 文件的字段的个数\n    field_info     fields[fields_count];//一个类会可以有个字段\n```\n\n字段表（field info）用于**描述接口或类中声明的变量。字段包括类级变量以及实例变量**，但不包括在方法内部声明的局部变量。\n\n**field info(字段表) 的结构:**\n\n![](statistic/asynccode-183.png)\n\n- **access_flags:** 字段的作用域（`public` ,`private`,`protected`修饰符），是实例变量还是类变量（`static`修饰符）,可否被序列化（transient 修饰符）,可变性（final）,可见性（volatile 修饰符，是否强制从主内存读写）。\n    \n- **name_index:** 对常量池的引用，表示的字段的名称；\n    \n- **descriptor_index:** 对常量池的引用，表示字段和方法的描述符；\n    \n- **attributes_count:** 一个字段还会拥有一些额外的属性，attributes_count 存放属性的个数；\n    \n- **attributes[attributes_count]:** 存放具体属性具体内容。\n    \n\n上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫什么名字、字段被定义为什么数据类型这些都是无法固定的，只能引用常量池中常量来描述。\n\n**字段的 access_flag 的取值:**\n\n![](statistic/asynccode-182.png)\n\n## 方法表集合（Methods）\n\n```Java\n    u2             methods_count;//Class 文件的方法的数量\n    method_info    methods[methods_count];//一个类可以有个多个方法\n```\n\nmethods_count 表示方法的数量，而 method_info 表示方法表。\n\nClass 文件存储格式中对**方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项**。\n\n**method_info(方法表的) 结构:**\n\n![](statistic/asynccode-183.png)\n\n**方法表的 access_flag 取值：**\n\n![](statistic/asynccode-184.png)\n\n注意：因为**`volatile`****修饰符和****`transient`****修饰符不可以修饰方法，所以方法表的访问标志中没有这两个对应的标志，但是增加了****`synchronized`****、****`native`****、****`abstract`****等关键字修饰方法，所以也就多了这些关键字对应的标志。**\n\n## 属性表集合（Attributes）\n\n```Java\n   u2             attributes_count;//此类的属性表中的属性数\n   attribute_info attributes[attributes_count];//属性表集合\n```\n\n在 Class 文件，**字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息**。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":[]},"/Java/JVM/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6":{"title":"JVM垃圾回收","content":"# 堆空间的基本结构\n\nJava 自动内存管理最核心的功能是 **堆** 内存中对象的分配与回收。\n\n**Java 堆是垃圾收集器管理的主要区域**，因此也被称作 **GC 堆（Garbage Collected Heap）**。\n\n在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分：\n\n1. 新生代内存(Young Generation)\n    \n    1. Eden 区、两个 Survivor 区 S0 和 S1 都属于新生代\n        \n2. 老生代(Old Generation)\n    \n3. 永久代(Permanent Generation)\n    \n\n下图所示的 Eden 区、两个 Survivor 区 S0 和 S1 都属于新生代，中间一层属于老年代，最下面一层属于永久代。\n\n![](statistic/asynccode-185.png)\n\n**JDK 8 版本之后 PermGen(永久) 已被 Metaspace(元空间) 取代，元空间使用的是直接内存** 。\n\n关于堆空间结构更详细的介绍，可以回过头看看 Java 内存区域详解 这篇文章。\n\n# 内存分配和回收原则\n\n## 对象优先在 Eden 区分配\n\n- 大多数情况下，对象在新生代中 Eden 区分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。\n    \n- 当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。\n    \n    - GC 期间虚拟机存入Survivor 空间，所以只好通过 **分配担保机制** 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 Eden 区的话，还是会在 Eden 区分配内存。可以执行如下代码验证：\n        \n\n## 大对象直接进入老年代\n\n大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。\n\n大对象直接进入老年代主要是为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。\n\n## 长期存活的对象将进入老年代\n\n虚拟机给每个对象一个对象年龄（Age）计数器。\n\n- 大部分情况，对象都会首先在 Eden 区域分配。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间（s0 或者 s1）中，并将对象年龄设为 1(Eden 区-\u003eSurvivor 区后对象的初始年龄变为 1)。\n    \n- 对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。\n    \n\n\u003e 修正（[issue552open in new window](https://github.com/Snailclimb/JavaGuide/issues/552)）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的 50% 时（默认值是 50%，可以通过 `-XX:TargetSurvivorRatio=percent` 来设置，参见 [issue1199open in new window](https://github.com/Snailclimb/JavaGuide/issues/1199) ），取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\n\u003e \n\u003e jdk8 官方文档引用 ：https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html 。\n\u003e \n\u003e 额外补充说明([issue672open in new window](https://github.com/Snailclimb/JavaGuide/issues/672))：**关于默认的晋升年龄是 15，这个说法的来源大部分都是《深入理解 Java 虚拟机》这本书。** 如果你去 Oracle 的官网阅读[相关的虚拟机参数open in new window](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html)，你会发现`-XX:MaxTenuringThreshold=threshold`这里有个说明\n\u003e \n\u003e **Sets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是 15，这个是要区分垃圾收集器的，CMS 就是 6.**\n\n## 主要进行 gc 的区域\n\n周志明先生在《深入理解 Java 虚拟机》第二版中 P92 如是写道：\n\n\u003e ~~_“老年代 GC（Major GC/Full GC），指发生在老年代的 GC……”_~~\n\n上面的说法已经在《深入理解 Java 虚拟机》第三版中被改正过来了。感谢 R 大的回答：\n\n**总结：**\n\n针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种：\n\n部分收集 (Partial GC)：\n\n- 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集；\n    \n- 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集；\n    \n- 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。\n    \n\n整堆收集 (Full GC)：收集整个 Java 堆和方法区。\n\n## 空间分配担保\n\n**空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。**\n\n《深入理解 Java 虚拟机》第三章对于空间分配担保的描述如下：\n\n\u003e JDK 6 Update 24 之前，在发生 Minor GC 之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次 Minor GC 可以确保是安全的。如果不成立，则虚拟机会先查看 `-XX:HandlePromotionFailure` 参数的设置值是否允许担保失败(Handle Promotion Failure);如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次 Minor GC，尽管这次 Minor GC 是有风险的;如果小于，或者 `-XX: HandlePromotionFailure` 设置不允许冒险，那这时就要改为进行一次 Full GC。\n\u003e \n\u003e JDK 6 Update 24 之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小，就会进行 Minor GC，否则将进行 Full GC。\n\n  \n\n# 各个区域的Gc\n\n## **Minor GC/Young GC**、**Major GC/Old GC**、**Mixed GC**、**Full GC** 都是什么意思？\n\n- 部分收集（Partial GC）：指目标不是完整收集整个Java堆的垃圾收集，其中又分为：\n    \n    - 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集。\n        \n    - 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。\n        \n    - 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收 集。目前只有G1收集器会有这种行为。\n        \n- 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集\n    \n\n## **Minor GC/Young GC**什么时候触发？\n\n新创建的对象优先在新生代Eden区进行分配，如果Eden区没有足够的空间时，就会触发Young GC来清理新生代\n\n# 什么时候会触发Full Gc\n\n![](statistic/asynccode-186.png)\n\n- Young GC之前检查老年代 ：**在要进行 Young GC 的时候，发现 老年代可用的连续 内存空间 \u003c 新生代历次Young GC后升入老年代的对象总和的平均大小** ，说明本次 Young GC后可能升入老年代的对象大小，可能超过了老年代当前可用内存空间, 那就会触发 Full GC。\n    \n- Young GC之后老年代空间不足 ：**执行Young GC之后有一批对象需要放入老年 代，此时老年代就是没有足够的内存空间存放这些对象了**，此时必须立即触发一 次Full GC\n    \n- 老年代空间不足 ，**老年代内存使用率过高，达到一定比例**，也会触发Full GC。\n    \n- 空间分配担保失败 （ Promotion Failure），**新生代的 To 区放不下从 Eden 和 From 拷贝过来对象，或者新生代对象 GC 年龄到达阈值需要晋升这两种情况，** 老年代如果放不下的话都会触发 Full GC。\n    \n- 方法区内存空间不足 ：**如果方法区由永久代实现，永久代空间不足 Full GC。**\n    \n- **System.gc()等命令**触发 ：System.gc()、jmap -dump 等命令会触发 full gc。\n    \n\n  \n\n# 死亡对象判断方法\n\n## 引用计数法\n\n给对象中添加一个引用计数器：\n\n- 每当有一个地方引用它，计数器就加 1；\n    \n- 当引用失效，计数器就减 1；\n    \n- 任何时候计数器为 0 的对象就是不可能再被使用的。\n    \n\n**这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。**\n\n  \n\n## 可达性分析算法\n\n这个算法的基本思想就是通过一系列的称为 **“GC Roots”** 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。\n\n### **哪些对象可以作为 GC Roots 呢？**\n\n- 虚拟机栈(栈帧中的本地变量表)中引用的对象\n    \n- 本地方法栈(Native 方法)中引用的对象\n    \n- 方法区中类静态属性引用的对象\n    \n- 方法区中常量引用的对象\n    \n- 所有被同步锁持有的对象\n    \n\n### **对象可以被回收，就代表一定会被回收吗？**\n\n- 可达性分析法中不可达的对象被第一次标记并且进行一次筛选，\n    \n    - 筛选的条件是此对象是否有必要执行 `finalize` 方法。当对象没有覆盖 `finalize` 方法，或 `finalize` 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。\n        \n    - 被判定为需要执行的对象将**会被放在一个队列中进行第二次标记**，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。\n        \n\n\u003e `Object` 类中的 `finalize` 方法一直被认为是一个糟糕的设计，成为了 Java 语言的负担，影响了 Java 语言的安全和 GC 的性能。JDK9 版本及后续版本中各个类中的 `finalize` 方法会被逐渐弃用移除。忘掉它的存在吧！\n\u003e \n\u003e 参考：\n\u003e \n\u003e - [JEP 421: Deprecate Finalization for Removalopen in new window](https://openjdk.java.net/jeps/421)\n\u003e     \n\u003e - [是时候忘掉 finalize 方法了open in new window](https://mp.weixin.qq.com/s/LW-paZAMD08DP_3-XCUxmg)\n\u003e     \n\n## 引用类型总结\n\n无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，**判定对象的存活都与“引用”有关。**\n\n  \n\n### **1．强引用（StrongReference）**\n\n以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似**于必不可少的生活用品**，垃**圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题**。\n\n### **2．软引用（SoftReference）**\n\n如果一个对象只具有软引用，那就类似于**可有可无的生活用品**。**如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。**只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。\n\n软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。\n\n### **3．弱引用（WeakReference）**\n\n如果一个对象只具有弱引用，那就类似于**可有可无的生活用品**。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，**一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存**。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。\n\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n\n### **4．虚引用（PhantomReference）**\n\n\"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，**虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收**。\n\n### **虚引用主要用来跟踪对象被垃圾回收的活动**。\n\n**虚引用与软引用和弱引用的一个区别在于：** 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。\n\n特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为**软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生**。\n\n## 如何判断一个常量是废弃常量？\n\n运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？\n\n~~**JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。**~~\n\n\u003e **🐛 修正（参见：****[issue747open in new window](https://github.com/Snailclimb/JavaGuide/issues/747)****，****[referenceopen in new window](https://blog.csdn.net/q5706503/article/details/84640762)****）** ：\n\u003e \n\u003e 1. **JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时 hotspot 虚拟机对方法区的实现为永久代**\n\u003e     \n\u003e 2. **JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是 hotspot 中的永久代** 。\n\u003e     \n\u003e 3. **JDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)**\n\u003e     \n\n假如在字符串常量池中存在字符串 \"abc\"**，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池了**。\n\n## 如何判断一个类是无用的类\n\n方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？\n\n判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 **“无用的类”** ：\n\n- **该类所有的实例都已经被回收**，也就是 Java 堆中不存在该类的任何实例。\n    \n- **加载该类的** **`ClassLoader`** **已经被回收**。\n    \n- **该类对应的** **`java.lang.Class`** **对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。**\n    \n\n  \n\n## finalize方法的作用\n\n垃圾回收就是古代的秋后问斩，finalize()就是刀下留人\n\n- 如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记，随后进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法。\n    \n- 如果对象在在finalize()中成功拯救自己——只要重新与引用链上的任何一个对象建立 关联即可，\n    \n\n# 垃圾收集算法\n\n## 标记-清除算法\n\n该算法分为“标记”和“清除”阶段：\n\n- 首先标记出所有不需要回收的对象，\n    \n- 在标记完成后统一回收掉所有没有被标记的对象。\n    \n\n![](statistic/asynccode-187.png)\n\n它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：\n\n1. **效率问题**\n    \n2. **空间问题（标记清除后会产生大量不连续的碎片）**\n    \n\n## 标记-复制算法\n\n- 它可以将内存分为大小相同的两块，每次使用其中的一块。\n    \n- 当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n    \n\n![](statistic/asynccode-186.png)\n\n## 标记-整理算法\n\n- 标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。\n    \n\n![](statistic/asynccode-192.png)\n\n## 分代收集算法\n\n当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。\n\n- **比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。**\n    \n- **而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。**\n    \n\n![](statistic/asynccode-193.png)\n\n**延伸面试问题：** HotSpot 为什么要分为新生代和老年代？\n\n根据上面的对分代收集算法的介绍回答。\n\n# 垃圾收集器\n\n**如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。**\n\n  \n\n![](statistic/asynccode-192.png)\n\n![](statistic/asynccode-194.png)\n\n## Serial 收集器\n\nSerial（串行）收集器它的 **“单线程”** 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候**必须暂停其他所有的工作线程**（ **\"Stop The World\"** ），直到它收集结束。\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法。**\n\n![](statistic/asynccode-190.png)\n\n  \n\n## ParNew 收集器\n\n**ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。**\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法。**\n\n![](statistic/asynccode-189.png)\n\n## Parallel Scavenge 收集器\n\nParallel Scavenge 收集器也是使用标记-复制算法的多线程收集器，它看上去几乎和 ParNew 都一样。 **那么它有什么特别之处呢？**\n\n```Plain\n-XX:+UseParallelGC\n\n    使用 Parallel 收集器+ 老年代串行\n\n-XX:+UseParallelOldGC\n\n    使用 Parallel 收集器+ 老年代并行\n```\n\n**Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。**\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法。**\n\n![](statistic/asynccode-188.png)\n\n## Serial Old 收集器\n\n**Serial 收集器的老年代版本**，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。\n\n## Parallel Old 收集器\n\n**Parallel Scavenge 收集器的老年代版本**。使用多线程和**“标记-整理”算法**。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。\n\n## CMS 收集器\n\n**CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。**\n\n**CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。**\n\n从名字中的**Mark Sweep**这两个词可以看出，CMS 收集器是一种 **“标记-清除”算法**实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：\n\n- **初始标记：** 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ；\n    \n- **并发标记：** **同时开启 GC 和用户线程，用一个闭包结构去记录可达对象**。但在这个阶段结束，这个**闭包结构并不能保证包含当前所有的可达对象**。因为用户线程可能会不断的更新引用域，所以 G**C 线程无法保证可达性分析的实时性**。所以这个算法里会跟踪记录这些发生引用更新的地方。\n    \n- **重新标记：** 重新标记阶段就是为了**修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，**这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短\n    \n- **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。\n    \n\n![](statistic/asynccode-189.png)\n\n从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：\n\n- **对 CPU 资源敏感；**\n    \n- **无法处理浮动垃圾；**\n    \n- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**\n    \n\n## G1 收集器\n\n**G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.**\n\n被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备以下特点：\n\n- **并行与并发**：G1 能充分利用 CPU、多核环境下的硬件优势**，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间**。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。\n    \n- **分代收集**：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。\n    \n- **空间整合**：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。\n    \n- **可预测的停顿**：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。\n    \n\nG1 收集器的运作大致分为以下几个步骤：\n\n- **初始标记**\n    \n- **并发标记**\n    \n- **最终标记**\n    \n- **筛选回收**\n    \n\n**G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)** 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。\n\n## ZGC 收集器\n\n与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。\n\n在 ZGC 中出现 Stop The World 的情况会更少！\n\n详情可以看 ： [《新一代垃圾回收器 ZGC 的探索与实践》](https://tech.meituan.com/2020/08/06/new-zgc-practice-in-meituan.html)\n\n  \n\n  \n\n## 了解）JVM 的常用参数\n\nJVM 的参数非常之多，这里只列举比较重要的几个，通过各种各样的搜索引擎也可以得知这些信息。\n\n|   |   |   |   |\n|---|---|---|---|\n|参数名称|含义|默认值|说明|\n|-Xms|初始堆大小|物理内存的 1/64(\u003c1GB)|默认(MinHeapFreeRatio 参数可以调整)空余堆内存小于 40%时，JVM 就会增大堆直到-Xmx 的最大限制.|\n|-Xmx|最大堆大小|物理内存的 1/4(\u003c1GB)|默认(MaxHeapFreeRatio 参数可以调整)空余堆内存大于 70%时，JVM 会减少堆直到 -Xms 的最小限制|\n|-Xmn|年轻代大小(1.4or later)||注意：此处的大小是（eden+ 2 survivor space).与 jmap -heap 中显示的 New gen 是不同的。整个堆大小=年轻代大小 + 老年代大小 + 持久代（永久代）大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun 官方推荐配置为整个堆的 3/8|\n|-XX:NewSize|设置年轻代大小(for 1.3/1.4)|||\n|-XX:MaxNewSize|年轻代最大值(for 1.3/1.4)|||\n|-XX:PermSize|设置持久代(perm gen)初始值|物理内存的 1/64||\n|-XX:MaxPermSize|设置持久代最大值|物理内存的 1/4||\n|-Xss|每个线程的堆栈大小||JDK5.0 以后每个线程堆栈大小为 1M,以前每个线程堆栈大小为 256K.根据应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在 3000~5000 左右一般小的应用， 如果栈不是很深， 应该是 128k 够用的 大的应用建议使用 256k。这个选项对性能影响比较大，需要严格的测试。（校长）和 threadstacksize 选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:-Xss is translated in a VM flag named ThreadStackSize”一般设置这个值就可以了|\n|-XX:NewRatio|年轻代(包括 Eden 和两个 Survivor 区)与年老代的比值(除去持久代)||-XX:NewRatio=4 表示年轻代与年老代所占比值为 1:4,年轻代占整个堆栈的 1/5Xms=Xmx 并且设置了 Xmn 的情况下，该参数不需要进行设置。|\n|-XX:SurvivorRatio|Eden 区与 Survivor 区的大小比值||设置为 8,则两个 Survivor 区与一个 Eden 区的比值为 2:8,一个 Survivor 区占整个年轻代的 1/10|\n|-XX:+DisableExplicitGC|关闭 System.gc()||这个参数需要严格的测试|\n|-XX:PretenureSizeThreshold|对象超过多大是直接在旧生代分配|0|单位字节 新生代采用 Parallel ScavengeGC 时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.|\n|-XX:ParallelGCThreads|并行收集器的线程数||此值最好配置与处理器数目相等 同样适用于 CMS|\n|-XX:MaxGCPauseMillis|每次年轻代垃圾回收的最长时间(最大暂停时间)||如果无法满足此时间,JVM 会自动调整年轻代大小,以满足此值.|\n\n其实还有一些打印及 CMS 方面的参数，这里就不以一一列举了\n\n  \n\n## 什么是**Stop The World ?** 什么是 **OopMap** ？什么是安全点？\n\n- 进行垃圾回收的过程中，会涉及对象的移动。为了保证对象引用更新的正确性，**必须暂停所有的用户线程，像这样的停顿**，虚拟机设计者形象描述为 Stop The World 。也简称为STW\n    \n\n  \n\n- 在HotSpot中，有个数据结构（映射表）称为 OopMap 。一旦类加载动作完成的时 候**，HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，记录到 OopMap**。在即时编译过程中，也会在特定的位置 生成 OopMap，记录下栈上和寄存 器里哪些位置是引用。这些特定的位置主要在：\n    \n    - 循环的末尾（非 counted 循环）\n        \n    - 方法临返回前 / 调用方法的call指令后\n        \n    - 可能抛异常的位置\n        \n- 这些位置就叫作安全点**(safepoint)**。 **用户程序执行时并非在代码指令流的任意位置 都能够在停顿下来开始垃圾收集，而是必须是执行到安全点才能够暂停**。\n    \n\n## 有了**CMS**，为什么还要引入**G1**？\n\n优点：CMS最主要的优点在名字上已经体现出来——并发收集、低停顿。\n\n缺点：CMS同样有三个明显的缺点。\n\n- Mark Sweep算法会导致内存碎片比较多\n    \n- CMS的并发能力比较依赖于CPU资源，并发回收时垃圾收集线程可能会抢占用户线程的资源，导致用户程序性能下降。\n    \n- 并发清除阶段，用户线程依然在运行，会产生所谓的理“浮动垃圾”（Floating Garbage），本次垃圾收集无法处理浮动垃圾，必须到下一次垃圾收集才能处 理。如果浮动垃圾太多，会触发新的垃圾回收，导致性能降低。 G1主要解决了内存碎片过多的问题。\n    \n\n## 你们线上用的什么垃圾收集器？为什么要用它？\n\n默认 UseParallelGC = Parallel Scavenge + Parallel Old ，表示的是新生代用的 Parallel Scavenge 收集器，老年代用的是 Parallel Old 收集器。\n\n  \n\n- 提高吞吐量**(Parallel Scavenge + Parallel Old)**\n    \n      高吞吐，我们可以回答：因为我们系统是业务相对复杂，但并发并不是非常高，所以希望尽可能的利用处理器资源，出于提高吞吐量的考虑采用 Parallel Scavenge + Parallel Old 的组合。\n    \n- 降低停顿时间**（Parallel Scavenge + Parallel Old）**\n    \n- 不仅满足我们低停顿的要 求，而且解决了CMS的浮动垃圾问题、内存碎片问题***（G1）**\n    \n\n  \n\n  \n\n这里简单地列一下上面提到的一些收集器的适用场景：\n\n- Serial ：如果应用程序有一个很小的内存空间（大约100 MB）亦或它在没有停顿时间要求的单线程处理器上运行。\n    \n- Parallel：如果优先考虑应用程序的峰值性能，并且没有时间要求要求，或者可以接受1秒或更长的停顿时间。\n    \n- CMS/G1：如果响应时间比吞吐量优先级高，或者垃圾收集暂停必须保持在大约 1秒以内。\n    \n- ZGC：如果响应时间是高优先级的，或者堆空间比较大\n    \n\n  \n\n  \n\n# 对象一定分配在堆中吗？有没有了解逃逸分析技术？\n\n## 对象一定分配在堆中吗？有没有了解逃逸分析技术？\n\n随着**JIT编译期的发展与逃逸分析技术逐渐成熟**，所有的对象都分配到堆上也渐渐变得不那么“绝对”了,其中一种重要的技术叫做逃逸分析\n\n## 什么是逃逸分析？\n\n逃逸分析是指**分析指针动态范围的方法,**当变量（或者对象）在方法中分配后，**其指针有可能被返回或者被全局 引用，这样就会被其他方法或者线程所引用**，这种现象称作指针（或者引用）的逃逸(Escape)\n\n![](statistic/asynccode-191.png)\n\n## 逃逸分析的好处\n\n- 栈上分配\n    \n    - 如果确定一个对象不会逃逸到线程之外，那么久可以考虑将这个对象在栈上分配， 对象占用的内存随着栈帧出栈而销毁\n        \n- 同步消除\n    \n    - 线程同步本身是一个相对耗时的过程**，如果逃逸分析能够确定一个变量不会逃逸出线程，无法被其他线程访问**，那么这个变量的**读写肯定就不会有竞争**， 对这个变 实施的同步措施也就可以安全地消除掉\n        \n- 标量替换\n    \n    - 如果一个数据是基本数据类型，不可拆分，它就被称之为标量。把一个Java对象拆散，将其用到的成员变量恢复为原始类型来访问，这个过程就称为**标量替换**\n        \n    - 假如 逃逸分析能够证明一个对象不会被方法外部访问，并且这个对象可以被拆散，**那么 可以不创建对象，直接用创建若干个成员变量代替**，可以让对象的成员变量在栈上 分配和读写。","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/JVM/JVM-%E8%B0%83%E4%BC%98":{"title":"JVM 调优","content":"# 有哪些常用的命令行性能监控和故障处理工具？\n\n- 操作系统工具\n    \n    - top：显示系统整体资源使用情况\n        \n    - vmstat：监控内存和CPU\n        \n    - iostat：监控IO使用\n        \n    - netstat：监控网络使用JDK性能监控工具\n        \n- JDK性能监控工具\n    \n    - jps：虚拟机进程查看\n        \n    - jstat：虚拟机运行时信息查看\n        \n    - jinfo：虚拟机配置查看\n        \n    - jmap：内存映像（导出）\n        \n    - jhat：堆转储快照分析\n        \n    - jstack：Java堆栈跟踪\n        \n    - jcmd：实现上面除了jstat外所有命令的功能\n        \n\n# 了解哪些可视化的性能监控和故障处理工具\n\n- JConsole\n    \n\n![](statistic/asynccode-161.png)\n\n- VisualVM\n    \n\n![](statistic/asynccode-166.png)\n\n- Java Mission Control\n    \n\n![](statistic/asynccode-169.png)\n\n- MAT :Java 堆内存分析工具。\n    \n- GChisto:GC 日志分析工具。\n    \n- GCViewer :GC 日志分析工具。\n    \n- JProfiler:商用的性能分析利器。\n    \n- arthas :阿里开源诊断工具。\n    \n- async-profiler:Java 应用性能分析工具，开源、火焰图、跨平台。\n    \n\n  \n\n# **JVM**的常见参数配置知道哪些\n\n一些常见的参数配置：\n\n- 堆配置：\n    \n    - -Xms:初始堆大小\n        \n    - -Xms：最大堆大小\n        \n    - -XX:NewSize=n:设置年轻代大小\n        \n    - -XX:NewRatio=n:设置年轻代和年老代的比值。如：为3表示年轻代和年老代比值\n        \n    - 为1：3，年轻代占整个年轻代年老代和的1/4\n        \n    - -XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区\n        \n    - 有两个。如3表示Eden： 3 Survivor：2，一个Survivor区占整个年轻代的1/5\n        \n    - -XX:MaxPermSize=n:设置持久代大小\n        \n- 收集器设置：\n    \n    - -XX:+UseSerialGC:设置串行收集器\n        \n    - -XX:+UseParallelGC:设置并行收集器\n        \n    - -XX:+UseParalledlOldGC:设置并行年老代收集器\n        \n    - -XX:+UseConcMarkSweepGC:设置并发收集器\n        \n- 并行收集器设置\n    \n    - -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程 数\n        \n    - -XX:MaxGCPauseMillis=n:设置并行收集最大的暂停时间（如果到这个时间了， 垃圾回收器依然没有回收完，也会停止回收）\n        \n    - -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为： 1/(1+n)\n        \n    - -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况-XX:ParallelGCThreads=n:设置并发收集器年轻代手机方式为并行收集时，使用 的CPU数。并行收集线程数\n        \n- 打印**GC**回收的过程日志信息\n    \n    - -XX:+PrintGC\n        \n    - -XX:+PrintGCDetails\n        \n    - -XX:+PrintGCTimeStamps\n        \n    - -Xloggc:filename\n        \n\n  \n\n# **JVM**调优流程\n\n![](statistic/asynccode-167.png)\n\n![](statistic/asynccode-165.png)\n\n  \n\n# 线上服务CPU占用过高如何排查？\n\n![](statistic/asynccode-162.png)\n\n# 内存飚高如何排查\n\n![](statistic/asynccode-168.png)\n\n# 频繁 **minor gc** 怎么办？\n\n优化Minor GC频繁问题：通常情况下，由于新生代空间较小，Eden区很快被填满， 就会导致频繁Minor GC，因此可以通过增大新生代空间 -Xmn 来降低Minor GC的频 率\n\n## 频繁**Full GC**怎么办？\n\n![](statistic/asynccode-164.png)\n\n![](statistic/asynccode-169.png)\n\n  \n\n## 有没有处理过内存泄漏问题？是如何定位的？\n\n内存泄漏是内在病源，外在病症表现可能有：\n\n- 应用程序长时间连续运行时性能严重下降\n    \n- CPU 使用率飙升，甚至到 100%\n    \n- 频繁 Full GC，各种报警，例如接口超时报警等\n    \n- 应用程序抛出 OutOfMemoryError 错误\n    \n- 应用程序偶尔会耗尽连接对象\n    \n\n严重内存泄漏往往**伴随频繁的 Full GC**，所以分析排查内存泄漏问题首先还得从查 看 Full GC 入手。主要有以下操作步骤：\n\n1. 使用 jps 查看运行的 Java 进程 ID\n    \n2. 使用 top -p [pid] 查看进程使用 CPU 和 MEM 的情况\n    \n3. 使用 top -Hp [pid] 查看进程下的所有线程占 CPU 和 MEM 的情况\n    \n4. 将线程 ID 转换为 16 进制： printf \"%x\\n\" [pid] ，输出的值就是线程栈信 息中的 **nid**。\n    \n    1. 例如： printf \"%x\\n\" 29471 ，换行输出 **731f**。\n        \n5. 抓取线程栈： jstack 29452 \u003e 29452.txt ，可以多抓几次做个对比。 在线程栈信息中找到对应线程号的 16 进制值，如下是 **731f** 线程的信息。线程 栈分析可使用 Visualvm 插件 **TDA**。\n    \n\n![](statistic/asynccode-163.png)\n\n6. 使用 jstat -gcutil [pid] 5000 10 每隔 5 秒输出 GC 信息，输出 10 次， 查看 **YGC** 和 **Full GC** 次数。通常会出现 YGC 不增加或增加缓慢，而 Full GC 增加很快。\n    \n    1. 或使用 jstat -gccause [pid] 5000 ，同样是输出 GC 摘要信息。\n        \n    2. 或使用 jmap -heap [pid] 查看堆的摘要信息，关注老年代内存使用是否达到 阀值，若达到阀值就会执行 Full GC。\n        \n7. 如果发现 Full GC 次数太多，就很大概率存在内存泄漏了\n    \n8. 使用 jmap -histo:live [pid] 输出每个类的对象数量，内存大小(字节单位) 及全限定类名。\n    \n9. 生成 dump 文件，借助工具分析哪 个对象非常多，基本就能定位到问题在那了 使用 jmap 生成 dump 文件：\n    \n10. 10. dump 文件分析\n    \n    1. 可以使用 **jhat** 命令分析： jhat -port 8000 29471.dump ，浏览器访问 jhat 服务，端口是 8000。\n        \n    2. 通常使用图形化工具分析，如 JDK 自带的 **jvisualvm**，从菜单 \u003e 文件 \u003e 装入 dump 文件。\n        \n    3. 或使用第三方式具分析的，如 **JProfiler** 也是个图形化工具，**GCViewer** 工具。\n        \n    4. Eclipse 或以使用 MAT 工具查看。或使用在线分析平台 **GCEasy**。\n        \n    5. 注意：如果 dump 文件较大的话，分析会占比较大的内存。\n        \n11. 11. 在 dump 文析结果中查找存在大量的对象，再查对其的引用。\n    \n\n基本上就可以定位到代码层的逻辑了。","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/JVM/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F":{"title":"Java内存区域","content":"## 运行时数据区域\n\nJava 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK 1.8 和之前的版本略有不同，下面会介绍到。\n\n**JDK 1.8 之前** ：\n\n![](statistic/asynccode-196.png)\n\n**JDK 1.8 之后** ：\n\n![](statistic/asynccode-206.png)\n\n**线程私有的：**\n\n- 程序计数器\n    \n- 虚拟机栈\n    \n- 本地方法栈\n    \n\n**线程共享的：**\n\n- 堆\n    \n- 方法区\n    \n- 直接内存 (非运行时数据区的一部分)\n    \n\nJava 虚拟机规范对于运行时数据区域的规定是相当宽松的。以堆为例：堆可以是连续空间，也可以不连续。堆的大小可以固定，也可以在运行时按需扩展 。虚拟机实现者可以使用任何垃圾回收算法管理堆，甚至完全不进行垃圾收集也是可以的。\n\n## 程序计数器\n\n**程序计数器是一块较小的内存空间**，可以看作是当前线程**所执行的字节码的行号指示器**。字节码解释器工作时通过**改变这个计数器的值来选取下一条需要执行的字节码指令**，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。\n\n另外**，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器**，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。\n\n从上面的介绍中我们知道了程序计数器主要有两个作用：\n\n- 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。\n    \n- 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。\n    \n\n⚠️ 注意 ：**程序计数器是唯一一个不会出现** **`OutOfMemoryError`** **的内存区域**，**它的生命周期随着线程的创建而创建，随着线程的结束而死亡。**\n\n## Java 虚拟机栈\n\n- 与程序计数器一样，**Java 虚拟机栈（后文简称栈）也是线程私有的，它的生命周期和线程相同，随着线程的创建而创建，随着线程的死亡而死亡。**\n    \n- **所有的 Java 方法调用都是通过栈来实现的（也需要和其他运行时数据区域比如程序计数器配合）**。**方法调用的数据需要通过栈进行传递，每一次方法调用都会有一个对应的栈帧被压入栈中，每一个方法调用结束后，都会有一个栈帧被弹出。**\n    \n\n而每个栈帧中都拥有：\n\n- 局部变量表\n    \n- 操作数栈\n    \n- 动态链接\n    \n- 方法返回地址。和数据结构上的栈类似，两者都是先进后出的数据结构，只支持出栈和入栈两种操作。\n    \n\n![](statistic/asynccode-200.png)\n\n### **局部变量表**\n\n**主要存放了编译期可知的各种数据类型**（boolean、byte、char、short、int、float、long、double）、**对象引用**（reference 类型，它不同于对象本身，**可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置**）。\n\n![](statistic/asynccode-204.png)\n\n### **操作数栈**\n\n主要作为方法调用的中转站使用，**用于存放方法执行过程中产生的中间计算结果**。另外，计算过程中产生的临时变量也会放在操作数栈中。\n\n### **动态链接**\n\n**主要服务一个方法需要调用其他方法的场景**。Class 文件的常量池里保存有大量的符号引用比如方法引用的符号引用。\n\n当一个方法要调用其他方法，\n\n- 需要将常量池中指向方法的**符号引用转化为其在内存地址中的直接引用**。\n    \n- **动态链接的作用就是为了将符号引用转换为调用方法的直接引用**，这个过程也被称为 **动态连接** 。\n    \n\n![](statistic/asynccode-195.png)\n\n栈空间虽然不是无限的，但一般正常调用的情况下是不会出现问题的。**不过，如果函数调用陷入无限循环的话，就会导致栈中被压入太多栈帧而占用太多空间，导致栈空间过深。那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出** **`StackOverFlowError`** **错误**。\n\nJava 方法有两种返回方式，一种是 return 语句正常返回，一种是抛出异常。不管哪种返回方式，都会导致栈帧被弹出。也就是说， **栈帧随着方法调用而创建，随着方法结束而销毁。无论方法正常完成还是异常完成都算作方法结束。**\n\n  \n\n简单总结一下程序运行中栈可能会出现两种错误：\n\n- **`StackOverFlowError`****：** 若栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 `StackOverFlowError` 错误。\n    \n- **`OutOfMemoryError`****：** 如果栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出`OutOfMemoryError`异常。\n    \n\n## 本地方法栈\n\n和虚拟机栈所发挥的作用非常相似，区别是： **虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。** 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。\n\n本地方法被执行的时候，在**本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。**\n\n方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 **`StackOverFlowError`** **和** **`OutOfMemoryError`** **两种错误。**\n\n## 堆\n\nJava 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。**\n\nJava 世界中“几乎”所有的对象都在堆中分配，但是，随着 JIT 编译器的发展与逃逸分析技术逐渐成熟，**栈上分配、标量替换优化技术将会导致一些微妙的变化**，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从 JDK 1.7 开始已经**默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。**\n\nJava 堆是垃圾收集器管理的主要区域，因此也被称作 **GC 堆（Garbage Collected Heap）**。从垃圾回收的角度，由于现在收集器基本都采用**分代垃圾收集算法。**\n\n- 新生代\n    \n    - Eden\n        \n    - Survivor\n        \n    - Old\n        \n- 老年代\n    \n\n在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分：\n\n1. 新生代内存(Young Generation)\n    \n2. 老生代(Old Generation)\n    \n3. 永久代(Permanent Generation)\n    \n\n下图所示的 Eden 区、两个 Survivor 区 S0 和 S1 都属于新生代，中间一层属于老年代，最下面一层属于永久代。\n\n![](statistic/asynccode-203.png)\n\n**JDK 8 版本之后 PermGen(永久) 已被 Metaspace(元空间) 取代，元空间使用的是本地内存。** （我会在方法区这部分内容详细介绍到）。\n\n大部分情况，**对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区-\u003eSurvivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数** **`-XX:MaxTenuringThreshold`** **来设置**。\n\n\u003e **🐛 修正（参见：****[issue552open in new window](https://github.com/Snailclimb/JavaGuide/issues/552)****）** ：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\n\u003e \n\u003e **动态年龄计算的代码如下**\n\u003e \n\u003e ```C++\n\u003e uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) {\n\u003e         //survivor_capacity是survivor空间的大小\n\u003e size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100);\n\u003e size_t total = 0;\n\u003e uint age = 1;\n\u003e while (age \u003c table_size) {\n\u003e total += sizes[age];//sizes数组是每个年龄段对象大小\n\u003e if (total \u003e desired_survivor_size) break;\n\u003e age++;\n\u003e }\n\u003e uint result = age \u003c MaxTenuringThreshold ? age : MaxTenuringThreshold;\n\u003e         ...\n\u003e }\n\u003e ```\n\n堆这里最容易出现的就是 `OutOfMemoryError` 错误，并且出现这种错误之后的表现形式还会有几种，比如：\n\n1. **`java.lang.OutOfMemoryError: GC Overhead Limit Exceeded`** ： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。\n    \n2. **`java.lang.OutOfMemoryError: Java heap space`** **:假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误**。(和配置的最大堆内存有关，且受制于物理内存大小。最大堆内存可通过`-Xmx`参数配置，若没有特别配置，将会使用默认值，详见：[Default Java 8 max heap sizeopen in new window](https://stackoverflow.com/questions/28272923/default-xmxsize-in-java-8-max-heap-size))\n    \n3. ......\n    \n\n## 方法区\n\n方法区属于是 **JVM 运行时数据区域的一块逻辑区域，是各个线程共享的内存区域**。\n\n当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。方法区会存储已被虚拟机加载的 **类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据**。\n\n**方法区和永久代以及元空间是什么关系呢？**\n\n**永久代以及元空间是 HotSpot 虚拟机对虚拟机规范中方法区的两种实现方式**。并且，永久代是 JDK 1.8 之前的方法区实现，JDK 1.8 及以后方法区的实现变成了元空间。\n\n![](statistic/asynccode-195.png)\n\n**为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?**\n\n下图来自《深入理解 Java 虚拟机》第 3 版 2.2.5\n\n![](statistic/asynccode-196.png)\n\n1、**整个永久代有一个 JVM 本身设置的固定大小上限，无法进行调整**，而**元空间使用的是本地内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小**。\n\n\u003e 当元空间溢出时会得到如下错误： `java.lang.OutOfMemoryError: MetaSpace`\n\n你可以使用 `-XX：MaxMetaspaceSize` 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。`-XX：MetaspaceSize` 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。\n\n2、**元空间里面存放的是类的元数据**，这样加载多少类的元数据就不由 `MaxPermSize` 控制了, 而由系统的实际可用空间来控制，**这样能加载的类就更多了。**\n\n3、在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。\n\n  \n\n### 运行时常量池\n\nClass 文件中除了有**类的版本、字段、方法、接口等描述信息外，还有用于存放编译期生成的各种字面量（Literal）和符号引用（Symbolic Reference）的 常量池表(Constant Pool Table) 。**\n\n字面量是源代码中的固定值的表示法，即通过字面我们就能知道其值的含义**。字面量包括整数、浮点数和字符串字面量。常见的符号引用包括类符号引用、字段符号引用、方法符号引用、接口方法符号。**\n\n《深入理解 Java 虚拟机》7.34 节第三版对符号引用和直接引用的解释如下：\n\n![](statistic/asynccode-176.png)\n\n常量池表会在类加载后存放到方法区的运行时常量池中。\n\n**运行时常量池的功能类似于传统编程语言的符号表，尽管它包含了比典型符号表更广泛的数据。**\n\n既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 `OutOfMemoryError` 错误。\n\n### 字符串常量池\n\n**字符串常量池** 是 JVM 为了提升性能和减少内存**消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建**。\n\n```Java\n// 在堆中创建字符串对象”ab“\n// 将字符串对象”ab“的引用保存在字符串常量池中\nString aa = \"ab\";\n// 直接返回字符串常量池中字符串对象”ab“的引用\nString bb = \"ab\";\nSystem.out.println(aa==bb);// true\n```\n\n- **HotSpot 虚拟机中字符串常量池的实现是** **`src/hotspot/share/classfile/stringTable.cpp`** **,****`StringTable`** **本质上就是一个****`HashSet\u003cString\u003e`**\n    \n- **`StringTable`** **中保存的是字符串对象的引用，字符串对象的引用指向堆中的字符串对象。**\n    \n\nJDK1.7 之前，**字符串常量池存放在永久代**。JDK1.7 字**符串常量池和静态变量从永久代移动了 Java 堆中**。\n\n![](statistic/asynccode-197.png)\n\n![](statistic/asynccode-198.png)\n\n**JDK 1.7 为什么要将字符串常量池移动到堆中？**\n\n**主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC**。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。\n\n相关问题：[JVM 常量池中存储的是对象还是引用呢？ - RednaxelaFX - 知乎open in new window](https://www.zhihu.com/question/57109429/answer/151717241)\n\n最后再来分享一段周志明老师在[《深入理解 Java 虚拟机（第 3 版）》样例代码\u0026勘误open in new window](https://github.com/fenixsoft/jvm_book) Github 仓库的 [issue#112open in new window](https://github.com/fenixsoft/jvm_book/issues/112) 中说过的话：\n\n\u003e **运行时常量池、方法区、字符串常量池这些都是不随虚拟机实现而改变的逻辑概念，是公共且抽象的，Metaspace、Heap 是与具体某种虚拟机实现相关的物理概念，是私有且具体的。**\n\n## 直接内存\n\n直接内存是一种特殊的内存缓冲区，**并不在 Java 堆或方法区中分配的，而是通过 JNI 的方式在本地内存上分配的**。\n\n直接内存并**不是虚拟机运行时数据区的一部**分，也不是虚拟机规范中定义的内存区域，但**是这部分内存也被频繁地使用。而且也可能导致** **`OutOfMemoryError`** **错误出现**。\n\n- **NIO(New Input/Output) 类**，引入了一种基于**通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据**。\n    \n\n  \n\n类似的概念还有 **堆外内存** 。在一些文章中将直接内存等价于堆外内，个人觉得不是特别准确。\n\n堆外内存就是把内存对象分配在堆（新生代+老年代+永久代）以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够**在一定程度上减少垃圾回收对应用程序造成的影响**。\n\n# HotSpot 虚拟机对象探秘\n\n通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。\n\n## 对象的创建\n\nJava 对象的创建过程我建议最好是能默写出来，并且要掌握每一步在做什么。\n\n### Step1:类加载检查\n\n虚拟机遇到一条 new 指令时，\n\n- 首先将去检查这个指令的参数是否能在**常量池中定位到这个类的符号引用**，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。\n    \n\n### Step2:分配内存\n\n- 在**类加载检查**通过后，接下来虚拟机将为新生对象**分配内存**。对**分配方式**有 **“指针碰撞”** 和 **“空闲列表”** 两种，**选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定**。\n    \n\n#### **内存分配的两种方式** （补充内容，需要掌握）：\n\n- 指针碰撞 ：\n    \n    - 适用场合 ：堆内存规整（即没有内存碎片）的情况下。\n        \n    - 原理 ：用过的内存全部整合到一边，没有用过的内存放在另一边，中间有一个分界指针，只需要向着没用过的内存方向将该指针移动对象内存大小位置即可。\n        \n    - 使用该分配方式的 GC 收集器：Serial, ParNew\n        \n- 空闲列表 ：\n    \n    - 适用场合 ： 堆内存不规整的情况下。\n        \n    - 原理 ：虚拟机会维护一个列表，该列表中会记录哪些内存块是可用的，在分配的时候，找一块儿足够大的内存块儿来划分给对象实例，最后更新列表记录。\n        \n    - 使用该分配方式的 GC 收集器：CMS\n        \n\n选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是\"标记-清除\"，还是\"标记-整理\"（也称作\"标记-压缩\"），值得注意的是，复制算法内存也是规整的。\n\n#### **内存分配并发问题（补充内容，需要掌握），堆发生了抢占**\n\n在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全：\n\n- **CAS+失败重试：** CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。**虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。**\n    \n- **TLAB：** 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配\n    \n\n### Step3:初始化零值\n\n内存分配完成后，虚拟机需要将**分配到的内存空间都初始化为零值（不包括对象头**），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。\n\n### Step4:设置对象头\n\n初始化零值完成之后，**虚拟机要对对象进行必要的设置**，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 **这些信息存放在对象头中。** 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。\n\n### Step5:执行 init 方法\n\n在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，`\u003cinit\u003e` 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 `\u003cinit\u003e` 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。\n\n![](statistic/asynccode-202.png)\n\n# 对象的内存布局\n\n在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：**对象头**、**实例数据**和**对齐填充**。\n\n**Hotspot 虚拟机的对象头包括两部分信息**，\n\n- **第一部分用于存储对象自身的运行时数据**（哈希码、GC 分代年龄、锁状态标志等等），**另一部分是类型指针**，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。\n    \n- **实例数据部分是对象真正存储的有效信息**，也是在程序中所定义的各种类型的字段内容。\n    \n- **对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。** **因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍**，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。\n    \n\n![](statistic/asynccode-201.png)\n\n# 对象的访问定位\n\n建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有：**使用句柄**、**直接指针**。\n\n### 句柄\n\n如果使用句柄的话，那**么 Java 堆中将会划分出一块内存来作为句柄池**，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息。\n\n![](statistic/asynccode-205.png)\n\n### 直接指针\n\n如果使用直接指针访问，reference 中存储的直接就是对象的地址。\n\n![](statistic/asynccode-199.png)\n\n这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。\n\nHotSpot 虚拟机主要使用的就是这种方式来进行对象访问。","lastmodified":"2023-07-31T03:20:22.313088387Z","tags":[]},"/Java/overview":{"title":"overview","content":"\n\n","lastmodified":"2023-07-31T03:20:22.31708838Z","tags":["journal"]},"/Kubernetes/%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86-DaemonSet":{"title":"作业管理-DaemonSet","content":"**另外一类代表在线业务 API 对象：DaemonSet**，它会在 **Kubernetes 集群的每个节点上都运行一个 Pod**，就好像是 Linux 系统里的“守护进程”（Daemon）。\n\n# 为什么要有 DaemonSet\n\n**Deployment 并不关心这些 Pod 会在集群的哪些节点上运行，在它看来，Pod 的运行环境与功能是无关的，只要 Pod 的数量足够，应用程序应该会正常工作**。\n\n  \n\n有一些业务比较特殊**，它们不是完全独立于系统运行的，而是与主机存在“绑定”关系，必须要依附于节点才能产生价值**，比如说：\n\n- 网络应用（如 kube-proxy），必须每个节点都运行一个 Pod，否则节点就无法加入 Kubernetes 网络\n    \n- 监控应用（如 Prometheus），必须每个节点都有一个 Pod 用来监控节点的状态，实时上报信息。\n    \n- 日志应用（如 Fluentd），必须在每个节点上运行一个 Pod，才能够搜集容器运行时产生的日志数据。\\\n    \n- 安全应用，同样的，每个节点都要有一个 Pod 来执行安全审计、入侵检查、漏洞扫描等工作。\n    \n\n**DaemonSet 的目标是在集群的每个节点上运行且仅运行一个 Pod，就好像是为节点配上一只“看门狗”，忠实地“守护”着节点，这就是 DaemonSet 名字的由来**\n\n  \n\n# 如何使用 YAML 描述 DaemonSet\n\n**DaemonSet 和 Deployment 都属于在线业务，所以它们也都是“apps”组，使用命令 kubectl api-resources 可以知道它的简称是 ds** ，YAML 文件头信息应该是：\n\n```Go\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: xxx-ds\n```\n\n  \n\nkubectl 不提供自动创建 DaemonSet YAML 样板的功能，也就是说，我们不能用命令 kubectl create 直接创建出一个 DaemonSet 对象\n\n![](statistic/asynccode-603.png)\n\n（https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/）上找到一份 DaemonSet 的 YAML 示例，把它拷贝下来，再去掉多余的部分，就可以做成自己的一份样板文件，大概是下面的这个样子：\n\n```Go\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis-ds\n  labels:\n    app: redis-ds\n\nspec:\n  selector:\n    matchLabels:\n      name: redis-ds\n\n  template:\n    metadata:\n      labels:\n        name: redis-ds\n    spec:\n      containers:\n      - image: redis:5-alpine\n        name: redis\n        ports:\n        - containerPort: 6379\n```\n\n，DaemonSet 在 spec 里没有 replicas 字段，这是它与 Deployment 的一个关键不同点，意味着它不会在集群里创建多个 Pod 副本，而是要在每个节点上只创建出一个 Pod 实例。也就是说，**DaemonSet 仅仅是在 Pod 的部署调度策略上和 Deployment 不同，其他的都是相同的，某种程度上我们也可以把 DaemonSet 看做是 Deployment 的一个特例。**\n\n![](statistic/asynccode-604.png)\n\n## 如何在 Kubernetes 里使用 DaemonSet\n\n![](statistic/asynccode-609.png)\n\n看这张截图，虽然我们没有指定 DaemonSet 里 Pod 要运行的数量，但它自己就会去查找集群里的节点，在节点里创建 Pod。因为我们的实验环境里有一个 Master 一个 Worker，而 Master 默认是不跑应用的，所**以 DaemonSet 就只生成了一个 Pod，运行在了“worker”节点上。**\n\n暂停一下，你发现这里有什么不对劲了吗？\n\n按照 DaemonSet 的本意，**应该在每个节点上都运行一个 Pod 实例才对，但 Master 节点却被排除在外了**，这就不符合我们当初的设想了，**它的设计与 Kubernetes 集群的工作机制发生了冲突，有没有办法解决呢？**\n\n为了应对 Pod 在某些节点的“调度”和“驱逐”问题，它定义了两个新的概念：**污点（taint）和容忍度（toleration）。**\n\n**什么是污点（taint）和容忍度（toleration）**\n\n- **“污点”**是 Kubernetes 节点的一个属性，**它的作用也是给节点“贴标签”，**但为了不和已有的 labels 字段混淆，就改成了 taint。\n    \n- 和“污点”相对的，就是 Pod 的“容忍度”，顾名思义，就是 Pod 能否“容忍”污点。\n    \n\n  \n\n你可以用 kubectl describe node 来查看 Master 和 Worker 的状态：\n\n```Go\n\nkubectl describe node master\n\nName:     master\nRoles:    control-plane,master\n...\nTaints:   node-role.kubernetes.io/master:NoSchedule\n...\n\nkubectl describe node worker\n\nName:     worker\nRoles:    \u003cnone\u003e\n...\nTaints:   \u003cnone\u003e\n...\n```\n\nMaster 节点默认有一个 taint，名字是 node-role.kubernetes.io/master，**它的效果是 NoSchedule，**也就是说这个**污点会拒绝 Pod 调度到本节点上运行，而 Worker 节点的 taint 字段则是空的。**\n\n  \n\n你就知道该怎么让 DaemonSet 在 Master 节点（或者任意其他节点）上运行了，方法有两种。\n\n- 第一种方法是去掉 Master 节点上的 taint，让 Master 变得和 Worker 一样“纯洁无瑕”，DaemonSet 自然就不需要再区分 Master/Worker。\n    \n    ```Go\n    \n    kubectl taint node master node-role.kubernetes.io/master:NoSchedule-\n    ```\n    \n    - 这种方法修改的是 Node 的状态，影响面会比较大，可能会导致很多 Pod 都跑到这个节点上运行，所以我们可以保留 Node 的“污点”，**为需要的 Pod 添加“容忍度”，只让某些 Pod 运行在个别节点上，实现“精细化”调度**。\n        \n- 第二种方法，为 Pod 添加字段 tolerations，让它能够“容忍”某些“污点”，就可以在任意的节点上运行了。\n    \n\n```Go\n\ntolerations:\n- key: node-role.kubernetes.io/master\n  effect: NoSchedule\n  operator: Exists\n```\n\n  \n\n# 什么是静态Pod\n\ndaemonSet 是在 Kubernetes 里运行节点专属 Pod 最常用的方式，但它不是唯一的方式，Kubernetes 还支持另外一种叫“静态 Pod”的应用部署手段。\n\n静态 Pod”非常特殊，**它不受 Kubernetes 系统的管控，不与 apiserver、scheduler 发生关系，所以是“静态”的**。\n\n但既然它是 Pod，也必然会“跑”在容器运行时上，也会有 YAML 文件来描述它，**而唯一能够管理它的 Kubernetes 组件也就只有在每个节点上运行的 kubelet 了**。“静态 Pod”的 YAML 文件默认都存放在节点的 /etc/kubernetes/manifests 目录下，它是 Kubernetes 的专用目录。\n\n![](statistic/asynccode-602.png)\n\n下面的这张截图就是 Master 节点里目录的情况：你可以看到，**Kubernetes 的 4 个核心组件 apiserver、etcd、scheduler、controller-manager 原来都以静态 Pod 的形式存在的**，这也是为什么它们能够先于 Kubernetes 集群启动的原因。\n\n如果你有一些 DaemonSet 无法满足的特殊的需求，可以**考虑使用静态 Pod，编写一个 YAML 文件放到这个目录里，节点的 kubelet 会定期检查目录里的文件，发现变化就会调用容器运行时创建或者删除静态 Pod**。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86-Deployment":{"title":"作业管理-Deployment","content":"Deployment”，顾名思义，它是专门用来部署应用程序的，能够让应用永不宕机，多用来发布无状态的应用，是 Kubernetes 里最常用也是最有用的一个对象\n\n  \n\n# 为什么要有Delplyment\n\n除了“离线业务”，另一大类业务——也就是“在线业务”，在 Kubernetes 里应该如何处理呢？\n\n我们先看看用 Pod 是否就足够了。因为它在 YAML 里使用“containers”就可以任意编排容器，而且还有一个**“restartPolicy”字段，默认值就是 Always**，可以监控 Pod 里容器的状态，一旦发生异常，就会自动重启容器。\n\n- **“restartPolicy”只能保证容器正常工作**。不知你有没有想到，如果容器之外的 Pod 出错了该怎么办呢？比如说，有人不小心用 kubectl delete 误删了 Pod，或者 Pod 运行的节点发生了断电故障，那么 Pod 就会在集群里彻底消失，对容器的控制也就无从谈起了\n    \n\n  \n\n- 在线业务远不是单纯启动一个 Pod 这么简单**，还有多实例、高可用、版本更新等许多复杂的操作**。比如最简单的多实例需求，为了提高系统的服务能力，应对突发的流量和压力，我们需要创建多个应用的副本，还要即时监控它们的状态。如果还是只使用 Pod，那就会又走回手工管理的老路，没有利用好 Kubernetes 自动化运维的优势。\n    \n\n  \n\nKubernetes 已经给我们提供了处理这种问题的思路，就是**“单一职责”和“对象组合”**。既然 Pod 管理不了自己，那么我们就再创建一个新的对象，由它来管理 Pod\n\n  \n\n# 如何使用YAML 描述deployment\n\n可以用kubectl api-resources 看看Deployment的基本信息\n\n```Go\n\nkubectl api-resources\n\nNAME         SHORTNAMES   APIVERSION   NAMESPACED   KIND\ndeployments  deploy       apps/v1      true        Deployment\n```\n\n  \n\n我们还是可以使用命令 kubectl create 来创建 Deployment 的 YAML 样板，免去反复手工输入的麻烦。\n\n```Go\n\nexport out=\"--dry-run=client -o yaml\"\nkubectl create deploy ngx-dep --image=nginx:alpine $out\n```\n\n  \n\n得到的Deployment 是这个样子的\n\n```Go\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ngx-dep\n  name: ngx-dep\n  \nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ngx-dep\n      \n  template:\n    metadata:\n      labels:\n        app: ngx-dep\n    spec:\n      containers:\n      - image: nginx:alpine\n        name: nginx\n```\n\n把它和 Job/CronJob 对比一下，你会发现有相似也有不同。相似的地方是都有“spec”“template”字段，“template”字段里也是一个 Pod；不同的地方在于它的“spec”部分多了 replicas、selector 这两个新字段\n\n- replicas：指定要在 Kubernetes 集群里运行多少个 Pod 实例。\n    \n- selector，它的作用是**“筛选”出要被 Deployment 管理的 Pod 对象**，下属字段**“matchLabels”定义了 Pod 对象应该携带的 label，它必须和“template”里 Pod 定义的“labels”完全相同**，否则 Deployment 就会找不到要控制的 Pod 对象，apiserver 也会告诉你 YAML 格式校验错误无法创建。\n    \n\n  \n\n**为什么要这么麻烦？为什么不能像 Job 对象一样，直接用“template”里定义好的 Pod 就行了呢？**\n\n- 离线业务中的 Pod 基本上是一次性的，只与这个业务有关，紧紧地绑定在 Job 对象里，一般不会被其他对象所使用。\n    \n- 在线业务就要复杂得多了，因为 Pod 永远在线，除了要在 Deployment 里部署运行，还可能会被其他的 API 对象引用来管理，比如负责负载均衡的 Service 对象。\n    \n- Deployment 和 Pod 实际上是一种松散的组合关系，Deployment 实际上并不“持有”Pod 对象，它只是帮助 Pod 对象能够有足够的副本数量运行，仅此而已\n    \n\n  \n\n通过标签这种设计，Kubernetes 就解除了 Deployment 和模板里 Pod 的强绑定，把组合关系变成了“弱引用”。\n\n![](statistic/asynccode-601.png)\n\n# 如何使用kubectl 操作deployment\n\n```Go\n\nkubectl apply -f deploy.yml\n```\n\n查看 Deployment 的状态，仍然是用 kubectl get 命令：\n\n```Go\n\nkubectl get deploy\n```\n\n![](statistic/asynccode-599.png)\n\n它显示的信息都很重要：\n\n- READY 表示运行的 Pod 数量，前面的数字是当前数量，后面的数字是期望数量，所以“2/2”的意思就是要求有两个 Pod 运行，现在已经启动了两个 Pod。\n    \n- UP-TO-DATE 指的是当前已经更新到最新状态的 Pod 数量。因为如果要部署的 Pod 数量很多或者 Pod 启动比较慢，Deployment 完全生效需要一个过程，**UP-TO-DATE 就表示现在有多少个 Pod 已经完成了部署，达成了模板里的“期望状态”**。A\n    \n- VAILABLE 要比 READY、UP-TO-DATE 更进一步，不仅要求已经运行，还必须是健康状态，能够正常对外提供服务，它才是我们最关心的 Deployment 指标。\n    \n- 最后一个 AGE 就简单了，表示 Deployment 从创建到现在所经过的时间，也就是运行的时间。\n    \n\n需要用 kubectl get pod 命令来看看 Pod 的状态：\n\n```Go\n\nkubectl get pod\n```\n\n![](statistic/asynccode-600.png)\n\nDeployment 管理的 Pod 自动带上了名字，命名的规则是 Deployment 的名字加上两串随机数（**其实是 Pod 模板的 Hash 值**）\n\n  \n\n**在 Deployment 部署成功之后，你还可以随时调整 Pod 的数量，实现所谓的“应用伸缩”**\n\n  \n\n如下面的这条命令，就把 Nginx 应用扩容到了 5 个：\n\n```Go\n\nkubectl scale --replicas=5 deploy ngx-dep\n```\n\nkubectl scale 是命令式操作，扩容和缩容**只是临时的措施**，如果应用需要长时间保持一个确定的 Pod 数量，最好还是编辑 Deployment 的 YAML 文件，改动“replicas”，再以声明式的 kubectl apply 修改对象的状态。\n\n  \n\n通过 labels 为对象“贴”了各种“标签”，在使用 kubectl get 命令的时候，加上参数 -l，使用 ==、!=、in、notin 的表达式，就能够很容易地用“标签”筛选、过滤出所要查找的对象（有点类似社交媒体的 #tag 功能），效果和 Deployment 里的 selector 字段是一样的。\n\n```Go\n\nkubectl get pod -l app=nginx\nkubectl get pod -l 'app in (ngx, nginx, ngx-dep)'\n```","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86-Job-%E4%B8%8E-CronJob":{"title":"作业管理-Job 与 CronJob","content":"Pod 已经是一个相对完善的对象，专门负责管理容器，**那么我们就不应该再“画蛇添足”地盲目为它扩充功能**，而是要保持它的独立性，**容器之外的功能就需要定义其他的对象，把 Pod 作为它的一个成员“组合”**进去。\n\n  \n\n# 为什么要有 Job/CronJob\n\nNginx 和 busybox，它们分别代表了 Kubernetes 里的两大类业务。一类是像 Nginx 这样长时间运行的**“在线业务”，**另一类是像 busybox 这样短时间运行的**“离线业务**”\n\n- 在线业务”类型的应用有很多，比如 Nginx、Node.js、MySQL、Redis 等等，一**旦运行起来基本上不会停，也就是永远在线。**\n    \n- 离线业务”的特点是必定会退出，**不会无期限地运行下去**，所以它的调度策略也就与“在线业务”存在很大的不同，需要考虑运行超时、状态检查、失败重试、获取计算结果等管理事项\n    \n\n---\n\n- “临时任务”就是 API 对象 Job，\n    \n- “定时任务”就是 API 对象 CronJob，\n    \n- 使用这两个对象你就能够在 Kubernetes 里调度管理任意的离线业务了。\n    \n\n  \n\n# 如何使用 YAML 描述 Job\n\n- apiVersion 不是 v1，而是 batch/v1。\n    \n- kind 是 Job，这个和对象的名字是一致的。\n    \n- metadata 里仍然要有 name 标记名字，也可以用 labels 添加任意的标签。如果记不住这些也不要紧，你还可以使用命令 kubectl explain job 来看它的字段说明。不过想要生成 YAML 样板文件的话**不能使用 kubectl run**，因为 kubectl run 只能创建 Pod，要创建 Pod 以外的其他 API 对象，**需要使用命令 kubectl create**，再加上对象的类型名。\n    \n\n```Go\n\nexport out=\"--dry-run=client -o yaml\"              # 定义Shell变量\nkubectl create job echo-job --image=busybox $out\n```\n\n  \n\n  \n\n会生成一个基本的 YAML 文件，保存之后做点修改，就有了一个 Job 对象：\n\n```Go\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: echo-job\n\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - image: busybox\n        name: echo-job\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/echo\"]\n        args: [\"hello\", \"world\"]\n```\n\n**\"spec”字段里，多了一个 template 字段，然后又是一个“spec”**，\n\n- **Job 对象里应用了组合模式**，t**emplate 字段定义了一个“应用模板”，里面嵌入了一个 Pod，这样 Job 就可以从这个模板来创建出 Pod。**\n    \n- 这个 Pod 因为受 Job 的管理控制，不直接和 apiserver 打交道，也就没必要重复 apiVersion 等“头字段”，只需要定义好关键的 spec\n    \n\n![](statistic/asynccode-610.png)\n\n  \n\n# 开Kubernetes 中操作Job\n\n创建 Job 对象\n\n```Go\n\nkubectl apply -f job.yml\n```\n\n用 kubectl get job、kubectl get pod 来分别查看 Job 和 Pod 的状态：\n\n```Go\nkubectl get jobkubectl get pod\n```\n\n可以在 Job 级别、Pod 级别添加任意的字段来定制业务，这种优势是简单的容器技术无法相比的。\n\n这里我列出**几个控制离线作业的重要字段**，其他更详细的信息可以参考 Job 文档：\n\n- activeDeadlineSeconds，设置 Pod 运行的超时时间。\n    \n- backoffLimit，设置 Pod 的失败重试次数。\n    \n- completions，Job 完成需要运行多少个 Pod，默认是 1 个。\n    \n- parallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源。\n    \n\n创建一个 Job 对象，名字叫“sleep-job”，它随机睡眠一段时间再退出，模拟运行时间较长的作业（比如 MapReduce）。Job 的参数设置成 15 秒超时，最多重试 2 次，总共需要运行完 4 个 Pod，但同一时刻最多并发 2 个 Pod：\n\n```Go\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sleep-job\n\nspec:\n  activeDeadlineSeconds: 15\n  backoffLimit: 2\n  completions: 4\n  parallelism: 2\n\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - image: busybox\n        name: echo-job\n        imagePullPolicy: IfNotPresent\n        command:\n          - sh\n          - -c\n          - sleep $(($RANDOM % 10 + 1)) \u0026\u0026 echo done\n```\n\n  \n\n# 如何使用 YAML 描述 CronJob\n\n注意两点。\n\n- 第一，因为 CronJob 的名字有点长，所以 Kubernetes 提供了简写 cj，这个简写也可以使用命令 kubectl api-resources 看到；\n    \n- 第二，CronJob 需要定时运行，所以我们在命令行里还需要指定参数 --schedule\n    \n\n  \n\n```Go\n\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: echo-cj\n\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - image: busybox\n            name: echo-cj\n            imagePullPolicy: IfNotPresent\n            command: [\"/bin/echo\"]\n            args: [\"hello\", \"world\"]\n```\n\n重点关注它的 spec 字段，你会发现它居然连续有三个 spec 嵌套层次：\n\n- 第一个 spec 是 CronJob 自己的对象规格声明\n    \n- 第二个 spec 从属于“jobTemplate”，它定义了一个 Job 对象。\n    \n- 第三个 spec 从属于“template”，它定义了 Job 里运行的 Pod。\n    \n\n![](statistic/asynccode-611.png)\n\nCronJob 还有一个新字段就是“schedule”，用来定义任务周期运行的规则。它使用的是标准的 Cron 语法，指定分钟、小时、天、月、周，和 Linux 上的 crontab 是一样的","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86-Pod":{"title":"作业管理-Pod","content":"# 为什么要有 Pod\n\n**Namespace 做隔离，Cgroups 做限制，rootfs 做文件系统”这样的“三句箴言”可以朗朗上口了，为什么 Kubernetes 项目又突然搞出一个 Pod 来呢？**\n\n为了解决这样**多应用联合运行的问题，同时还要不破坏容器的隔离**，就需要在容器外面再建立一个“收纳舱”，让多个容器既保持相对独立，又能够小范围共享网络、存储等资源，而且永远是“绑在一起”的状态。\n\n**Kubernetes 项目所做的，其实就是将“进程组”的概念映射到了容器技术中，这就是Pod**\n\n  \n\n# 为什么Pod 是Kubernetes 的核心对象\n\nKubernetes 让 Pod 去编排处理容器，然后把 Pod 作为应用调度部署的最小单位，\n\n  \n\n从 Pod 开始，扩展出了 Kubernetes 里的一些重要 API 对象，比如配置信息 ConfigMap、离线作业 Job、多实例部署 Deployment 等等，它们都分别对应到现实中的各种实际运维需求。\n\n![](statistic/asynccode-612.png)\n\n  \n\n# 容器的实现原理\n\n- 首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。\n    \n    - Pod，其实是一组共享了某些资源的容器。\n        \n- Pod 里的所有容器，共享的是同一个 **Network Namespace**，并且可以声明共享同一个 Volume。\n    \n\n# 如何用YAML 描述Pod\n\nPod 也是 API 对象，所以它也必然具有 apiVersion、kind、metadata、spec 这四个基本组成部分。\n\nmetadata”里应该有 name 和 labels 这两个字段。\n\n- name 只是一个基本的标识，信息有限\n    \n- labels 字段就派上了用处。它可以添加任意数量的 Key-Value，给 Pod“贴”上归类的标签，结合 name 就更方便识别和管理了。\n    \n\n  \n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busy-pod\n  labels:\n    owner: chrono\n    env: demo\n    region: north\n    tier: back\n```\n\nspec”字段由于需要管理、维护 Pod 这个 Kubernetes 的基本调度单元，里面有非常多的关键信息\n\ncontainers\n\n“containers”是一个数组，里面的每一个元素又是一个 container 对象，也就是容器。\n\n- name:和 Pod 一样，container 对象也必须要有一个 name 表示名字，是必需的\n    \n- image:说明它使用的镜像，是必需的\n    \n- ports：列出容器对外暴露的端口，和 Docker 的 -p 参数有点像。\n    \n- imagePullPolicy：指定镜像的拉取策略，可以是 Always/Never/IfNotPresent，一般默认是 IfNotPresent，也就是说只有本地不存在才会远程拉取镜像，可以减少网络消耗。\n    \n- env：定义 Pod 的环境变量，和 Dockerfile 里的 ENV 指令有点类似，但它是运行时指定的，更加灵活可配置。command：定义容器启动时要执行的命令，相当于 Dockerfile 里的 ENTRYPOINT 指令。\n    \n- args：它是 command 运行时的参数，相当于 Dockerfile 里的 CMD 指令，这两个命令和 Docker 的含义不同，要特别注意。\n    \n\n  \n\n  \n\n# 使用 kubectl 操作 Pod\n\nkubectl apply -f 参数指定 YAML 文件创建 Pod\n\n```Go\n\nkubectl apply -f busy-pod.yml\nkubectl delete -f busy-pod.yml\n```\n\nkubectl delete -f 参数指定 YAML 文件删除 Pod，\n\n```Go\nkubectl delete -f busy-pod.yml\n```\n\nKubernetes 的 Pod 不会在前台运行，只能在后台（相当于默认使用了参数 -d），所以输出信息不能直接看到。我们可以用命令 kubectl logs，它会把 Pod 的标准输出流信息展示给我们看\n\n```Go\n\nkubectl logs busy-pod\n```\n\n使用命令 kubectl get pod 可以查看 Pod 列表和运行状态\n\n```Go\n\nkubectl get pod\n```\n\n![](statistic/asynccode-613.png)\n\n这个 Pod 运行有点不正常，状态是“CrashLoopBackOff”，那么我们可以使用命令 kubectl describe 来检查它的详细状态\n\n```Go\n\nkubectl describe pod busy-pod\n```\n\n![](statistic/asynccode-614.png)\n\n末尾的**“Events”部分，它显示的是 Pod 运行过程中的一些关键节点事件**。对于这个 busy-pod，因为它只执行了一条 echo 命令就退出了，而 Kubernetes 默认会重启 Pod，所以就会进入一个反复停止 - 启动的循环错误状态\n\nkubectl 也提供与 docker 类似的 cp 和 exec 命令，kubectl cp 可以把本地文件拷贝进 Pod，kubectl exec 是进入 Pod 内部执行 Shell 命令\n\n```Go\n\necho 'aaa' \u003e a.txt\nkubectl cp a.txt ngx-pod:/tmp\n```\n\nkubectl exec 的命令格式与 Docker 有一点小差异，需要在 Pod 后面加上 --，把 kubectl 的命令与 Shell 命令分隔开，你在用的时候需要小心一些\n\n```Go\n\nkubectl exec -it ngx-pod -- sh\n```","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86-StatefulSet":{"title":"作业管理-StatefulSet","content":"eployment 和 DaemonSet 两种 API 对象，它们是在 Kubernetes 集群里部署应用的重要工具，不过它们也有一个缺点，只能管理“无状态应用”（Stateless Application），**不能管理“有状态应用”（Stateful Application）。**\n\n  \n\n  \n\n有状态应用”的处理比较复杂，要考虑的事情很多，但是这些问题我们其实**可以通过组合Deployment、Service、PersistentVolume** 等对象来解决。\n\n  \n\n# 什么是有状态的应用\n\n状态”，应用保存的数据，实际上就是它某个时刻的“运行状态”。\n\n  \n\n- **只是有的应用的状态信息不是很重要，即使不恢复状态也能够正常运行**，这就是我们常说的“无状态应用”。\n    \n- 还有一些应用，**运行状态信息就很重要了，如果因为重启而丢失了状态是绝对无法接受的，这样的应用就是“有状态应用”**。\n    \n\n  \n\n**Deployment 加上 PersistentVolume，在 Kubernetes 里是不是可以轻松管理有状态的应用了呢？**\n\n- Deployment 来保证高可用，用 PersistentVolume 来存储数据，确实可以部分达到管理“有状态应用”的目的\n    \n- 它认为“状态”**不仅仅是数据持久化**，在集群化、分布式的场景里，还有多实例的依赖关系、启动顺序和网络标识等问题需要解决\n    \n- “有状态应用”，**多个实例之间可能存在依赖关系，比如 master/slave、active/passive，需要依次启动才能保证应用正常运行，外界的客户端也可能要使用固定的网络标识来访问实例**，而且这些信息还必须要保证在 Pod 重启后不变。\n    \n\n  \n\n# 如何使用 YAML 描述 StatefulSet\n\n```Go\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-sts\n\nspec:\n  serviceName: redis-svc\n  replicas: 2\n  selector:\n    matchLabels:\n      app: redis-sts\n\n  template:\n    metadata:\n      labels:\n        app: redis-sts\n    spec:\n      containers:\n      - image: redis:5-alpine\n        name: redis\n        ports:\n        - containerPort: 6379\n```\n\n在 spec 里还多出了一个“serviceName”字段，其余的部分和 Deployment 是一模一样的，比如 replicas、selector、template 等等。这两个不同之处其实就是 StatefulSet 与 Deployment 的关键区别。\n\n想要真正理解这一点，我们得结合 StatefulSet 在 Kubernetes 里的使用方法来分析\n\n# 如何在 Kubernetes 里使用 StatefulSet\n\n用 kubectl apply 创建 StatefulSet 对象，用 kubectl get 先看看它是什么样的：\n\n```Go\n\nkubectl apply -f redis-sts.yml\nkubectl get sts\nkubectl get pod\n```\n\n![](statistic/asynccode-596.png)\n\nStatefulSet 所管理的 **Pod 不再是随机的名字了，而是有了顺序编号**，从 0 开始分别被命名为 redis-sts-0、redis-sts-1，**Kubernetes 也会按照这个顺序依次创建（0 号比 1 号的 AGE 要长一点），这就解决了“有状态应用”的第一个问题：启动顺序。**\n\n  \n\n**用该怎么知道自己的身份，进而确定互相之间的依赖关系呢？**\n\nKubernetes 给出的方法是使用 hostname，也就是每个 Pod 里的主机名\n\n  \n\n**第三个问题：网络标识，这就需要用到 Service 对象。**\n\n```Go\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-svc\n\nspec:\n  selector:\n    app: redis-sts\n\n  ports:\n  - port: 6379\n    protocol: TCP\n    targetPort: 6379\n```\n\n![](statistic/asynccode-595.png)\n\n**普通Service 自己会有一个域名，格式是“对象名. 名字空间”，每个 Pod 也会有一个域名，形式是“IP 地址. 名字空间”**。但因为 IP 地址不稳定，所以 Pod 的域名并不实用，一般我们会使用稳定的 Service 域名。\n\n  \n\n**有状态应用，需要有稳定的网络标识，所以就会为 Pod 再多创建出一个新的域名，格式是“Pod 名. 服务名. 名字空间.svc.cluster.local”。**当然，这个域名也可以简写成“Pod 名. 服务名”\n\n  \n\n  \n\n**通过 StatefulSet 和 Service 的联合使用，Kubernetes 就解决了“有状态应用”的依赖关系、启动顺序和网络标识这三个问题，**\n\n  \n\n关于 Service，有一点值得再多提一下。\n\n**Service 原本的目的是负载均衡，**应该由它在 Pod 前面来转发流量**，但是对 StatefulSet 来说，这项功能反而是不必要的，因为 Pod 已经有了稳定的域名，**外界访问服务就不应该再通过 Service 这一层了。所以，从安全和节约系统资源的角度考虑，我**们可以在 Service 里添加一个字段 clusterIP: None ，告诉 Kubernetes 不必再为这个对象分配 IP 地址**。\n\n  \n\nstatefulSet 与 Service 对象的关系，你可以参考一下它们字段之间的互相引用：\n\n![](statistic/asynccode-597.png)\n\n  \n\n# 如何实现 StatefulSet 的数据持久化\n\n  \n\n为了强调持久化存储与 StatefulSet 的一对一绑定关系，**Kubernetes 为 StatefulSet 专门定义了一个字段“volumeClaimTemplates**”，**直接把 PVC 定义嵌入 StatefulSet 的 YAML 文件里**。\n\n- 这样能保证创建 StatefulSet 的同时\n    \n- 为每个 Pod 自动创建 PVC，让 StatefulSet 的可用性更高。\n    \n\n让我们把刚才的 Redis StatefulSet 对象稍微改造一下，加上持久化存储功能：\n\n```Go\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-pv-sts\n\nspec:\n  serviceName: redis-pv-svc\n\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-100m-pvc\n    spec:\n      storageClassName: nfs-client\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 100Mi\n\n  replicas: 2\n  selector:\n    matchLabels:\n      app: redis-pv-sts\n\n  template:\n    metadata:\n      labels:\n        app: redis-pv-sts\n    spec:\n      containers:\n      - image: redis:5-alpine\n        name: redis\n        ports:\n        - containerPort: 6379\n\n        volumeMounts:\n        - name: redis-100m-pvc\n          mountPath: /data\n```\n\n完整的关系图\n\n![](statistic/asynccode-598.png)\n\n  \n\n由于 StatefulSet 和 Deployment 一样会监控 Pod 的实例，发现 Pod 数量少了就会很快创建出新的 Pod，并且名字、网络标识也都会和之前的 Pod 一模一样","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86-YAML":{"title":"作业管理-YAML","content":"# 声明式与命令式是怎么回事\n\n“声明式”，在 Kubernetes 出现之前比较少见，它与“命令式”完全相反，不关心具体的过程，更注重结果。我们不需要“教”计算机该怎么做，只要告诉它一个目标状态，它自己就会想办法去完成任务，相比起来自动化、智能化程度更高。\n\n  \n\n# 什么是 YAML\n\nYAML 的官网（https://yaml.org/）有对语言规范的完整介绍，所以我就不在这里列举语言的细节了，只讲一些与 Kubernetes 相关的要点，帮助你快速掌握。\n\nYAML 是 JSON 的超集，支持整数、浮点数、布尔、字符串、数组和对象等数据类型\n\n  \n\n但和 JSON 比起来，YAML 的语法更简单，形式也更清晰紧凑，比如：\n\n- 使用空白与缩进表示层次（有点类似 Python），可以不使用花括号和方括号。\n    \n- 可以使用 # 书写注释，比起 JSON 是很大的改进。\n    \n- 对象（字典）的格式与 JSON 基本相同，但 Key 不需要使用双引号。\n    \n- 数组（列表）是使用 - 开头的清单形式（有点类似 MarkDown）。\n    \n- 表示对象的 : 和表示数组的 - 后面都必须要有空格。\n    \n- 可以使用 --- 在一个文件里分隔多个 YAML 对象\n    \n\n  \n\n**数组**\n\n```Go\n\n# YAML数组(列表)\nOS:\n  - linux\n  - macOS\n  - Windows\n  \n```\n\n等价的JSON\n\n```Go\n\n{\n  \"OS\": [\"linux\", \"macOS\", \"Windows\"]\n}\n```\n\n**对象**\n\n```Go\n\n# YAML数组(列表)\nOS:\n  - linux\n  - macOS\n  - Windows\n```\n\n等价的JSON\n\n```Go\n\n{\n  \"Kubernetes\": {\n    \"master\": 1,\n    \"worker\": 3\n  }\n}\n```\n\n![](statistic/asynccode-618.png)\n\n  \n\n# 什么是api对象\n\n因为 YAML 语言只相当于“语法”，要与 Kubernetes 对话，我们还必须有足够的“词汇”来表示“语义”。\n\nkubernetes 归纳总结了 Google 多年的经验，在理论层面抽象出了很多个概念，用来描述系统的管理运维工作，这些概念就叫做“**API 对象**”。\n\n  \n\napiserver 是 Kubernetes 系统的唯一入口，外部用户和内部组件都必须和它通信，而它采用了 HTTP 协议的 URL 资源理念，API 风格也用 RESTful 的 GET/POST/DELETE 等等，所以，这些概念很自然地就被称为是“API 对象”了。\n\n  \n\n可以使用 kubectl api-resources 来查看当前 Kubernetes 版本支持的所有对象\n\n```Go\n\nkubectl api-resources\n```\n\n![](statistic/asynccode-615.png)\n\n在使用 kubectl 命令的时候，你还可以加上一个参数 --v=9，它会显示出详细的命令执行过程，清楚地看到发出的 HTTP 请求\n\n```Go\n\nkubectl get pod --v=9\n```\n\n![](statistic/asynccode-617.png)\n\nkubectl 客户端等价于调用了 curl，向 8443 端口发送了 HTTP GET 请求，URL 是 /api/v1/namespaces/default/pods。\n\n  \n\n# 如何描述API 对象\n\n我们来把它改写成“声明式”的 YAML，说清楚我们想要的 Nginx 应用是个什么样子，也就是“目标状态”，让 Kubernetes 自己去决定如何拉取镜像运行\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ngx-pod\n  labels:\n    env: demo\n    owner: chrono\n\nspec:\n  containers:\n  - image: nginx:alpine\n    name: ngx\n    ports:\n    - containerPort: 80\n```\n\n“header”包含的是 API 对象的基本信息，有三个字段：apiVersion、kind、metadata。\n\n- apiVersion 表示操作这种资源的 API 版本号，由于 Kubernetes 的迭代速度很快，不同的版本创建的对象会有差异，为了区分这些版本就需要使用 apiVersion 这个字段，比如 v1、v1alpha1、v1beta1 等等。\n    \n- kind 表示资源对象的类型，这个应该很好理解，比如 Pod、Node、Job、Service 等等。\n    \n- metadata 这个字段顾名思义，表示的是资源的一些“元信息”，也就是**用来标记对象**，方便 Kubernetes 管理的一些信息。\n    \n\n而“body”部分则会与对象特定相关，每种对象会有不同的规格定义，在 YAML 里就表现为 **spec 字段**（即 specification），**表示我们对对象的“期望状态”（desired status）**。\n\n```Go\n\nspec:\n  containers:\n  - image: nginx:alpine\n    name: ngx\n    ports:\n    - containerPort: 80\n```\n\n  \n\n  \n\n# 如何编写YAML\n\n官方文档https://kubernetes.io/docs/reference/kubernetes-api/\n\n  \n\n  \n\n- kubectl api-resources 命令，它会显示出资源对象相应的 API 版本和类型，比如 Pod 的版本是“v1”，Ingress 的版本是“networking.k8s.io/v1”，照着它写绝对不会错\n    \n- kubectl explain，它相当于是 Kubernetes 自带的 API 文档，会给出对象字段的详细说明，这样我们就不必去网上查找了。比如想要看 Pod 里的字段该怎么写，就可以这样：\n    \n\n```Go\n\nkubectl explain pod\nkubectl explain pod.metadata\nkubectl explain pod.spec\nkubectl explain pod.spec.containers\n```\n\n![](statistic/asynccode-616.png)\n\n- 第三个技巧就是 kubectl 的两个特殊参数 --dry-run=client 和 -o yaml，前者是空运行，后者是生成 YAML 格式，结合起来使用就会让 kubectl 不会有实际的创建动作，而只生成 YAML 文件。\n    \n\n```Go\n\nkubectl run ngx --image=nginx:alpine --dry-run=client -o yaml\n```\n\n生成一个绝对正确的 YAML 文件\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: ngx\n  name: ngx\nspec:\n  containers:\n  - image: nginx:alpine\n    name: ngx\n    resources: {}\n  dnsPolicy: ClusterFirst\n  restartPolicy: Always\nstatus: {}\n```","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4":{"title":"命名空间","content":"# 为啥要有命名空间\n\nKubernetes 的名字空间并不是一个实体对象，**只是一个逻辑上的概念。**\n\n它可以把集群切分成一个个彼此独立的区域，然后我们把对象放到这些区域里，就实现了类似容器技术里 namespace 的隔离效果，**应用只能在自己的名字空间里分配资源和运行，不会干扰到其他名字空间里的应用。**\n\n  \n\nKubernetes面对大规模集群、海量节点时的一种现实考虑。）\n\n- 可能会有大量的 Pod，这就使得资源争抢和命名冲突的概率大大增加了。\n    \n- 多团队、多项目共用 Kubernetes 的时候，我们就需要把集群给适当地“局部化”，为每一类用户创建出只属于它自己的“工作空间”\n    \n- 区分测试，生产环境\n    \n\n  \n\n  \n\n# 如何使用名字空间\n\n创建\n\n```Go\n\nkubectl create ns test-ns \nkubectl get ns\n```\n\n想要把一个对象放入特定的名字空间，需要在它的 metadata 里添加一个 namespace 字段\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ngx\n  namespace: test-ns\n\nspec:\n  containers:\n  - image: nginx:alpine\n    name: ngx\n```\n\nkubectl apply 创建这个对象之后，我们直接用 kubectl get 是看不到它的，因为默认查看的是“default”名字空间，想要操作其他名字空间的对象必须要用 -n 参数明确指定：\n\n```Go\n\nkubectl get pod -n test-ns\n```\n\n# 什么是资源配额\n\n名字空间的资源配额需要使用一个专门的 API 对象，叫做 **ResourceQuota**，简称是 quota，我们可以使用命令 kubectl create 创建一个它的样板文件\n\n```Go\n名字空间的资源配额需要使用一个专门的 API 对象，叫做 ResourceQuota，简称是 quota，我们可以使用命令 kubectl create 创建一个它的样板文件\n```\n\n下面我们先创建一个名字空间“dev-ns”，再创建一个资源配额对象“dev-qt”：\n\n```Go\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev-ns\n\n---\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-qt\n  namespace: dev-ns\n\nspec:\n  ... ...\n```\n\n在 ResourceQuota 里可以设置各类资源配额，字段非常多，我简单地归了一下类，你可以课后再去官方文档上查找详细信息\n\n```Go\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-qt\n  namespace: dev-ns\n\nspec:\n  hard:\n    requests.cpu: 10\n    requests.memory: 10Gi\n    limits.cpu: 10\n    limits.memory: 20Gi\n\n    requests.storage: 100Gi\n    persistentvolumeclaims: 100\n\n    pods: 100\n    configmaps: 100\n    secrets: 100\n    services: 10\n\n    count/jobs.batch: 1\n    count/cronjobs.batch: 1\n    count/deployments.apps: 1\n```\n\n# 默认资源配额\n\n在名字空间加上了资源配额限制之后，它会有一个合理但比较“烦人”的约束：**要求所有在里面运行的 Pod 都必须用字段 resources 声明资源需求，否则就无法创建**。\n\n```Go\n\nkubectl run ngx --image=nginx:alpine -n dev-ns\n```\n\n![](statistic/asynccode-626.png)\n\n  \n\n这个时候就要用到一个很小但很有用的辅助对象了—— LimitRange，简称是 limits，它能为 API 对象添加默认的资源配额限制。\n\n可以用命令 kubectl explain limits 来查看它的 YAML 字段详细说明，这里说几个要点：\n\n- spec.limits 是它的核心属性，描述了默认的资源限制\n    \n- type 是要限制的对象类型，可以是 Container、Pod、PersistentVolumeClaim。\n    \n- default 是默认的资源上限，对应容器里的 resources.limits，只适用于 Container。\n    \n- defaultRequest 默认申请的资源，对应容器里的 resources.requests，同样也只适用于 Container。\n    \n- max、min 是对象能使用的资源的最大最小值。\n    \n\n```Go\n\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: dev-limits\n  namespace: dev-ns\n\nspec:\n  limits:\n  - type: Container\n    defaultRequest:\n      cpu: 200m\n      memory: 50Mi\n    default:\n      cpu: 500m\n      memory: 100Mi\n  - type: Pod\n    max:\n      cpu: 800m\n      memory: 200Mi\n```","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E5%AD%98%E5%82%A8-PersistentVolume":{"title":"存储-PersistentVolume","content":"一起来了解 Volume 的高级用法，看看 Kubernetes 管理存储资源的 API 对象 P**ersistentVolume、PersistentVolumeClaim、StorageClass**，然后使用本地磁盘来创建实际可用的存储卷。\n\n  \n\n# 什么是 PersistentVolume\n\nPersistentVolume专门用来**表示持久存储设备**，但隐藏了存储的底层实现，我们只需要知道**它能安全可靠地保管数据就可以了**（由于 PersistentVolume 这个词很长，一般都把它简称为 PV）。\n\n  \n\n**作为存储的抽象，PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS**，甚至是本地磁盘，管理它们已经超出了 Kubernetes 的能力范围，所以，一般会由系统管理员单独维护，然后再在 Kubernetes 里创建对应的 PV\n\n  \n\n**PV 属于集群的系统资源，是和 Node 平级的一种对象，Pod 对它没有管理权，只有使用权。**\n\n  \n\n# **什么是 PersistentVolumeClaim/StorageClass**\n\n让 Pod 直接去选择 PV 也很不灵活。于是 Kubernetes 就又增加了两个新对象，**PersistentVolumeClaim 和 StorageClass，用的还是“中间层”的思想，把存储卷的分配管理过程再次细化。**\n\n  \n\n- **PersistentVolumeClaim，简称 PVC，从名字上看比较好理解，就是用来向 Kubernetes 申请存储资源的。**PVC 是给 Pod 使用的对象**，它相当于是 Pod 的代理，代表 Pod 向系统申请 PV，**一旦资源申请成功，Kubernetes 就会把 PV 和 PVC 关联在一起，这个动作叫做**“绑定”（bind**）。\n    \n- 但是，系统里的存储资源非常多，如果要 PVC 去直接遍历查找合适的 PV 也很麻烦，所以就要用到 **StorageClass。，它抽象了特定类型的存储系统（比如 Ceph、NFS），**在 PVC 和 PV 之间充当“协调人”的角色，帮助 PVC 找到合适的 PV。**也就是说它可以简化 Pod 挂载“虚拟盘”的过程，让 Pod 看不到 PV 的实现细节**\n    \n\n![](statistic/asynccode-586.png)\n\n  \n\n# 如何使用 YAML 描述 PersistentVolume\n\nKubernetes 里有很多种类型的 PV，我们先看看最容易的**本机存储“HostPath”**，它和 Docker 里挂载本地目录的 -v 参数非常类似，可以用它来初步认识一下 PV 的用法\n\n  \n\n  \n\nPod 会在集群的任意节点上运行，所以首先，我们要作为系统管理员在每个节点上创建一个目录，它将会作为本地存储卷挂载到 Pod 里\n\n  \n\n为了省事，我就在 /tmp 里建立名字是 host-10m-pv 的目录，表示一个只有 10MB 容量的存储设备。\n\n只能用 kubectl api-resources、kubectl explain 查看 PV 的字段说明，手动编写 PV 的 YAML 描述文件。\n\n  \n\n```Go\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: host-10m-pv\n\nspec:\n  storageClassName: host-test\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: 10Mi\n  hostPath:\n    path: /tmp/host-10m-pv/\n```\n\n- storageClassName，对存储类型的抽象 StorageClass。对存储类型的抽象 StorageClass。这个 PV 是我们手动管理的，名字可以任意起\n    \n- “accessModes”定义了存储设备的访问模式，简单来说**就是虚拟盘的读写权限**，和 Linux 的文件访问模式差不多，目前 Kubernetes 里有 3 种\n    \n    - ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。\n        \n    - ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载\n        \n    - ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载\n        \n    - 这 3 种访问模式限制的对象是节点而不是 Pod，因为存储是系统级别的概念，不属于 Pod 里的进程。\n        \n- capacity：表示存储设备的容量，\n    \n- hostPath：存储卷的本地路径，也就是我们在节点上创建的目录\n    \n\n  \n\n# 如何使用YAML 描述 PersistentVolumeClaim\n\n  \n\n```Go\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: host-5m-pvc\n\nspec:\n  storageClassName: host-test\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Mi\n```\n\nPVC 的内容与 PV 很像，但它不表示实际的存储，而是一个“申请”或者“声明”，spec 里的字段描述的是对存储的“期望状态”。\n\n  \n\n**PVC 里的 storageClassName、accessModes 和 PV 是一样的，但不会有字段 capacity，而是要用 resources.request 表示希望要有多大的容量**。\n\n  \n\n# 如何在kubernetes里面使用PersistentVolume\n\n首先需要用 kubectl apply 创建 PV 对象：\n\n```Go\n\nkubectl apply -f host-path-pv.yml\n```\n\n首先需要用 kubectl apply 创建 PV 对象：\n\n```Go\n\nkubectl get pv\n```\n\n![](statistic/asynccode-587.png)\n\n  \n\n从截图里我们可以看到，这个 PV 的容量是 10MB，访问模式是 RWO（ReadWriteOnce），StorageClass 是我们自己定义的 host-test，状态显示的是 Available，也就是处于可用状态，可以随时分配给 Pod 使用。\n\n  \n\n接下来我们创建 PVC，申请存储资源：\n\n```Go\n\nkubectl apply -f host-path-pvc.yml\nkubectl get pvc\n```\n\n一旦 PVC 对象创建成功，**Kubernetes 就会立即通过 StorageClass、resources 等条件在集群里查找符合要求的 PV**，如果找到合适的存储对象就会把它俩“绑定”在一起。\n\n  \n\n# 如何为Pod 挂载PersistentVolume\n\nPV 和 PVC 绑定好了，有了持久化存储，现在我们就可以为 Pod 挂载存储卷。**spec.volumes 定义存储卷，然后在 containers.volumeMounts 挂载进容器。**\n\n不过因为我们用的是 PVC，所以要在 volumes 里用字段 **persistentVolumeClaim 指定 PVC 的名字。**\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: host-pvc-pod\n\nspec:\n  volumes:\n  - name: host-pvc-vol\n    persistentVolumeClaim:\n      claimName: host-5m-pvc\n\n  containers:\n    - name: ngx-pvc-pod\n      image: nginx:alpine\n      ports:\n      - containerPort: 80\n      volumeMounts:\n      - name: host-pvc-vol\n        mountPath: /tmp\n```\n\n  \n\n![](statistic/asynccode-588.png)\n\n因为 Pod 产生的数据已经通过 PV 存在了磁盘上，**所以如果 Pod 删除后再重新创建，挂载存储卷时会依然使用这个目录，数据保持不变，也就实现了持久化存储**。\n\n  \n\nHostPath 类型的 PV 一般用来做测试，或者是用于 DaemonSet 这样与节点关系比较密切的应用，我们下节课再讲**实现真正任意的数据持久化**\n\n\n\n# NFS\n\n要想让存储卷真正能被 Pod 任意挂载，我们需要变更存储的方式，**不能限定在本地磁盘，而是要改成网络存储**\n\n有很多知名的产品，比如 AWS、Azure、Ceph，Kubernetes 还专门定义了 CSI（Container Storage Interface）规范，不过这些存储类型的安装、使用都比较复杂，在我们的实验环境里部署难度比较高\n\n**相对来说比较简单的 NFS 系统（Network File System）**\n\n  \n\n# 如何安装 NFS 服务器\n\nhttps://time.geekbang.org/column/article/542458","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E4%B8%8EKubernetes":{"title":"容器编排与Kubernetes","content":"容器技术的核心概念是容器、镜像、仓库，使用这三大基本要素我们就可以轻松地完成应用的打包、分发工作，实现“一次开发，到处运行”的梦想。\n\n现实生产环境的复杂程度实在是太高了，除了最基本的安装，还会有各式各样的需求，\n\n- 服务发现\n    \n- 负载均衡\n    \n- 状态监控\n    \n- 健康检查\n    \n- 扩容缩容\n    \n- 应用迁移\n    \n- 高可用\n    \n- 等等\n    \n\n这些容器之上的**管理、调度工作**，就是这些年最流行的词汇：**“容器编排”**（Container Orchestration）。\n\n# 什么是 Kubernetes\n\nhttps://kubernetes.io/zh/\n\nGoogle 拥有数量庞大的服务器集群，为了提高资源利用率和部署运维效率，它专门开发了一个集群应用管理系统，代号 **Borg**\n\n**Kubernetes 就是一个生产级别的容器编排平台和集群管理系统，不仅能够创建、调度容器，还能够监控、管理服务器**\n\n  \n\n**Kubernetes 可以说是一个集群级别的操作系统，主要功能就是资源管理和作业调度。**\n\n  \n\n# Kubernetes 的基本架构\n\n![](statistic/asynccode-563.png)\n\nKubernetes 采用了现今流行的“控制面 / 数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为**“节点”（Node）**\n\n- 控制面的节点在 Kubernetes 里叫做 **Master Node，一般简称为 Master**，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。\n    \n- 数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。\n    \n\n以使用命令 kubectl get node 来查看 Kubernetes 的节点状态：\n\n![](statistic/asynccode-560.png)\n\n## 节点内部的结构\n\nKubernetes 的节点内部也具有复杂的结构，是由很多的模块构成的，这些模块又可以分成**组件（Component）和插件（Addon）两类**。\n\n- 组件实现了 Kubernetes 的核心功能特性，没有这些组件 Kubernetes 就无法启动，而\n    \n- 插件则是 Kubernetes 的一些附加功能，属于“锦上添花”，不安装也不会影响 Kubernetes 的正常运行。\n    \n\n  \n\n## Master 里的组件有哪些\n\nMaster 里有 4 个组件，分别是 apiserver、etcd、scheduler、controller-manager。\n\n![](statistic/asynccode-562.png)\n\n- apiserver 是 Master 节点——同时也是**整个 Kubernetes 系统的唯一入口**，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员。\n    \n- etcd 是一个**高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。**注意它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。\n    \n- scheduler 负责**容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员**。因为节点状态和 Pod 信息都存储在 etcd 里，所以 scheduler 必须通过 apiserver 才能获得。\n    \n- controller-manager **负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员**。同样地，它也必须通过 apiserver 获得存储在 etcd 里的信息，才能够实现对资源的各种操作。\n    \n\n## Node 里的组件有哪些\n\nNode 里的 3 个组件了，分别是 kubelet、kube-proxy、container-runtime。\n\n![](statistic/asynccode-561.png)\n\n- kubelet 是 Node 的代理，**负责管理 Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信**，实现状态报告、命令下发、启停容器等功能，相当于是 Node 上的一个“小管家”。\n    \n- kube-proxy 的作用有点特别，它是 **Node 的网络代理**，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包，相当于是专职的“小邮差”。\n    \n- 第三个组件 container-runtime 我们就比较熟悉了，它是**容器和镜像的实际使用者**，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期，是真正干活的“苦力”。\n    \n\n这 3 个组件中只有 kube-proxy 被容器化了，而 kubelet 因为必须要管理整个节点，容器化会限制它的能力，所以它必须在 container-runtime 之外运行。\n\n# Kubernetes 的大致工作流程\n\n- 每个 Node 上的 kubelet 会定期向 apiserver 上报节点状态，apiserver 再存到 etcd 里。\n    \n- 每个 Node 上的 kube-proxy 实现了 TCP/UDP 反向代理，让容器对外提供稳定的服务。\n    \n- scheduler 通过 apiserver 得到当前的节点状态，调度 Pod，然后 apiserver 下发命令给某个 Node 的 kubelet，kubelet 调用 container-runtime 启动容器。\n    \n- controller-manager 也通过 apiserver 得到实时的节点状态，监控可能的异常情况，再使用相应的手段去调节恢复。\n    \n\n# 插件（Addons）有哪些\n\n个人认为比较重要的有两个：DNS 和 Dashboard。\n\nDNS 你应该比较熟悉吧，**它在 Kubernetes 集群里实现了域名解析服务**，能够让我们以域名而不是 IP 地址的方式来互相通信，是服务发现和负载均衡的基础。\n\nDashboard 就是仪表盘，为 Kubernetes 提供了一个图形化的操作界面，非常直观友好，虽然大多数 Kubernetes 工作都是使用命令行 kubectl，","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E5%AE%B9%E5%99%A8-%E5%AE%B9%E5%99%A8%E7%9A%84%E6%9C%AC%E8%B4%A8%E9%9A%94%E7%A6%BB%E7%9A%84%E8%BF%9B%E7%A8%8B":{"title":"容器-容器的本质，隔离的进程","content":"\u003e - 容器就是操作系统里一个特殊的“沙盒”环境，里面运行的进程只能看到受限的信息，与外部系统实现了隔离。\n\u003e     \n\u003e - 容器隔离的目的是为了系统安全，限制了进程能够访问的各种资源。\n\u003e     \n\u003e - 相比虚拟机技术，容器更加轻巧、更加高效，消耗的系统资源非常少，在云计算时代极具优势。\n\u003e     \n\u003e - 容器的基本实现技术是 Linux 系统里的 namespace、cgroup、chroot。\n\u003e     \n\n广义上来说，容器技术是动态的容器、静态的镜像和远端的仓库这三者的组合。\n\n# 容器是什么\n\n容器，就是一个特殊的隔离环境，它能够让进程只看到这个环境里的有限信息，不能对外界环境施加影响。\n\n## 为什么要隔离\n\n一个个不受任何限制的应用程序。是这个进程能够看到系统里所有的文件、所有的进程、所有的网络流量，访问内存里的任何数据，那么恶意程序很容易就会把系统搞瘫痪，正常程序也可能会因为无意的 Bug 导致信息泄漏或者其他安全事故。\n\n**使用容器技术，我们就可以让应用程序运行在一个有严密防护的“沙盒”（Sandbox）环境之内**\n\n容器技术的另一个本领就是为应用程序**加上资源隔离，在系统里切分出一部分资源**，让它只能使用指定的配额，\n\n# 与虚拟机的区别\n\n- 容器并不直接运行在 Docker 上，Docker 只是辅助建立隔离环境，让容器基于 Linux 操作系统运行\n    \n\n![](statistic/asynccode-559.png)\n\n- 从实现的角度来看，虚拟机虚拟化出来的是硬件，需要在上面再安装一个操作系统后才能够运行应用程序\n    \n- 容器（即图中的 Docker），它直接利用了下层的计算机硬件和操作系统，因为比虚拟机少了一层，\n    \n\n![](statistic/asynccode-558.png)\n\n# **隔离是如何实现的**\n\nnamespace、cgroup、chroot\n\n- namespace：它可以创建出独立的文件系统、主机名、进程号、网络等资源空间，相当于给进程盖了一间小板房，这样就实现了系统全局资源和进程局部资源的隔离。**Namespace则是用来修改进程视图的主要方法。**\n    \n- cgroup：它的全称是 Linux Control Group，用来实现对进程的 CPU、内存等资源的优先级和配额限制，相当于给进程的小板房加了一个天花板。**Cgroups 技术是用来制造约束的主要手段**\n    \n- chroot ：它可以更改进程的根目录，也就是限制访问文件系统，相当于给进程的小板房铺上了地砖。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E5%AE%B9%E5%99%A8-docker":{"title":"容器-docker","content":"\n\n# Docker 相关内容\n\n\n[[如何容器化应用]]\n\n[[dockerfile]]\n\n[[与外界资源的共享]]\n\n\n\n# 什么是 CRI\n\nNCF 已经成立一年了，而 Kubernetes 也已经发布了 1.0 版，可以正式用于生产环境，这些都标志着 Kubernetes 已经成长起来了，不再需要“看脸色吃饭”。于是它就宣布加入了 CNCF，成为了第一个 CNCF 托管项目，想要借助基金会的力量联合其他厂商，一起来“扳倒”Docker。\n\nKubernetes **引入了一个新的接口标准：CRI** ，Container Runtime Interface。\n\n- CRI 采用了 ProtoBuffer 和 gPRC，规定 kubelet 该如何调用容器运行时去管理容器和镜像，但这是一套全新的接口，和之前的 Docker 调用完全不兼容\n    \n\nkubernetes 也只能同时提供一个“折中”方案，在 **kubelet 和 Docker 中间加入一个“适配器”**，把 Docker 的接口转换成符合 CRI 标准的接口\n\n![](statistic/asynccode-571.png)\n\n# 什么是 containerd\n\nocker 也没有“坐以待毙”，而是采取了“断臂求生”的策略，推动自身的重构，把原本单体架构的 Docker Engine 拆分成了多个模块，其中的 **Docker daemon 部分就捐献给了 CNCF，形成了 containerd**\n\n  \n\ncontainerd 作为 CNCF 的托管项目，自然是要符合 CRI 标准的。但 **Docker 出于自己诸多原因的考虑，它只是在 Docker Engine 里调用了 containerd，外部的接口仍然保持不变**，也就是说还不与 CRI 兼容。\n\n  \n\n由于 Docker 的“固执己见”，这时 Kubernetes 里就出现了两种调用链：\n\n- 第一种是用 CRI 接口调用 dockershim，然后 dockershim 调用 Docker，Docker 再走 containerd 去操作容器。\n    \n- 第二种是用 CRI 接口直接调用 containerd 去操作容器。\n\n# Kubernetes“弃用Docker”是怎么回事\n\n\n如果你理解了前面讲的 **CRI 和 containerd 这两个项目**，就会知道 Kubernetes 的这个举动也没有什么值得大惊小怪的，一切都是“水到渠成”的：它实际上只是“弃用了 dockershim”这个小组件，**也就是说把 dockershim 移出了 kubelet，并不是“弃用了 Docker”这个软件产品**\n\n![](statistic/asynccode-572.png)\n\n\n\n# Docker 的未来\n\n**容器器镜像格式已经被标准化了**（OCI 规范，Open Container Initiative），Docker 镜像仍然可以在 Kubernetes 里正常使用，原来的开发测试、CI/CD 流程都不需要改动，我们仍然可以拉取 Docker Hub 上的镜像，或者编写 Dockerfile 来打包应用。\n\nDocker 是一个完整的软件产品线，不止是 containerd，它还包括了镜像构建、分发、测试等许多服务，甚至在 Docker Desktop 里还内置了 Kubernetes。\n\n虽然 Kubernetes 已经不再包含 dockershim，但 Docker 公司却把这部分代码接管了过来，另建了一个叫 cri-dockerd（https://github.com/mirantis/cri-dockerd）的项目","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E6%BB%9A%E5%8A%A8%E6%9B%B4%E6%96%B0":{"title":"滚动更新","content":"在 Kubernetes 里，使用命令 **kubectl scale**，我们就可以轻松调整 Deployment,StatefulSet 下属的 Pod 数量。\n\n除了“应用伸缩”，其他的运维操作比如应用更新、版本回退等工作\n\n滚动更新，使用 **kubectl rollout 实现用户无感知的应用升级和降级**\n\n  \n\n# Kubernetes 如何定义应用版本\n\n  \n\n在 Kubernetes 里，版本更新使用的不是 API 对象，而是两个命令：**kubectl apply 和 kubectl rollout**，当然它们也要搭配部署应用所需要的 **Deployment、DaemonSet 等 YAML 文件**。\n\n  \n\n- 在 Kubernetes 里应用都是以 Pod 的形式运行的，而 Pod 通常又会被 Deployment 等对象来管理，**所以应用的“版本更新”实际上更新的是整个 Pod。**\n    \n\n  \n\n在 **Kubernetes 里应用的版本变化**就是 **template 里 Pod 的变化**，哪怕 template 里只变动了一个字段，那也会形成一个新的版本，也算是版本变化\n\n  \n\n![](statistic/asynccode-627.png)\n\n\"6796……”就是 Pod 模板的 Hash 值，也就是 Pod 的“版本号”\n\n  \n\n  \n\nkubectl rollout status，来查看应用更新的状态\n\n```Go\nkubectl rollout status，来查看应用更新的状态：\n```\n\n![](statistic/asynccode-631.png)\n\nkubectl rollout status 的输出信息，你可以发现，Kubernetes 不是把旧 Pod 全部销毁再一次性创建出新 Pod，**而是在逐个地创建新 Pod，同时也在销毁旧 Pod**，保证系统里始终有足够数量的 Pod 在运行，不会有“空窗期”中断服务。\n\n版本更新的过程\n\n![](statistic/asynccode-629.png)\n\n# Kubernetes 如何管理应用更新\n\n查看更新历史使用的命令是 kubectl rollout history\n\n```Go\n\nkubectl rollout history deploy ngx-dep\n```\n\n![](statistic/asynccode-628.png)\n\n但 **kubectl rollout history 的列表输出的有用信息太少，你可以在命令后加上参数 --revision** 来查看每个版本的详细信息，包括标签、镜像名、环境变量、存储卷等等\n\n```Go\n\nkubectl rollout history deploy --revision=2\n```\n\n![](statistic/asynccode-630.png)\n\n想要回退到上一个版本，就可以使用命令 kubectl rollout undo，也可以加上参数 --to-revision 回退到任意一个历史版本\n\n```Go\n\nkubectl rollout undo deploy ngx-dep\n```\n\n![](statistic/asynccode-632.png)\n\n这个 V2 到 V1 的“版本降级”的过程我同样画了一张图，它和从 V1 到 V2 的“版本升级”过程是完全一样的，不同的只是版本号的变化方向：\n\n![](statistic/asynccode-633.png)\n\n# Kubernetes 如何添加更新描述\n\n只需要在 Deployment 的 metadata 里加上一个新的字段 annotations。\n\n- annotations 字段的含义是“注解”“注释”，形式上和 labels 一样，都是 Key-Value，也都是给 API 对象附加一些额外的信息，但是用途上区别很大。\n    \n    - annotations 添加的信息一般是给 Kubernetes 内部的各种对象使用的，有点像是“扩展属性”；\n        \n    - labels 主要面对的是 Kubernetes 外部的用户，用来筛选、过滤对象的\n        \n\nannotations 就是包装盒里的产品说明书，而 labels 是包装盒外的标签贴纸。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7Metrics-Server%E5%92%8CPrometheus":{"title":"系统监控：Metrics Server和Prometheus","content":"  \n\n观测集群是不能用“探针”这种简单的方式的，所以今天我就带你一起来看看 Kubernetes 为集群提供的两种系统级别的监控项目：**Metrics Server 和 Prometheus，**以及基于它们的水平自动伸缩对象 **HorizontalPodAutoscaler。**\n\n  \n\n# Metrics Server\n\nhttps://github.com/kubernetes-sigs/metrics-server\n\n**top 能够实时显示当前系统的 CPU 和内存利用率**，它是性能分析和调优的基本工具，非常有用。Kubernetes 也提供了类似的命令，就是 **kubectl top**，不过默认情况下这个命令不会生效，必须要安装一个插件 **Metrics Server 才可以**\n\n  \n\n  \n\nM**etrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具**，它定时从所有节点的 kubelet 里采集信息，但是对集群的整体性能影响极小，每个节点只大约会占用 1m 的 CPU 和 2MB 的内存，所以性价比非常高\n\n  \n\n它调用 **kubelet 的 API 拿到节点和 Pod 的指标，再把这些信息交给 apiserver，这样 kubectl、HPA 就可以利用 apiserver 来读取指标了**\n\n![](statistic/asynccode-623.png)\n\nMetrics Server 的所有依赖都放在了一个 YAML 描述文件里\n\n```Go\nMetrics Server 的所有依赖都放在了一个 YAML 描述文件里\n```\n\nkubectl apply 创建对象之前，我们还有两个准备工作要做。\n\n- 第一个工作，是修改 YAML 文件。你需要在 Metrics Server 的 Deployment 对象里，加上一个额外的运行参数 --kubelet-insecure-tls，也就是这样,\n    \n\n```Go\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metrics-server\n  namespace: kube-system\nspec:\n  ... ... \n  template:\n    spec:\n      containers:\n      - args:\n        - --kubelet-insecure-tls\n        ... ... \n        \n        \n是因为 Metrics Server 默认使用 TLS 协议，要验证证书才能与 kubelet 实现安全通信，而我们的实验环境里没有这个必要，加上这个参数可以让我们的部署工作简单很多（生产环境里就要慎用\n```\n\n- 第二个工作，是预先下载 Metrics Server 的镜像。看这个 YAML 文件，你会发现 Metrics Server 的镜像仓库用的是 gcr.io，下载很困难。好在它也有国内的镜像网站.\n    \n\n```Go\n出一段 Shell 脚本代码，\n\nrepo=registry.aliyuncs.com/google_containers\n\nname=k8s.gcr.io/metrics-server/metrics-server:v0.6.1\nsrc_name=metrics-server:v0.6.1\n\ndocker pull $repo/$src_name\n\ndocker tag $repo/$src_name $name\ndocker rmi $repo/$src_name\n```\n\nYAML 部署 Metrics Server 了\n\n```Go\nYAML 部署 Metrics Server 了\n```\n\n在有了 Metrics Server 插件，我们就可以使用命令 kubectl top 来查看 Kubernetes 集群当前的资源状态了。它有两个子命令，node 查看节点的资源使用率，pod 查看 Pod 的资源使用率。\n\n```Go\n\nkubectl top node\nkubectl top pod -n kube-system\n```\n\n![](statistic/asynccode-624.png)\n\n# HorizontalPodAutoscaler\n\n有了 Metrics Server，我们就可以轻松地查看集群的资源使用状况了，不过它另**外一个更重要的功能是辅助实现应用的“水平自动伸缩”。**\n\nKubernetes 为此就定义了一个新的 API 对象，叫做“HorizontalPodAutoscaler”，简称是“hpa”。顾名思义，它是专门用来自动伸缩 Pod 数量的对象，**适用于 Deployment 和 StatefulSet**\n\n  \n\n  \n\n下面我们就来看看该怎么使用 HorizontalPodAutoscaler，首先要定义 Deployment 和 Service，创建一个 Nginx 应用，作为自动伸缩的目标对象：\n\n```Go\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ngx-hpa-dep\n\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ngx-hpa-dep\n\n  template:\n    metadata:\n      labels:\n        app: ngx-hpa-dep\n    spec:\n      containers:\n      - image: nginx:alpine\n        name: nginx\n        ports:\n        - containerPort: 80\n\n        resources:\n          requests:\n            cpu: 50m\n            memory: 10Mi\n          limits:\n            cpu: 100m\n            memory: 20Mi\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: ngx-hpa-svc\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: ngx-hpa-dep\n```\n\n注意在它的 s**pec 里一定要用 resources 字段写清楚资源配额，否则 HorizontalPodAutoscaler 会无法获取 Pod 的指标，也就无法实现自动化扩缩容。**\n\n接下来我们要用命令 kubectl autoscale 创建一个 HorizontalPodAutoscaler 的样板 YAML 文件，它有三个参数：*\n\n- min，Pod 数量的最小值，也就是缩容的下限。\n    \n- max，Pod 数量的最大值，也就是扩容的上限。\n    \n- cpu-percent，CPU 使用率指标，当大于这个值时扩容，小于这个值时缩容。\n    \n\n  \n\n```Go\n \nexport out=\"--dry-run=client -o yaml\"              # 定义Shell变量\nkubectl autoscale deploy ngx-hpa-dep --min=2 --max=10 --cpu-percent=5 $out\n```\n\n```Go\n\n\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ngx-hpa\n\nspec:\n  maxReplicas: 10\n  minReplicas: 2\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ngx-hpa-dep\n  targetCPUUtilizationPercentage: 5\n```\n\n  \n\n# Prometheus\n\nhttps://github.com/prometheus-operator/kube-prometheus/）\n\n有了 Metrics Server 和 HorizontalPodAutoscaler 的帮助，我们的应用管理工作又轻松了一些。不过，Metrics Server 能够获取的指标还是太少了，**只有 CPU 和内存**，想要监控到更多更全面的应用运行状况，还得请出这方面的权威项目**“Prometheus”**。\n\n![](statistic/asynccode-619.png)\n\nPrometheus 系统的核心是它的 Server，**里面有一个时序数据库 TSDB**，**用来存储监控数据，另一个组件 Retrieval 使用拉取（Pull）的方式从各个目标收集数据**，再通过 HTTP Server 把这些数据交给外界使用。\n\nPrometheus Server 之外还有三个重要的组件：\n\n- Push Gateway，用来适配一些特殊的监控目标，把默认的 Pull 模式转变为 Push 模式。\n    \n- Alert Manager，告警中心，预先设定规则，发现问题时就通过邮件等方式告警。\n    \n- Grafana 是图形化界面，可以定制大量直观的监控仪表盘。\n    \n\n  \n\n**第一步，是修改 prometheus-service.yaml、grafana-service.yaml**。这两个文件定义了 Prometheus 和 Grafana 服务对象，我们可以给它们添加 type: NodePort（参考第 20 讲），这样就可以直接通过节点的 IP 地址访问（当然你也可以配置成 Ingress）。\n\n  \n\n**第二步，是修改 kubeStateMetrics-deployment.yaml、prometheusAdapter-deployment.yaml，**因为它们里面有两个存放在 gcr.io 的镜像，必须解决下载镜像的问题。\n\n我们要执行两个 kubectl create 命令来部署 Prometheus，先是 manifests/setup 目录，创建名字空间等基本对象，然后才是 manifests 目录：\n\n```Go\n\nkubectl create -f manifests/setup\nkubectl create -f manifests\n```\n\nPrometheus 的对象都在名字空间“monitoring”里，创建之后可以用 kubectl get 来查看状态：\n\n![](statistic/asynccode-620.png)\n\n![](statistic/asynccode-621.png)\n\n![](statistic/asynccode-621.png)\n\nPrometheus 的 Web 界面比较简单，通常只用来调试、测试，不适合实际监控。\n\n  \n\n![](statistic/asynccode-622.png)","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E7%BD%91%E7%BB%9C-CNI":{"title":"网络-CNI","content":"讲讲 Kubernetes 的网络接口标准 CNI，以及 **Calico、Cilium** 等性能更好的网络插件。\n\n# Kubernetes 的网络模型\n\nDocker 的 null、host 和 bridge 三种网络模式还有印象吗？这里我重新画了一张图，描述了 Docker 里最常用的 **bridge 网络模式**：\n\n  \n\n![](statistic/asynccode-607.png)\n\nDocker 会创建一个名字叫“docker0”的网桥，默认是私有网段“172.17.0.0/16”。每个容器都会创建一个虚拟网卡对（veth pair），两个虚拟网卡分别“插”在容器和网桥上，这样容器之间就可以互联互通了\n\n  \n\n**Docker 的网络方案简单有效，但问题是它只局限在单机环境里工作，跨主机通信非常困难（需要做端口映射和网络地址转换）。**\n\nKubernetes 提出了一个自己的网络模型**“IP-per-pod”**，能够很好地适应集群系统的网络需求，它有下面的这 4 点基本假设：\n\n- 集群里的每个 Pod 都会有唯一的一个 IP 地址。\n    \n- Pod 里的所有容器共享这个 IP 地址。\n    \n- 集群里的所有 Pod 都属于同一个网段。\n    \n- Pod 直接可以基于 IP 地址直接访问另一个 Pod，不需要做麻烦的网络地址转换（NAT）。\n    \n\n![](statistic/asynccode-606.png)\n\n  \n\n这个网络让 Pod 摆脱了主机的硬限制，**是一个“平坦”的网络模型，**很好理解，通信自然也非常简单。因为 Pod 都具有独立的 IP 地址，相当于一台虚拟机，而且直连互通，也就**可以很容易地实施域名解析、负载均衡、服务发现等工作**\n\n# 什么是 CNI\n\nKubernetes 定义的这个网络模型很完美，但要把这个模型落地实现就不那么容易了。所以 Kubernetes 就专门制定了一个标准：**CNI（Container Networking Interface）**。\n\n**CNI 为网络插件定义了一系列通用接口，开发者只要遵循这个规范就可以接入 Kubernetes，为 Pod 创建虚拟网卡、分配 IP 地址、设置路由规则，最后就能够实现“IP-per-pod”网络模型。**\n\n  \n\nCNI 插件可以大致上分成“Overlay”“Route”和“Underlay”三种。\n\n- **Overlay** 的原意是“覆盖”，**是指它构建了一个工作在真实底层网络之上的“逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包**。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。\n    \n\n  \n\n- **Route** 也是在**底层网络之上工作**，但它没有封包和拆包，而是**使用系统内置的路由功能来实现 Pod 跨主机通信**。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了。\n    \n- **Underlay** 就是**直接用底层网络来实现 CNI**，也就是说 **Pod 和宿主机都在一个网络里，Pod 和宿主机是平等的**。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。\n    \n\n  \n\n  \n\n# 网络插件\n\n- Flannel（https://github.com/flannel-io/flannel/）：最早是一种 **Overlay 模式的网络插件**，使用 UDP 和 VXLAN 技术，后来又用 Host-Gateway 技术支持了 **Route 模式**\n    \n- Calico（https://github.com/projectcalico/calico）是一种 **Route 模式的网络插件**，使用 BGP 协议（Border Gateway Protocol）来维护路由信息，性能要比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能。\n    \n- Cilium（https://github.com/cilium/cilium）是一个比较新的网络插件，同时支持 **Overlay 模式和 Route 模式**，它的特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，所以性能很高，可以灵活实现各种功能。在 2021 年它加入了 CNCF，成为了孵化项目，是非常有前途的 CNI 插件。\n    \n\n  \n\n  \n\n# CNI 插件是怎么工作的\n\nFlannel 比较简单，我们先以它为例看看 CNI 在 Kubernetes 里的工作方式。\n\n```Go\n\nkubectl create deploy ngx-dep --image=nginx:alpine --replicas=3\n```\n\n使用命令 kubectl get pod 可以看到，有两个 Pod 运行在 master 节点上，IP 地址分别是“10.10.0.3”“10.10.0.4”，另一个 Pod 运行在 worker 节点上，IP 地址是“10.10.1.77”：\n\n![](statistic/asynccode-605.png)\n\nFlannel 默认使用的是基于 VXLAN 的 Overlay 模式，整个集群的网络结构我画了一张示意图，你可以对比一下 Docker 的网络结构\n\n![](statistic/asynccode-608.png)\n\nFlannel 的网络结构和 Docker 几乎是一模一样的，只不过网桥换成了“cni0”，而不是“docker0”。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E7%BD%91%E7%BB%9C-Ingress":{"title":"网络-Ingress","content":"集群进出流量的总管\n\n对网络流量的管理方案还是太简单，**离复杂的现代应用架构需求还有很大的差距**，所以 Kubernetes 就在 Service 之上又提出了一个新的概念：Ingress。\n\n  \n\n  \n\n# 为什么要有 Ingress\n\n**Service 的功能和运行机制，它本质上就是一个由 kube-proxy 控制的四层负载均衡，在 TCP/IP 协议栈上转发流量**\n\n  \n\n![](statistic/asynccode-577.png)\n\n但在四层上的负载均衡功能还是太有限了，**只能够依据 IP 地址和端口号做一些简单的判断和组合**，而我们现在的绝**大多数应用都是跑在七层的 HTTP/HTTPS 协议上的，有更多的高级路由条件，比如主机名、URI、请求头、证书等等，而这些在 TCP/IP 网络栈里是根本看不见的**。\n\n  \n\nService 还有一个缺点，**它比较适合代理集群内部的服务**。如果想要把**服务暴露到集群外部，就只能使用 NodePort 或者 LoadBalancer 这两种方式**，而它们都缺乏足够的灵活性，难以管控，这就导致了一种很无奈的局面：我们的服务空有一身本领，却没有合适的机会走出去大展拳脚。\n\n  \n\n那么我再引入一个新的 API 对象，**在七层上做负载均衡**是不是就可以了呢\n\n- 不过除了七层负载均衡，这个对象还应该承担更多的职责，也就是作为流量的总入口，统管集群的进出口数据\n    \n- “扇入”“扇出”流量（也就是我们常说的“南北向”），让外部用户能够安全、顺畅、便捷地访问内部服务（图片来源）：\n    \n\n![](statistic/asynccode-578.png)\n\n  \n\n  \n\n# 为啥要用Ingress Controller\n\n  \n\n- Ingress 可以说是在七层上另一种形式的 Service，它同样会代理一些后端的 Pod，也有一些路由规则来定义流量应该如何分配、转发，只不过这些规则都使用的是 HTTP/HTTPS 协议。\n    \n- **Service 本身是没有服务能力的**，它只是一些 iptables 规则，**真正配置、应用这些规则的实际上是节点里的 kube-proxy 组件**。\n    \n- **Ingress 也只是一些 HTTP 路由规则的集合**，相当于一份静态的描述文件，**真正要把这些规则在集群里实施运行，还需要有另外一个东西，这就是 Ingress Controller**，它的作用就相当于 Service 的 kube-proxy，能够读取、应用 Ingress 规则，处理、调度流量。\n    \n\n  \n\n不过 Ingress Controller 要做的事情太多，与上层业务联系太密切，所以 Kubernetes 把 Ingress Controller 的实现交给了社区\n\n  \n\n有名的Ingress-controller\n\n- 社区的kubernetes Ingress Controller(https://github.com/kubernetes/ingress-nginx )\n    \n- Nginx 公司自己的 Nginx Ingress Controller（https://github.com/nginxinc/kubernetes-ingress）、\n    \n- 还有基于 OpenResty 的 Kong Ingress Controller（https://github.com/Kong/kubernetes-ingress-controller）\n    \n\n  \n\n# 为啥要有IngressClass\n\n有了 Ingress 和 Ingress Controller，我们是不是就可以完美地管理集群的进出流量了呢？\n\n有些问题需要解决\n\n- 由于某些原因，项目组需要引入不同的 Ingress Controller，但 Kubernetes 不允许这样做；\n    \n- Ingress 规则太多，都交给一个 Ingress Controller 处理会让它不堪重负；\n    \n- 多个 Ingress 对象没有很好的逻辑分组方式，管理和维护成本很高；\n    \n- 集群里有不同的租户，他们对 Ingress 的需求差异很大甚至有冲突，无法部署在同一个 Ingress Controller 上。\n    \n\n  \n\nKubernetes 就又提出了一个 Ingress Class 的概念，让它插在 Ingress 和 Ingress Controller 中间，Kubernetes 用户可以转向管理 Ingress Class，用它来定义不同的业务逻辑分组，简化 Ingress 规则的复杂度。\n\n![](statistic/asynccode-576.png)\n\n我们可以用 Class A 处理博客流量、Class B 处理短视频流量、Class C 处理购物流量。\n\n  \n\n# 如何使用YAML描述Ingress/Ingress Class\n\n首先应当用命令 kubectl api-resources 查看它们的基本信息，输出列在这里了\n\n```Go\n\nkubectl api-resources\n\nNAME          SHORTNAMES   APIVERSION           NAMESPACED   KIND\ningresses       ing          networking.k8s.io/v1   true         Ingress\ningressclasses               networking.k8s.io/v1   false        IngressClass\n```\n\n## Ingress\n\nIngress 也是可以使用 kubectl create 来创建样板文件的，和 Service 类似，它也需要用两个附加参数：\n\n- --class，指定 Ingress 从属的 Ingress Class 对象。\n    \n- --rule，指定路由规则，基本形式是“URI=Service”，也就是说是访问 HTTP 路径就转发到对应的 Service 对象，再由 Service 对象转发给后端的 Pod。\n    \n\n```Go\n\nexport out=\"--dry-run=client -o yaml\"\nkubectl create ing ngx-ing --rule=\"ngx.test/=ngx-svc:80\" --class=ngx-ink $out\n```\n\n```Go\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ngx-ing\n  \nspec:\n\n  ingressClassName: ngx-ink\n  \n  rules:\n  - host: ngx.test\n    http:\n      paths:\n      - path: /\n        pathType: Exact\n        backend:\n          service:\n            name: ngx-svc\n            port:\n              number: 80\n```\n\n- ingressClassName\n    \n- rules\n    \n    - “rules”的格式比较复杂，嵌套层次很深。不过仔细点看就会发现它是把路由规则拆散了，有 host 和 http path，在 path 里又指定了路径的匹配方式，可以是精确匹配（Exact）或者是前缀匹配（Prefix），再用 backend 来指定转发的目标 Service 对象。\n        \n\n  \n\n## Ingress Class\n\nIngress Class 本身并没有什么实际的功能，只是起到联系 Ingress 和 Ingress Controller 的作用，所以它的定义非常简单，在“spec”里只有一个必需的字段“controller”，表示要使用哪个 Ingress Controller，具体的名字就要看实现文档了。\n\n```Go\n\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: ngx-ink\n\nspec:\n  controller: nginx.org/ingress-controller\n```\n\nIngress 和 Service、Ingress Class 的关系\n\n![](statistic/asynccode-580.png)\n\n  \n\n  \n\n# 如何在 Kubernetes 里使用 Ingress Controller\n\n你可以在 GitHub 上找到 Nginx Ingress Controller 的项目（https://github.com/nginxinc/kubernetes-ingress），因为它以 Pod 的形式运行在 Kubernetes 里，所以同时支持 Deployment 和 DaemonSet 两种部署方式。这里我选择的是 Deployment，相关的 YAML 也都在我们课程的项目（https://github.com/chronolaw/k8s_study/tree/master/ingress）里复制了一份。\n\n  \n\nNginx Ingress Controller 的安装略微麻烦一些，有很多个 YAML 需要执行，但如果只是做简单的试验，就只需要用到 4 个 YAML：\n\n```Go\n\nkubectl apply -f common/ns-and-sa.yaml\nkubectl apply -f rbac/rbac.yaml\nkubectl apply -f common/nginx-config.yaml\nkubectl apply -f common/default-server-secret.yaml\n```\n\n- 前两条命令为 Ingress Controller 创建了一个独立的名字空间“nginx-ingress”，还有相应的账号和权限，这是为了访问 apiserver 获取 Service、Endpoint 信息用的；\n    \n- 后两条则是创建了一个 ConfigMap 和 Secret，用来配置 HTTP/HTTPS 服务。\n    \n\n1. 部署 Ingress Controller 不需要我们自己从头编写 Deployment，Nginx 已经为我们提供了示例 YAML，但创建之前为了适配我们自己的应用还必须要做几处小改动：\n    \n\n- metadata 里的 name 要改成自己的名字，比如 ngx-kic-dep。\n    \n- spec.selector 和 template.metadata.labels 也要修改成自己的名字，比如还是用 ngx-kic-dep。\n    \n- containers.image 可以改用 apline 版本，加快下载速度，比如 nginx/nginx-ingress:2.2-alpine。\n    \n- 最下面的 args 要加上 -ingress-class=ngx-ink，也就是前面创建的 Ingress Class 的名字，这是让 Ingress Controller 管理 Ingress 的关键。修改完之后，Ingress Controller 的 YAML 大概是这个样子：\n    \n\n```Go\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ngx-kic-dep\n  namespace: nginx-ingress\n\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ngx-kic-dep\n\n  template:\n    metadata:\n      labels:\n        app: ngx-kic-dep\n    ...\n    spec:\n      containers:\n      - image: nginx/nginx-ingress:2.2-alpine\n        ...\n        args:\n          - -ingress-class=ngx-ink\n```\n\n有了 Ingress Controller，这些 API 对象的关联就更复杂了，你可以用下面的这张图来看出它们是如何使用对象名字联系起来的\n\n![](statistic/asynccode-579.png)\n\n不过还有最后一道工序，因为 **Ingress Controller 本身也是一个 Pod，想要向外提供服务还是要依赖于 Service 对象。所以你至少还要再为它定义一个 Service，使用 NodePort 或者 LoadBalancer 暴露端口**\n\n  \n\n- 这条命令就把本地的 8080 端口映射到了 Ingress Controller Pod 的 80 端口\n    \n\n```Go\n\nkubectl port-forward -n nginx-ingress ngx-kic-dep-8859b7b86-cplgp 8080:80 \u0026\n```\n\n![](statistic/asynccode-575.png)","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E7%BD%91%E7%BB%9C-Service":{"title":"网络-Service","content":"而在云原生时代，微服务无疑是应用的主流形态。为了更好地支持微服务以及服务网格这样的应用架构，Kubernetes 又专门定义了一个新的对象：**Service，它是集群内部的负载均衡机制，用来解决服务发现的关键问题**\n\n  \n\n  \n\n# 为什么要有 Service\n\n  \n\n**解决服务发现的问题，实现负载均衡**，它们在前端与后端之间加入了一个“中间层”，屏蔽后端的变化，为前端提供一个稳定的服务。\n\n  \n\nLVS、Nginx 毕竟不是云原生技术，所以 Kubernetes 就按照这个思路，定义了新的 API 对象**：Service。**\n\n  \n\n  \n\nService 的工作原理和 LVS、Nginx 差不多，**Kubernetes 会给Service 分配一个静态 IP 地址**，然后它再去自动管理、维护后面动态变化的 Pod 集合，当客户端访问 Service，它就根据某种策略，把**流量转发给后面的某个 Pod**。\n\n  \n\n![](statistic/asynccode-577.png)\n\n  \n\n这里 **Service 使用了 iptables 技术，每个节点上的 kube-proxy 组件自动维护 iptables 规则**，客户不再关心 Pod 的具体地址，只要访问 Service 的固定 IP 地址，Service 就会根据 iptables 规则转发请求给它管理的多个 Pod，是典型的负载均衡架构\n\n  \n\nService实现负载均衡的的技术：\n\n- Iptables\n    \n- Userspace (性能更差)\n    \n- ipvs（性能更好）\n    \n\n  \n\n# **如何通过YAML 描述Service**\n\nService与 Pod 一样，属于 Kubernetes 的核心对象，不关联业务应用，与 Job、Deployment 是不同的。\n\n  \n\n\u003e 虽然它可以自动创建 YAML 样板，但不是用命令 kubectl create，而是另外一个命令 kubectl expose，也许 Kubernetes 认为“expose”能够更好地表达 Service“暴露”服务地址的意思吧。\n\u003e \n\u003e 所以 kubectl expose 支持从多种对象创建服务，Pod、Deployment、DaemonSet 都可以。\n\u003e \n\u003e 使用 kubectl expose 指令时还需要用参数 --port 和 --target-port 分别指定映射端口和容器端口\n\u003e \n\u003e ```Go\n\u003e \n\u003e export out=\"--dry-run=client -o yaml\"\n\u003e kubectl expose deploy ngx-dep --port=80 --target-port=80 $out\n\u003e ```\n\n  \n\n  \n\n```Go\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: ngx-svc\n  \nspec:\n  selector:\n    app: ngx-dep\n    \n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n```\n\nselector 和 Deployment/DaemonSet 里的作用是一样的，用来过滤出要代理的那些 Pod。\n\nports 就很好理解了，里面的三个字段分别表示外部端口、内部端口和使用的协议，在这里就是内外部都使用 80 端口，协议是 TCP。当然，你在这里也可以把 ports 改成“8080”等其他的端口，这样外部服务看到的就是 Service 给出的端口，而不会知道 Pod 的真正服务端口。\n\n  \n\n![](statistic/asynccode-585.png)\n\n  \n\n# 如何在Kubernets 中使用Service\n\n首先，我们创建一个 ConfigMap，定义一个 Nginx 的配置片段，它会输出服务器的地址、主机名、请求的 URI 等基本信息\n\n```Go\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ngx-conf\n\ndata:\n  default.conf: |\n    server {\n      listen 80;\n      location / {\n        default_type text/plain;\n        return 200\n          'srv : $server_addr:$server_port\\nhost: $hostname\\nuri : $request_method $host $request_uri\\ndate: $time_iso8601\\n';\n      }\n    }\n```\n\n  \n\n然后我们在 Deployment 的“template.volumes”里定义存储卷，再用“volumeMounts”把配置文件加载进 Nginx 容器里\n\n```Go\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ngx-dep\n\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ngx-dep\n\n  template:\n    metadata:\n      labels:\n        app: ngx-dep\n    spec:\n      volumes:\n      - name: ngx-conf-vol\n        configMap:\n          name: ngx-conf\n\n      containers:\n      - image: nginx:alpine\n        name: nginx\n        ports:\n        - containerPort: 80\n\n        volumeMounts:\n        - mountPath: /etc/nginx/conf.d\n          name: ngx-conf-vol\n```\n\n创建\n\n```Go\napiVersion: v1\nkind: Service\nmetadata:\n  name: ngx-svc\n  \nspec:\n  selector:\n    app: ngx-dep\n    \n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n```\n\n```Go\n\nkubectl apply -f svc.yml\n```\n\n![](statistic/asynccode-581.png)\n\n你可以看到，Kubernetes 为 Service 对象自动分配了一个 IP 地址“10.96.240.115”，这个地址段是独立于 Pod 地址段的（比如第 17 讲里的 10.10.xx.xx）。而且 Service 对象的 IP 地址还有一个特点，**它是一个“虚地址”，不存在实体，只能用来转发流量。**\n\n  \n\n想要看 Service 代理了哪些后端的 Pod，你可以用 kubectl describe 命令：kubectl describe svc ngx-svc\n\n```Go\n\nkubectl describe svc ngx-svc\n```\n\n![](statistic/asynccode-582.png)\n\n  \n\n# 如何以域名的方式使用 Service\n\n我们先来看看 **DNS 域名**。**Service 对象的 IP 地址是静态的，保持稳定，这在微服务里确实很重要**， **DNS 插件就派上了用处**，它可以为 Service 创建易写易记的域名，让 Service 更容易使用。\n\n  \n\n## 名字空间（namespace）\n\nKubernetes 有一个默认的名字空间，叫“default”，如果不显式指定，API 对象都会在这个“default”名字空间里。\n\n而其他的名字空间都有各自的用途，比如“kube-system”就包含了 apiserver、etcd 等核心组件的 Pod。\n\n  \n\nDNS 是一种层次结构，为了避免太多的域名导致冲突，Kubernetes 就把名字空间作为域名的一部分，减少了重名的可能性\n\n  \n\nservice 对象的域名完全形式是“**对象. 名字空间.svc.cluster.loca**l”，但很多时候也可以省略后面的部分，直接写**“对象. 名字空间**”甚至“**对象名**”就足够了，默认会使用对象所在的名字空间（比如这里就是 default）。\n\n  \n\nDNS 域名的用法，还是先 kubectl exec 进入 Pod，然后用 curl 访问 ngx-svc、ngx-svc.default 等域名：\n\n![](statistic/asynccode-583.png)\n\n\u003e （顺便说一下，Kubernetes 也为每个 Pod 分配了域名，形式是“IP 地址. 名字空间.pod.cluster.local”，但需要把 IP 地址里的 . 改成 - 。比如地址 10.10.1.87，它对应的域名就是 10-10-1-87.default.pod。）\n\n  \n\n# 如何让Service对外暴露\n\n**由于 Service 是一种负载均衡技术，所以它不仅能够管理 Kubernetes 集群内部的服务，还能够担当向集群外部暴露服务的重任。**\n\n  \n\nService Type\n\n- ClusterIP:Service 的静态 IP 地址只能在集群内访问\n    \n- ExternalName:由云服务商提供\n    \n- LoadBalancer:由云服务商提供\n    \n- Service: 除了会对后端的 Pod 做负载均衡之外，还会在**集群里的每个节点上创建一个独立的端口，用这个端口对外提供服务，这也正是“NodePort”这个名字的由来**\n    \n    - 这个端口号属于节点，外部能够直接访问，**所以现在我们就可以不用登录集群节点或者进入 Pod 内部，直接在集群外使用任意一个节点的 IP 地址**，就能够访问 Service 和它代理的后端服务了。\n        \n\n  \n\nNodePort 与 Service、Deployment 的对应关系画成了图，你看了应该就能更好地明白它的工作原理：\n\n![](statistic/asynccode-584.png)\n\nNodePort 类型的缺点\n\n- 第一个缺点是它的端口数量很有限。Kubernetes 为了避免端口冲突，默认只在“30000~32767”这个范围内随机分配，只有 2000 多个，而且都不是标准端口号，这对于具有大量业务应用的系统来说根本不够用。\n    \n- 第二个缺点是它会在每个节点上都开端口，然后使用 kube-proxy 路由到真正的后端 Service，这对于有很多计算节点的大集群来说就带来了一些网络通信成本，不是特别经济。\n    \n- 第三个缺点**，它要求向外界暴露节点的 IP 地址，这在很多时候是不可行的**，**为了安全还需要在集群外再搭一个反向代理，增加了方案的复杂度**。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E8%B5%84%E6%BA%90%E5%92%8C%E6%8E%A2%E9%92%88":{"title":"资源和探针","content":"  \n\n# 容器资源配额\n\ncgroup、chroot。其中的 namespace 实现了独立的进程空间，chroot 实现了独立的文件系统，但唯独没有看到 cgroup 的具体应用。cgroup 的作用是管控 CPU、内存，保证容器不会无节制地占用基础资源，进而影响到系统里的其他应用。\n\n  \n\nCPU、内存直接“内置”在系统里的，不像硬盘那样需要“外挂”，所以申请和管理的过程也就会简单很多。具体的申请方法很简单，**只要在 Pod 容器的描述部分添加一个新字段 resources 就可以了**，\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ngx-pod-resources\n\nspec:\n  containers:\n  - image: nginx:alpine\n    name: ngx\n\n    resources:\n      requests:\n        cpu: 10m\n        memory: 100Mi\n      limits:\n        cpu: 20m\n        memory: 200Mi\n```\n\n- containers.resources，它下面有两个字段：“\n    \n    - requests”，意思是容器要申请的资源，也就是说要求 Kubernetes 在创建 Pod 的时候必须分配这里列出的资源，否则容器就无法运行。\n        \n    - “limits”，意思是容器使用资源的上限，不能超过设定值，否则就有可能被强制停止运行\n        \n\n  \n\nKubernetes 会根据每个 Pod 声明的需求，**像搭积木或者玩俄罗斯方块一样，把节点尽量“塞满**”，充分利用每个节点的资源，让集群的效益最大化。\n\n  \n\n如果 Pod 不写 resources 字段，Kubernetes 会如何处理呢？\n\n- 这就意味着 Pod 对运行的资源要求“既没有下限，也没有上限”，Kubernetes 不用管 CPU 和内存是否足够，**可以把 Pod 调度到任意的节点上**，而且**后续 Pod 运行时也可以无限制地使用 CPU 和内存**\n    \n\n  \n\n  \n\n# 什么是容器状态探针\n\n应该用什么手段来检查应用的健康状态呢？Kubernetes 用一根小棉签在应用的“检查口”里提取点数据，就可以从这些信息来判断应用是否“健康”了，这项功能也就被形象地命名**为“探针”（Probe）**，也可以叫“探测器”。\n\n  \n\nKubernetes 为检查应用状态定义了三种探针\n\n- Startup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。\n    \n- Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。\n    \n- Readiness，就绪探针，用来检查应用是否可以接收流量，是否能够对外提供服务。\n    \n\n  \n\n需要注意这三种探针是递进的关系\n\n- 应用程序先启动，加载完配置文件等基本的初始化数据就进入了 Startup 状态，\n    \n- 之后如果没有什么异常就是 Liveness 存活状态，但可能有一些准备工作没有完成，还不一定能对外提供服务，\n    \n- 只有到最后的 Readiness 状态才是一个**容器最健康可用**的状态。\n    \n\n![](statistic/asynccode-625.png)\n\n如果一个 Pod 里的容器配置了探针，Kubernetes 在启动容器后就会不断地调用探针来检查容器的状态：\n\n- 如果 Startup 探针失败，Kubernetes 会认为容器没有正常启动，**就会尝试反复重启，当然其后面的 Liveness 探针和 Readiness 探针也不会启动**。\n    \n- 如果 Liveness 探针失败，Kubernetes 就会认为容器发生了异常，**也会重启容器**\n    \n- 如果 Readiness 探针失败，Kubernetes 会认为容器虽然在运行，但内部有错误，不能正常提供服务，就会把容器从 **Service 对象的负载均衡集合中排除，不会给它分配流量**。\n    \n\n  \n\n# 如何使用容器状态探针\n\n  \n\nstartupProbe、livenessProbe、readinessProbe 这三种探针的配置方式都是一样的，关键字段有这么几个：\n\n- periodSeconds，**执行探测动作的时间间隔**，默认是 10 秒探测一次。\n    \n- timeoutSeconds，**探测动作的超时时间**，**如果超时就认为探测失败，默认是 1 秒**。\n    \n- successThreshold，连续几次探测成功才认为是正常，**对于 startupProbe 和 livenessProbe 来说它只能是 1**。\n    \n- failureThreshold，连续探测失败几次才认为是真正发生了异常，默认是 3 次。\n    \n\n  \n\n  \n\n至于探测方式，Kubernetes 支持 3 种：Shell、TCP Socket、HTTP GET，它们也需要在探针里配置\n\n- exec，执行一个 Linux 命令，比如 ps、cat 等等，和 container 的 command 字段很类似。\n    \n- tcpSocket，使用 TCP 协议尝试连接容器的指定端口。\n    \n- httpGet，连接端口并发送 HTTP GET 请求。\n    \n\n  \n\n这里我还是以 Nginx 作为示例，用 ConfigMap 编写一个配置文件：\n\n```Go\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ngx-conf\n\ndata:\n  default.conf: |\n    server {\n      listen 80;\n      location = /ready {\n        return 200 'I am ready';\n      }\n    }\n```\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ngx-pod-probe\n\nspec:\n  volumes:\n  - name: ngx-conf-vol\n    configMap:\n      name: ngx-conf\n\n  containers:\n  - image: nginx:alpine\n    name: ngx\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - mountPath: /etc/nginx/conf.d\n      name: ngx-conf-vol\n\n    startupProbe:\n      periodSeconds: 1\n      exec:\n        command: [\"cat\", \"/var/run/nginx.pid\"]\n\n    livenessProbe:\n      periodSeconds: 10\n      tcpSocket:\n        port: 80\n\n    readinessProbe:\n      periodSeconds: 5\n      httpGet:\n        path: /ready\n        port: 80\n```","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/Kubernetes/%E9%85%8D%E7%BD%AE-ConfigMap-Secret":{"title":"配置-ConfigMap Secret","content":"配置方式\n\n- 第一种是编写 Dockerfile，用 COPY 指令把配置文件打包到镜像里；\n    \n- 第二种是在运行时使用 docker cp 或者 docker run -v，把本机的文件拷贝进容器。\n    \n\n。第一种方法相当于是在镜像里固定了配置文件，不好修改，不灵活，第二种方法则显得有点“笨拙”，不适合在集群中自动化运维管理。\n\n  \n\n# ConfigMap/Secret\n\n首先你要知道，应用程序有很多类别的配置信息，但从数据安全的角度来看可以分成两类：\n\n- 一类是明文配置，也就是不保密，可以任意查询修改，比如服务端口、运行参数、文件路径等等。\n    \n- 另一类则是机密配置，由于涉及敏感信息需要保密，不能随便查看，比如密码、密钥、证书等等。\n    \n\n  \n\nKubernetes 也就定义了两个 API 对象，ConfigMap 用来保存明文配置，Secret 用来保存秘密配置。\n\n# 什么是 ConfigMap\n\n我们仍然可以用命令 kubectl create 来创建一个它的 YAML 样板。注意，它有简写名字“cm”，所以命令行里没必要写出它的全称：\n\n```Go\n\nexport out=\"--dry-run=client -o yaml\"        # 定义Shell变量\nkubectl create cm info $out\n```\n\n```Go\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: info\n```\n\nConfigMap 的 YAML 和之前我们学过的 Pod、Job 不一样，除了熟悉的“apiVersion”“kind”“metadata”，居然就没有其他的了，**最重要的字段“spec”哪里去了**？这是因为 **ConfigMap 存储的是配置数据，是静态的字符串，并不是容器**，所以它们就不需要用“spec”字段来说明运行时的“规格”。\n\n既然 ConfigMap 要存储数据，我们就需要用另一个含义更明确的字段**“data”。**\n\n**要生成带有“data”字段的 YAML 样板**，你需要在 kubectl create 后面多加一个参数 --from-literal ，表示从字面值生成一些数据：\n\n```Go\nexport out=\"--dry-run=client -o yaml\"\nkubectl create cm info --from-literal=k=v $out\n```\n\n```Go\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: info\n\ndata:\n  count: '10'\n  debug: 'on'\n  path: '/etc/systemd'\n  greeting: |\n    say hello to kubernetes.\n```\n\n我们还是可以用 kubectl get、kubectl describe 来查看 ConfigMap 的状态：\n\n```Go\n\nkubectl get cm\nkubectl describe cm info\n```\n\n![](statistic/asynccode-590.png)\n\n![](statistic/asynccode-591.png)\n\n# **什么是Secret**\n\n它和 ConfigMap 的结构和用法很类似，不过在 Kubernetes 里 Secret 对象又细分出很多类:\n\n- 访问私有镜像仓库的认证信息\n    \n- 身份识别的凭证信息\n    \n- HTTPS 通信的证书和私钥\n    \n- 一般的机密信息（格式由用户自行解释）\n    \n\n前几种我们现在暂时用不到，所以就只使用最后一种，创建 YAML 样板的命令是 kubectl create secret generic ，同样，也要使用参数 --from-literal 给出 Key-Value 值：\n\n```Go\nexport out=\"--dry-run=client -o yaml\"\nkubectl create secret generic user --from-literal=name=root $out\n```\n\nSecret 对象\n\n```Go\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: user\n\ndata:\n  name: cm9vdA==\n```\n\n这串“乱码”就是 Secret 与 ConfigMap 的不同之处，不让用户直接看到原始数据，起到一定的保密作用。不过它的手法非常简单，只是做了 Base64 编码，根本算不上真正的加密，所以我们完全可以绕开 kubectl，自己用 Linux 小工具“base64”来对数据编码，然后写入 YAML 文件\n\n  \n\n我们再来重新编辑 Secret 的 YAML，为它添加两个新的数据，方式可以是参数 --from-literal 自动编码，也可以是自己手动编码：\n\n```Go\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: user\n\ndata:\n  name: cm9vdA==  # root\n  pwd: MTIzNDU2   # 123456\n  db: bXlzcWw=    # mysql\n```\n\n使用 kubectl apply、kubectl get、kubectl describe：\n\n```Go\n\nkubectl apply  -f secret.yml\nkubectl get secret\nkubectl describe secret user\n```\n\n![](statistic/asynccode-594.png)\n\n![](statistic/asynccode-589.png)\n\n# 如何使用\n\n因为 **ConfigMap 和 Secret 只是一些存储在 etcd 里的字符串，所以如果想要在运行时产生效果，就必须要以某种方式“注入”到 Pod 里**，让应用去读取。在这方面的处理上 Kubernetes 和 Docker 是一样的，也是两种途径**：环境变量和加载文件。**\n\n## 如何以环境变量的方式使用 ConfigMap/Secret\n\n“env”，它定义了 Pod 里容器能够看到的环境变量。\n\n- “value”，把环境变量的值写“死”在了 YAML 里，\n    \n- “valueFrom”字段，从 ConfigMap 或者 Secret 对象里获取值，这样就实现了把配置信息以环境变量的形式注入进 Pod，也就是配置与应用的解耦。\n    \n\n  \n\n```Go\n\nkubectl explain pod.spec.containers.env.valueFrom\n```\n\n“valueFrom”字段指定了环境变量值的来源，**可以是“configMapKeyRef”或者“secretKeyRef”**，然后你要再进一步指定应用的 ConfigMap/Secret 的“name”和它里面的“key”，**要当心的是这个“name”字段是 API 对象的名字**，而不是 Key-Value 的名字。\n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: env-pod\n\nspec:\n  containers:\n  - env:\n      - name: COUNT\n        valueFrom:\n          configMapKeyRef:\n            name: info\n            key: count\n      - name: GREETING\n        valueFrom:\n          configMapKeyRef:\n            name: info\n            key: greeting\n      - name: USERNAME\n        valueFrom:\n          secretKeyRef:\n            name: user\n            key: name\n      - name: PASSWORD\n        valueFrom:\n          secretKeyRef:\n            name: user\n            key: pwd\n\n    image: busybox\n    name: busy\n    imagePullPolicy: IfNotPresent\n    command: [\"/bin/sleep\", \"300\"]\n```\n\n你需要重点关注的是它的“env”字段，里面定义了 4 个环境变量，COUNT、GREETING、USERNAME、PASSWORD。\n\n  \n\n对于明文配置数据， COUNT、GREETING 引用的是 ConfigMap 对象，所以使用字段“configMapKeyR**ef”，里面的“name”是 ConfigMap 对象的名字，也就是之前我们创建的“info”**，**而“key”字段分别是“info”对象里的 count 和 greeting。**\n\n![](statistic/asynccode-593.png)\n\n# 如何以 Volume 的方式使用 ConfigMap/Secret\n\n**Kubernetes 为 Pod 定义了一个“Volume”的概念，可以翻译成是“存储卷”。**\n\n在 Pod 里挂载 Volume 很容易，只需要在“spec”里增加一个“volumes”字段，然后再定义卷的名字和引用的 ConfigMap/Secret 就可以了\n\n```Go\n\nspec:\n  volumes:\n  - name: cm-vol\n    configMap:\n      name: info\n  - name: sec-vol\n    secret:\n      secretName: user\n```\n\n有了 Volume 的定义之后，就可以在容器里挂载了，这要用到“volumeMounts”字段，正如它的字面含义，可以把定义好的 Volume 挂载到容器里的某个路径下，所以需要在里面用“mountPath”“name”明确地指定挂载路径和 Volume 的名字。\n\n```Go\n\n  containers:\n  - volumeMounts:\n    - mountPath: /tmp/cm-items\n      name: cm-vol\n    - mountPath: /tmp/sec-items\n      name: sec-vol\n```\n\n![](statistic/asynccode-592.png)\n\n现在我把 Pod 的完整 YAML 描述列出来，然后使用 kubectl apply 创建它\n\n  \n\n```Go\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vol-pod\n\nspec:\n  volumes:\n  - name: cm-vol\n    configMap:\n      name: info\n  - name: sec-vol\n    secret:\n      secretName: user\n\n  containers:\n  - volumeMounts:\n    - mountPath: /tmp/cm-items\n      name: cm-vol\n    - mountPath: /tmp/sec-items\n      name: sec-vol\n\n    image: busybox\n    name: busy\n    imagePullPolicy: IfNotPresent\n    command: [\"/bin/sleep\", \"300\"]\n```","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MongoDB/%E4%B9%A6":{"title":"书","content":"![[mongo权威指南(第三版).pdf]]","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E4%BA%8B%E5%8A%A1":{"title":"事务","content":"# 事务的特性\n\n- **原子性（Atomicity）**：一个事务中的所有操作，要么全部完成，要么全部不完成\n    \n- **一致性（Consistency**）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。\n    \n- **隔离性（Isolation）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致\n    \n- **持久性（Durability）**：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。\n    \n\n  \n\n# InnoDB引起通过什么技术来保证事务的四个特性\n\n- 持久性是通过 **redo log （重做日志）**来保证的；\n    \n- 原子性是通过 **undo log（回滚日志）** 来保证的；\n    \n- 隔离性是通过 **MVCC（多版本并发控制） 或锁机制来保证**的；\n    \n- 一致性则是通过持**久性+原子性+隔离性来保证**；\n    \n\n# 并行事务会引发的问题\n\n脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）\n\n- 脏读：如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象\n    \n- 不可重复读：在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。\n    \n- 幻读：在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。\n    \n\n  \n\n# 隔离性和隔离级别\n\n  \n\n当多个事务并发执行时可能会遇到「脏读、不可重复读、幻读」的现象，这些现象会对事务的一致性产生不同程序的影响。\n\n- 脏读：读到其他事务未提交的数据；\n    \n- 不可重复读：前后读取的数据不一致；\n    \n- 幻读：前后读取的记录数量不一致。\n    \n\n这三个现象的严重性排序如下：\n\n![](statistic/asynccode-311.png)\n\n- 读未提交（read uncommitted）是指，一个事务还没提交时，它做的变更就能被别的事务看到。\n    \n- 读提交（read committed）是指，一个事务提交之后，它做的变更才会被其他事务看到。\n    \n- 可重复读（repeatable read）是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。\n    \n- 串行化（serializable ），顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”\n    \n\n![](statistic/asynccode-307.png)\n\n不同的隔离级别v1,v2.v3 有不同的的返回值\n\n|   |   |\n|---|---|\n|隔离级别|说明|\n|读未提交|则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2|\n|读提交|则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2|\n|可重复读|则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的|\n|串行化|则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。|\n\n![](statistic/asynccode-305.png)\n\nMySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了，详见这篇[文章 (opens new window)](https://xiaolincoding.com/mysql/transaction/phantom.html)），解决的方案有两种：\n\n- 针对**快照读**（普通 select 语句），**是通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。\n    \n- 针对**当前读**（select ... for update 等语句），**是通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。\n    \n\n  \n\n# 事务隔离的实现\n\n在实现上，数据库里面会创建一个视图（read-view(读视图)），访问的时候以视图的逻辑结果为准。\n\n- 在“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；\n    \n- 在“读提交”隔离级别下**，这个视图是在每个 SQL 语句开始执行的时候创建的。**\n    \n- 在“可重复读”隔离级别下，**这个视图是在事务启动时创建的，整个事务存在期间都用这个视图**。\n    \n- 而“串行化”隔离级别下直接用加锁的方式来避免并行访问。\n    \n\n## Read View 在MVCC 你面试如何工作的?\n\n我们需要了解两个知识：\n\n- Read View 中四个字段作用；\n    \n- 聚簇索引记录中两个跟事务有关的隐藏列；\n    \n\n那 Read View 到底是个什么东西？\n\n![](statistic/asynccode-304.png)\n\nRead View 有四个重要的字段：\n\n- **m_ids** ：指的是在创建 Read View 时，**当前数据库中「活跃事务」的事务 id 列表**，\n    \n- **min_trx_id** ：指的是在创建 Read View 时，**当前数据库中「活跃事务」中事务 id 最小的事务**，也就是 m_ids 的最小值。\n    \n- **max_trx_id** ：这个并不是 m_ids 的最大值，而是创建 Read View 时**当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1**；\n    \n- **creator_trx_id** ：**指的是创建该 Read View 的事务的事务 id**。\n    \n\n知道了 Read View 的字段，我们还需要了解聚簇索引记录中的两个隐藏列。\n\n假设在账户余额表插入一条小林余额为 100 万的记录，然后我把这两个隐藏列也画出来，该记录的整个示意图如下：\n\n![](statistic/asynccode-314.png)\n\n对于使用 InnoDB 存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：\n\n- **trx_id**，**当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里**；\n    \n- **roll_pointer**，每次对某条聚簇索引记录进行改动时，**都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录**。\n    \n\n在创建 R**ead View 后，我们可以将记录中的 trx_id 划分这三种情况**：\n\n![](statistic/asynccode-308.png)\n\n一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：\n\n- 如果记录的 trx_id 值\u003c `min_trx_id` ，表示这个版本的记录是在**创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见**。\n    \n- 如果记录的 trx_id \u003e= `max_trx_id` 值，表示这个版本的记录是**在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见**。\n    \n- 如果记录的 trx_id 值在 Read View 的 **`min_trx_id`** **和** **`max_trx_id`** **之间，需要判断 trx_id 是否在 m_ids 列表中：**\n    \n    - 如果记录的 trx_id 在 `m_ids` 列表中，**表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见**。\n        \n    - 如果记录的 trx_id 不在 `m_ids`列表中，**表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见**。\n        \n\n这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。\n\n## 可重复读是如何工作的？\n\n**可重复读隔离级别是启动事务时生成一个 Read View**，**然后整个事务期间都在用这个 Read View**。\n\n假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，那这两个事务创建的 Read View 如下：\n\n![](statistic/asynccode-309.png)\n\n事务 A 和 事务 B 的 Read View 具体内容如下：\n\n- 在事务 A 的 Read View 中，它的事务 id 是 51，由于它是第一个启动的事务，所以此时活跃事务的事务 id 列表就只有 51，活跃事务的事务 id 列表中最小的事务 id 是事务 A 本身，下一个事务 id 则是 52。\n    \n- 在事务 B 的 Read View 中，它的事务 id 是 52，由于事务 A 是活跃的，所以此时活跃事务的事务 id 列表是 51 和 52，活跃的事务 id 中最小的事务 id 是事务 A，下一个事务 id 应该是 53。\n    \n\n接着，在可重复读隔离级别下，事务 A 和事务 B 按顺序执行了以下操作：\n\n|   |   |\n|---|---|\n|事务A|事务B|\n|创建事务||\n||创建事务|\n||事务 B 读取小林的账户余额记录，读到余额是 100 万；|\n|事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务；||\n||事务 B 读取小林的账户余额记录，读到余额还是 100 万；|\n|事务 A 提交事务；||\n||事务 B 读取小林的账户余额记录，读到余额依然还是 100 万；|\n\n  \n\n接下来，跟大家具体分析下。\n\n- 事务 B 第一次读小林的账户余额记录，在找到记录后，它会先看这条记录的 trx_id，\n    \n    - 此时发现 trx_id 为 50，比事务 B 的 Read View 中的 min_trx_id 值（51）还小，这意味着修改这条记录的事务早就在事务 B 启动前提交过了，所以该版本的记录对事务 B 可见的，也就是事务 B 可以获取到这条记录。\n        \n- 接着，事务 A 通过 update 语句将这条记录修改了（还未提交事务），将小林的余额改成 200 万，这时 MySQL 会记录相应的 undo log，并以链表的方式串联起来，形成版本链，如下图：\n    \n    - 你可以在上图的「记录的字段」看到，**由于事务 A 修改了该记录，以前的记录就变成旧版本记录了，于是最新记录和旧版本记录通过链表的方式串起来**，而且最新记录的 trx_id 是事务 A 的事务 id（trx_id = 51）。\n        \n\n![](statistic/asynccode-308.png)\n\n  \n\n- 然后事务 B 第二次去读取该记录，发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，\n    \n    - **判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录**，所以\n        \n    - 事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。\n        \n- 最后，当事物 A 提交事务后，**由于隔离级别时「可重复读」**，所以事务 B 再次读取记录时，\n    \n    - 还是基于启动事务时创建的 Read View 来判断当前版本的记录是否可见。所以，即使事物 A 将小林余额修改为 200 万并提交了事务， 事务 B 第三次读取记录时，**读到的记录都是小林余额是 100 万的这条记录**。\n        \n\n就是通过这样的方式实现了，「可重复读」隔离级别下在事务期间读到的记录都是事务启动前的记录\n\n## 读提交是如何工作的？\n\n读提交隔离级别是在每次读取数据时，都会生成一个新的 Read View。\n\n也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。\n\n那读提交隔离级别是怎么工作呢？我们还是以前面的例子来聊聊。\n\n假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，接着按顺序执行了以下操作：\n\n|   |   |\n|---|---|\n|事务A|事务B|\n|创建事务||\n||创建事务|\n||事务 B 读取小林的账户余额记录，读到余额是 100 万；|\n|事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务；||\n||事务 B 读取小林的账户余额记录，读到余额还是 100 万；|\n|事务 A 提交事务；||\n||事务 B 读取小林的账户余额记录，读到余额依然还是 100 万；|\n\n那具体怎么做到的呢？我们重点看事务 B 每次读取数据时创建的 Read View。前两次 事务 B 读取数据时创建的 Read View 如下图：\n\n![](statistic/asynccode-306.png)\n\n我们来分析下为什么事务 B 第二次读数据时，读不到事务 A （还未提交事务）修改的数据？\n\n- 事务 B 在找到小林这条记录时，会看这条记录的 trx_id 是 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，接下来需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，\n    \n    - 那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。\n        \n    - 而是，沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。\n        \n\n我们来分析下为什么事务 A 提交后，事务 B 就可以读到事务 A 修改的数据？\n\n在事务 A 提交后，**由于隔离级别是「读提交」，所以事务 B 在每次读数据的时候，会重新创建 Read View**，此时事务 B 第三次读取数据时创建的 Read View 如下：\n\n![](statistic/asynccode-309.png)\n\n- 事务 B 在找到小林这条记录时**，会发现这条记录的 trx_id 是 51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小**，这意味着修改这条记录的事务早就在创建 Read View 前提交过了，所以该版本的记录对事务 B 是可见的。\n    \n\n正是因为在读提交隔离级别下，事务每次读数据时都重新创建 Read View，那么在事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。\n\n  \n\n# 另外的解释\n\n**实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值 undo log**\n\n**undo log** 在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时（**当系统里么有比这个回滚日志更早的read-view的时候**），回滚日志会被删除\n\n设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。\n\n![](statistic/asynccode-310.png)\n\n**当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。**\n\n如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的**多版本并发控制（MVCC）**\n\n突的。\n\n**长事务与undo log**\n\n**长事务意味着系统里面会存在很老的事务视图。**由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n\n# 事务的启动方式\n\n1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。\n    \n2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且**并不会自动提交**。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接\n    \n\n建议的方式：\n\n- 如果考虑多一次交互问题，可以使用commit work and chain语法\n    \n- 在autocommit=1的情况下用begin显式启动事务，如果执行commit则提交事务。如果执行commit work and chain则提交事务并自动启动下一个事务\n    \n\n**如何避免长事务的影响**\n\n应用开发端\n\n1. 确认是否使用了 set autocommit=0。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成\n    \n2. 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin/commit 框起来。\n    \n3. 通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）\n    \n\n数据库端\n\n1. 监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 / 或者 kill；\n    \n2. Percona 的 pt-kill 这个工具不错，推荐使用；\n    \n3. 在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；\n    \n4. 如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E4%BA%8B%E5%8A%A1%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E4%BA%86%E5%90%97":{"title":"事务可重复读，完全解决幻读了吗？","content":"MySQL InnoDB 引擎的默**认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种：**\n\n- **针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，**因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。\n    \n- **针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，**因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。\n    \n\n这两个解决方案是很大程度上解决了幻读现象，但是还是有个别的情况造成的幻读现象是无法解决的。\n\n这次，就跟大家好好聊这个问题。\n\n# 什么是幻读？\n\n首先来看看 MySQL 文档是怎么定义幻读（Phantom Read）的:\n\n翻译：**当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。**\n\n举个例子，假设一个事务在 T1 时刻和 T2 时刻分别执行了下面查询语句，途中没有执行其他任何语句：\n\n```SQL\nSELECT * FROM t_test WHERE id \u003e 100;\n```\n\n只要 T1 和 T2 时刻执行产生的结果集是不相同的，那就发生了幻读的问题，比如：\n\n- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 6 条行记录，那就发生了幻读的问题。\n    \n- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 4 条行记录，也是发生了幻读的问题。\n    \n\n# 快照读是如何避免幻读的？\n\n可重复读隔离级是由 MVCC（多版本并发控制）实现的，**实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。**\n\n做个实验，数据库表 t_stu 如下，其中 id 为主键。\n\n![](statistic/asynccode-319.png)\n\n然后在可重复读隔离级别下，有两个事务的执行顺序如下：\n\n![](statistic/asynccode-323.png)\n\n从这个实验结果可以看到，即使事务 B 中途插入了一条记录，事务 A 前后两次查询的结果集都是一样的，并没有出现所谓的幻读现象。\n\n# 当前读是如何避免幻读的？\n\nMySQL 里除了普通查询是快照读，其他都是当前读，比如 update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。\n\n这很好理解，假设你要 update 一个记录，另一个事务已经 delete 这条记录并且提交事务了，这样不是会产生冲突吗，所以 update 的时候肯定要知道最新的数据。\n\n另外，`select ... for update` 这种查询语句是当前读，每次执行的时候都是读取最新的数据。\n\n接下来，我们假设`select ... for update`当前读是不会加锁的（实际上是会加锁的），在做一遍实验。\n\n![](statistic/asynccode-320.png)\n\n这时候，**事务 B 插入的记录，就会被事务 A 的第二条查询语句查询到（因为是当前读），这样就会出现前后两次查询的结果集合不一样，这就出现了幻读。**\n\n**所以，Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了间隙锁。**\n\n假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。\n\n![](statistic/asynccode-322.png)\n\n举个具体例子，场景如下：\n\n![](statistic/asynccode-321.png)\n\n事务 A 执行了这面这条锁定读语句后，就在对表中的记录加上 id 范围为 (2, +∞] 的 next-key lock（next-key lock 是间隙锁+记录锁的组合）。\n\n然后，事务 B 在执行插入语句的时候，判断到插入的位置被事务 A 加了 next-key lock，于是事物 B 会生成一个插入意向锁，同时进入等待状态，直到事务 A 提交了事务。这就避免了由于事务 B 插入新记录而导致事务 A 发生幻读的现象。\n\n# 幻读被完全解决了吗？\n\n可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。\n\n我举例一个可重复读隔离级别发生幻读现象的场景。\n\n# 第一个发生幻读现象的场景\n\n还是以这张表作为例子：\n\n![](statistic/asynccode-319.png)\n\n事务 A 执行查询 id = 5 的记录，此时表中是没有该记录的，所以查询不出来。\n\n```Go\n事务 A\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from t_stu where id = 5;\nEmpty set (0.01 sec)\n```\n\n  \n\n然后事务 B 插入一条 id = 5 的记录，并且提交了事务。\n\n```Go\n事务 B\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e insert into t_stu values(5, '小美', 18);\nQuery OK, 1 row affected (0.00 sec)\n\nmysql\u003e commit;\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n  \n\n此时，事务 A 更新 id = 5 这条记录，对没错，**事务 A 看不到 id = 5 这条记录，但是他去更新了这条记录，这场景确实很违和，然后再次查询 id = 5 的记录，事务 A 就能看到事务 B 插入的纪录了，幻读就是发生在这种违和的场**景。\n\n```Go\n# 事务 A\nmysql\u003e update t_stu set name = '小林coding' where id = 5;\nQuery OK, 1 row affected (0.01 sec)Rows matched: 1  Changed: 1  Warnings: 0\n\nmysql\u003e select * from t_stu where id = 5;\n+----+--------------+------+\n| id | name         | age  |\n+----+--------------+------+\n|  5 | 小林coding   |   18 |\n+----+--------------+------+\n1 row in set (0.00 sec)\n```\n\n整个发生幻读的时序图如下：\n\n![](statistic/asynccode-324.png)\n\n在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。\n\n因为这种特殊现象的存在，所以我们认为 **MySQL Innodb 中的 MVCC 并不能完全避免幻读现象**。\n\n## 第二个发生幻读现象的场景\n\n除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。\n\n- T1 时刻：事务 A 先执行「**快照读语句**」：select * from t_test where id \u003e 100 得到了 3 条记录。\n    \n- T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；\n    \n- T3 时刻：事务 A 再执行「**当前读语句**」 select * from t_test where id \u003e 100 for update 就会得到 4 条记录，此时也发生了幻读现象。\n    \n\n要避免这类特殊场景下发生幻读的现象的话，就是**尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。**\n\n## **总结**\n\nMySQL InnoDB 引擎的可重复读隔离级别（默认隔离级），根据不同的查询方式，分别提出了避免幻读的方案：\n\n- 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读。\n    \n- 针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。\n    \n\n我举例了两个发生幻读场景的例子。\n\n第一个例子：对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。\n\n第二个例子：对于当前读，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。\n\n所以，MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。\n\n要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E4%BA%8B%E5%8A%A1-%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%92%8CMVCC":{"title":"事务-隔离级别的实现和MVCC","content":"**已知的矛盾**\n\n- **只查询**。可重复读隔离级别，事务 T 启动的时候会创建一个视图 read-view，之后事务 T 执行期间，即使有其他事务修改了数据，事务 T 看到的仍然跟在启动时看到的一样。\n    \n- 更新后查询。在写方面 一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁，它又不能这么超然了，**会被锁住，进入等待状态**。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值是啥。\n    \n    ```Plain\n    mysql\u003e CREATE TABLE `t` (\n      `id` int(11) NOT NULL,\n      `k` int(11) DEFAULT NULL,\n      PRIMARY KEY (`id`)\n    ) ENGINE=InnoDB;\n    insert into t(id, k) values(1,1),(2,2);\n    ```\n    \n\n对于，上述的表格采用一下操作\n\n![](statistic/asynccode-303.png)\n\nbegin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。\n\n事务的启动时机\n\n- 一致性视图是在执行第一个快照读语句时创建的；\n    \n- 一致性视图是在执行 start transaction with consistent snapshot 时创建的\n    \n\n上述的操作的结构为：事务 B 查到的 k 的值是 3（只查询），而事务 A 查到的 k 的值是 1（更新后查询）\n\n![](statistic/asynccode-9.jpg)\n\n# “快照”在 MVCC 里是怎么工作的？\n\n事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。\n\n## 快照的的实现创建\n\n  \n\n1. InnoDB 里面**每个事务有一个唯一的事务 ID，叫作 transaction id**。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按**申请顺序严格递增的.**\n    \n2. 每行数据也都是有多个版本的，每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。\n    \n\n![](statistic/asynccode-300.png)\n\n  \n\n1. 三个虚线箭头，就是 undo log；而 V1、V2、V3 并**不是物理上真实存在的**，而是每次需要的时候根据**当前版本**和 undo log 计算出来的\n    \n2. 可重复读事务 的实现只需要在启动的时候声明说，**以我启动的时刻为准**，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本， 如果“上一个版本”也不可见，那就得继续往前找。\n    \n3. 在实现上， InnoDB 为每个事务构造了一个数组，**用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID**。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。**开启事务的先后影响了版本的可见性，先开先见**\n    \n\n![](statistic/asynccode-301.png)\n\n对于当前事务的启动瞬间来说，一个数据版本的 row trx_id\n\n1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的 ,只有一个版本。\n    \n2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；\n    \n3. 如果落在黄色部分，那就包括两种情况\n    \n\na. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；\n\n  b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。\n\n1. 这个视图数组和高水位，就组成了当前事务的**一致性视图（read-view）即“快照”**\n    \n2. InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了**“秒级创建快照”**的能力。\n    \n\n## 查询逻辑\n\n假设\n\n1. 事务 A 开始前，系统里面只有一个活跃事务 ID 是 99；\n    \n2. 事务 A、B、C 的版本号分别是 100、101、102，且当前系统里只有这四个事务；\n    \n3. 三个事务开始前，(1,1）这一行数据的 row trx_id 是 90。\n    \n\n![](statistic/asynccode-300.png)\n\n分析：\n\n1. 第一个有效更新是事务 C，把数据从 (1,1) 改成了 (1,2)。这时候，**这个数据的最新版本的 row trx_id 是 102**，而 90 这个版本已经成为了历史版本\n    \n2. 第二个有效更新是事务 B，把数据从 (1,2) 改成了 (1,3)。这时候，**这个数据的最新版本（即 row trx_id）是 101**，而 102 又成为了历史版本。\n    \n3. 现在事务 A 要来读数据了，它的视图数组是[99,100]。在事务 A 查询的时候，其实事务 B 还没有提交，但是它生成的 (1,3) 这个版本已经变成当前版本了。但这个版本对事务 A 必须是不可见的，否则就变成脏读。**当前版本为90**\n    \n    1. 找到 (1,3) 的时候，判断出 row trx_id=101，比高水位大，处于红色区域，不可见；\n        \n    2. 接着，找到上一个历史版本，一看 row trx_id=102，比高水位大，处于红色区域，不可见；\n        \n    3. 再往前找，终于找到了（1,1)，它的 row trx_id=90，比低水位小，处于绿色区域，可见。\n        \n\n虽然期间这一行数据被修改过，但是事务 A 不论在什么时候查询，看到这行数据的结果都是一致的，所以我们称之为**一致性读**。\n\n## 更新逻辑\n\n**上面的事务B，如果按照一致性读结果不对**\n\n**如果事务 B 在更新之前查询一次数据，这个查询返回的 k 的值确实是 1。**\n\n但是，当它要去更新数据的时候，就**不能再在历史版本上更新**了，**否则事务 C 的更新就丢失**了。因此，事务 B 此时的 set k=k+1 是在（1,2）的基础上进行的操作。\n\n更新逻辑存在这样一条重要规则**：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。**\n\n  \n\n除了 update 语句外，select 语句如果加锁，也是当前读。\n\nselect * from t where id=1 修改一下，加上 lock in share mode 或 for update，也都可以读到版本号是 101 的数据，返回的 k 的值是 3。下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）\n\n```Plain\nmysql\u003e select k from t where id=1 lock in share mode;\nmysql\u003e select k from t where id=1 for update;\n```\n\n如果事务C不马上提交，即C‘\n\n![](statistic/asynccode-301.png)\n\n1. 事务 C’的不同是，更新后并没有马上提交，在它提交前，事务 B 的更新语句先发起了。\n    \n2. 两阶段锁协议”就要上场了。事务 C’没提交，也就是说 (1,2) 这个版本上的写锁还没释放。**C’提交了才能释放锁。事务B的操作阻塞**\n    \n\n**重点：**\n\n1. 可重复读的核心就是一致性读（consistent read）；\n    \n2. 而事务更新数据的时候，只能用当前读。\n    \n3. 如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。\n    \n\n**读提交的逻辑和可重复读的逻辑类似，它们最主要的区别**\n\n1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；\n    \n2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8":{"title":"数据存储","content":"\u003e 解锁这些面试题：\n\u003e \n\u003e - MySQL 的 NULL 值会占用空间吗？\n\u003e     \n\u003e - MySQL 怎么知道 varchar(n) 实际占用数据的大小？\n\u003e     \n\u003e - varchar(n) 中 n 最大取值为多少？\n\u003e     \n\u003e - 行溢出后，MySQL 是怎么处理的？\n\u003e     \n\n  \n\n## MySQL 的数据存放在哪个文件？\n\nMySQL 存储的行为是由存储引擎实现的，MySQL 支持多种存储引擎，不同的存储引擎保存的文件自然也不同。\n\nInnoDB 是我们常用的存储引擎，也是 MySQL 默认的存储引擎。\n\n  \n\n看看数据库的文件地址\n\n  \n\n```SQL\nmysql\u003e SHOW VARIABLES LIKE 'datadir';\n+---------------+-----------------+\n| Variable_name | Value           |\n+---------------+-----------------+\n| datadir       | /var/lib/mysql/ |\n+---------------+-----------------+\n1 row in set (0.00 sec)\n```\n\n  \n\n每创建一个 database（数据库） 都会在 /var/lib/mysql/ 目录里面创建一个**以 database 为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。**\n\n有一个名为 my_test 的 database，该 database 里有一张名为 t_order 数据库表\n\n![](statistic/asynccode-232.png)\n\n然后，我们进入 /var/lib/mysql/my_test 目录，看看里面有什么文件？\n\n/var/lib/mysql/my_test 目录，看看里面有什么文件\n\n```SQL\n[root@xiaolin ~]#ls /var/lib/mysql/my_test\ndb.opt  \nt_order.frm  \nt_order.ibd\n```\n\n- db.opt，用来存储当前数据库的**默认字符集和字符校验规则**。\n    \n- t_order.frm ，t_order 的**表结构**会保存在这个文件**。在 MySQL 中建立一张表都会生成一个.frm 文件，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。**\n    \n- t_order.ibd，t_order 的**表数据**会保存在这个文件。表数据既可以存在**共享表空间文件（文件名：ibdata1**）里，也可以存放在**独占表空间文件（文件名：表名字.ibd**）**。**这个行为是由参数 innodb_file_per_table 控制的，若设置了参数 innodb_file_per_table 为 1，则会将存储的数据、索引等信息单独存储在一个独占表空间，从 MySQL 5.6.6 版本开始，它的默认值就是 1 了，因此从这个版本之后， MySQL 中每一张表的数据都存放在一个独立的 .ibd 文件。\n    \n\n  \n\n# 表空间文件的结构\n\n表空间由**段（segment）、区（extent）、页（page）、行（row）**组成，InnoDB存储引擎的逻辑存储结构大致如下图：\n\nInnoDB存储引擎的逻辑存储结构大致如下图：\n\n![](statistic/asynccode-231.png)\n\n下面我们从下往上一个个看看。\n\n## 1、行（row）\n\n数据库表中的记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。\n\n后面我们详细介绍 InnoDB 存储引擎的行格式[行结构](https://jovbd87bon.feishu.cn/wiki/wikcntv38f3wMF0kyBEUuhKTZjb)\n\n## 2、页（page）\n\n记录是按照行来存储的，但是**数据库的读取并不以「行」为单位**，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。\n\n因此，**InnoDB 的数据是按「页」为单位来读写的，也就是说，当需要读一条记录的时候，并不是将这个行记录从磁盘读出来，而是以页为单位，将其整体读入内存。**\n\n默认**每个页的大小为 16KB**，也就是最多能保证 16KB 的连续存储空间。\n\n  \n\n页的类型有很多，常见的有**数据页、undo 日志页、溢出页**等等。\n\n数据表中的行记录是用「数据页」来管理的，数据页的结构这里我就不讲细说了，之前文章有说过，感兴趣的可以去看这篇文章：[换一个角度看 B+ 树(opens new window)](https://xiaolincoding.com/mysql/index/page.html)\n\n总之知道表中的记录存储在「数据页」里面就行。\n\n## 3、区（extent）\n\n我们知道 InnoDB 存储引擎是用 B+ 树来组织数据的。\n\n**B+ 树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的**，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢的。\n\n解决这个问题也很简单，**就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序 I/O 了**，那么在范围查询（扫描叶子节点）的时候性能就会很高。\n\n那具体怎么解决呢？\n\n在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。**每个区的大小为 1MB**，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了。\n\n## 4、段（segment）\n\n**表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。**\n\n- 索引段：存放 B + 树的非叶子节点的区的集合；\n    \n- 数据段：存放 B + 树的叶子节点的区的集合；\n    \n- 回滚段：存放的是回滚数据的区的集合，之前讲[事务隔离 (opens new window)](https://xiaolincoding.com/mysql/transaction/mvcc.html)的时候就介绍到了 MVCC 利用了回滚段实现了多版本查询数据。\n    \n\n好了，终于说完表空间的结构了。接下来，就具体讲一下 InnoDB 的行格式了。\n\n之所以要绕一大圈才讲行记录的格式，主要是想让大家知道行记录是存储在哪个文件，以及行记录在这个表空间文件中的哪个区域，有一个从上往下切入的视角，这样理解起来不会觉得很抽象","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8-%E6%95%B0%E6%8D%AE%E9%A1%B5%E7%BB%93%E6%9E%84":{"title":"数据存储-数据页结构","content":"这点没错，但是大家知道 B+ 树里的节点里存放的是什么呢？查询数据的过程又是怎样的？\n\n这次，我们从数据页的角度看 B+ 树，看看每个节点长啥样。\n\n![](statistic/asynccode-236.png)\n\n# InnoDB 是如何存储数据的？\n\n  \n\n- 记录是按**照行来存储的**，但是**数据库的读取并不以「行」为单位**，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。\n    \n- InnoDB 的数据是按「数据页」为单位来读写的\n    \n- InnoDB 数据页的默认大小是 16KB\n    \n\n## 数据页包括七个部分\n\n结构如下图：\n\n![](statistic/asynccode-233.png)\n\n这 7 个部分的作用如下图：\n\n![](statistic/asynccode-237.png)\n\n在 File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表，如下图所示：\n\n![](statistic/asynccode-235.png)\n\n采用链表的结构是让数据页之间不需要是物理上的连续的，而是逻辑上的连续。\n\n### **User Records 是怎么组织数据的--数据页**\n\n- 数据页中的记录按照「主键」顺序组成**单向链表**，单向链表的特点就是**插入、删除非常方便**，但是**检索效率不高**，最差的情况下需要遍历链表上的所有节点才能完成检索\n    \n- 数据页中有一个**页目录**，起到记录的索引作用，就像我们书那样，针对书中内容的每个章节设立了一个目录\n    \n\n  \n\n### 如何给记录创建页目录\n\n页目录与记录的关系如下图：\n\n![](statistic/asynccode-237.png)\n\n页目录创建的过程如下：\n\n1. 将**所有的记录划分成几个组**，这些记录包括最小记录和最大记录，但**不包括标记为“已删除”的记录**；\n    \n2. 每个记录组的**最后一条记录就是组内最大的那条记录**，并且最**后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段**（上图中粉红色字段）\n    \n3. **页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot）**，每个槽相当于指针指向了不同组的最后一个记录。\n    \n\n  \n\n- 页目录就是由多个槽组成的，**槽相当于分组记录的索引**\n    \n- 我们通过槽查找记录时，可以使用**二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**\n    \n\n  \n\n  \n\n如果**某个槽内的记录很多，然后因为记录都是单向链表串起来的，那这样在槽内查找某个记录的时间复杂度不就是 O(n) 了吗**？\n\n- 第一个分组中的记录只能有 1 条记录；\n    \n- 最后一个分组中的记录条数范围只能在 1-8 条之间；\n    \n- 剩下的分组中记录条数范围只能在 4-8 条之间。\n    \n\n# B+ 树是如何进行查询的？\n\n**数据页中的检索**\n\n一个数据页中的记录是有限的，且主键值是有序的，所以通过对所有记录进行分组，然后将组号（槽号）存储到页目录，使其起到索引作用，通过二分查找的方法快速检索到记录在哪个分组，来降低检索的时间复杂度。\n\n  \n\n## 多个数据页的检索\n\n  \n\nInnoDB 采用了 **B+ 树作为索**引。“矮胖”的 B+ 树数据结构，这样所需要进行的磁盘 I/O 次数更少，而且 B+ 树 更适合进行关键字的范围查询\n\n  \n\n**回顾一个m阶的B树具有如下几个特征：**\n\n1. 根结点至少有两个子女\n    \n2. 每个中间节点都包含k-1个元素和k个孩子，其中 m/2 \u003c= k \u003c= m\n    \n3. 每一个叶子节点都包含k-1个元素，其中 m/2 \u003c= k \u003c= m\n    \n4. 所有的叶子结点都位于同一层。\n    \n5. 每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。\n    \n\n**一个m阶的B+树具有如下几个特征：**\n\n1. **有k个子树的中间节点包含有k个元素（B树中是k-1个元素）**，每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。\n    \n2. 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接\n    \n3. **所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素**\n    \n\n  \n\nInnoDB 里的 B+ 树中的每个节点都是一个数据页，结构示意图如下：\n\n![](statistic/asynccode-238.png)\n\nB+树是B树的变体，具有比B树更高的查询性能\n\n  \n\n通过上图，我们看出 B+ 树的特点：\n\n- 只有**叶子节点（最底层的节点）才存放了数据**，非叶子节点（其他上层节）仅用来存放目录项作为索引。\n    \n- 非叶子节点分为不同层次，通过分层来降低每一层的搜索量；\n    \n- 所有节点按照索引键大小排序，构成一个双向链表，便于范围查询；\n    \n\n我们再看看 B+ 树如何实现快速查找主键为 6 的记录，以上图为例子：\n\n- 从根节点开始，通过二分法快速定位到符合页内范围包含查询值的页，因为查询的主键值为 6，在[1, 7)范围之间，所以到页 30 中查找更详细的目录项；\n    \n- 在非叶子节点（页30）中，继续通过二分法快速定位到符合页内范围包含查询值的页，主键值大于 5，所以就到叶子节点（页16）查找记录；\n    \n- 接着，在叶子节点（页16）中，通过槽查找记录时，使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到主键为 6 的记录。\n    \n\n可以看到，**在定位记录所在哪一个页时，也是通过二分法快速定位到包含该记录的页。定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号），最后在分组内进行遍历查找**\n\n  \n\n## 聚簇索引和二级索引\n\n另外，索引又可以分成**聚簇索引和非聚簇索引（二级索引）**，它们区别就在于叶子节点存放的是什么数据：\n\n- 聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点；\n    \n- 二级索引的叶子节点存放的是**主键值**，而不是实际数据。\n    \n\n**因为表的数据都是存放在聚簇索引的叶子节点里，所以 InnoDB 存储引擎一定会为表创建一个聚簇索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个。**\n\nInnoDB 在**创建聚簇索引**时，会根据不同的场景选择不同的列作为索引：\n\n- 如果有主键，默认会使用主键作为聚簇索引的索引键；\n    \n- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键；\n    \n- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键；\n    \n\n一张表只能有一个聚簇索引，那为了实现非主键字段的快速搜索，就引出了二级索引（非聚簇索引/辅助索引），它也是利用了 B+ 树的数据结构，但是二级索引的叶子节点存放的是主键值，不是实际数据。\n\n**二级索引的 B+ 树如下图，数据部分为主键值**：\n\n![](statistic/asynccode-234.png)\n\n因此，**如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作「回表」**，也就是说要查两个 B+ 树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8_%E8%A1%8C%E7%BB%93%E6%9E%84":{"title":"数据存储_行结构","content":"# InnoDB 行格式有哪些？\n\n行格式（row_format），就是一条记录的存储结构。\n\nInnoDB 提供了 4 种行格式，分别是 Redundant、Compact、Dynamic和 Compressed 行格式。\n\n- Redundant 是很古老的行格式了， MySQL 5.0 版本之前用的行格式，现在基本没人用了。\n    \n- 由于 Redundant 不是一种紧凑的行格式，所以 MySQL 5.0 之后引入了 Compact 行记录存储方式，Compact 是一种**紧凑的行格式**，设计的初衷就是为了让一个数据页中可以存放更多的行记录，**从 MySQL 5.1 版本之后，行格式默认设置成 Compact。**\n    \n- Dynamic 和 Compressed 两个都是紧凑的行格式，它们的行格式都和 Compact 差不多，因为都是基于 Compact 改进一点东西。**从 MySQL5.7 版本之后，默认使用 Dynamic 行格式。**\n    \n\nRedundant 行格式我这里就不讲了，因为现在基本没人用了，这次重点介绍 Compact 行格式，因为 Dynamic 和 Compressed 这两个行格式跟 Compact 非常像。\n\n所以，弄懂了 Compact 行格式，之后你们在去了解其他行格式，很快也能看懂。\n\n# COMPACT 行格式长什么样？\n\n先跟 Compact 行格式混个脸熟，它长这样：\n\n![](statistic/asynccode-226.png)\n\n可以看到，一条完整的记录分为「记录的额外信息」和「记录的真实数据」两个部分。\n\n接下里，分别详细说下。\n\n## 记录的额外信息\n\n记录的额外信息包含 3 个部分：变长字段长度列表、NULL 值列表、记录头信息。\n\n### 变长字段长度列表\n\nvarchar(n) 和 char(n) 的区别是什么，相信大家都非常清楚，**char 是定长的，varchar 是变长的，变长字段实际存储的数据的长度（大小）不固定的。**\n\n所以，**在存储数据的时候，也要把数据占用的大小存起来，存到「变长字段长度列表」里面，读取数据的时候才能根据这个「变长字段长度列表」去读取对应长度的数据**。**其他 TEXT、BLOB 等变长字段也是这么实现的。**\n\n```SQL\nCREATE TABLE `t_user` (\n`id` int(11) NOT NULL,\n`name` VARCHAR(20) DEFAULT NULL,\n`phone` VARCHAR(20) DEFAULT NULL,`age` int(11) DEFAULT NULL,\nPRIMARY KEY (`id`) USING BTREE\n) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT;\n```\n\n现在 t_user 表里有这三条记录：\n\n![](statistic/asynccode-219.png)\n\n  \n\n先来看第一条记录：\n\n- name 列的值为 a，真实数据占用的字节数是 1 字节，十六进制 0x01；\n    \n- phone 列的值为 123，真实数据占用的字节数是 3 字节，十六进制 0x03；\n    \n- age 列和 id 列不是变长字段，所以这里不用管。\n    \n\n这些**变长字段的真实数据占用的字节数会按照列的顺序逆序存放**（等下会说为什么要这么设计），所以「变长字段长度列表」里的内容是「 03 01」，而不是 「01 03」。\n\n![](statistic/asynccode-224.png)\n\n同样的道理，我们也可以得出第二条记录的行格式中，「变长字段长度列表」里的内容是「 04 02」，如下图：\n\n![](statistic/asynccode-227.png)\n\n第三条记录中 phone 列的值是 NULL，**NULL 是不会存放在行格式中记录的真实数据部分里的**，所以「变长字段长度列表」里不需要保存值为 NULL 的变长字段的长度。\n\n![](statistic/asynccode-228.png)\n\n\u003e 为什么「变长字段长度列表」的信息要按照逆序存放？\n\n这个设计是有想法的，主要是因为**「记录头信息」中指向下一个记录的指针，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，**这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。\n\n「变长字段长度列表」中的信息之所以要逆序存放，**是因为这样可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率**。\n\n同样的道理， NULL 值列表的信息也需要逆序存放。\n\n如果你不知道什么是 CPU Cache，可以看[这篇文章 (opens new window)](https://xiaolincoding.com/os/1_hardware/how_to_make_cpu_run_faster.html)，这属于计算机组成的知识。\n\n\u003e 每个数据库表的行格式都有「变长字段字节数列表」吗？\n\n其实变长字段字节数列表不是必须的。\n\n**当数据表没有变长字段的时候，比如全部都是 int 类型的字段，这时候表里的行格式就不会有「变长字段长度列表」了，因为没必要，不如去掉以节省空间**。\n\n所以「变长字段长度列表」只出现在数据表有变长字段的时候。\n\n### NULL 值列表\n\n表中的某些列可能会存储 NULL 值，如果**把这些 NULL 值都放到记录的真实数据中会比较浪费空间，所以 Compact 行格式把这些值为 NULL 的列存储到 NULL值列表**中。\n\n如果存在允许 NULL 值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排列。\n\n- **二进制位的值为****`1`****时，代表该列的值为NULL。**\n    \n- **二进制位的值为****`0`****时，代表该列的值不为NULL。**\n    \n\n另外，NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 `0`。\n\n还是以 t_user 表的这三条记录作为例子：\n\n![](statistic/asynccode-219.png)\n\n接下来，我们看看看看这三条记录的行格式中的 NULL 值列表是怎样存储的。\n\n先来看第一条记录，第一条记录所有列都有值，不存在 NULL 值，所以用二进制来表示是酱紫的：\n\n![](statistic/asynccode-229.png)\n\n但是 InnoDB 是用整数字节的二进制位来表示 NULL 值列表的，现在不足 8 位，所以要在高位补 0，最终用二进制来表示是酱紫的：\n\n![](statistic/asynccode-218.png)\n\n所以，对于第一条数据，NULL 值列表用十六进制表示是 0x00。\n\n接下来看第二条记录，第二条记录 age 列是 NULL 值，所以，对于第二条数据，NULL值列表用十六进制表示是 0x04。\n\n![](statistic/asynccode-220.png)\n\n最后第三条记录，第三条记录 phone 列 和 age 列是 NULL 值，所以，对于第三条数据，NULL 值列表用十六进制表示是 0x06。\n\n![](statistic/asynccode-223.png)\n\n我们把三条记录的 NULL 值列表都填充完毕后，它们的行格式是这样的：\n\n![](statistic/asynccode-222.png)\n\n\u003e 每个数据库表的行格式都有「NULL 值列表」吗？\n\n**NULL 值列表也不是必须的。**\n\n当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了。\n\n所以在设计数据库表的时候，通常都是建议将字段设置为 NOT NULL，这样可以至少节省 1 字节的空间（NULL 值列表至少占用 1 字节空间）。\n\n\u003e 「NULL 值列表」是固定 1 字节空间吗？如果这样的话，一条记录有 9 个字段值都是 NULL，这时候怎么表示？\n\n「NULL 值列表」的空间不是固定 1 字节的。\n\n当一条记录有 9 个字段值都是 NULL，那么就会创建 2 字节空间的「NULL 值列表」，以此类推。\n\n### 记录头信息\n\n记录头信息中包含的内容很多，我就不一一列举了，这里说几个比较重要的：\n\n- delete_mask ：标识此条数据是否被删除。从这里可以知道，我们执行 detele 删除记录的时候，并不会真正的删除记录，只是将这个记录的 delete_mask 标记为 1。\n    \n- next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面我也提到了，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。\n    \n- record_type：表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录\n    \n\n# 记录真实数据\n\n记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer，我们来看下这三个字段是什么。\n\n![](statistic/asynccode-225.png)\n\n- row_id\n    \n\n如果我们建表的时候指定了主**键或者唯一约束列，那么就没有 row_id 隐藏字段了**。如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。\n\n- trx_id\n    \n\n**事务id，表**示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。\n\n- roll_pointer\n    \n\n这条记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字节。\n\n如果你熟悉 MVCC 机制，你应该就清楚 trx_id 和 roll_pointer 的作用了，[1.6 隔离级别的实现和MVCC](https://jovbd87bon.feishu.cn/wiki/wikcnAwoo5cQNH2hCZZSKEQbkAc)\n\n  \n\n# varchar(n) 中 n 最大取值为多少？\n\nMySQL 规定除了 **TEXT、BLOBs 这种大对象类型之外**，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 **65535 个字节**。\n\nvarchar(n) 字段类型的 **n 代表的是最多存储的字符数量，并不是字节大小哦**。\n\n要算 varchar(n) 最大能允许存储的字节数，还要看数据库表的字符集，因为字符集代表着，1个字符要占用多少字节，比如 **ascii 字符集， 1 个字符占用 1 字节，那么 varchar(100) 意味着最大能允许存储 100 字节的数据**。\n\n  \n\n- 一行数据的最大字节数是 65535（不包含 TEXT、BLOBs 这种大对象类型），其中包含了 storage overhead。\n    \n\n其实就是「变长字段长度列表」和 「NULL 值列表」，也就是说一行数据的最大字节数 65535，其实是包含「变长字段长度列表」和 「NULL 值列表」所占用的字节数的。\n\n# 行溢出后，MySQL 是怎么处理的？\n\nMySQL 中磁盘和内存交互的基本单位是页，一个页的大小一般是 `16KB`，也就是 `16384字节`，而一个 varchar(n) 类型的列最多可以存储 `65532字节`，一些大对象如 TEXT、BLOB 可能存储更多的数据，这时一个页可能就存不了一条记录。**这个时候就会发生行溢出，多的数据就会存到另外的「溢出页」中**。\n\n当发生行溢出时，在记录的真实数据处**只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中**，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。大致如下图所示。\n\n![](statistic/asynccode-221.png)\n\n上面这个是 Compact 行格式在发生行溢出后的处理。\n\nCompressed 和 Dynamic 这两个行格式和 Compact 非常类似，主要的区别在于处理行溢出数据时有些区别。\n\n这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，**只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中，看起来就像下面这样**：\n\n![](statistic/asynccode-230.png)","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8_Change-buffer-%E5%92%8C-buffer-pool":{"title":"数据存储_Change buffer 和 buffer pool","content":"# **Change Buffer**\n\n## change buffer\n\n- 当需要更新一个数据页时，**如果数据页在内存中就直接更新**，\n    \n- 而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下**，InnoDB 会将这些更新操作缓存在 change buffer 中**，\n    \n- 在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。\n    \n- change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上\n    \n- change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge.\n    \n    - 除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。\n        \n    - 在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作.\n        \n\n  \n\n## 使用change buffer的条件\n\n对于唯一索引来说，不需要使用change buffer\n\n1. 所有的更新操作都要先判断这个操作是否违反唯一性约束。需要查询进内存\n    \n2. 如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。\n    \n\n  \n\n实际上也只有普通索引可以使用change bufffer change buffer 用的是 buffer pool 里的内存，因此不能无限增大\n\n  \n\n如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。\n\n![](statistic/asynccode-6.jpg)\n\n# Buffer Pool\n\n## 为什么需要 Buffer Pool？\n\n那修改完这条记录是选择直接写回到磁盘,缓存起来，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。\n\n为此，In**nodb 存储引擎设计了一个缓冲池（Buffer Pool）**，来提高数据库的读写性能。\n\n![](statistic/asynccode-239.png)\n\n有了 Buffer Poo 后：\n\n- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。\n    \n- 当修改数据时，如果数据存在于 Buffer Pool 中，**那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页**（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由**后台线程选择一个合适的时机**将脏页写入到磁盘。\n    \n\n## Buffer Pool 有多大？\n\nBuffer Pool 是在 MySQL 启动的时候，向操作系统申请的一片连续的内存空间，默认配置下 Buffer Pool 只有 `128MB` 。\n\n可以通过调整 `innodb_buffer_pool_size` 参数来设置 Buffer Pool 的大小，一般建议设置成可用物理内存的 60%~80%。\n\n## Buffer Pool 缓存什么？\n\n- 索引页\n    \n- 数据页\n    \n- Undo 页\n    \n- 插入缓存\n    \n- 自适应哈希索引锁信息\n    \n    ![](statistic/asynccode-242.png)\n    \n\n在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的****`16KB`****的大小划分出一个个的页**， Buffer Pool 中的页就叫做**缓存页**。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。\n\n所以，MySQL 刚启动的时候，你会观察到使用的虚拟内存空间很大，而使用到的物理内存空间却很小，**这是因为只有这些虚拟内存被访问后，操作系统才会触发缺页中断，申请物理内存，接着将虚拟地址和物理地址建立映射关系。**\n\n## Buffer Pool 中的缓存页的设计\n\n为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一个**控制块**，控制块信息包括「**缓存页的表空间、页号、缓存页地址、链表节点**」等等。\n\n控制块也是占有内存空间的，它是放在 Buffer Pool 的最前面，接着才是缓存页，如下图：\n\n![](statistic/asynccode-240.png)\n\n### 为啥会有碎片空间？\n\n每一个控制块都对应一个缓存页，那在分配足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，自然就用不到喽，这个用不到的那点儿内存空间就被称为碎片了\n\n### 查询一条记录，就只需要缓冲一条记录吗？\n\n当我们查询一条记录时，InnoDB 是会把整个页的数据加载到 Buffer Pool 中，因为，通过索引只能定位到磁盘中的页，而不能定位到页中的一条记录。将页加载到 Buffer Pool 后，再通过页里的页目录去定位到某条具体的记录。\n\n  \n\n## 如果管理Buffer Pool\n\n### 如何管理空闲页？\n\n为了能够快速找到空闲的缓存页，可以使用链表结构，将空闲缓存页的**「控制块」作为链表的节点**，这个链表称为 Free 链表（空闲链表）\n\n  \n\n![](statistic/asynccode-243.png)\n\n- Free 链表上除了有控制块，**还有一个头节点，该头节点包含链表的头节点地址，尾节点地址**，以及当前链表中节点的数量等信息。\n    \n- Free 链表节点是一个一个的控制块，而每个控制块包含着对应缓存页的地址，所以相当于 Free 链表节点都对应一个空闲的缓存页。\n    \n- 有了 Free 链表后，每当需要从磁盘中加载一个页到 Buffer Pool 中时，就从 Free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上，然后把该缓存页对应的控制块从 Free 链表中移除。\n    \n\n  \n\n### 如何管理脏页？\n\n更新数据的时候，不需要每次都要写入磁盘，而是将 Buffer Pool 对应的缓存页标记为脏页，然后再由**后台线程将脏页写入到磁盘**。\n\n为了能快速知道哪些缓存页是脏的，于是就设计出 **Flush 链表**，它跟 Free 链表类似的，链表的节点也是控制块，区别在于 **Flush 链表的元素都是脏页**。\n\n![](statistic/asynccode-241.png)\n\n  \n\n### 如何提高缓存命中率\n\n频繁访问的数据我们希望可以一直留在 Buffer Pool 中，而一些很少访问的数据希望可以在某些时机可以淘汰掉\n\n最容易想到的就是 LRU（Least recently used）算法。\n\n#### **简单的 LRU 算法**\n\n- 当访问的页在 Buffer Pool 里，就直接把该页对应的 LRU 链表节点移动到链表的头部。\n    \n- 当访问的页不在 Buffer Pool 里，除了要把页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点。\n    \n\nBuffer Pool 里有三种页和链表来管理数据：\n\n- Free Page（空闲页），表示此页未被使用，位于 Free 链表；\n    \n- Clean Page（干净页），表示此页已被使用，但是页面未发生修改，位于LRU 链表。\n    \n- Dirty Page（脏页），表示此页「已被使用」且「已经被修改」，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。脏页同时存在于 LRU 链表和 Flush 链表。\n    \n\n普通的LRU 算法没有被MySQL 使用\n\n- 预读失效；\n    \n- Buffer Pool 污染；\n    \n\n#### 预读失效\n\n- MySQL 的预读机制。程序是有空间局部性的，靠近当前被访问数据的数据，在未来很大概率会被访问到，会提前把它相邻的数据页一并加载进来\n    \n- 被提前加载进来的数据页，并没有被访问，相当于这个预读是白做了，这个就是**预读失效。**\n    \n- 如果这些预读页如果一直不会被访问到，不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是频繁访问的页，这样就大大降低了缓存命中率\n    \n\n#### 如何解决预读失效导致缓存命中率降低的问题\n\n让**预读的页停留在 Buffer Pool 里的时间要尽可能的短**，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长\n\n将 LRU 划分了 2 个区域：old 区域 和 young 区域。\n\nyoung 区域在 LRU 链表的前半部分，old 区域则是在后半部分，如下图：\n\n![](statistic/asynccode-244.png)\n\nld 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pct` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。\n\n划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部。\n\n  \n\n#### 什么是 Buffer Pool 污染？\n\n**当某一个 SQL 语句扫描了大量的数据时，在 Buffer Pool 空间比较有限的情况下，可能会将 Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 **Buffer Pool 污染**。\n\n#### 怎么解决出现 Buffer Pool 污染而导致缓存命中率下降的问题？\n\n**LRU 链表中 young 区域就是热点数据，只要我们提高进入到 young 区域的门槛，就能有效地保证 young 区域里的热点数据不会被替换掉**\n\n  \n\n增加了一个停留在 old 区域的时间判断。\n\n- 如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该缓存页就不会被从 old 区域移动到 young 区域的头部；\n    \n- 如果后续的访问时间与第一次访问的时间不在某个时间间隔内，那么该缓存页移动到 young 区域的头部；\n    \n\n这个间隔时间是由 `innodb_old_blocks_time` 控制的，默认是 1000 ms。\n\n也就说，只有同**时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部，这样就解决了 Buffer Pool 污染的问题 。**\n\n  \n\n### 脏页什么时候会被刷入磁盘？\n\n脏页需要被刷入磁盘，保证缓存和磁盘数据一致，但是若每次修改数据都刷入磁盘，则性能会很差，因此一般都会在**一定时机进行批量刷盘**。\n\n果在脏页还没有来得及刷入到磁盘时，MySQL 宕机了，不就丢失数据了吗？\n\n这个不用担心，I**nnoDB 的更新操作采用的是 Write Ahead Log 策略，即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力**\n\n下面几种情况会触发脏页的刷新：\n\n- 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；\n    \n- Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；\n    \n- MySQL 认为空闲时，后台线程会定期将适量的脏页刷入到磁盘；\n    \n- MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；\n    \n\n如果你发现**「偶尔」会出现一些用时稍长的 SQL**，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E6%97%A5%E5%BF%97":{"title":"日志","content":"Update 的执行过程\n\n```SQL\nUPDATE t_user SET name = 'xiaolin' WHERE id = 1;\n```\n\n  \n\n- 客户端先通过连接器建立连接，连接器自会判断用户身份；\n    \n- 因为这是一条 update 语句，所以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存清空的，所以说查询缓存很鸡肋，在 MySQL 8.0 就被移除这个功能了；\n    \n- 解析器会通过词法分析识别出关键字 update，表名等等，构建出语法树，接着还会做语法分析，判断输入的语句是否符合 MySQL 语法；\n    \n- 预处理器会判断表和字段是否存在；\n    \n- 优化器确定执行计划，因为 where 条件中的 id 是主键索引，所以决定要使用 id 这个索引；\n    \n- 执行器负责具体执行，找到这一行，然后更新。\n    \n\n与查询流程不一样的是，更新流程还涉及三个个重要的日志模块，它们正是我们今天要讨论的主角：**redo log（重做日志）和 binlog（归档日志），undo log（回滚日志）**\n\n\n\n| 日志                 | 作用                                                                           |\n| -------------------- | ------------------------------------------------------------------------------ |\n| Redo log（重做日志） | 是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC  |\n| binlog（归档日志）   | 是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复； |\n| undo log（回滚日志)  | - 用于数据备份和主从复制；                                                     | \n\n\n\n# Undo log\n\n## 为什么需要undo log\n\n一个事务在执行过程中，在还没有提交事务之前，如果MySQL 发生了崩溃，要怎么回滚到事务之前的数据呢?\n\n每次在事务执行过程中，都记录下回滚时需要的信息到一个日志里，那么在事务执行中途发生了 MySQL 崩溃后，就不用担心无法回滚到事务之前的数据,这就是**undo log（回滚日志）**，它保证了事务的 [ACID 特性 (opens new window)](https://xiaolincoding.com/mysql/transaction/mvcc.html#%E4%BA%8B%E5%8A%A1%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E6%80%A7)中的原子性（Atomicity）\n\n## **Undo log 得个过程**\n\n![](statistic/asynccode-274.png)\n\n  \n\nnnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如：\n\n- 在插入一条记录时，要把这条记录的**主键值**记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了；\n    \n- 在删除一条记录时，要把这条记录中的**内容都**记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了；\n    \n- 在更新一条记录时，要把被更新的列的**旧值**记下来，这样之后回滚时再把这些列更新为旧值就好了。\n    \n\n## Undo log 格式\n\n条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：\n\n- 通过 trx_id 可以知道该记录是被哪个事务修改的；\n    \n- 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链；\n    \n\n版本链如下图：\n\n![](statistic/asynccode-276.png)\n\n## Undo log 与 MVCC\n\nundo log 还有一个作用，通过 ReadView + undo log 实现 MVCC（多版本并发控制）。\n\n对于「读提交」和「可重复读」隔离级别的事务来说，它们的快照读（普通 select 语句）是通过 Read View + undo log 来实现的，它们的区别在于创建 Read View 的时机不同\n\n- 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。\n    \n- 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。\n    \n\n这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列（trx_id 和 roll_pointer）」的比对，如果不满足可见行，就会顺着 **undo log 版本链**里找到满足其可见性的记录，从而控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。\n\n# redo log --**引擎层**\n\nBuffer Pool() 是提高了读写效率没错，但是问题来了，**Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。**\n\n- redo log 是物理日志，**记录了某个数据页做了什么修改**，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。\n    \n- 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。\n    \n- 在内存修改该 Undo 页面后，需要记录对应的 redo log\n    \n\n  \n\n## **Redo log 和undo log 的区别**\n\n- redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值；\n    \n- undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值；\n    \n\n  \n\n事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务，如下图：\n\n![](statistic/asynccode-287.png)\n\n## **redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？**\n\n- 写入 redo log 的方式使用了追加操作， 所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。\n    \n- 实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；\n    \n\n  \n\n**WAL技术（Write-Ahead Logging）它的关键点就是先写日志，再写磁盘**\n\n- 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就**会先把记录写到 redo log（粉板- redo-buffer-log）里面**，并更新内存，这个时候更新就算完成了。\n    \n- 同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做\n    \n\n**InnoDB 的 redo log 是固定大小的，工作在引擎层，**从头开始写，写到末尾就又回到开头循环写\n\n![](statistic/asynccode-290.png)\n\n过程：\n\n1. write pos 是当前记录的位置，一边写一边后移，checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件\n    \n2. write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下\n    \n    1. 这个时候MySql 会阻塞\n        \n    2. 会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针）\n        \n\n**有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。**\n\n![](statistic/asynccode.webp)\n\nredo log buffer 默认大小 16 MB，可以通过 `innodb_log_Buffer_size` 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。\n\n## redo log 什么时候刷盘\n\n- MySQL 正常关闭时；\n    \n- 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；\n    \n- InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。\n    \n- 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。\n    \n    - `innodb_flush_log_at_trx_commit`\n        \n        - 0:每次事务提交时 ,将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。\n            \n        - 1:每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。\n            \n        - 2:表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache（如果你想了解 Page Cache，可以看[这篇 (opens new window)](https://xiaolincoding.com/os/6_file_system/pagecache.html)），Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。\n            \n\n![](statistic/asynccode-281.png)\n\n# binlog --server层\n\n## 为啥使用binlog？\n\n- 开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM\n    \n- MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档\n    \n- 而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的\n    \n\n## **Rebo log 和binlog的区别：**\n\n- redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\n    \n- redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”,Redo log不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。\n    \n- Binlog有两种模式，\n    \n    - statement 格式的话是记sql语句，\n        \n    - row格式会记录行的内容，记两条，更新前和更新后都有\n        \n    - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；\n        \n- redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志\n    \n- _用途不同：_\n    \n    - binlog 用于备份恢复、主从复制；·\n        \n    - redo log 用于掉电等故障恢复。\n        \n\n## 执行器和InnoDB的update流程\n\nupdate T set c=c+1 where ID=2;\n\n1. 执行器先找引擎取 ID=2 这一行，如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n    \n2. 执行器拿到引擎给的行数据，把这个值加上 1，再调用引擎接口写入这行新数据\n    \n3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。\n    \n4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。\n    \n5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。\n    \n\n![](statistic/asynccode-277.png)\n\n  \n\n## 主从复制如何实现\n\nMySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复**制的过程就是将 binlog 中的数据从主库传输到从库上。**\n\n这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。\n\n![](statistic/asynccode-277.png)\n\nMySQL 集群的主从复制过程梳理成 3 个阶段：\n\n- 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。\n    \n- 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。\n    \n- 回放 Binlog：回放 binlog，并更新存储引擎中的数据。\n    \n\n具体详细过程如下：\n\n- MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。\n    \n- 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。\n    \n- 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。\n    \n\n在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。\n\n![](statistic/asynccode-275.png)\n\n## 从库是不是越多越好\n\n因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。\n\n## 主从复制模型\n\n- 同步复制：MySQL 主库提交事务的线程要**等待所有从库的复制成功响应，才返回客户端结果**。这种方式在实际项目中，基本上没法用，\n    \n    - 一是性能很差，因为要复制到所有节点才返回响应；\n        \n    - 二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。\n        \n- 异步复制（默认模型）：MySQL **主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失**。\n    \n- 半同步复制：介于两者之间，事务线程不用等待所有的从库复制成功响应，只**要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端**。\n    \n\n## binlog 什么时候刷盘？\n\n事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。\n\n**一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大**（比如有很多条语句），也要保证一次性写入，这样**如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。**\n\nMySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 **binlog cache**\n\n#### 什么时候 binlog cache 会写到 binlog 文件？\n\n在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。\n\n![](statistic/asynccode-282.png)\n\n虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件：\n\n- 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。\n    \n- 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。\n    \n\nMySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率：\n\n- sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；\n    \n- sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；\n    \n- sync_binlog =N(N\u003e1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。\n    \n\n  \n\n# 两阶段提交\n\n## 为啥要有两阶段提交\n\nredo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成**两份日志之间的逻辑不一致。**\n\n**两阶段提交的必要性---使用反证法**\n\n假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash\n\n- 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0\n    \n- 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同\n    \n\n保证了cashe-safe\n\n  \n\n## 2 个阶段\n\n分别是「准备（Prepare）阶段」和「提交（Commit）阶段」，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。。\n\n举个拳击比赛的例子，两位拳击手（参与者）开始比赛之前，裁判（协调者）会在中间确认两位拳击手的状态，类似于问你准备好了吗？\n\n- 准备阶段：裁判（协调者）会依次询问两位拳击手（参与者）是否准备好了，然后拳击手听到后做出应答，如果觉得自己准备好了，就会跟裁判说准备好了；如果没有自己还没有准备好（比如拳套还没有带好），就会跟裁判说还没准备好。\n    \n- 提交阶段：如果两位拳击手（参与者）都回答准备好了，裁判（协调者）宣布比赛正式开始，两位拳击手就可以直接开打；如果任何一位拳击手（参与者）回答没有准备好，裁判（协调者）会宣布比赛暂停，对应事务中的回滚操作。\n    \n\n## 两阶段提交的过程\n\nMySQL 使用了内部 XA 事务（是的，也有外部 XA 事务，跟本文不太相关，我就不介绍了），**内部 XA 事务由 binlog 作为协调者，存储引擎是参与者**\n\nMySQL 内部开启一个 XA 事务，分两阶段来完成 XA 事务的提交，如下图：\n\n![](statistic/asynccode-278.png)\n\n从图中可看出，事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下：\n\n- prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；\n    \n- commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；\n    \n\n## 异常重启\n\n下图中有时刻 A 和时刻 B 都有可能发生崩溃：\n\n![](statistic/asynccode-279.png)\n\n不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），还是时刻 B （redo log 和 binlog 都已经写入磁盘，还没写入 commit 标识）崩溃，此时的 redo log 都处于 prepare 状态。\n\n在 MySQL 重启后会按顺序扫描 redo log 文件，**碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID：**\n\n- 如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。\n    \n- 如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。\n    \n\n所以说，**两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了**，就意味着能在 binlog 中查找到与 redo log 相同的 XID。\n\n#### **处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计?**\n\nbinlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。\n\n所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。\n\n#### 事务没提交的时候，redo log 会被持久化到磁盘吗？\n\n会的。\n\n事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。\n\n也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。\n\n## 两阶段提交有什么问题？\n\n两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响：\n\n- 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。\n    \n- 锁竞争激烈：两阶段提交虽然能够\n    \n    - 保证「单事务」两个日志的内容一致，\n        \n    - 但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，**还需要加一个锁来保证提交的原子性**，从而保证多事务的情况下，两个日志的提交顺序一致。\n        \n\n#### 为什么两阶段提交的磁盘 I/O 次数会很高？\n\nbinlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一**般我们为了避免日志丢失的风险，会将这两个参数设置为 1：**\n\n- 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘；\n    \n- 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘；\n    \n\n可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会至少调用 2 次刷盘操作，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。\n\n#### 为什么锁竞争激烈？\n\n在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，**在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁**，下个事务才可以继续进行 prepare 操作。\n\n  \n\n# 组提交\n\nMySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数\n\n引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，**将 commit 阶段拆分为三个过程**：\n\n- flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；\n    \n- sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；\n    \n- commit 阶段：各个事务按顺序做 InnoDB commit 操作；\n    \n\n上面的每个阶段都有一个队列，**每个阶段有锁进行保护，因此保证了事务写入的顺序**，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。\n\n![](statistic/asynccode-284.png)\n\n对每个阶段引入了队列后，**锁就只针对每个队列进行保护，不再锁住提交事务的整个过程**，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。\n\n## 有 binlog 组提交，那有 redo log 组提交吗？\n\n这个要看 MySQL 版本，MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。\n\n所以在 MySQL 5.7 版本中，做了个改进**，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段**。\n\n这个优化是将 **redo log 的刷盘延迟到了 flush 阶段之中**，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。\n\n接下来介绍每个阶段的过程，注意下面的过程针对的是“双 1” 配置（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）。\n\n## flush 阶段\n\n第一个事务会成为 flush 阶段的 Leader，此时后面到来的事务都是 Follower ：\n\n![](statistic/asynccode-284.png)\n\n接着，获取队列中的事务组，由**绿色事务组的 Leader 对 rodo log 做一次 write + fsyn**c，即一次将同组事务的 redolog 刷盘：\n\n![](statistic/asynccode-280.png)\n\n完成了 prepare 阶段后，**将绿色这一组事务执行过程中产生的 binlog 写入 binlog 文件（**调用 write，不会调用 fsync，所以不会刷盘，binlog 缓存在操作系统的文件系统中）。\n\n![](statistic/asynccode-280.png)\n\n从上面这个过程，可以知道 **flush 阶段队列的作用是用于支撑 redo log 的组提交**。\n\n如果在这一步完成后数据库崩溃，由于 binlog 中没有该组事务的记录，所以 MySQL 会在重启后回滚该组事务。\n\n## sync 阶段\n\n绿色这一组事务的 binlog 写入到 binlog 文件后，并**不会马上执行刷盘的操作，而是会等待一段时间，**这个等待的时长由 `Binlog_group_commit_sync_delay` 参数控制，**目的是为了组合更多事务的 binlog，然后再一起刷盘，如下过程：**\n\n![](statistic/asynccode-285.png)\n\n不过，**在等待的过程中，如果事务的数量提前达到了** **`Binlog_group_commit_sync_no_delay_count`** **参数设置的值，就不用继续等待了，就马上将 binlog 刷盘**，如下图：\n\n![](statistic/asynccode-283.png)\n\n从上面的过程，可以知道 s**ync 阶段队列的作用是用于支持 binlog 的组提交**。\n\n如果想提升 binlog 组提交的效果，可以通过设置下面这两个参数来实现：\n\n- `binlog_group_commit_sync_delay= N`，表示在等待 N 微妙后，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘，也就是将「 binlog 文件」持久化到磁盘。\n    \n- `binlog_group_commit_sync_no_delay_count = N`，表示如果队列中的事务数达到 N 个，就忽视binlog_group_commit_sync_delay 的设置，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘。\n    \n\n如果在这一步完成后数据库崩溃，由于 binlog 中已经有了事务记录，MySQL会在重启后通过 redo log 刷盘的数据继续进行事务的提交。\n\n## commit 阶段\n\n最后进入 commit 阶段，**调用引擎的提交事务接口，将 redo log 状态设置为 commit。**\n\n![](statistic/asynccode-286.png)\n\ncommit 阶段队列的作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率。\n\n# **Update 更新流程**\n\n具体更新一条记录 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 的流程如下:\n\n1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：\n    \n    1. 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；\n        \n    2. 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。\n        \n2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：\n    \n    1. 如果一样的话就不进行后续更新流程；\n        \n    2. 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；\n        \n3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。\n    \n4. InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。\n    \n5. 至此，一条记录更新完了。\n    \n6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。\n    \n7. 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）：\n    \n    1. prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；\n        \n    2. commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；\n        \n8. 至此，一条更新语句执行完成。\n    \n\n# 备份周期设置\n\n首先，是恢复数据丢失的时间，既然需要恢复，肯定是数据丢失了。如果一天一备份的话，只要找到这天的全备，加入这天某段时间的binlog来恢复，如果一周一备份，假设是周一，而你要恢复的数据是周日某个时间点，那就，需要全备+周一到周日某个时间点的全部binlog用来恢复，时间相比前者需要增加很多；看业务能忍受的程度\n\n其次，是数据库丢失，如果一周一备份的话，需要确保整个一周的binlog都完好无损，否则将无法恢复；而一天一备，只要保证这天的binlog都完好无损；当然这个可以通过校验，或者冗余等技术来实现，相比之下，上面那点更重要","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E7%B4%A2%E5%BC%95":{"title":"索引","content":"索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。\n\n# 常见的索引模型\n\n![](statistic/asynccode-8.jpg)\n\n**索引是在存储引擎层实现的**，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同\n\n# InnoDB的索引模型\n\n在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为**索引组织表**。\n\nInnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。\n\n## B+树的结构\n\nB+树是B树的变体，具有比B树更高的查询性能\n\n**回顾一个m阶的B树具有如下几个特征：**\n\n1. 根结点至少有两个子女\n    \n2. **除根节点以外的所有非叶结点至少有 M/2 棵子树，至多有 M 个子树（关键字数为子树减一）**\n    \n3. 每一个叶子节点**至少有 M/2-1 棵子树，至多有 M -1个子树**\n    \n4. 所有的叶子结点都位于同一层。\n    \n5. 每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。\n    \n\n**一棵 3 阶的 B 树（即节点最多有三个子树），每个节点的关键字数最少为 1，最多为 2，如果要添加数据的子树的关键字数已经是最多，就需要拆分节点，调整树的结构。**\n\n**一个m阶的B+树具有如下几个特征：**\n\n1. **有k个子树的中间节点包含有k个元素（B树中是k-1个元素）**，每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。\n    \n2. 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接\n    \n3. **所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素**\n    \n\n![](statistic/asynccode-273.png)\n\n## B+树的构造\n\n**B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数**\n\n现在有表\n\n```Plain\nmysql\u003e create table T(\nid int primary key, \nk int not null, \nname varchar(16),\nindex (k))engine=InnoDB;\n```\n\n表中有值：R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)\n\n![](statistic/asynccode-272.png)\n\n索引的查询\n\n- 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树\n    \n- 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为**回表**\n    \n- 基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n    \n\n![](statistic/asynccode-7.jpg)\n\n# 如何维护索引\n\nB+树为了维护索引的有序性，在插入值的时候要进行维护\n\n以上面这个图为例，\n\n- 如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。-----**递增插入**\n    \n- 如果新插入的 ID 值为 400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。----**挪动其他记录**\n    \n- 而更糟的是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。-------**页分裂**\n    \n\n**当然有分裂就有合并**。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程\n\n**主键自增的场景** NOT NULL PRIMARY KEY AUTO_INCREMENT\n\n- 自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。不涉及到**挪动其他记录**，也不会触发叶子节点的分裂\n    \n- 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高\n    \n- 键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 。从性能和存储空间方面考量，自增主键往往是更合理的选择\n    \n\n**业务字段做主键的场景**\n\n- 只有一个索引；\n    \n- 该索引必须是唯一索引。\n    \n- 这就是典型的 KV 场景\n    \n\n# 避免回表\n\n由于查询结果所需要的数据只在主键索引上有，所以不得不回表。但是可能经过索引优化，避免回表\n\n## 覆盖索引\n\n```Plain\nselect ID from T where k  between 3 and 5，\n```\n\n  \n\n这时只需要查 ID 的值，而 **ID 的值已经在 k 索引树上**了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为**覆盖索引**\n\n**正确使用联合索引可以形成覆盖索引，提高效率**\n\n如要根据市民的身份证号查询他的姓名，联合索引（身份证号、姓名）就有意义了。它用到覆盖索引，不再需要回表。\n\n## 最左前缀原则\n\n**在建立联合索引的时候，如何安排索引内的字段顺序。**\n\n1. 如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。\n    \n\n最左前缀可以用于在索引中定位记录\n\n## 索引下推\n\n检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的\n\n```Plain\nmysql\u003e select * from tuser where name like '张%' and age=10 and ismale=1;\n```\n\n这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录，对于联合索引的其他字段， **可以在索引遍历过程中，对索引中包含的字段先做判断**，直接过滤掉不满足条件的记录，减少回表次数。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E7%B4%A2%E5%BC%95-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"索引-常见面试题","content":"# 什么是索引\n\n  \n\n那换到数据库中，索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就**是索引是数据的目录**。\n\n# 索引的分类。\n\n我们可以按照四个角度来分类索引。\n\n- 按「数据结构」分类：B+tree索引、Hash索引、Full-text索引。\n    \n- 按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）。\n    \n- 按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引。\n    \n- 按「字段个数」分类：单列索引、联合索引。\n    \n\n接下来，按照这些角度来说说各类索引的特点\n\n  \n\n## 按数据结构分类\n\n每一种存储引擎支持的索引类型不一定相同，我在表中总结了 MySQL 常见的存储引擎 InnoDB、MyISAM 和 Memory 分别支持的索引类型。\n\n![](statistic/asynccode-254.png)\n\nInnoDB 是在 MySQL 5.5 之后成为默认的 MySQL 存储引擎，B+Tree 索引类型也是 MySQL 存储引擎采用最多的索引类型。\n\n在创建表时，InnoDB 存储引擎会根据不同的场景选择不同的列作为索引：\n\n- 如果有主键，默认会使用主键作为聚簇索引的索引键（key）；\n    \n- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）；\n    \n- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）；\n    \n\n其它索引都属于辅助索引（Secondary Index），也被称为**二级索引或非聚簇索引**。创建的主键索引和二级索引默认使用的是 B+Tree 索引。\n\nB+Tree 是一种多叉树，**叶子节点才存放数据，非叶子节点只存放索引**，而且每个节点里的数据是按主键顺序存放的。每一层父节点的索引值都会出现在下层子节点的索引值中，因此在叶子节点**中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。**\n\n### 通过主键查询商品数据的过程\n\n比如，我们执行了下面这条查询语句：\n\n```SQL\nselect * from product where id= 5;\n```\n\n这条语句使用了主键索引查询 id 号为 5 的商品。查询过程是这样的，B+Tree 会自顶向下逐层进行查找：\n\n- 将 5 与根节点的索引数据 (1，10，20) 比较，5 在 1 和 10 之间，所以根据 B+Tree的搜索逻辑，找到第二层的索引数据 (1，4，7)；\n    \n- 在第二层的索引数据 (1，4，7)中进行查找，因为 5 在 4 和 7 之间，所以找到第三层的索引数据（4，5，6）；\n    \n- 在叶子节点的索引数据（4，5，6）中进行查找，然后我们找到了索引值为 5 的行数据。\n    \n\n数据库的索引和数据都是存储在硬盘的，我们可以把读取一个节点当作一次磁盘 I/O 操作。那么上面的整个查询过程一共经历了 3 个节点，也就是进行了 3 次 I/O 操作。\n\nB+Tree 存储千万级的数据只需要 3-4 层高度就可以满足，这意味着从千万级的表查询目标数据最多需要 3-4 次磁盘 I/O，所以B+Tree 相比于 B 树和二叉树来说，最大的优势在于查询效率很高，**因为即使在数据量很大的情况，查询一个数据的磁盘 I/O 依然维持在 3-4次。**\n\n  \n\n### 通过二级索引查询商品数据的过程\n\n主键索引的 B+Tree 和二级索引的 B+Tree 区别如下：\n\n- 主键索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；\n    \n- 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。\n    \n\n我这里将前面的商品表中的 product_no （商品编码）字段设置为二级索引，那么二级索引的 B+Tree 如下图（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）。\n\n其中非叶子的 key 值是 product_no（图中橙色部分），叶子节点存储的数据是主键值（图中绿色部分）。\n\n如果我用 product_no 二级索引查询商品，如下查询语句：\n\n```SQL\nselect * from product where product_no = '0002';\n```\n\n会先检二级索引中的 B+Tree 的索引值（商品编码，product_no），找到对应的叶子节点，然后获取主键值，然后再通过主键索引中的 B+Tree 树查询到对应的叶子节点，然后获取整行数据。这**个过程叫「回表」，也就是说要查两个 B+Tree 才能查到数据**。如下图（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）：\n\n![](statistic/asynccode-258.png)\n\n不过，当查询的数据是能在二级索引的 B+Tree 的叶子节点里查询到，这时就不用再查主键索引查，比如下面这条查询语句：\n\n```SQL\nselect id from product where product_no = '0002';\n```\n\n这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「**覆盖索引」**，也就是只需要查一个 B+Tree 就能找到数据。\n\n  \n\n  \n\n这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「覆盖索引」，也就是只需要查一个 B+Tree 就能找到数据。\n\n### 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？\n\n前面已经讲了 B+Tree 的索引原理，现在就来回答一下 B+Tree 相比于 B 树、二叉树或 Hash 索引结构的优势在哪儿？\n\n之前我也专门写过一篇文章，想详细了解的可以看这篇：「[女朋友问我：为什么 MySQL 喜欢 B+ 树？我笑着画了 20 张图 (opens new window)](https://mp.weixin.qq.com/s/w1ZFOug8-Sa7ThtMnlaUtQ)」，这里就简单做个比对。\n\n_1、B+Tree vs B Tree_\n\nB+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。\n\n另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。\n\n_2、B+Tree vs 二叉树_\n\n对于有 N 个叶子节点的 B+Tree，其搜索复杂度为`O(logdN)`，其中 d 表示节点允许的最大子节点个数为 d 个。\n\n在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。\n\n而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 `O(logN)`，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。\n\n_3、B+Tree vs Hash_\n\nHash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。\n\n但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因\n\n  \n\n## 按物理存储分类\n\n从物理存储的角度来看，索引分为**聚簇索引（主键索引）、二级索引（辅助索引）**。\n\n这两个区别在前面也提到了：\n\n- 主键索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；\n    \n- 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。\n    \n\n所以，在查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是覆盖索引。如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表。\n\n## 按字段特性分类\n\n从字段特性的角度来看，索引分为主键索引、唯一索引、普通索引、前缀索引。\n\n#### 主键索引\n\n主键索引就是建立在主键字段上的索引，通常在创建表的时候一起创建，一张表最多只有一个主键索引，索引列的值不允许有空值。\n\n在创建表时，创建主键索引的方式如下：\n\n```SQL\nCREATE TABLE table_name  (....PRIMARY KEY (index_column_1) USING BTREE);\n```\n\n#### 唯一索引\n\n唯一索引建立在 UNIQUE 字段上的索引，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值。\n\n在创建表时，创建唯一索引的方式如下：\n\n```SQL\nCREATE TABLE table_name  (....UNIQUE KEY(index_column_1,index_column_2,...) \n);\n```\n\n建表后，如果要创建唯一索引，可以使用这面这条命令：\n\n```SQL\nCREATE UNIQUE INDEX index_name\nON table_name(index_column_1,index_column_2,...); \n```\n\n#### 普通索引\n\n普通索引就是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为 UNIQUE。\n\n在创建表时，创建普通索引的方式如下：\n\n```SQL\nCREATE TABLE table_name  (....INDEX(index_column_1,index_column_2,...) \n);\n```\n\n建表后，如果要创建普通索引，可以使用这面这条命令：\n\n```SQL\nCREATE INDEX index_name\nON table_name(index_column_1,index_column_2,...); \n```\n\n#### 前缀索引\n\n前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。\n\n使用前缀索引的目的是为了减少索引占用的存储空间，提升查询效率。\n\n在创建表时，创建前缀索引的方式如下：\n\n```SQL\nCREATE TABLE table_name(\n    column_list,INDEX(column_name(length))); \n```\n\n建表后，如果要创建前缀索引，可以使用这面这条命令：\n\n```SQL\nCREATE INDEX index_name\nON table_name(column_name(length)); \n```\n\n## 按字段个数分类\n\n从字段个数的角度来看，索引分为单列索引、联合索引（复合索引）。\n\n- 建立在单列上的索引称为单列索引，比如主键索引；\n    \n- 建立在多列上的索引称为联合索引；\n    \n\n### 联合索引\n\n通过将**多个字段组合成一个索引，该索引就被称为联合索引**。\n\n比如，将商品表中的 product_no 和 name 字段组合成联合索引`(product_no, name)`，创建联合索引的方式如下：\n\n```SQL\nCREATE INDEX index_product_no_name ON product(product_no, name);\n```\n\n联合索引`(product_no, name)` 的 B+Tree 示意图如下（图中叶子节点之间我画了单向链表，但是实际上是双向链表，原图我找不到了，修改不了，偷个懒我不重画了，大家脑补成双向链表就行）。\n\n![](statistic/asynccode-257.png)\n\n可以看到，联合索引的非叶子节点用两个字段的值作为 B+Tree 的 key 值。当在联合索引查询数据时，先按 product_no 字段比较，在 product_no 相同的情况下再按 name 字段比较。\n\n也就是说，**联合索引查询的 B+Tree 是先按 product_no 进行排序，然后再 product_no 相同的情况再按 name 字段排序。**\n\n因此，**使用联合索引时，存在最左匹配原则，也就是按照最左优先的方式进行索引的匹配。**在使用联合索引进行查询的时候，如果**不遵循「最左匹配原则」，联合索引会失效，这样就无法利用到索引快速查询的特性**了。\n\n```SQL\n如果创建了一个 (a, b, c) 联合索引，如果查询条件是以下这几种，就可以匹配上联合索引：\nwhere a=1；\nwhere a=1 and b=2 and c=3；\nwhere a=1 and b=2；\n需要注意的是，因为有查询优化器，所以 a 字段在 where 子句的顺序并不重要。\n但是，如果查询条件是以下这几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效:\nwhere b=2；\nwhere c=3；\nwhere b=2 and c=3；\n上面这些查询条件之所以会失效，是因为(a, b, c) 联合索引，是先按 a 排序，在 a 相同的情况再按 b 排序，在 b 相同的情况再按 c 排序。所以，b 和 c 是全局无序，局部相对有序的，这样在没有遵循最左匹配原则的情况下，是无法利用到索引的。\n```\n\n  \n\n我这里举联合索引（a，b）的例子，该联合索引的 B+ Tree 如下\n\n![](statistic/asynccode-255.png)\n\n可以看到，a 是全局有序的（1, 2, 2, 3, 4, 5, 6, 7 ,8），而 b 是全局是无序的（12，7，8，2，3，8，10，5，2）。因此，直接执行`where b = 2`这种查询条件没有办法利用联合索引的，利用索引的前提是索引里的 key 是有序的。\n\n**只有在 a 相同，b 才是有序的，**比如 a 等于 2 的时候，b 的值为（7，8），这时就是有序的，这个有序状态是局部的，因此，执行`where a = 2 and b = 7`是 a 和 b 字段能用到联合索引的，也就是联合索引生效了。\n\n#### 联合索引范围查询\n\n联合索引有一些特殊情况，**并不是查询过程使用了联合索引查询，就代表联合索引中的所有字段都用到了联合索引进行索引查询，也就是可能存在部分字段用到联合索引的 B+Tree，部分字段没有用到联合索引的 B+Tree 的情况。**\n\n这种特殊情况就发生在**范围查询**。联合索引的最左匹配原则会一直向右匹配直到遇到「范围查询」就会停止匹配。也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。\n\n  \n\n\u003e Q1: `select * from t_table where a \u003e 1 and b = 2`，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？\n\u003e \n\u003e 由于联合索引（二级索引）是先按照 a 字段的值排序的，所以符合 a \u003e 1 条件的二级索引记录肯定是相邻，于是在进行索引扫描的时候，可以定位到符合 a \u003e 1 条件的第一条记录，然后沿着记录所在的链表向后扫描，直到某条记录不符合 a \u003e 1 条件位置。**所以 a 字段可以在联合索引的 B+Tree 中进行索引查询**。\n\u003e \n\u003e 但是在符合 a \u003e 1 条件的二级索引记录的范围里，b 字段的值是无序的。比如前面图的联合索引的 B+ Tree 里，下面这三条记录的 **a 字段的值都符合 a \u003e 1 查询条件，而 b 字段的值是无序的**：\n\u003e \n\u003e 因此，Q**1 这条查询语句只有 a 字段用到了联合索引进行索引查询，而 b 字段并没有使用到联合索引**。\n\n我们也可以**在执行计划中的 key_len 知道这一点，**在使用联合索引进行查询的时候，通过 k**ey_len 我们可以知道优化器具体使用了多少个字段的搜索条件来形成扫描区间的边界条**件。\n\n举例个例子 ，a 和 b 都是 int 类型且不为 NULL 的字段，那么 Q1 这条查询语句执行计划如下，可以看到 key_len 为 4 字节（如果字段允许为 NULL，就在字段类型占用的字节数上加 1，也就是 5 字节），说明只**有 a 字段用到了联合索引进行索引查询，而且可以看到，即使 b 字段没用到联合索引，key 为 idx_a_b，说明 Q1 查询语句使用了 idx_a_b 联合索引。**\n\n![](statistic/asynccode-261.png)\n\n  \n\n\u003e Q2: `select * from t_table where a \u003e= 1 and b = 2`，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？\n\u003e \n\u003e Q2 和 Q1 的查询语句很像，**唯一的区别就是 a 字段的查询条件「大于等于」。**\n\u003e \n\u003e 由于联合索引（二级索引）是先按照 a 字段的值排序的，所以符合 \u003e= 1 条件的二级索引记录肯定是相邻，于是在进行索引扫描的时候，可以定位到符合 \u003e= 1 条件的第一条记录，然后沿着记录所在的链表向后扫描，直到某条记录不符合 a\u003e= 1 条件位置。**所以 a 字段可以在联合索引的 B+Tree 中进行索引查询**。\n\u003e \n\u003e 虽然在符合 a\u003e= 1 条件的二级索引记录的范围里，**b 字段的值是「无序」的，但是对于符合 a = 1 的二级索引记录的范围里，b 字段的值是「有序」的（因为对于联合索引，是先按照 a 字段的值排序，然后在 a 字段的值相同的情况下，再按照 b 字段的值进行排序）**。\n\u003e \n\u003e 于是，在确定需要扫描的二级索引的范围时，当二级索引记录的 a 字段值为 1 时，可以通过 b = 2 条件减少需要扫描的二级索引记录范围（b 字段可以利用联合索引进行索引查询的意思）。**也就是说，从符合 a = 1 and b = 2 条件的第一条记录开始扫描，而不需要从第一个 a 字段值为 1 的记录开始扫描。**\n\u003e \n\u003e **所以在执行 Q1 这条查询语句的时候，对应的扫描区间是 (2, + ∞)，形成该扫描区间的边界条件是 a \u003e 1，与 b = 2 无关。**\n\u003e \n\u003e 所以，**Q2 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询。**\n\n  \n\n  \n\n\u003e Q3: `SELECT * FROM t_table WHERE a BETWEEN 2 AND 8 AND b = 2`，联合索引（a, b）哪一个字段用到了联合索引的 B+Tree？\n\u003e \n\u003e Q3 查询条件中 `a BETWEEN 2 AND 8` 的意思是查询 a 字段的值在 2 和 8 之间的记录。不同的数据库对 BETWEEN ... AND 处理方式是有差异的。**在 MySQL 中，BETWEEN 包含了 value1 和 value2 边界值，类似于 \u003e= and =\u003c**。而有的数据库则不包含 value1 和 value2 边界值（类似于 \u003e and \u003c）。\n\u003e \n\u003e 这里我们只讨论 MySQL。由于 MySQL 的 BETWEEN 包含 value1 和 value2 边界值，所以类似于 Q2 查询语句，因此 **Q3 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询**。\n\n  \n\n\u003e Q4: `SELECT * FROM t_user WHERE name like 'j%' and age = 22`，联合索引（name, age）哪一个字段用到了联合索引的 B+Tree？\n\u003e \n\u003e 由于联合索引（二级索引）是先按照 name 字段的值排序的，所以前缀为 ‘j’ 的 name 字段的二级索引记录都是相邻的， 于是在进行索引扫描的时候，可以定位到符合前缀为 ‘j’ 的 name 字段的第一条记录，然后沿着记录所在的链表向后扫描，直到某条记录的 name 前缀不为 ‘j’ 为止。\n\u003e \n\u003e 所以 a 字段可以在联合索引的 B+Tree 中进行索引查询，形成的扫描区间是['j','k')。注意， j 是闭区间。如下图：\n\u003e \n\u003e ![](statistic/asynccode-260.png)\n\u003e \n\u003e 虽然在符合前缀为 ‘j’ 的 name 字段的二级索引记录的范围里，age 字段的值是「无序」的，但是对于符合 name = j 的二级索引记录的范围里，age字段的值是「有序」的（因为对于联合索引，是先按照 name 字段的值排序，然后在 name 字段的值相同的情况下，再按照 age 字段的值进行排序）。\n\u003e \n\u003e 于是，在确定需要扫描的二级索引的范围时，**当二级索引记录的 name 字段值为 ‘j’ 时，可以通过 age = 22 条件减少需要扫描的二级索引记录范围（age 字段可以利用联合索引进行索引查询的意思**）。也就是说，从符合 `name = 'j' and age = 22` 条件的第一条记录时开始扫描，而不需要从第一个 name 为 j 的记录开始扫描 。如下图的右边：\n\u003e \n\u003e ![](statistic/asynccode-263.png)\n\u003e \n\u003e 所以**，Q4 这条查询语句 a 和 b 字段都用到了联合索引进行索引查询。**\n\n  \n\n  \n\n综上所示，**联合索引的最左匹配原则，在遇到范围查询（如 \u003e、\u003c）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引**。注意，**对于 \u003e=、\u003c=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配**\n\n#### 索引下推\n\n现在我们知道，对于联合索引（a, b），在执行 `select * from table where a \u003e 1 and b = 2` 语句的时候，**只有 a 字段能用到索引**，那在联合索引的 B+Tree 找到第一个满足条件的主键值（ID 为 2）后，还需要判断其他条件是否满足（看 b 是否等于 2），那是在联合索引里判断？还是回主键索引去判断呢？\n\n- 在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。\n    \n- 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， **可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。**\n    \n\n当你的查询语句的执行计划里，出现了 Extra 为 `Using index condition`，那么说明使用了索引下推的优化。\n\n  \n\n#### 联合索引进行排序\n\n这里出一个题目，针对针对下面这条 SQL，你怎么通过索引来提高查询效率呢？\n\n```SQL\nselect * from order where status = 1 order by create_time asc\n```\n\n有的同学会认为，单独给 status 建立一个索引就可以了。\n\n但是更好的方式给 status 和 create_time 列建立一个联合索引，因为这样**可以避免 MySQL 数据库发生文件排序**。\n\n\u003e 文件排序：[MySQL filesort原理及优化](https://zhuanlan.zhihu.com/p/311933050)\n\n# 索引区分度\n\n区分度就是某个字段 column 不同值的个数「除以」表的总行数，计算公式如下：\n\n![](statistic/asynccode-256.png)\n\n区分度越小，越不适合建立索引；\n\n# 什么时候需要 / 不需要创建索引？\n\n## 什么时候适用索引？\n\n- **字段有唯一性限制的**，比如商品编码；\n    \n- 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。\n    \n- 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，**这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。**\n    \n\n## 什么时候不需要创建索引？\n\n- **`WHERE`** **条件，****`GROUP BY`****，****`ORDER BY`** **里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的**，因为索引是会占用物理空间的。\n    \n- 字段中存在大量重复数据，不需要创建索引，比如性**别字段，只有男女**，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因**为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候**，它一般会忽略索引，进行全表扫描。\n    \n- 表数据太少的时候，不需要创建索引；\n    \n- 经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的\n    \n\n# 优化索引\n\n这里说一下几种常见优化索引的方法：\n\n- 前缀索引优化；\n    \n- 覆盖索引优化；\n    \n- 主键索引最好是自增的；\n    \n- 防止索引失效；\n    \n\n  \n\n  \n\n## 前缀索引优化\n\n前缀索引顾名思义就是使用某个字段中字符串的前几个字符建立索引\n\n使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。\n\n- order by 就无法使用前缀索引；\n    \n- 无法把前缀索引用作覆盖索引；\n    \n\n  \n\n## 覆盖索引优化\n\n覆盖索引是指 SQL 中 query 的所有字段，在索引 **B+Tree 的叶子节点上都能找得到**的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作\n\n## 主键索引最好是自增的\n\n同一个叶子节点内的各个数据是按主键顺序存放的，因此，每当有一条新的数据插入时，数据库会根据主键将其插入到对应的叶子节点中。\n\n**如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，**当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。\n\n**如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入**，**甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂**。\n\n  \n\n主键字段的长度不要太大，因为主**键字段长度越小，意味着二级索引的叶子节点越小**（二级索引的叶子节点存放的数据是主键值）\n\n## 防止索引失效\n\n\u003e [谁还没碰过索引失效呢](https://mp.weixin.qq.com/s/lEx6iRRP3MbwJ82Xwp675w)\n\n  \n\n- 当我们使用左或者**左右模糊匹配**的时候，也就是 `like %xx` 或者 `like %xx%` 这两种方式都会造成索引失效；\n    \n- 当我们在查询条件中对索引列**使用函数**，就会导致索引失效。\n    \n- 当我们在查询条件中对索引列进行**表达式计算**，也是无法走索引的。\n    \n- MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。如果字符串是索引列，而条件语句中的输入参数是数字的话，那么索引列会发生隐式类型转换，由于**隐式类型转换是通过 CAST 函数实现的，等同于对索引列使用了函数**，所以就会导致索引失效。\n    \n- 联合索引要能正确使用需要遵循最左匹配原则，也就是按**照最左优先的方式进行索引的匹配**，否则就会导致索引失效。\n    \n- 在 WHERE 子句中，如果在 **OR 前的条件列是索引列，而在 OR 后的条件列不是索引列**，那么索引会失效。\n    \n\n  \n\n  \n\n# 执行计划\n\n就是一个没有使用索引，并且是一个全表扫描的查询语句。\n\n![](statistic/asynccode-259.png)\n\n## 执行计划参数\n\n- possible_keys 字段表示可能用到的索引；\n    \n- key 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引；\n    \n- key_len 表示**索引的长度**；\n    \n- rows 表示扫描的**数据行数**。\n    \n- type 表示数据**扫描类型**，我们需要重点看这个。\n    \n- extra:其他\n    \n\n## type 字段\n\ntype 字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的执行效率从低到高的顺序为：\n\n- All（全表扫描）；\n    \n    - all 是最坏的情况，因为采用了全表扫描的方式\n        \n- index（全索引扫描）；\n    \n    - index 和 all 差不多，只不过 index 对索引表进行全扫描，这样做的好处是不再需要对数据进行排序，但是开销依然很大\n        \n- range（索引范围扫描）；\n    \n    - range 表示采用了索引范围扫描，一般在 where 子句中使用 \u003c 、\u003e、in、between 等关键词，只检索给定范围的行，属于范围查找\n        \n- ref（非唯一索引扫描）；\n    \n    - ref 类型表示采用了非唯一索引，或者是唯一索引的非唯一性前缀，返回数据返回可能是多条。因为虽然使用了索引，但该索引列的值并不唯一，有重复。这样即使使用索引快速查找到了第一条数据，仍然不能停止，要进行目标值附近的小范围扫描。但它的好处是它并不需要扫全表，因为索引是有序的，即便有重复值，也是在一个非常小的范围内扫描。\n        \n- eq_ref（唯一索引扫描）；\n    \n    - eq_ref 类型是使用主键或唯一索引时产生的访问方式，通常使用在**多表联查**中。比如，对两张表进行联查，\n        \n- const（结果只有一条的主键或唯一索引扫描）。\n    \n    - const 类型表示使用了主键或者唯一索引与常量值进行比较，比如 select name from product where id=1。\n        \n\n## extra字段\n\n关注 extra 显示的结果，这里说几个重要的参考指标：\n\n- Using filesort ：当查询语句中包含 group by 操作，而且**无法利用索引完成排序操作的时候**， 这时不得不选择相应的排序算法进行，**甚至可能会通过文件排序**，效率是很低的，所以要避免这种问题的出现。\n    \n- Using temporary：**使了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by**。效率低，要避免这种问题的出现。\n    \n- Using index：**所需数据只需在索引即可全部获得，不须要再到表中取数据，也就是使用了覆盖索引，避免了回表操作**，效率不错。\n    \n\n  \n\n  \n\n![](statistic/asynccode-262.png)","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E7%B4%A2%E5%BC%95-%E7%90%86%E8%A7%A3-B-%E6%A0%91B+-%E6%A0%91%E7%89%B9%E7%82%B9%E5%8F%8A%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF":{"title":"索引-理解 B 树、B+ 树特点及使用场景","content":"  \n\n\u003e [为什么 MySQL 采用 B+ 树作为索引？](https://xiaolincoding.com/mysql/index/why_index_chose_bpuls_tree.html#%E6%80%8E%E6%A0%B7%E7%9A%84%E7%B4%A2%E5%BC%95%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%98%AF%E5%A5%BD%E7%9A%84)\n\n  \n\n# **B 树**\n\nB 树就是常说的“B 减树（B- 树）”，又名**平衡多路（即不止两个子树）查找树**，它和平衡二叉树的不同有这么几点：\n\n1. 平衡二叉树节点最多有两个子树，而 B 树每个节点可以有多个子树，**M 阶 B 树表示该树每个节点最多有 M 个子树**\n    \n2. 平衡二叉树每个节点只有**一个数据和两个指向孩子的指针**，而 **B 树每个中间节点有 k-1 个关键字（可以理解为数据）和 k 个子树（ **k 介于阶数 M 和 M/2 之间，M/2 向上取整）**\n    \n3. B 树的所有叶子节点都在同一层，并且叶子节点只有关键字，指向孩子的指针为 null\n    \n\n和平衡二叉树相同的点在于：B 树的节点数据大小也是按照左小右大，子树与节点的大小比较决定了子树指针所处位置。\n\n看着概念可能有点难理解，来看看图对比下平衡二叉树和 B 树。\n\n## **对比平衡二叉树和 B 树**\n\n首先是节点， 平衡二叉树的节点如下图所示，每个节点有一个数据和最多两个子树：\n\n![](statistic/asynccode-267.png)\n\nB 树中的每个节点由两部分组成：\n\n1. 关键字（可以理解为数据）\n    \n2. 指向孩子节点的指针\n    \n\nB 树的节点如下图所示，每个节点可以有不只一个数据，同时拥有**数据数加一**个子树，同时每个节点左子树的数据比当前节点都小、右子树的数据都比当前节点的数据大：\n\n![](statistic/asynccode-271.png)\n\n\u003e 上图是为了方便读者理解 B 树每个节点的内容，实际绘制图形还是以圆表示每个节点。\n\n了解了节点的差异后，来看看 B 树的定义，**一棵 B 树必须满足以下条件**：\n\n1. **若根结点不是终端结点，则至少有2棵子树**\n    \n2. **除根节点以外的所有非叶结点至少有 M/2 棵子树，至多有 M 个子树（关键字数为子树减一）**\n    \n3. **所有的叶子结点都位于同一层**\n    \n\n用一张图对比平衡二叉树和 B 树：\n\n![](statistic/asynccode-269.png)\n\n可以看到，**B 树的每个节点可以表示的信息更多，因此整个树更加“矮胖”，这在从磁盘中查找数据（先读取到内存、后查找）的过程中，可以减少磁盘 IO 的次数，从而提升查找速度。**\n\n## **B 树中如何查找数据**\n\n因为 B 树的子树大小排序规则，因此在 B 树中查找数据时，一般需要这样：\n\n1. 从根节点开始，如果查找的数据比根节点小，就去左子树找，否则去右子树\n    \n2. 和子树的多个关键字进行比较，找到它所处的范围，然后去范围对应的子树中继续查找\n    \n3. 以此循环，直到找到或者到叶子节点还没找到为止\n    \n\n## **B 树如何保证平衡**\n\n我们知道，平衡的树之所以能够加快查找速度，是因为在添加、删除的时候做了某些操作以保证平衡。\n\n平衡二叉树的平衡条件是：**左右子树的高度差不大于 1**；而 B 树的平衡条件则有三点：\n\n1. 叶子节点都在同一层\n    \n2. 每个节点的关键字数为子树个数减一（子树个数 k 介于树的阶 M 和它的二分之一）\n    \n3. 子树的关键字保证左小右大的顺序\n    \n\n也就是说，**一棵 3 阶的 B 树（即节点最多有三个子树），每个节点的关键字数最少为 1，最多为 2，如果要添加数据的子树的关键字数已经是最多，就需要拆分节点，调整树的结构。**\n\n网上找到一张很不错的动图，我们来根据它分析下 B 树添加元素时如何保证平衡。\n\n这个图用以表示往 4 阶 B 树中依次插入下面这组数据的过程：\n\n6 10 4 14 5 11 15 3 2 12 1 7 8 8 6 3 6 21 5 15 15 6 32 23 45 65 7 8 6 5 4\n\n![](statistic/asynccode.gif)\n\n**建议放大图查看**。\n\n由于我比较懒，我们来根据前几步分析下 B 树的添加流程：\n\n1. 首先明确：**4 阶 B 树表示每个节点最多有 4 个子树、3 个关键字，最少有 2 个子树、一个关键字**\n    \n2. 添加 6，第一个节点，没什么好说的\n    \n3. 添加 10，根节点最多能放三个关键字，按顺序添到根节点中\n    \n4. 添加 4，还能放到根节点中\n    \n5. 添加 14，这时超出了关键字最大限制，需要把 14 添加为子树，同时为了保证“所有叶子节点在同一层”，就需要拆几个关键字作为子树：\n    \n\n![](statistic/asynccode-268.png)\n\n拆为：\n\n![](statistic/asynccode-266.png)\n\n这个拆的过程比较复杂，首先要确定根节点保留几个关键字，由于**“非叶子节点的根节点至少有 2 棵子树”**的限制，那就至少需要两个关键字分出去，又因为**“子树数是关键字数+1”**，如果根节点有两个关键字，就得有三个子树，无法满足，所以只好把除 6 以外的三个关键字都拆为子树。\n\n谁和谁在一个子树上呢，根据**“左子树比关键字小、右子树比关键字大”**的规律，4 在左子树，10 和 14 在右子树。\n\n继续添加 ：\n\n1. 添加 5，放到 4 所在的子树上\n    \n2. 添加 11，放在 10 和 14 所在的右子树上\n    \n3. 添加 15，按大小应该放到 10、11 和 14 所在的子树上，但因为超过了关键字数限制，又得拆分\n    \n\n![](statistic/asynccode-268.png)\n\n因为**“根节点必须都在同一层”**，因此我们不能给现有的左右子树添加子树，只能添加给 6 了；但是如果 6 有三个子树，就必须得有 2 个关键字，提升谁做关键字好呢，这得看谁做 6 中间的子树，因为右子树的所有关键字都得比父节点的关键字大，所以这个提升的关键字只能比未来右子树中的关键字都小，那就只有 10 和 11 可以考虑了。\n\n提升 10 吧，没有比它小的做子树，那就只能提升 11 了：\n\n![](statistic/asynccode-264.png)\n\n再添加元素也是类似的逻辑：\n\n1. 首先考虑要插入的子树是否已经超出了关键字数的限制\n    \n2. 超出的话，如果要插入的位置是叶子节点，就只能拆一个关键字添加到要插入位置的父节点\n    \n3. 如果非叶子节点，就得从其他子树拆子树给新插入的元素做孩子\n    \n\n删除也是一样的，要考虑删除孩子后，父节点是否还满足子树 k 介于 M/2 和 M 的条件，不满足就得从别的节点拆子树甚至修改相关子树结构来保持平衡。\n\n总之添加、删除的过程很复杂，要考虑的条件很多，具体实现就不细追究了，这里我们有个基本认识即可。\n\n正是这个复杂的保持平衡操作，使得平衡后的 B 树能够发挥出磁盘中快速查找的作用。\n\n## **使用场景**\n\n文件系统和[数据库](https://cloud.tencent.com/solution/database?from=10680)系统中常用的B/B+ 树，他通过对每个节点存储个数的扩展，使得对连续的数据能够进行较快的定位和访问，能够有效减少查找时间，提高存储的空间局部性从而减少IO操作。他广泛用于文件系统及数据库中，如：\n\n- Windows：HPFS 文件系统\n    \n- Mac：HFS，HFS+ 文件系统\n    \n- Linux：ResiserFS，XFS，Ext3FS，JFS 文件系统\n    \n- 数据库：ORACLE，MYSQL，SQLSERVER 等中\n    \n- 数据库：ORACLE，MYSQL，SQLSERVER 等中\n    \n\n# **B+ 树**\n\n了解了 B 树后再来了解下它的变形版：B+ 树，它比 B 树的查询性能更高。\n\n一棵 B+ 树需要满足以下条件：\n\n1. 节点的子树数和关键字数相同（B 树是关键字数比子树数少一）\n    \n2. 节点的关键字表示的是子树中的最大数，在子树中同样含有这个数据\n    \n3. 叶子节点包含了全部数据，同时符合左小右大的顺序\n    \n\n![](statistic/asynccode-265.png)\n\n简单概括下 B+ 树的三个特点：\n\n1. 关键字数和子树相同\n    \n2. 非叶子节点仅用作索引，它的关键字和子节点有重复元素\n    \n3. 叶子节点用指针连在一起\n    \n\n首先第一点不用特别介绍了，在 B 树中，节点的关键字用于在查询时确定查询区间，因此关键字数比子树数少一；而在 B+ 树中，节点的关键字代表子树的最大值，因此关键字数等于子树数。\n\n第二点，除叶子节点外的所有节点的关键字，都在它的下一级子树中同样存在，最后所有数据都存储在叶子节点中。\n\n\u003e 根节点的最大关键字其实就表示整个 B+ 树的最大元素。\n\n第三点，叶子节点包含了全部的数据，并且按顺序排列，B+ 树使用一个链表将它们排列起来，这样在查询时效率更快。\n\n由于 B+ 树的中间节点不含有实际数据，只有子树的最[大数据](https://cloud.tencent.com/solution/bigdata?from=10680)和子树指针，因此磁盘页中可以容纳更多节点元素，也就是说同样数据情况下，B+ 树会 B 树更加“矮胖”，因此查询效率更快。\n\nB+ 树的查找必会查到叶子节点，更加稳定。\n\n**有时候需要查询某个范围内的数据，由于 B+ 树的叶子节点是一个有序链表，只需在叶子节点上遍历即可，不用像 B 树那样挨个中序遍历比较大小。**\n\nB+ 树的三个优点：\n\n1. 层级更低，IO 次数更少\n    \n2. 每次都需要查询到叶子节点，查询性能稳定\n    \n3. 叶子节点形成有序链表，范围查询方便","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E7%B4%A2%E5%BC%95-Count%E6%95%88%E7%8E%87":{"title":"索引-Count效率","content":"  \n\n# Count(*)的实现方式\n\n  \n\n在不同的 MySQL 引擎中，count(*) 有不同的实现方式:\n\n- MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；\n    \n- 而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。\n    \n\n  \n\n  \n\n什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢?\n\n- 这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的\n    \n\n# 不同的 count 用法\n\n  \n\n在 select count(?) from t 这样的查询语句里面，count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能，有哪些差别?\n\n  \n\ncount() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。\n\n- count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；\n    \n- 而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。\n    \n\n  \n\n- 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。\n    \n- 对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。\n    \n- 对于 count(字段) 来说：\n    \n    - 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，**判断不能为 nul**l，按行累加；\n        \n    - 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累\n        \n\n  \n\n  \n\nserver 层要什么字段，InnoDB 就返回什么字段。\n\ncount(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。\n\n  \n\n  \n\n所以结论是：按照效率排序的话，**count(字段)\u003ccount(主键 id)\u003ccount(1)≈count(*)**，所以我建议你，尽量使用 count(*)。\n\n  \n\n# 为什么要通过遍历的方式来计数？\n\n在 MyISAM 存储引擎里，执行 count 函数的方式是不一样的，通常在没有任何查询条件下的 count(*)，MyISAM 的查询速度要明显快于 InnoDB。\n\n而 InnoDB 存储引擎是支持事务的，同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的，所以无法像 MyISAM一样，只维护一个 row_count 变量。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E7%B4%A2%E5%BC%95-MySQL-%E4%BD%BF%E7%94%A8-like-x%E7%B4%A2%E5%BC%95%E4%B8%80%E5%AE%9A%E4%BC%9A%E5%A4%B1%E6%95%88%E5%90%97":{"title":"索引-MySQL 使用 like “%x“，索引一定会失效吗？","content":"  \n\n\u003e [谁还没碰过索引失效呢](https://mp.weixin.qq.com/s/lEx6iRRP3MbwJ82Xwp675w)\n\u003e \n\u003e   \n\n**使用左模糊匹配（like \"%xx\"）并不一定会走全表扫描，关键还是看数据表中的字段。**\n\n**如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。**\n\n![](statistic/asynccode-295.png)\n\n  \n\n# 题目一\n\n数据库表如下，id 是主键索引，name 是二级索引，其他字段都是非索引字段\n\n![](statistic/asynccode-298.png)\n\n这四条模糊匹配的查询语句，**第一条和第二条都会走索引扫描，而且都是选择扫描二级索引（index_name）**，我贴个第二条查询语句的执行计划结果图：\n\n![](statistic/asynccode-296.png)\n\n而**第三和第四条会发生索引失效，执行计划的结果 type= ALL**，代表了全表扫描。\n\n![](statistic/asynccode-297.png)\n\n# 题目二\n\n数据库表特别之处在于，只有两个字段，一个是主键索引 id，另外一个是二级索引 name。\n\n![](statistic/asynccode-292.png)\n\n针对题目 2 的数据表，**第一条和第二条模糊查询语句也是一样可以走索引扫描，第二条查询语句的执行计划如下，Extra 里的 Using index 说明用上了覆盖索引**：\n\n![](statistic/asynccode-299.png)\n\n我们来看一下第三条查询语句的执行计划（第四条也是一样的结果）可**以看到 key=index_name，也就是说用上了二级索引，而且从 Extra 里的 Using index 说明用上了覆盖索引。**\n\n![](statistic/asynccode-294.png)\n\n  \n\n# 这是为什么呢？\n\n首先，这张表的字段**没有「非索引」字段**，所以 `select *` 相当于 `select id,name`，然后这个查询的数据都在二级索引的 B+ 树，**因为二级索引的 B+ 树的叶子节点包含「索引值+主键值」，所以查二级索引的 B+ 树就能查到全部结果了，这个就是覆盖索引。**\n\n但是执行计划里的 type 是 `index`，这代表着是通过全扫描二级索引的 B+ 树的方式查询到数据的，也就是**遍历了整颗索引树。**\n\n而第一和第二条查询语句的执行计划中 type 是 `range`，表示对索引列进行范围查询，也**就是利用了索引树的有序性的特点，通过查询比较的方式，快速定位到了数据行。**\n\n所以，type=range 的查询效率会比 type=index 的高一些。\n\n\u003e 为什么选择全扫描二级索引树，而不扫描聚簇索引树呢？\n\n因为二级索引树的记录东西很少，就只有「索引列+主键值」，而聚簇索引记录的东西会更多，比如聚簇索引中的叶子节点则记录了主键值、事务 id、用于事务和 MVCC 的回滚指针以及所有的剩余列。\n\n再加上，这个 **select * 不用执行回表操作。**\n\n所以， MySQL 优化器认为直接遍历二级索引树要比遍历聚簇索引树的成本要小的多，**因此 MySQL 选择了「全扫描二级索引树」的方式查询数据。**\n\n\u003e 为什么这个数据表加了非索引字段，执行同样的查询语句后，怎么变成走的是全表扫描呢？\n\n加了其他字段后，`select * from t_user where name like \"%xx\";` 要查询的数据就不能只在二级索引树里找了，得需要回表操作才能完成查询的工作，再加上是左模糊匹配，无法利用索引树的有序性来快速定位数据，所以得在二级索引树逐一遍历，获取主键值后，再到聚簇索引树检索到对应的数据行，这样实在太累了。\n\n所以，优化器认为上面这样的查询过程的成本实在太高了，所以直接选择全表扫描的方式来查询数据。\n\n---\n\n从这个思考题我们知道了，**使用左模糊匹配（like \"%xx\"）并不一定会走全表扫描，关键还是看数据表中的字段。**\n\n**如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。**\n\n再说一个相似，我们都知道联合索引要遵循最左匹配才能走索引，但是如果数据库表中的字段都是索引的话，即使查询过程中，没有遵循最左匹配原则，也是走全扫描二级索引树(type=index)，比如下图：\n\n![](statistic/asynccode-293.png)\n\n就说到这了，下次见啦","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E7%B4%A2%E5%BC%95-MySQL-%E5%8D%95%E8%A1%A8%E4%B8%8D%E8%A6%81%E8%B6%85%E8%BF%87-2000W-%E8%A1%8C%E9%9D%A0%E8%B0%B1%E5%90%97":{"title":"索引-MySQL 单表不要超过 2000W 行，靠谱吗","content":"\u003e [mysql 最大建议行数2000w,靠谱吗? - 京东云开发者的个人空间 - OSCHINA - 中文开源技术交流社区](https://my.oschina.net/u/4090830/blog/5559454)\n\n  \n\n# 总结\n\n- MySQL 的表数据是以页的形式存放的，页在磁盘中不一定是连续的。\n    \n- 页的空间是 16K, 并不是所有的空间都是用来存放数据的，会有一些固定的信息，如，页头，页尾，页码，校验码等等。\n    \n- 在 B+ 树中，叶子节点和非叶子节点的数据结构是一样的，区别在于，叶子节点存放的是实际的行数据，而非叶子节点存放的是主键和页号。\n    \n- 索引结构不会影响单表最大行数，2000W 也只是推荐值，超过了这个值可能会导致 B + 树层级更高，影响查询性能。\n    \n\n  \n\n  \n\n# 前置\n\n```SQL\nCREATE TABLE person(\n    id int NOT NULL AUTO_INCREMENT PRIMARY KEY comment '主键',\n    person_id tinyint not null comment '用户id',\n    person_name VARCHAR(200) comment '用户名称',\n    gmt_create datetime comment '创建时间',\n    gmt_modified datetime comment '修改时间') comment '人员信息表';\n```\n\n插入一条数据\n\n```SQL\ninsert into person values(1, 1,'user_1', NOW(), now());\n```\n\n利用 MySQL 伪列 rownum 设置伪列起始点为 1\n\n```SQL\nselect (@i:=@i+1) as rownum, person_name from person, (select @i:=100) as init; \nset @i=1;\n```\n\n运行下面的 sql，连续执行 20 次，就是 2 的 20 次方约等于 100w 的数据；执行 23 次就是 2 的 23 次方约等于 800w , 如此下去即可实现千万测试数据的插入。\n\n如果不想翻倍翻倍的增加数据，而是想少量，少量的增加，有个技巧，就是在 SQL 的后面增加 where 条件，如 id \u003e 某一个值去控制增加的数据量即可。\n\n```SQL\ninsert into person(id, person_id, person_name, gmt_create, gmt_modified)select @i:=@i+1,left(rand()*10,10) as person_id,\nconcat('user_',@i%2048),\ndate_add(gmt_create,interval + @i*cast(rand()*100 as signed) SECOND),\ndate_add(date_add(gmt_modified,interval +@i*cast(rand()*100 as signed) SECOND), interval + cast(rand()*1000000 as signed) SECOND)from person;\n```\n\n此处需要注意的是，也许你在执行到近 800w 或者 1000w 数据的时候，会报错：The total number of locks exceeds the lock table size。\n\n这是由于你的临时表内存设置的不够大，只需要扩大一下设置参数即可。\n\n```SQL\nSET GLOBAL tmp_table_size =512*1024*1024; （512M）\nSET global innodb_buffer_pool_size= 1*1024*1024*1024 (1G);\n```\n\n![](statistic/asynccode-252.png)\n\n# 表空间\n\n下面我们再来看看索引的结构，我们下面讲内容都是基于 Innodb 引擎的，大家都知道 Innodb 的索引内部用的是 B+ 树。\n\n![](statistic/asynccode-251.png)\n\n这张表数据，在硬盘上存储也是类似如此的，它实际是放在一个叫 person.ibd （innodb data）的文件中，也叫做表空间；虽然数据表中，他们看起来是一条连着一条，但**是实际上在文件中它被分成很多小份的数据页，而且每一份都是 16K。**\n\n大概就像下面这样，当然这只是我们抽象出来的，在表空间中还有段、区、组等很多概念，但是我们需要跳出来看。\n\n![](statistic/asynccode-253.png)\n\n# 页的数据结构\n\n实际页的内部结构像是下面这样的：\n\n![](statistic/asynccode-245.png)\n\n从图中可以看出，一个 InnoDB 数据页的存储空间大致被划分成了 7 个部分，有的部分占用的字节数是确定的，有的部分占用的字节数是不确定的。\n\n在页的 7 个组成部分中，我们自己存储的记录会按照我们指定的行格式存储到 `User Records` 部分。\n\n但是在一开始生成页的时候，其实并没有 User Records 这个部分，每当我们插入一条记录，都会从 Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到 User Records 部分。\n\n当 **Free Space 部分的空间全部被 User Records 部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了**。\n\n这个过程的图示如下：\n\n![](statistic/asynccode-247.png)\n\n刚刚上面说到了数据的新增的过程。\n\n那下面就来说说，数据的查找过程，假如我们需要查找一条记录，我们可以把表空间中的每一页都加载到内存中，然后对记录挨个判断是不是我们想要的。\n\n在数据量小的时候，没啥问题，内存也可以撑。但是现实就是这么残酷，不会给你这个局面。\n\n为了解决这问题，MySQL 中就有了索引的概念，大家都知道索引能够加快数据的查询，那到底是怎么个回事呢？下面我就来看看。\n\n# 索引的数据结构\n\n在 MySQL 中索引的数据结构和刚刚描述的页几乎是一模一样的，**而且大小也是 16K**,。\n\n但是在**索引页中记录的是页 (数据页，索引页) 的最小主键 id 和页号**，以及在索引页中增加了层级的信息，从 0 开始往上算，所以页与页之间就有了上下层级的概念。\n\n![](statistic/asynccode-246.png)\n\n看到这个图之后，是不是有点似曾相似的感觉，是不是像一棵二叉树啊，对，没错！它就是一棵树。\n\n只不过我们在这里只是简单画了三个节点，2 层结构的而已，如果数据多了，可能就会扩展到 3 层的树，这个就是我们常说的 B+ 树，最下面那一层的 page level =0, 也就是叶子节点，其余都是非叶子节点。\n\n![](statistic/asynccode-250.png)\n\n看上图中，我们是单拿一个节点来看，首先它是一个非叶子节点（索引页），在它的内容区中有 id 和 页号地址两部分：\n\n- id ：对应页中记录的最小记录 id 值；\n    \n- 页号：地址是指向对应页的指针；\n    \n\n而数据页与此几乎大同小异，区别在于数据页记录的是真实的行数据而不是页地址，而且 id 的也是顺序的。\n\n## 单表建议值\n\n下面我们就以 3 层，2 分叉（实际中是 M 分叉）的图例来说明一下查找一个行数据的过程。\n\n![](statistic/asynccode-248.png)\n\n比如说我们需要查找一个 id=6 的行数据：\n\n- 因为在非叶子节点中存放的是页号和该页最小的 id，所以我们从顶层开始对比，首先看页号 10 中的目录，有 [id=1, 页号 = 20],[id=5, 页号 = 30], 说明左侧节点最小 id 为 1，右侧节点最小 id 是 5。6\u003e5, 那按照二分法查找的规则，肯定就往右侧节点继续查找；\n    \n- 找到页号 30 的节点后，发现这个节点还有子节点（非叶子节点），那就继续比对，同理，6\u003e5 \u0026\u0026 6\u003c7, 所以找到了页号 60；\n    \n- 找到页号 60 之后，发现此节点为叶子节点（数据节点），于是将此页数据加载至内存进行一一对比，结果找到了 id=6 的数据行。\n    \n\n**从上述的过程中发现，我们为了查找 id=6 的数据，总共查询了三个页，如果三个页都在磁盘中（未提前加载至内存），那么最多需要经历三次的磁盘 IO**。\n\n需要注意的是，图中的页号只是个示例，实际情况下并不是连续的，在磁盘中存储也不一定是顺序的。\n\n至此，我们大概已经了解了表的数据是怎么个结构了，也大概知道查询数据是个怎么的过程了，这样我们也就能大概估算这样的结构能存放多少数据了。\n\n从上面的图解我们知道 **B+ 数的叶子节点才是存在数据的，而非叶子节点是用来存放索引数据的**。\n\n所以，同样一个 16K 的页，非叶子节点里的每条数据都指向新的页，而新的页有两种可能\n\n- 如果是叶子节点，那么里面就是一行行的数据\n    \n- 如果是非叶子节点的话，**那么就会继续指向新的页**\n    \n\n假设\n\n- **非叶子节点内指向其他页的数量为 x**\n    \n- **叶子节点内能容纳的数据行数为 y**\n    \n- **B+ 数的层数为 z**\n    \n\n如下图中所示，**Total =x^(z-1) *y 也就是说总数会等于 x 的 z-1 次方 与 Y 的乘积**。\n\n![](statistic/asynccode-249.png)\n\n\u003e X =？\n\n在文章的开头已经介绍了页的结构，索引也也不例外，都会有 **File Header (38 byte)、Page Header (56 Byte)、Infimum + Supermum（26 byte）、File Trailer（8byte）, 再加上页目录，大概 1k 左右。**\n\n我们就当做它就是 1K, 那整个页的大小是 16K, **剩下 15k 用于存数据，在索引页中主要记录的是主键与页号，主键我们假设是 Bigint (8 byte), 而页号也是固定的（4Byte）**, **那么索引页中的一条数据也就是 12byte**。\n\n**所以 x=15*1024/12≈1280 行**。\n\n\u003e Y=？\n\n叶子节点和非叶子节点的结构是一样的，同理，**能放数据的空间也是 15k**。\n\n但是叶子节点中存放的是真正的行数据，这个影响的因素就会多很多，比如，字段的类型，字段的数量。每行数据占用空间越大，页中所放的行数量就会越少。\n\n这边我们暂时**按一条行数据 1k 来算，那一页就能存下 15 条**，Y = 15*1024/1000 ≈15。\n\n算到这边了，是不是心里已经有谱了啊。\n\n根据上述的公式，**Total =x^(z-1) *y，已知 x=1280，y=15：**\n\n- 假设 B+ 树是两层，那就是 z = 2， Total = （1280 ^1 ）*15 = 19200\n    \n- 假设 B+ 树是三层，那就是 z = 3， Total = （1280 ^2） *15 = 24576000 （约 2.45kw）\n    \n\n哎呀，妈呀！这不是正好就是文章开头说的最大行数建议值 2000W 嘛！对的**，一般 B+ 数的层级最多也就是 3 层。**\n\n\u003e 到这里难道就完了？\n\n不。\n\n我们刚刚在说 Y 的值时候假设的是 1K ，那比如我实际当行的数据占用空间不是 1K , 而是 5K, 那么单个数据页最多只能放下 3 条数据。\n\n同样，还是按照 z = 3 的值来计算，那 Total = （1280 ^2） *3 = 4915200 （近 500w）\n\n所以，在保持相同的层级（相似查询性能）的情况下，在行数据大小不同的情况下，其实这个最大建议值也是不同的，而且影响查询性能的还有很多其他因素，比如，数据库版本，服务器配置，sql 的编写等等。\n\n**MySQL 为了提高性能，会将表的索引装载到内存中，在 InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。**\n\n但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降，所以增加硬件配置（比如把内存当磁盘使），可能会带来立竿见影的性能提升哈。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E9%94%81":{"title":"锁","content":"根据加锁的范围可以分为全局锁，表级锁，行锁。\n\n![[Pasted image 20230727224159.png]]\n\n# 全局锁\n\n全局锁就是对整个数据库实例加锁。\n\n## 全局锁的用法\n\n**全局读锁**：MySQL 提供了一个加全局读锁的方法，命令是 **Flush tables with read lock** (FTWRL)\n\n![](statistic/asynccode-10.jpg)\n\nFTWRL\n\n- FTWRL 前有读写的话 **,FTWRL 都会等待 读写执行完毕后才执行**\n    \n- FTWRL 执行的时候**要刷脏页的数据到磁盘**,因为要保持数据的一致性 ，理解的执行FTWRL时候是 所有事务 都提交完毕的时候\n    \n\n**FTWRL主要应用于全库逻辑备份**\n\n## **全局读锁用于备份的局限**\n\n- 如果你在主库上备份，那么在备份期间都不能执行更新，**业务基本上就得停摆**；\n    \n- 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致**主从延迟**\n    \n\n## 然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免\n\n但是不使用全局锁。比如先备份一张表，再备份另一张表，会破坏一致性。\n\n数据库的引擎支持的事务支持**可重复读的隔离级别**，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。\n\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的\n\nFTWRL的作用\n\n- single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。\n    \n\n# 表级锁\n\n  \n![[Pasted image 20230727224225.png]]\n\n  \n\n## 表锁\n\n  \n\n对学生表加锁\n\n```Go\n//表级别的共享锁，也就是读锁；\nlock tables t_student read;\n\n//表级别的独占锁，也就是写锁；\nlock tables t_stuent write;\n```\n\n- 表锁除了会限制别的线程的读写外，**也会限制本线程接下来的读写操作。**\n    \n- 就是说如果本线程对学生表加了**「共享表锁」**，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。\n    \n\n  \n\n## 元数据锁\n\n  \n\n我们对数据库表进行操作时，**会自动给这个表加上 MDL：**\n\n- 对一张表进行 CRUD 操作时，加的是 **MDL 读锁**；\n    \n- 对一张表做**结构变更操作的时候**，加的是 **MDL 写锁**；\n    \n\n### 互斥关系\n\n- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。\n    \n- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。\n    \n\n### MDL 不需要显示调用，那它是在什么时候释放的?\n\n**MDL 是在事务提交后才会释放，这意味着事务执行期间，MDL 是一直持有的**。\n\n### 锁升级\n\n![](statistic/asynccode-317.png)\n\nsession A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。\n\n之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。\n\n如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了\n\n**online ddl 对表操作增加字段的过程**\n\n1. 拿MDL写锁\n    \n2. 降级成MDL读锁\n    \n3. 真正做DDL\n    \n4. 升级成MDL写锁\n    \n5. 释放MDL锁\n    \n\n1、2、4、5如果没有锁冲突，执行时间非常短。第3步占用了DDL绝大部分时间，这期间这个表可以正常读写数据，是因此称为“online ”\n\n  \n\n  \n\n### 为什么线程 C 因为申请不到 MDL 写锁，而导致后续的申请读锁的查询操作也会被阻塞？\n\n申请 **MDL 锁的操作会形成一个队列，队列中写锁获取优先级高于读锁**，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作。\n\n### **如何安全的给小表加字段**\n\n1. 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁\n    \n2. 在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。\n    \n\n## 意向锁\n\n- 在使用 InnoDB 引擎的表里对某些记录**加上「共享锁」之前**，需要先在表级别加上一个「意向共享锁」；\n    \n- 在使用 InnoDB 引擎的表里对某些纪录**加上「独占锁」之前**，需要先在表级别加上一个「意向独占锁」；\n    \n\n也就是，**当执行插入、更新、删除操作，需要先对表加上「意向独占锁」**，然后对该记录加独占锁。\n\n  \n\n**意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（**_**lock tables ... read**_**）和独占表锁（**_**lock tables ... write**_**）发生冲突。**\n\n意向锁的目的是为了**快速判断表里是否有记录被加锁**\n\n  \n\n## AUTO_INC锁\n\n  \n\n数据库会自动给主键赋值递增的值，这主要是通过 **AUTO-INC 锁实现的。**\n\nAUTO-INC 锁是特殊的表锁机制，**锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。**\n\n# 行锁\n\nInnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。\n\n共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。\n\n![](statistic/asynccode-318.png)\n\n  \n![[Pasted image 20230727224245.png]]\n### Record Lock\n\nRecord Lock 称为记录锁，**锁住的是一条记录**。而且记录锁是有 S 锁和 X 锁之分的：\n\n- （S 型与 S 锁兼容）\n    \n- （S 型与 X 锁不兼容）\n    \n- （X 型与 X 锁不兼容）\n    \n\n### Gap Lock\n\nGap Lock 称为**间隙锁，只存在于可重复读隔离级别**，目的是**为了解决可重复读隔离级别下幻读的现象。**\n\n- 表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。\n    \n\n![](statistic/asynccode-315.png)\n\n间隙锁虽然**存在 X 型间隙锁和 S 型间隙锁**，但是并没有什么区别，间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目**的是防止插入幻影记录而提出的**。\n\n  \n\n### Next-Key Lock\n\nNext-Key Lock 称为临键锁，是 Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。\n\n假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改 id = 5 这条记录。\n\n![](statistic/asynccode-318.png)\n\nt-key lock **即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中。**\n\nnext-key lock **是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。\n\n虽然相同范围的间隙锁是多个事务相互兼容的，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的。\n\n  \n\n## 插入意向锁\n\n- 一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。\n    \n    - 如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻）\n        \n    - 在此期间会生成一个**插入意向锁**，表明**有事务想在某个区间插入新记录**，但是现在处于等待状态\n        \n\n  \n\n假设事务 A 已经对表加了一个范围 id 为（3，5）间隙锁。\n\n![](statistic/asynccode-316.png)\n\n当事务 A 还没提交的时候，事务 B 向该表插入一条 id = 4 的新记录，这时会判断到插入的位置已经被事务 A 加了间隙锁，于是事物 B 会生成一个插入意向锁**，然后将锁的状态设置为等待状态**\n\n\u003e _MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁_\n\n插入意向锁名字虽然有意向锁，但是它并不是意向锁，**它是一种特殊的间隙锁，属于行级别锁**。\n\n如果说间隙锁锁住的是一个区间，那么「**插入意向锁」锁住的就是一个点**。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。\n\n# 两阶段锁\n\n**在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**\n\n  \n\n**如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。**\n\n  \n\n需要提供死锁检测机制\n\n# 死锁和死锁检测\n\n出现死锁后的两种策略\n\n- 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。\n    \n- 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。\n    \n\n通常我们使用第二种。\n\n**如何解决热点数据更新导致的性能问题**\n\n- 在软件应用层就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。\n    \n- 另一个思路是控制并发度。限流。\n    \n- 将一行改成逻辑上的多行来减少锁冲突，如影院的账户总额等于这 10 个记录的值的总和","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E9%94%81-MySQL%E6%AD%BB%E9%94%81%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E":{"title":"锁-MySQL死锁了，怎么办","content":"# 死锁的发生\n\n我建了一张订单表，其中 **id 字段为主键索引，order_no 字段普通索引**，也就是非唯一索引：\n\n```SQL\nCREATE TABLE `t_order` (\n    `id` int NOT NULL AUTO_INCREMENT,\n    `order_no` int DEFAULT NULL,\n    `create_date` datetime DEFAULT NULL,\n    PRIMARY KEY (`id`),\n    KEY `index_order` (`order_no`) \n    USING BTREE\n) ENGINE=InnoDB ;\n```\n\n  \n\n![](statistic/asynccode-352.png)\n\n假设这时有两事务，一个事务要插入订单 1007 ，另外一个事务要插入订单 1008，因为需要对订单做幂等性校验，所以两个事务先要查询该订单是否存在，不存在才插入记录，过程如下：\n\n![](statistic/asynccode-348.png)\n\n两个事务都陷入了等待状态（前提没有打开死锁检测），也就是发生了死锁，因为都在相互等待对方释放锁。\n\n# 为啥会产生死锁的\n\nInnodb 引擎为了解决「可重复读」隔离级别下的幻读问题，就引出了 next-key 锁，它是记录锁和间隙锁的组合。\n\n- Record Lock，记录锁，锁的是记录本身；\n    \n- Gap Lock，间隙锁，锁的就是两个值之间的空隙，以防止其他事务在这个空隙间插入新的数据，从而避免幻读现象。\n    \n\n![](statistic/asynccode-347.png)\n\n在**线上千万不要执行没有带索引条件的 update 语句，不然会造成业务停滞，我有个读者就因为干了这个事情，然后被老板教育了一波**\n\n  \n\n**插入意向锁与间隙锁是冲突的，所以当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。而间隙锁与间隙锁之间是兼容的，所以所以两个事务中** **`select ... for update`** **语句并不会相互影响**。\n\n  \n\n## 为什么间隙锁与间隙锁之间是兼容的？\n\n**间隙锁的意义只在于阻止区间被插入，因此是可以共存的**。一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁，**共享和排他的间隙锁是没有区别的，他们相互不冲突**，且功能相同，即两个事务可以同时持有包含共同间隙的间隙锁。\n\n  \n\n这里的共同间隙包括两种场景：\n\n- 其一是两个间隙锁的间隙区间完全一样；\n    \n- 其二是一个间隙锁包含的间隙区间是另一个间隙锁包含间隙区间的子集。\n    \n\n**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。**\n\n  \n\n## 插入意向锁是什么？\n\n插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，**该锁只用于并发插入操作**\n\n插入意向锁与间隙锁的另一个非常重要的差别是：**尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。**\n\n  \n\n  \n\n# Insert 语句如何加行级锁的\n\ninsert 语句在正常执行时是不会生成锁结构的，它是靠聚簇索引记录自带的 trx_id 隐藏列来作为隐式锁来保护记录的。\n\n## 什么是隐式锁？\n\n当事务需要加锁的时，如果这个锁不可能发生冲突，InnoDB会跳过加锁环节，这种机制称为隐式锁。隐式锁是 InnoDB 实现的一种延迟加锁机制，其特点是只有在可能发生冲突时才加锁，从而减少了锁的数量，提高了系统整体性能。\n\n隐式锁就是在 Insert 过程中不加锁，只有在特殊情况下，才会将隐式锁转换为显示锁，这里我们列举两个场景。\n\n- **如果记录之间加有间隙锁，为了避免幻读，此时是不能插入记录的**；\n    \n- **如果 Insert 的记录和已有记录存在唯一键冲突，此时也不能插入记录**\n    \n\n  \n\n## 记录之间有间隙锁\n\n**每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，如果已加间隙锁，此时会生成一个插入意向锁，然后锁的状态设置为等待状态**（_PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁_），现象就是 Insert 语句会被阻塞。\n\n  \n\n## 遇到唯一键冲突\n\n如果在插入新记录时，插入了一个与「**已有的记录的主键或者唯一二级索引列值相同**」的记录（不过可以有多条记录的唯一二级索引列的值同时为NULL，这里不考虑这种情况），**此时插入就会失败，然后对于这条记录加上了 S 型的锁。**\n\n- 如果主键索引重复，插入新记录的事务会给已存在的主键值重复的聚簇索引记录添加 S 型记录锁。\n    \n- 如果唯一二级索引重复，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录添加 S 型 next-key 锁。\n    \n\n### 主键索引冲突\n\n下面举个「主键冲突」的例子，MySQL 8.0 版本，事务隔离级别为可重复读（默认隔离级别）。\n\nt_order 表中的 id 字段为主键索引，并且已经存在 id 值为 5 的记录，此时有个事务，插入了一条 id 为 5 的记录，就会报主键索引冲突的错误。\n\n![](statistic/asynccode-351.png)\n\n但是除了报错之外，还做一个很重要的事情，就是对 id 为 5 **的这条记录加上了 S 型的记录锁**。\n\n可以执行 `select * from performance_schema.data_locks\\G;` 语句，确定事务加了什么锁。\n\n![](statistic/asynccode-354.png)\n\n可以看到，主键索引为 5 （LOCK_DATA）的这条记录中加了锁类型为 S 型的记录锁。注意，这里 LOCK_TYPE 中的 RECORD 表示行级锁，而不是记录锁的意思。如果是 S 型记录锁的话，LOCK_MODE 会显示 `S, REC_NOT_GAP`。\n\n所以，在隔离级别是「可重复读」的情况下，如果在插入数据的时候，发生了主键索引冲突，插入新记录的事务会给已存在的主键值重复的聚簇索引记录添加 S 型记录锁。\n\n### 唯一二级索引冲突。\n\nt_order 表中的 order_no 字段为唯一二级索引，并且已经存在 order_no 值为 1001 的记录，此时事务 A，插入了 order_no 为 1001 的记录，就出现了报错。\n\n![](statistic/asynccode-349.png)\n\n但是除了报错之外，还做一个很重要的事情，就**是对 order_no 值为 1001 这条记录加上了 S 型的 next-key 锁**。\n\n我们可以执行 `select * from performance_schema.data_locks\\G;` 语句 ，确定事务加了什么类型的锁，这里只关注在记录上加锁的类型。\n\n![](statistic/asynccode-352.png)\n\n可以看到**，index_order 二级索引加了 S 型的 next-key 锁**，范围是(-∞, 1001]。注意，这里 LOCK_TYPE 中的 RECORD 表示行级锁，而不是记录锁的意思。如果是记录锁的话，LOCK_MODE 会显示 `S, REC_NOT_GAP`。\n\n此时，事务 B 执行了 select * from t_order where order_no = 1001 for update; 就会阻塞，因为这条语句想加 X 型的锁，是与 S 型的锁是冲突的，所以就会被阻塞。\n\n![](statistic/asynccode-352.png)\n\n我们也可以从 performance_schema.data_locks 这个表中看到，事务 B 的状态（LOCK_STATUS）是等待状态，加锁的类型 X 型的记录锁（LOCK_MODE: X,REC_NOT_GAP ）。\n\n![](statistic/asynccode-355.png)\n\n上面的案例是针对唯一二级索引重复而插入失败的场景。\n\n# 分析两个事务执行过程中，执行了相同的 insert 语句的场景。\n\n现在 t_order 表中，只有这些数据，order_no 为唯一二级索引。\n\n![](statistic/asynccode-350.png)\n\n在隔离级别可重复读的情况下，开启两个事务，前后执行相同的 Insert 语句，此时事务 B 的 Insert 语句会发生阻塞。\n\n![](statistic/asynccode-346.png)\n\n两个事务的加锁过程：\n\n- 事**务 A 先插入 order_no 为 1006 的记录，可以插入成功，此时对应的唯一二级索引记录被「隐式锁」保护，此时还没有实际的锁结构**（执行完这里的时候，你可以看查 performance_schema.data_locks 信息，可以看到这条记录是没有加任何锁的）；\n    \n- 接着，事务 B 也插入 order_no 为 1006 的记录，**由于事务 A 已经插入 order_no 值为 1006 的记录，所以事务 B 在插入二级索引记录时会遇到重复的唯一二级索引列值，此时事务 B 想获取一个 S 型 next-key 锁**，但是事务 A 并未提交，**事务 A 插入的 order_no 值为 1006 的记录上的「隐式锁」会变「显示锁」且锁类型为 X 型的记录锁**，所以事务 B 向获取 S 型 next-key 锁时会遇到锁冲突，事务 B 进入阻塞状态。\n    \n\n我们可以执行 `select * from performance_schema.data_locks\\G;` 语句 ，确定事务加了什么类型的锁，这里只关注在记录上加锁的类型。\n\n先看事务 A 对 order_no 为 1006 的记录加了什么锁？\n\n从下图可以看到，事务 A 对 order_no 为 1006 记录加上了类型为 X 型的记录锁（_注意，这个是在执行事务 B 之后才产生的锁，没执行事务 B 之前，该记录还是隐式锁_）。\n\n![](statistic/asynccode-352.png)\n\n然后看事务 B 想对 order_no 为 1006 的记录加什么锁？\n\n从下图可以看到，事务 B 想对 order_no 为 1006 的记录加 S 型的 next-key 锁，但是由于事务 A 在该记录上持有了 X 型的记录锁，这两个锁是冲突的，所以导致事务 B 处于等待状态。\n\n![](statistic/asynccode-352.png)\n\n从这个实验可以得知，**并发多个事务的时候，第一个事务插入的记录，并不会加锁，而是会用隐式锁保护唯一二级索引的记录**。\n\n但是当第一个事务还未提交的时候，有其他事务插入了与第一个事务相同的记录，第二个事务就会被阻塞，因为此时第一事务插入的记录中的隐式锁会变为显示锁且类型是 X 型的记录锁，而第二个事务是想对该记录加上 S 型的 next-key 锁，X 型与 S 型的锁是冲突的，所以导致第二个事务会等待，直到第一个事务提交后，释放了锁。\n\n如果 order_no 不是唯一二级索引，那么两个事务，前后执行相同的 Insert 语句，是不会发生阻塞的，就如前面的这个例子。\n\n![](statistic/asynccode-353.png)\n\n# 如何避免死锁？\n\n死锁的四个必要条件：互斥、占有且等待、不可强占用、循环等待。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。\n\n在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：\n\n- 设置事务等待锁的超时时间。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。\n    \n- 当发生超时后，就出现下面这个提示：\n    \n\n![](statistic/asynccode-348.png)\n\n- 开启主动死锁检测。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。\n    \n- 当检测到死锁后，就会出现下面这个提示：\n    \n\n![](statistic/asynccode-349.png)\n\n上面这个两种策略是「当有死锁发生时」的避免方式。\n\n我们可以回归业务的角度来预防死锁，对订单做幂等性校验的目的是为了保证不会出现重复的订单，那我们可以直接将 order_no 字段设置为唯一索引列，利用它的唯一性来保证订单表不会出现重复的订单，不过有一点不好的地方就是在我们插入一个已经存在的订单记录时就会抛出异常。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E9%94%81-MySQL-%E5%8F%AF%E4%BB%A5%E9%98%B2%E6%AD%A2%E5%88%A0%E9%99%A4%E6%93%8D%E4%BD%9C%E5%AF%BC%E8%87%B4%E7%9A%84%E5%B9%BB%E8%AF%BB%E5%90%97":{"title":"锁-MySQL 可以防止删除操作导致的幻读吗？","content":"昨天有位读者在美团二面的时候，被问到关于幻读的问题：\n\n![](statistic/asynccode-13.jpg)\n\n面试官反问的大概意思是，MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读吗？\n\n答案是可以的。\n\n接下来，通过几个小实验来证明这个结论吧，顺便再帮大家复习一下记录锁+间隙锁。\n\n## [#](https://xiaolincoding.com/mysql/lock/lock_phantom.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%B9%BB%E8%AF%BB)什么是幻读？\n\n首先来看看 MySQL 文档是怎么定义幻读（Phantom Read）的:\n\n_The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row._\n\n翻译：当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。\n\n举个例子，假设一个事务在 T1 时刻和 T2 时刻分别执行了下面查询语句，途中没有执行其他任何语句：\n\n```SQL\nSELECT * FROM t_test WHERE id \u003e 100;\n```\n\n只要 T1 和 T2 时刻执行产生的结果集是不相同的，那就发生了幻读的问题，比如：\n\n- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 6 条行记录，那就发生了幻读的问题。\n    \n- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 4 条行记录，也是发生了幻读的问题。\n    \n\n\u003e MySQL 是怎么解决幻读的？\n\nMySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了，详见这篇[文章 (opens new window)](https://xiaolincoding.com/mysql/transaction/phantom.html)），解决的方案有两种：\n\n- 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。\n    \n- 针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。\n    \n\n## [#](https://xiaolincoding.com/mysql/lock/lock_phantom.html#%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81)实验验证\n\n接下来，来验证「 MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读问题」的结论。\n\n实验环境：MySQL 8.0 版本，可重复读隔离级。\n\n现在有一张用户表（t_user），表里只有一个主键索引，表里有以下行数据：\n\n![](statistic/asynccode-358.png)\n\n现在有一个 A 事务执行了一条查询语句，查询到年龄大于 20 岁的用户共有 6 条行记录。\n\n![](statistic/asynccode-357.png)\n\n然后， B 事务执行了一条删除 id = 2 的语句：\n\n![](statistic/asynccode-356.png)\n\n此时，B 事务的删除语句就陷入了等待状态，说明是无法进行删除的。\n\n因此，MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读问题。\n\n### [#](https://xiaolincoding.com/mysql/lock/lock_phantom.html#%E5%8A%A0%E9%94%81%E5%88%86%E6%9E%90)加锁分析\n\n问题来了，A 事务在执行 select ... for update 语句时，具体加了什么锁呢？\n\n我们可以通过 `select * from performance_schema.data_locks\\G;` 这条语句，查看事务执行 SQL 过程中加了什么锁。\n\n输出的内容很多，共有 11 行信息，我删减了一些不重要的信息：\n\n![](statistic/asynccode-361.png)\n\n从上面输出的信息可以看到，共加了两种不同粒度的锁，分别是：\n\n- 表锁（`LOCK_TYPE: TABLE`）：X 类型的意向锁；\n    \n- 行锁（`LOCK_TYPE: RECORD`）：X 类型的 next-key 锁；\n    \n\n这里我们重点关注「行锁」，图中 `LOCK_TYPE` 中的 `RECORD` 表示行级锁，而不是记录锁的意思：\n\n- 如果 LOCK_MODE 为 `X`，说明是 next-key 锁；\n    \n- 如果 LOCK_MODE 为 `X, REC_NOT_GAP`，说明是记录锁；\n    \n- 如果 LOCK_MODE 为 `X, GAP`，说明是间隙锁；\n    \n\n然后通过 `LOCK_DATA` 信息，可以确认 next-key 锁的范围，具体怎么确定呢？\n\n- 根据我的经验，如果 LOCK_MODE 是 next-key 锁或者间隙锁，那么 LOCK_DATA 就表示锁的范围最右值，而锁范围的最左值为 LOCK_DATA 的上一条记录的值。\n    \n\n因此，此时事务 A 在主键索引（`INDEX_NAME : PRIMARY`）上加了 10 个 next-key 锁，如下：\n\n- X 型的 next-key 锁，范围：(-∞, 1]\n    \n- X 型的 next-key 锁，范围：(1, 2]\n    \n- X 型的 next-key 锁，范围：(2, 3]\n    \n- X 型的 next-key 锁，范围：(3, 4]\n    \n- X 型的 next-key 锁，范围：(4, 5]\n    \n- X 型的 next-key 锁，范围：(5, 6]\n    \n- X 型的 next-key 锁，范围：(6, 7]\n    \n- X 型的 next-key 锁，范围：(7, 8]\n    \n- X 型的 next-key 锁，范围：(8, 9]\n    \n- X 型的 next-key 锁，范围：(9, +∞]\n    \n\n这相当于把整个表给锁住了，其他事务在对该表进行增、删、改操作的时候都会被阻塞。\n\n只有在事务 A 提交了事务，事务 A 执行过程中产生的锁才会被释放。\n\n\u003e 为什么只是查询年龄 20 岁以上行记录，而把整个表给锁住了呢？\n\n这是因为事务 A 的这条查询语句是全表扫描，锁是在遍历索引的时候加上的，并不是针对输出的结果加锁。\n\n![](statistic/asynccode-359.png)\n\n因此，在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。\n\n\u003e 如果对 age 建立索引，事务 A 这条查询会加什么锁呢？\n\n接下来，我对 age 字段建立索引，然后再执行这条查询语句：\n\n![](statistic/asynccode-357.png)\n\n接下来，继续通过 `select * from performance_schema.data_locks\\G;` 这条语句，查看事务执行 SQL 过程中加了什么锁。\n\n具体的信息，我就不打印了，我直接说结论吧。\n\n因为表中有两个索引，分别是主键索引和 age 索引，所以会分别对这两个索引加锁。\n\n主键索引会加如下的锁：\n\n- X 型的记录锁，锁住 id = 2 的记录；\n    \n- X 型的记录锁，锁住 id = 3 的记录；\n    \n- X 型的记录锁，锁住 id = 5 的记录；\n    \n- X 型的记录锁，锁住 id = 6 的记录；\n    \n- X 型的记录锁，锁住 id = 7 的记录；\n    \n- X 型的记录锁，锁住 id = 8 的记录；\n    \n\n分析 age 索引加锁的范围时，要先对 age 字段进行排序。\n\n![](statistic/asynccode-363.png)\n\nage 索引加的锁：\n\n- X 型的 next-key lock，锁住 age 范围 (19, 21] 的记录；\n    \n- X 型的 next-key lock，锁住 age 范围 (21, 21] 的记录；\n    \n- X 型的 next-key lock，锁住 age 范围 (21, 23] 的记录；\n    \n- X 型的 next-key lock，锁住 age 范围 (23, 23] 的记录；\n    \n- X 型的 next-key lock，锁住 age 范围 (23, 39] 的记录；\n    \n- X 型的 next-key lock，锁住 age 范围 (39, 43] 的记录；\n    \n- X 型的 next-key lock，锁住 age 范围 (43, +∞] 的记录；\n    \n\n化简一下，age 索引 next-key 锁的范围是 (19, +∞]。\n\n可以看到，对 age 字段建立了索引后，查询语句是索引查询，并不会全表扫描，因此不会把整张表给锁住。\n\n![](statistic/asynccode-360.png)\n\n总结一下，在对 age 字段建立索引后，事务 A 在执行下面这条查询语句后，主键索引和 age 索引会加下图中的锁。\n\n![](statistic/asynccode-362.png)\n\n事务 A 加上锁后，事务 B、C、D、E 在执行以下语句都会被阻塞。\n\n![](statistic/asynccode-364.png)\n\n## [#](https://xiaolincoding.com/mysql/lock/lock_phantom.html#%E6%80%BB%E7%BB%93)总结\n\n在 MySQL 的可重复读隔离级别下，针对当前读的语句会对索引加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。\n\n有一点要注意的是，在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。\n\n完！","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E9%94%81-MySQL-%E6%98%AF%E5%A6%82%E4%BD%95%E5%8A%A0%E9%94%81%E7%9A%84":{"title":"锁-MySQL 是如何加锁的？","content":"InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁，所以后面的内容都是基于 InnoDB 引擎的。\n\n## 什么 SQL 语句会加行级锁？\n\n普通的 select 语句是**不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的。**\n\n  \n\n  \n\n如果要在查询时对记录加行级锁，可以使用下面这两个方式，这两种查询会加锁的语句称**为锁定读**。\n\n```Go\n//对读取的记录加共享锁(S型锁)\nselect ... lock in share mode;\n//对读取的记录加独占锁(X型锁)\nselect ... for update;\n```\n\n上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上\n\n除了上面这两条锁定读语句会加行级锁之外，**update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)。**\n\n```Go\n//对操作的记录加独占锁(X型锁)\nupdate table .... where id = 1;、\n\n//对操作的记录加独占锁(X型锁)\ndelete from table where id = 1;\n```\n\n![](statistic/asynccode-332.png)\n\n共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。\n\n  \n\n# MySQL是怎么加行级锁的\n\n## 行级锁加锁对象\n\n索引\n\n## 加锁的基本单位\n\nNext-key lock\n\n**next-key lock 是前开后闭区间，而间隙锁是前开后开区间。**\n\n## Next-key 锁在一定的场景会退化为**记录锁或者间隙锁**\n\n**在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成退化成记录锁或间隙锁**\n\n  \n\n## 唯一索引等值查询\n\n当我们用唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同：\n\n- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，**将该记录的索引中的 next-key lock 会退化成「记录锁」**。\n    \n- 当查询的记录是「不存在」的，在**索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」。**\n    \n\n  \n\n  \n\n#### 索引加的位置\n\n- 「唯一索引」是用「主键索引」作为案例说明的，**加锁只加在主键索引项上**。\n    \n- **如果是用二级索引（不管是不是非唯一索引，还是唯一索引）进行锁定读查询的时候，除了会对二级索引项加行级锁（如果是唯一索引的二级索引，加锁规则和主键索引的案例相同），而且还会对查询到的记录的主键索引项上加「记录锁」**。\n    \n\n  \n\n  \n\n### 1、记录存在的情况\n\n假设事务 A 执行了这条等值查询语句，查询的记录是「存在」于表中的。\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where id = 1 for update;+----+--------+-----+| id | name   | age |+----+--------+-----+|  1 | 路飞   |  19 |+----+--------+-----+1 row in set (0.02 sec)\n```\n\n那么，**事务 A 会为 id 为 1 的这条记录就会加上 X 型的记录锁**。\n\n![](statistic/asynccode-330.png)\n\n接下来，如果有其他事务，对 id 为 1 的记录进行更新或者删除操作的话，这些操作都会被阻塞，**因为更新或者删除操作也会对记录加 X 型的记录锁，而 X 锁和 X 锁之间是互斥关系。**\n\n比如，下面这个例子：\n\n![](statistic/asynccode-331.png)\n\n因为事务 A 对 id = 1的记录加了 X 型的记录锁，所以事务 B 在修改 id=1 的记录时会被阻塞，事务 C 在删除 id=1 的记录时也会被阻塞。\n\n#### 有什么命令可以分析加了什么锁？\n\n我们可以通过 `select * from performance_schema.data_locks\\G;` 这条语句，查看事务执行 SQL 过程中加了什么锁。\n\n我们以前面的事务 A 作为例子，分析下下它加了什么锁。\n\n![](statistic/asynccode-335.png)\n\n从上图可以看到，共加了两个锁，分别是：\n\n- 表锁：X 类型的意向锁；\n    \n- 行锁：X 类型的记录锁；\n    \n\n这里我们重点关注行级锁，图中 **LOCK_TYPE 中的 RECORD 表示行级锁，而不是记录锁的意思**。\n\n通过 LOCK_MODE 可以确认是 next-key 锁，还是间隙锁，还是记录锁：\n\n- 如果 LOCK_MODE 为 `X`，说明是 next-key 锁；\n    \n- 如果 LOCK_MODE 为 `X, REC_NOT_GAP`，说明是记录锁；\n    \n- 如果 LOCK_MODE 为 `X, GAP`，说明是间隙锁；\n    \n\n**此时事务 A 在 id = 1 记录的主键索引上加的是记录锁，锁住的范围是 id 为 1 的这条记录。**这样其他事务就无法对 id 为 1 的这条记录进行更新和删除操作了。\n\n从这里我们也可以得知，**加锁的对象是针对索引，因为这里查询语句扫描的 B+ 树是聚簇索引树，即主键索引树，所以是对主键索引加锁。将对应记录的主键索引加 记录锁后，就意味着其他事务无法对该记录进行更新和删除操作了。**\n\n#### 为什么唯一索引等值查询并且查询记录存在的场景下，该记录的索引中的 next-key lock 会退化成记录锁？\n\n原因就是在唯一索引等值查询并且查询记录存在的场景下，仅靠记录锁也能避免幻读的问题。\n\n幻读的定义就是，当一个事务前后两次查询的结果集，不相同时，就认为发生幻读**。所以，要避免幻读就是避免结果集某一条记录被其他事务删除，或者有其他事务插入了一条新记录**，\n\n- 由于主键具有唯一性，**所以其他事务插入 id = 1 的时候，会因为主键冲突，导致无法插入 id = 1 的新记录**。这样事务 A 在多次查询 id = 1 的记录的时候，不会出现前后两次查询的结果集不同，也就避免了幻读的问题。\n    \n- 由于对 id = 1 加了记录锁，**其他事务无法删除该记录**，这样事务 A 在多次查询 id = 1 的记录的时候，不会出现前后两次查询的结果集不同，也就避免了幻读的问题。\n    \n\n### 2、记录不存在的情况\n\n假设事务 A 执行了这条等值查询语句，查询的记录是「不存在」于表中的。\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where id = 2 for update;\nEmpty set (0.03 sec)\n```\n\n接下来，通过 `select * from performance_schema.data_locks\\G;` 这条语句，查看事务执行 SQL 过程中加了什么锁。\n\n![](statistic/asynccode-341.png)\n\n从上图可以看到，共加了两个锁，分别是：\n\n- 表锁：X 类型的意向锁；\n    \n- 行锁：X 类型的间隙锁；\n    \n\n因此，**此时事务 A 在 id = 5 记录的主键索引上加的是间隙锁，锁住的范围是 (1, 5)。**\n\n![](statistic/asynccode-330.png)\n\n- 如果有其他事务插入 id 值为 2、3、4 这一些记录的话，这些插入语句都会发生阻塞。\n    \n- 如果其他事务插入的 id = 1 或者 id = 5 的记录话，并不会发生阻塞，而是报主键冲突的错误，因为表中已经存在 id = 1 和 id = 5 的记录了。\n    \n\n比如，下面这个例子：\n\n![](statistic/asynccode-334.png)\n\n因为事务 A 在 id = 5 记录的主键索引上加了范围为 (1, 5) 的 X 型间隙锁，所以事务 B 在插入一条 id 为 3 的记录时会被阻塞住，即无法插入 id = 3 的记录。\n\n#### 间隙锁的范围`(1, 5)` ，是怎么确定的？\n\n根据我的经验，如果 LOCK_MODE 是 next-key 锁或者间隙锁，那么 LOCK_DATA 就表示锁的范围「右边界」，此次的事务 A 的 LOCK_DATA 是 5。\n\n然后锁范围的「左边界」是表中 id 为 5 的上一条记录的 id 值，即 1。\n\n因此，间隙锁的范围`(1, 5)`。\n\n#### 为什么唯一索引等值查询并且查询记录「不存在」的场景下，在索引树找到第一条大于该查询记录的记录后，要将该记录的索引中的 next-key lock 会退化成「间隙锁」？\n\n原因就是在**唯一索引等值查询并且查询记录不存在的场景下，仅靠间隙锁就能避免幻读**的问题。\n\n- 为什么 id = 5 记录上的主键索引的锁不可以是 next-key lock？\n    \n    - 如果是 next-key lock，就意味着其他事务无法删除 id = 5 这条记录，但是这次的案例是查询 id = 2 的记录，只要保证前后两次查询 id = 2 的结果集相同，就能避免幻读的问题了，所以即使 id =5 被删除，也不会有什么影响，那就没必须加 next-key lock，因此只需要在 id = 5 加间隙锁，避免其他事务插入 id = 2 的新记录就行了。\n        \n- 为什么不可以针对不存在的记录加记录锁？\n    \n    - 锁是加在索引上的，**而这个场景下查询的记录是不存在的，自然就没办法锁住这条不存在的记录**。\n        \n\n## 唯一索引范围查询\n\n**当唯一索引进行范围查询时，会对每一个扫描到的索引加 next-key 锁，然后如果遇到下面这些情况，会退化成记录锁或者间隙锁：**\n\n  \n\n  \n\n- 情况一：针对「大于等于」的范围查询，因为存在等值查询的条件，**那么如果等值查询的记录是存在于表中，那么该记录的索引中的 next-key 锁会退化成记录锁**。\n    \n- 情况二：针对「小于或者小于等于」的范围查询，要看条件值的记录是否存在于表中：\n    \n    - 当条件值的记录不在表中，那么不管是「小于」还是「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。\n        \n    - 当条件值的记录在表中，如果是「小于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁；如果「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引 next-key 锁不会退化成间隙锁。其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。\n        \n![[Pasted image 20230727224459.png]]\n  \n\n  \n\n  \n\n  \n\n### 1、针对「大于或者大于等于」的范围查询\n\n#### 实验一：针对「大于」的范围查询的情况。\n\n假设事务 A 执行了这条范围查询语句：\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where id \u003e 15 for update;\n+----+-----------+-----+\n| id | name      | age |\n+----+-----------+-----+\n| 20 | 香克斯    |  39 |\n+----+-----------+-----+\n1 row in set (0.01 sec)\n```\n\n事务 A 加锁变化过程如下：\n\n1. 最开始要找的第一行是 id = 20，由于查询该记录不是一个等值查询（不是大于等于条件查询），**所以对该主键索引加的是范围为 (15, 20] 的 next-key 锁**；\n    \n2. 由于是范围查找，就会继续往后找存在的记录，虽然我们看见表中最后一条记录是 id = 20 的记录，但是实际在 Innodb 存储引擎中，会用一个特殊的记录来标识最后一条记录，**该特殊的记录的名字叫 supremum pseudo-record** ，所以扫描第二行的时候，也就扫描到了这个特殊记录的时候，**会对该主键索引加的是范围为 (20, +∞] 的 next-key 锁。**\n    \n3. 停止扫描。\n    \n\n可以得知，事务 A 在主键索引上加了两个 X 型 的 next-key 锁：\n\n![](statistic/asynccode-334.png)\n\n- 在 id = 20 这条记录的主键索引上，加了范围为 (15, 20] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 20 的记录，同时无法插入 id 值为 16、17、18、19 的这一些新记录。\n    \n- 在特殊记录（ supremum pseudo-record）的主键索引上，加了范围为 (20, +∞] 的 next-key 锁，意味着其他事务无法插入 id 值大于 20 的这一些新记录。\n    \n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-336.png)\n\n从上图中的分析中，也可以得到事务 A 在主键索引上加了两个 X 型 的next-key 锁：\n\n- 在 id = 20 这条记录的主键索引上，加了范围为 (15, 20] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 20 的记录，同时无法插入 id 值为 16、17、18、19 的这一些新记录。\n    \n- 在特殊记录（ supremum pseudo-record）的主键索引上，加了范围为 (20, +∞] 的 next-key 锁，意味着其他事务无法插入 id 值大于 20 的这一些新记录。\n    \n\n#### 实验二：针对「大于等于」的范围查询的情况。\n\n假设事务 A 执行了这条范围查询语句：\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where id \u003e= 15 for update\n;+----+-----------+-----+\n| id | name      | age |\n+----+-----------+-----+\n| 15 | 乌索普    |  20 |\n| 20 | 香克斯    |  39 |\n+----+-----------+-----+\n2 rows in set (0.00 sec)\n```\n\n事务 A 加锁变化过程如下：\n\n1. 最开始要找的**第一行是 id = 15，由于查询该记录是一个等值查询（等于 15），所以该主键索引的 next-key 锁会退化成记录锁**，也就是仅锁住 id = 15 这一行记录。\n    \n2. 由于是范围查找，就会继续往后找存在的记录，**扫描到的第二行是 id = 20，于是对该主键索引加的是范围为 (15, 20] 的 next-key 锁；**\n    \n3. 接着扫描到第三行的时候，扫**描到了特殊记录（ supremum pseudo-record），于是对该主键索引加的是范围为 (20, +∞] 的 next-key 锁**。\n    \n4. 停止扫描。\n    \n\n可以得知，事务 A 在主键索引上加了三个 X 型 的锁，分别是：\n\n![](statistic/asynccode-333.png)\n\n- **在 id = 15 这条记录的主键索引上，加了记录锁**，范围是 id = 15 这一行记录；意味着其他事务无法更新或者删除 id = 15 的这一条记录；\n    \n- **在 id = 20 这条记录的主键索引上，加了 next-key 锁，范围是 (15, 20]** 。意味着其他事务即无法更新或者删除 id = 20 的记录，同时无法插入 id 值为 16、17、18、19 的这一些新记录。\n    \n- **在特殊记录（ supremum pseudo-record）的主键索引上，加了 next-key 锁，范围是 (20, +∞]** 。意味着其他事务无法插入 id 值大于 20 的这一些新记录。\n    \n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-340.png)\n\n通过前面这个实验，我们证明了：\n\n- 针对「大于等于」条件的唯一索引范围查询的情况下， 如果条件值的记录存在于表中，那么由于查询该条件值的记录是包含一个等值查询的操作，所以该记录的索引中的 next-key 锁会退化成记录锁。\n    \n\n### 2、针对「小于或者小于等于」的范围查询\n\n#### 实验一：针对「小于」的范围查询时，查询条件值的记录「不存在」表中的情况。\n\n假设事务 A 执行了这条范围查询语句，注意查询条件值的记录**（id 为 6）并不存在于表中**。\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where id \u003c 6 for update;\n+----+--------+-----+\n| id | name   | age |\n+----+--------+-----+\n|  1 | 路飞   |  19 |\n|  5 | 索隆   |  21 |\n+----+--------+-----+\n3 rows in set (0.00 sec)\n```\n\n事务 A 加锁变化过程如下：\n\n1. 最开始要找的第一行是 id = 1，于是对该主键索引加的是**范围为 (-∞, 1] 的 next-key 锁**；\n    \n2. 由于是范围查找，就会继续往后找存在的记录，扫描到的第二行是 **id = 5，所以对该主键索引加的是范围为 (1, 5] 的 next-key 锁**；\n    \n3. 由于扫描到的第二行记录（id = 5），满足 id \u003c 6 条件，而且也没有达到终止扫描的条件，接着会继续扫描。\n    \n4. 扫描到的第三行是 id = 10，该记录不满足 id \u003c 6 条件的记录，所以 id = 10 这一行记录的锁会退化成间隙锁，于是对该**主键索引加的是范围为 (5, 10) 的间隙锁**。\n    \n5. 由于扫描到的第三行记录（id = 10），不满足 id \u003c 6 条件，达到了终止扫描的条件，于是停止扫描。\n    \n\n从上面的分析中，可以得知事务 A 在主键索引上加了三个 X 型的锁：\n\n![](statistic/asynccode-334.png)\n\n- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 1 的这一条记录，同时也无法插入 id 小于 1 的这一些新记录。\n    \n- 在 id = 5 这条记录的主键索引上，加了范围为 (1, 5] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 5 的这一条记录，同时也无法插入 id 值为 2、3、4 的这一些新记录。\n    \n- 在 id = 10 这条记录的主键索引上，加了范围为 (5, 10) 的间隙锁，意味着其他事务无法插入 id 值为 6、7、8、9 的这一些新记录。\n    \n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-343.png)\n\n从上图中的分析中，也可以得知事务 A 在主键索引加的三个锁，就是我们前面分析出那三个锁。\n\n虽然这次范围查询的条件是「小于」，但是查询条件值的记录不存在于表中（ id 为 6 的记录不在表中），所以如果事务 A 的范围查询的条件改成 \u003c= 6 的话，加的锁还是和范围查询条件为 \u003c 6 是一样的。 大家自己也验证下这个结论。\n\n因此，针对「小于或者小于等于」的唯一索引范围查询，如果条件值的记录不在表中，那么不管是「小于」还是「小于等于」的范围查询，扫描到终止范围查询的记录时，该记录中索引的 next-key 锁会退化成间隙锁，其他扫描的记录，则是在这些记录的索引上加 next-key 锁。\n\n#### 实验二：针对「小于等于」的范围查询时，查询条件值的记录「存在」表中的情况。\n\n假设事务 A 执行了这条范围查询语句，注意查询条件值的记录（id 为 5）存在于表中。\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where id \u003c= 5 for update;+----+--------+-----+| id | name   | age |+----+--------+-----+|  1 | 路飞   |  19 ||  5 | 索隆   |  21 |+----+--------+-----+2 rows in set (0.00 sec)\n```\n\n事务 A 加锁变化过程如下：\n\n1. 最开始要找的第一行是 id = 1，**于是对该记录加的是范围为 (-∞, 1] 的 next-key 锁；**\n    \n2. 由于是范围查找，就会继续往后找存在的记录，**扫描到的第二行是 id = 5，于是对该记录加的是范围为 (1, 5] 的 next-key 锁。**\n    \n3. 由于主键索引具有唯一性，不会存在两个 id = 5 的记录，所以不会再继续扫描，于是停止扫描。\n    \n\n从上面的分析中，可以得到事务 A 在主键索引上加了 2 个 X 型的锁：\n\n![](statistic/asynccode-334.png)\n\n- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁。意味着其他事务即无法更新或者删除 id = 1 的这一条记录，同时也无法插入 id 小于 1 的这一些新记录。\n    \n- 在 id = 5 这条记录的主键索引上，加了范围为 (1, 5] 的 next-key 锁。意味着其他事务即无法更新或者删除 id = 5 的这一条记录，同时也无法插入 id 值为 2、3、4 的这一些新记录。\n    \n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-339.png)\n\n从上图中的分析中，可以得到事务 A 在主键索引上加了两个 X 型 next-key 锁，分别是：\n\n- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁；\n    \n- 在 id = 5 这条记录的主键索引上，加了范围为(1, 5 ] 的 next-key 锁。\n    \n\n#### 实验三：再来看针对「小于」的范围查询时，查询条件值的记录「存在」表中的情况。\n\n如果事务 A 的查询语句是小于的范围查询，且查询条件值的记录（id 为 5）存在于表中。\n\n```SQL\nselect * from user where id \u003c 5 for update;\n```\n\n事务 A 加锁变化过程如下：\n\n1. **最开始要找的第一行是 id = 1，于是对该记录加的是范围为 (-∞, 1] 的 next-key 锁**；\n    \n2. 由于是范围查找，就会继续往后找存在的记录，扫描到的第二行是 id = 5**，该记录是第一条不满足 id \u003c 5 条件的记录，于是该记录的锁会退化为间隙锁，锁范围是 (1,5)。**\n    \n3. 由于找到了第一条不满足 id \u003c 5 条件的记录，于是停止扫描。\n    \n\n可以得知，此时事务 A 在主键索引上加了两种 X 型锁：\n\n![](statistic/asynccode-338.png)\n\n- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 1 的这一条记录，同时也无法插入 id 小于 1 的这一些新记录。\n    \n- 在 id = 5 这条记录的主键索引上，加了范围为 (1,5) 的间隙锁，意味着其他事务无法插入 id 值为 2、3、4 的这一些新记录。\n    \n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-341.png)\n\n从上图中的分析中，可以得到事务 A 在主键索引上加了 X 型的范围为 (-∞, 1] 的 next-key 锁，和 X 型的范围为 (1, 5) 的间隙锁。\n\n因此，通过前面这三个实验，可以得知。\n\n在针对「小于或者小于等于」的唯一索引（主键索引）范围查询时，存在这两种情况会将索引的 next-key 锁会退化成间隙锁的：\n\n- 当条件值的记录「不在」表中时，那么不管是「小于」还是「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的主键索引中的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的主键索引上加 next-key 锁。\n    \n- 当条件值的记录「在」表中时：\n    \n    - 如果是「小于」条件的范围查询，扫描到终止范围查询的记录时，该记录的主键索引中的 next-key 锁会退化成间隙锁，其他扫描到的记录，都是在这些记录的主键索引上，加 next-key 锁。\n        \n    - 如果是「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的主键索引中的 next-key 锁「不会」退化成间隙锁，其他扫描到的记录，都是在这些记录的主键索引上加 next-key 锁。\n        \n\n## 非唯一索引等值查询\n\n当我们用非唯一索引进行等值查询的时候，**因为存在两个索引，一个是主键索引，一个是非唯一索引（二级索引），所以在加锁时，同时会对这两个索引都加锁，但是对主键索引加锁的时候，只有满足查询条件的记录才会对它们的主键索引加锁。**\n\n针对非唯一索引等值查询时，查询的记录存不存在，加锁的规则也会不同：\n\n- 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。\n    \n- 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。\n    \n\n接下里用两个实验来说明。\n\n### 1、记录不存在的情况\n\n#### 实验一：针对非唯一索引等值查询时，查询的值不存在的情况。\n\n先来说说非唯一索引等值查询时，查询的记录不存在的情况，因为这个比较简单。\n\n假设事务 A 对非唯一索引（age）进行了等值查询，且表中不存在 age = 25 的记录。\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where age = 25 for update;\nEmpty set (0.00 sec)\n```\n\n事务 A 加锁变化过程如下：\n\n- 定位到第一条不符合查询条件的二级索引记录，即扫描到 age = 39，于是该**二级索引的 next-key 锁会退化成间隙锁，范围是 (22, 39)。**\n    \n- 停止查询\n    \n\n事务 A 在 age = 39 记录的二级索引上，加了 X 型的间隙锁，范围是 (22, 39)。意味着其他事务无法插入 age 值为 23、24、25、26、....、38 这些新记录。不过**对于插入 age = 22 和 age = 39 记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，会在后面说**。\n\n![](statistic/asynccode-341.png)\n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-336.png)\n\n从上图的分析，可以看到，事务 A 在 age = 39 记录的二级索引上（INDEX_NAME: index_age ），加了范围为 (22, 39) 的 X 型间隙锁。\n\n此时，如果有其他事务插入了 age 值为 23、24、25、26、....、38 这些新记录，那么这些插入语句都会发生阻塞。不过对于插入 age = 39 记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，接下来我们就说！\n\n##### 当有一个事务持有二级索引的间隙锁 (22, 39) 时，什么情况下，可以让其他事务的插入 age = 22 或者 age = 39 记录的语句成功？又是什么情况下，插入 age = 22 或者 age = 39 记录时的语句会被阻塞？\n\n我们先要清楚，什么情况下插入语句会发生阻塞。\n\n插入语句在插入一条记录之前，需要先定位到该记录在 B+树 的位置，如果插入的位置的下一条记录的索引上有间隙锁，才会发生阻塞。\n\n在分析二级索引的间隙锁是否可以成功插入记录时，我们要先要知道二级索引树是如何存放记录的？\n\n二级索引树是按照二级索引值（age列）按顺序存放的，在相同的二级索引值情况下， 再按主键 id 的顺序存放。知道了这个前提，我们才能知道执行插入语句的时候，插入的位置的下一条记录是谁。\n\n基于前面的实验，事务 A 是在 age = 39 记录的二级索引上，加了 X 型的间隙锁，范围是 (22, 39)。\n\n插入 age = 22 记录的成功和失败的情况分别如下：\n\n- 当其他事务插入一条 age = 22，id = 3 的记录的时候，在二级索引树上定位到插入的位置，而该位置的下一条是 id = 10、age = 22 的记录，该记录的二级索引上没有间隙锁，所以这条插入语句可以执行成功。\n    \n- 当其他事务插入一条 age = 22，id = 12 的记录的时候，在二级索引树上定位到插入的位置，而该位置的下一条是 id = 20、age = 39 的记录，正好该记录的二级索引上有间隙锁，所以这条插入语句会被阻塞，无法插入成功。\n    \n\n插入 age = 39 记录的成功和失败的情况分别如下：\n\n- 当其他事务插入一条 age = 39，id = 3 的记录的时候，在二级索引树上定位到插入的位置，而该位置的下一条是 id = 20、age = 39 的记录，正好该记录的二级索引上有间隙锁，所以这条插入语句会被阻塞，无法插入成功。\n    \n- 当其他事务插入一条 age = 39，id = 21 的记录的时候，在二级索引树上定位到插入的位置，而该位置的下一条记录不存在，也就没有间隙锁了，所以这条插入语句可以插入成功。\n    \n\n所以，当有一个事务持有二级索引的间隙锁 (22, 39) 时，插入 age = 22 或者 age = 39 记录的语句是否可以执行成功，关键还要考虑插入记录的主键值，因为「二级索引值（age列）+主键值（id列）」才可以确定插入的位置，确定了插入位置后，就要看插入的位置的下一条记录是否有间隙锁，如果有间隙锁，就会发生阻塞，如果没有间隙锁，则可以插入成功。\n\n知道了这个结论之后，我们再回过头看，非唯一索引等值查询时，查询的记录不存在时，执行`select * from performance_schema.data_locks\\G;` 输出的结果。\n\n![](statistic/asynccode-337.png)\n\n在前面分析输出结果的时候，我说的结论是：「_事务 A 在 age = 39 记录的二级索引上（INDEX_NAME: index_age ），加了范围为 (22, 39) 的 X 型间隙锁_」。这个结论其实还不够准确，因为只考虑了 LOCK_DATA 第一个数值（39），没有考虑 LOCK_DATA 第二个数值（20）。\n\n那 `LOCK_DATA：39，20` 是什么意思？\n\n- LOCK_DATA 第一个数值，也就是 39， 它代表的是 age 值。从前面我们也知道了，LOCK_DATA 第一个数值是 next-key 锁和间隙锁锁住的范围的右边界值。\n    \n- LOCK_DATA 第二个数值，也就是 20， 它代表的是 id 值。\n    \n\n之所以 LOCK_DATA 要多显示一个数值（ID值），是因为针对「当某个事务持有非唯一索引的 (22, 39) 间隙锁的时候，其他事务是否可以插入 age = 39 新记录」的问题，还需要考虑插入记录的 id 值。而 LOCK_DATA 的第二个数值，就是说明在插入 age = 39 新记录时，哪些范围的 id 值是不可以插入的。\n\n因此， `LOCK_DATA：39，20` + `LOCK_MODE : X, GAP` 的意思是，事务 A 在 age = 39 记录的二级索引上（INDEX_NAME: index_age ），加了 age 值范围为 (22, 39) 的 X 型间隙锁，**同时针对其他事务插入 age 值为 39 的新记录时，不允许插入的新记录的 id 值小于 20** 。如果插入的新记录的 id 值大于 20，则可以插入成功。\n\n但是我们无法从`select * from performance_schema.data_locks\\G;` 输出的结果分析出「在插入 age =22 新记录时，哪些范围的 id 值是可以插入成功的」，这时候就得自己画出二级索引的 B+ 树的结构，然后确定插入位置后，看下该位置的下一条记录是否存在间隙锁，如果存在间隙锁，则无法插入成功，如果不存在间隙锁，则可以插入成功。\n\n### 2、记录存在的情况\n\n\u003e 实验二：针对非唯一索引等值查询时，查询的值存在的情况。\n\n假设事务 A 对非唯一索引（age）进行了等值查询，且表中存在 age = 22 的记录。\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where age = 22 for update;+----+--------+-----+| id | name   | age |+----+--------+-----+| 10 | 山治   |  22 |+----+--------+-----+1 row in set (0.00 sec)\n```\n\n事务 A 加锁变化过程如下：\n\n- 由于不是唯一索引，所以肯定存在值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，**最开始要找的第一行是 age = 22，于是对该二级索引记录加上范围为 (21, 22] 的 next-key 锁。**同时，**因为 age = 22 符合查询条件，于是对 age = 22 的记录的主键索引加上记录锁，即对 id = 10 这一行加记录锁。**\n    \n- **接着继续扫描，扫描到的第二行是 age = 39，该记录是第一个不符合条件的二级索引记录，所以该二级索引的 next-key 锁会退化成间隙锁，范围是 (22, 39)**。\n    \n- 停止查询。\n    \n\n可以看到，事务 A 对主键索引和二级索引都加了 X 型的锁：\n\n![](statistic/asynccode-345.png)\n\n- 主键索引：\n    \n    - 在 id = 10 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 10 的这一行记录。\n        \n- 二级索引（非唯一索引）：\n    \n    - 在 age = 22 这条记录的二级索引上，加了范围为 (21, 22] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 22 的这一些新记录，不过对于插入 age = 20 和 age = 21 新记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，会在后面说。\n        \n    - 在 age = 39 这条记录的二级索引上，加了范围 (22, 39) 的间隙锁。意味着其他事务无法插入 age 值为 23、24、..... 、38 的这一些新记录。不过对于插入 age = 22 和 age = 39 记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，会在后面说。\n        \n\n我们也可以通过 `select * from performance_schema.data_locks\\G;` 这条语句来看看事务 A 加了什么锁。\n\n输出结果如下，我这里只截取了行级锁的内容。\n\n![](statistic/asynccode-344.png)\n\n从上图的分析，可以看到，事务 A 对二级索引（INDEX_NAME: index_age ）加了两个 X 型锁，分别是：\n\n- 在 age = 22 这条记录的二级索引上，加了范围为 (21, 22] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 22 的这一些新记录，针对是否可以插入 age = 21 和 age = 22 的新记录，分析如下：\n    \n    - 是否可以插入 age = 21 的新记录，还要看插入的新记录的 id 值，如果插入 age = 21 新记录的 id 值小于 5，那么就可以插入成功，因为此时插入的位置的下一条记录是 id = 5，age = 21 的记录，该记录的二级索引上没有间隙锁。如果插入 age = 21 新记录的 id 值大于 5，那么就无法插入成功，因为此时插入的位置的下一条记录是 id = 10，age = 22 的记录，该记录的二级索引上有间隙锁。\n        \n    - 是否可以插入 age = 22 的新记录，还要看插入的新记录的 id 值，从 `LOCK_DATA : 22, 10` 可以得知，其他事务插入 age 值为 22 的新记录时，如果插入的新记录的 id 值小于 10，那么插入语句会发生阻塞；如果插入的新记录的 id 大于 10，还要看该新记录插入的位置的下一条记录是否有间隙锁，如果没有间隙锁则可以插入成功，如果有间隙锁，则无法插入成功。\n        \n- 在 age = 39 这条记录的二级索引上，加了范围 (22, 39) 的间隙锁。意味着其他事务无法插入 age 值为 23、24、..... 、38 的这一些新记录，针对是否可以插入 age = 22 和 age = 39 的新记录，分析如下：\n    \n    - 是否可以插入 age = 22 的新记录，还要看插入的新记录的 id 值，如果插入 age = 22 新记录的 id 值小于 10，那么插入语句会被阻塞，无法插入，因为此时插入的位置的下一条记录是 id = 10，age = 22 的记录，该记录的二级索引上有间隙锁（ age = 22 这条记录的二级索引上有 next-key 锁）。如果插入 age = 21 新记录的 id 值大于 10，也无法插入，因为此时插入的位置的下一条记录是 id = 20，age = 39 的记录，该记录的二级索引上有间隙锁。\n        \n    - 是否可以插入 age = 39 的新记录，还要看插入的新记录的 id 值，从 `LOCK_DATA : 39, 20` 可以得知，其他事务插入 age 值为 39 的新记录时，如果插入的新记录的 id 值小于 20，那么插入语句会发生阻塞，如果插入的新记录的 id 大于 20，则可以插入成功。\n        \n\n同时，事务 A 还对主键索引（INDEX_NAME: PRIMARY ）加了 X 型的记录锁：\n\n- 在 id = 10 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 10 的这一行记录。\n    \n\n\u003e 为什么这个实验案例中，需要在二级索引索引上加范围 (22, 39) 的间隙锁？\n\n要找到这个问题的答案，我们要明白 MySQL 在可重复读的隔离级别场景下，为什么要引入间隙锁？其实是为了避免幻读现象的发生。\n\n如果这个实验案例中：\n\n```SQL\nselect * from user where age = 22 for update;\n```\n\n如果事务 A 不在二级索引索引上加范围 (22, 39) 的间隙锁，只在二级索引索引上加范围为 (21, 22] 的 next-key 锁的话，那么就会有幻读的问题。\n\n前面我也说过，在非唯一索引上加了范围为 (21, 22] 的 next-key 锁，是无法完全锁住 age = 22 新记录的插入，因为对于是否可以插入 age = 22 的新记录，还要看插入的新记录的 id 值，从 `LOCK_DATA : 22, 10` 可以得知，其他事务插入 age 值为 22 的新记录时，如果插入的新记录的 id 值小于 10，那么插入语句会发生阻塞，如果插入的新记录的 id 值大于 10，则可以插入成功。\n\n也就是说，只在二级索引索引（非唯一索引）上加范围为 (21, 22] 的 next-key 锁，其他事务是有可能插入 age 值为 22 的新记录的（比如插入一个 age = 22，id = 12 的新记录），那么如果事务 A 再一次查询 age = 22 的记录的时候，前后两次查询 age = 22 的结果集就不一样了，这时就发生了幻读的现象。\n\n那么当在 age = 39 这条记录的二级索引索引上加了范围为 (22, 39) 的间隙锁后，其他事务是无法插入一个 age = 22，id = 12 的新记录，因为当其他事务插入一条 age = 22，id = 12 的新记录的时候，在二级索引树上定位到插入的位置，而该位置的下一条是 id = 20、age = 39 的记录，正好该记录的二级索引上有间隙锁，所以这条插入语句会被阻塞，无法插入成功，这样就避免幻读现象的发生。\n\n所以，为了避免幻读现象的发生，就需要在二级索引索引上加范围 (22, 39) 的间隙锁。\n\n## 非唯一索引范围查询\n\n非唯一索引和主键索引的范围查询的加锁也有所不同，不同之处在于非唯一索引范围查询，索引的 next-key lock 不会有退化为间隙锁和记录锁的情况，也就是非唯一索引进行范围查询时，对二级索引记录加锁都是加 next-key 锁。\n\n就带大家简单分析一下，事务 A 的这条范围查询语句：\n\n```SQL\nmysql\u003e begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql\u003e select * from user where age \u003e= 22  for update;+----+-----------+-----+| id | name      | age |+----+-----------+-----+| 10 | 山治      |  22 || 20 | 香克斯    |  39 |+----+-----------+-----+2 rows in set (0.01 sec)\n```\n\n事务 A 的加锁变化：\n\n- 最开始要找的第一行是 age = 22，**虽然范围查询语句包含等值查询，但是这里不是唯一索引范围查询，所以是不会发生退化锁的现象，因此对该二级索引记录加 next-key 锁，范围是 (21, 22]**。同时，对 age = 22 这条记录的主键索引加记录锁，即对 id = 10 这一行记录的主键索引加记录锁。\n    \n- 由于是范围查询，接着继续扫描已经存在的二级索引记录。扫**面的第二行是 age = 39 的二级索引记录，于是对该二级索引记录加 next-key 锁，范围是 (22, 39]**，同时，对 age = 39 这条记录的主键索引加记录锁，即对 id = 20 这一行记录的主键索引加记录锁。\n    \n- 虽然我们看见表中最后一条二级索引记录是 age = 39 的记录，但是实际在 Innodb 存储引擎中，会用一个特殊的记录来标识最后一条记录，该特殊的记录的名字叫 supremum pseudo-record ，所以扫描第二行的时候，也就扫描到了这个特殊记录的时候，**会对该二级索引记录加的是范围为 (39, +∞] 的 next-key 锁。**\n    \n- 停止查询\n    \n\n可以看到，事务 A 对主键索引和二级索引都加了 X 型的锁：\n\n![](statistic/asynccode-342.png)\n\n- 主键索引（id 列）：\n    \n    - 在 id = 10 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 10 的这一行记录。\n        \n    - 在 id = 20 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 20 的这一行记录。\n        \n- 二级索引（age 列）：\n    \n    - 在 age = 22 这条记录的二级索引上，加了范围为 (21, 22] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 22 的这一些新记录，不过对于是否可以插入 age = 21 和 age = 22 的新记录，还需要看新记录的 id 值，有些情况是可以成功插入的，而一些情况则无法插入，具体哪些情况，我们前面也讲了。\n        \n    - 在 age = 39 这条记录的二级索引上，加了范围为 (22, 39] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 39 的这一些记录，也无法插入 age 值为 23、24、25、...、38 的这一些新记录。不过对于是否可以插入 age = 22 和 age = 39 的新记录，还需要看新记录的 id 值，有些情况是可以成功插入的，而一些情况则无法插入，具体哪些情况，我们前面也讲了。\n        \n    - 在特殊的记录（supremum pseudo-record）的二级索引上，加了范围为 (39, +∞] 的 next-key 锁，意味着其他事务无法插入 age 值大于 39 的这些新记录。\n        \n\n\u003e 在 age \u003e= 22 的范围查询中，明明查询 age = 22 的记录存在并且属于等值查询，为什么不会像唯一索引那样，将 age = 22 记录的二级索引上的 next-key 锁退化为记录锁？\n\n因为 age 字段是非唯一索引，不具有唯一性，所以如果只加记录锁（记录锁无法防止插入，只能防止删除或者修改），就会导致其他事务插入一条 age = 22 的记录，这样前后两次查询的结果集就不相同了，出现了幻读现象。\n\n## 没有加索引的查询\n\n前面的案例，我们的查询语句都有使用索引查询，也就是查询记录的时候，是通过索引扫描的方式查询的，然后对扫描出来的记录进行加锁。\n\n如果锁定读查询语句，**没有使用索引列作为查询条件，或者查询语句没有走索引查询，导致扫描是全表扫描。那么，每一条记录的索引上都会加 next-key 锁**，这样就相当于锁住的全表，这时如果其他事务对该表进行增、删、改操作的时候，都会被阻塞。\n\n不只是锁定读查询语句不加索引才会导致这种情况，update 和 delete 语句如果查询条件不加索引，那么由于扫描的方式是全表扫描，于是就会对每一条记录的索引上都会加 next-key 锁，这样就相当于锁住的全表。\n\n因此，在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。\n\n## 总结\n\n这次我以 MySQL 8.0.26 版本，在可重复读隔离级别之下，做了几个实验，让大家了解了唯一索引和非唯一索引的行级锁的加锁规则。\n\n我这里总结下， MySQL 行级锁的加锁规则。\n\n唯一索引等值查询：\n\n- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。\n    \n- 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」。\n    \n\n非唯一索引等值查询：\n\n- 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。\n    \n- 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。\n    \n\n非唯一索引和主键索引的范围查询的加锁规则不同之处在于：\n\n- 唯一索引在满足一些条件的时候，索引的 next-key lock 退化为间隙锁或者记录锁。\n    \n- 非唯一索引范围查询，索引的 next-key lock 不会退化为间隙锁和记录锁。\n    \n\n其实理解 MySQL 为什么要这样加锁，主要要以避免幻读角度去分析，这样就很容易理解这些加锁的规则了。\n\n还有一件很重要的事情，在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。\n\n最后附上「@一只小铭」同学总结的流程图。\n\n唯一索引（主键索引）加锁的流程图如下。（_注意这个流程图是针对「主键索引」的，如果是二级索引的唯一索引，除了流程图中对二级索引的加锁规则之外，还会对查询到的记录的主键索引项加「记录锁」，流程图没有提示这一个点，所以在这里用文字补充说明下_）\n\n![](statistic/asynccode-12.jpg)\n\n非唯一索引加锁的流程图：\n\n![](statistic/asynccode-11.jpg)\n\n就说到这啦， 我们下次见啦！","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/%E9%94%81-Update%E6%B2%A1%E5%8A%A0%E7%B4%A2%E5%BC%95%E4%BC%9A%E9%94%81%E5%85%A8%E8%A1%A8%E5%90%97":{"title":"锁-Update没加索引会锁全表吗？","content":"大家好，我是小林。\n\n昨晚在群划水的时候，看到有位读者说了这么一件事。\n\n![](statistic/asynccode-329.png)\n\n大概就是，在线上执行一条 update 语句修改数据库数据的时候，where 条件没有带上索引，导致业务直接崩了，被老板教训了一波\n\n这次我们就来看看：\n\n- 为什么会发生这种的事故？\n    \n- 又该如何避免这种事故的发生？\n    \n\n说个前提，接下来说的案例都是基于 InnoDB 存储引擎，且事务的隔离级别是可重复读。\n\n## 为什么会发生这种的事故？\n\nInnoDB 存储引擎的默认事务隔离级别是「可重复读」，但是在这个隔离级别下，在多个事务并发的时候，会出现幻读的问题，所谓的幻读是指在同一事务下，连续执行两次同样的查询语句，第二次的查询语句可能会返回之前不存在的行。\n\n**因此 InnoDB 存储引擎自己实现了行锁，通过 next-key 锁（记录锁和间隙锁的组合）来锁住记录本身和记录之间的“间隙”，**防止其他事务在这个记录之间插入新的记录，从而避免了幻读现象。\n\n当我们执行 update 语句时，实际上是会对记录加独占锁（X 锁）的，如果其他事务对持有独占锁的记录进行修改时是会被阻塞的。另外，这个锁并不是执行完 update 语句就会释放的，而是会等事务结束时才会释放。\n\n在 InnoDB 事务中，对记录加锁带基本单位是 next-key 锁，**但是会因为一些条件会退化成间隙锁，或者记录锁。加锁的位置准确的说，锁是加在索引上的而非行上。**\n\n比如，在 **update 语句的 where 条件使用了唯一索引，那么 next-key 锁会退化成记录锁，也就是只会给一行记录加锁。**\n\n这里举个例子，这里有一张数据库表，其中 id 为主键索引。\n\n![](statistic/asynccode-326.png)\n\n假设有两个事务的执行顺序如下：\n\n![](statistic/asynccode-327.png)\n\n可以看到，事务 A 的 update 语句中 where 是等值查询，并且 id 是唯一索引，所以只会对 id = 1 这条记录加锁，因此，事务 B 的更新操作并不会阻塞。\n\n但是，**在 update 语句的 where 条件没有使用索引，就会全表扫描，于是就会对所有记录加上 next-key 锁（记录锁 + 间隙锁），相当于把整个表锁住了。**\n\n假设有两个事务的执行顺序如下：\n\n![](statistic/asynccode-328.png)\n\n可以看到，这次事务 B 的 update 语句被阻塞了。\n\n这是因为事务 A的 update 语句中 where 条件没有索引列，触发了全表扫描，在扫描过程中会对索引加锁，所以全表扫描的场景下，所有记录都会被加锁，也就是这条 update 语句产生了 4 个记录锁和 5 个间隙锁，相当于锁住了全表。\n\n![](statistic/asynccode-325.png)\n\n因此，当在数据量非常大的数据库表执行 update 语句时，如果没有使用索引，就会给全表的加上 next-key 锁， 那么锁就会持续很长一段时间，直到事务结束，而这期间除了 `select ... from`语句，其他语句都会被锁住不能执行，业务会因此停滞，接下来等着你的，就是老板的挨骂。\n\n那 update 语句的 where 带上索引就能避免全表记录加锁了吗？\n\n并不是。\n\n关键还得看这条语句在执行过程，优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%9F%BA%E7%A1%80/SQL%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B":{"title":"SQL的执行过程","content":"# MySQL 的基本架构图\n\n  \n\n![](statistic/asynccode-217.png)\n\nMySQL 可以分为 Server 层和存储引擎层两部分\n\n![](statistic/asynccode-4.jpg)\n\n  \n\n  \n\n# 连接器\n\n连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令为\n\n```Plain\nmysql -h$ip -P$port -u$user -p\n```\n\n连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在**完成经典的 TCP 握手后**，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。\n\n- 如果用户名或密码不对，你就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行\n    \n- 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限\n    \n\n一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置\n\n连接完成后，如果你没有后续的动作，这个连**接就处于空闲状态**，你可以在 show processlist 命令中看到它\n\n**客户端如果太长时间没动静，连接器就会自动将它断开**。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时\n\nMySQL 服务支持的最大连接数由 max_connections 参数控制\n\n### 长连接、短链接\n\n1. 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接\n    \n2. 短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个\n    \n\n**长连接导致占用内存激增**：\n\n- MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了\n    \n\n解决方法：\n\n1. **定期断开长连接。**使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连\n    \n2. 可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新**初始化连接资源**。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态\n    \n\n# 查询缓存\n\n执行逻辑就会来到第二步：查询缓存。\n\nMySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果\n\n**但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。**\n\n- **查询缓存的失效非常频繁**，只要有对一个表的更新，这个表上所有的查询缓存都会被清空\n    \n- MySQL **8.0 版本直接将查询缓存的整块功能删掉了**，也就是说 8.0 开始彻底没有这个功能了。\n    \n\n# 分析器\n\n分析器会先做词法分析再做语法分析\n\n![](statistic/asynccode-5.jpg)\n\n如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。这个错误在分析器阶段报出\n\n# 优化器\n\n**优化器主要负责将 SQL 查询语句的执行方案确定下来**\n\n优化器是在**表里面有多个索引的时候，决定使用哪个索引**；或者在一个语句有多表关联（join）的时候，决**定各个表的连接顺序**\n\n优化器是选择索引的，可能选择错\n\n# 执行器\n\n**MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做**\n\n- 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限\n    \n- 如果没有，就会返回没有权限的错误\n    \n- 在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限\n    \n\n```Plain\nselect * from T where ID=10;\n```\n\n上面语句的执行过程\n\n1. 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；\n    \n2. 调用引擎接口取“下一行”\n    \n\n  \n\n用三种执行过程，说说执行器和执行引擎的交互过程\n\n- 主键索引查询\n    \n- 全表扫描\n    \n- 索引下推\n    \n\n## 主键索引查询\n\n以本文开头查询语句为例，看看执行器是怎么工作的。\n\n```SQL\nselect * from product where id = 1;\n```\n\n这条查询语句的查询条件用到了**主键索引**，而且是**等值查询**，同时主键 id 是唯一，不会有 id 相同的记录，所以优化器决定选用访问类型为 const 进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的：\n\n- **执行器第一次查询**，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个**函数指针被指向为 InnoDB 引擎索引查询的接口**，把条件 `id = 1` 交给存储引擎，让存储引擎定位符合条件的第一条记录。\n    \n- **存储引擎**通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；\n    \n- 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，**如果符合则发送给客户端，如果不符合则跳过该记录。**\n    \n- 执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 **read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数**，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。\n    \n\n至此，这个语句就执行完成了。\n\n## 全表扫描\n\n举个全表扫描的例子：\n\n```Plain\nselect * from product where name = 'iphone';\n```\n\n这条查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的：\n\n- 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，让存储引擎读取表中的第一条记录；\n    \n- **执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过**；如果是则将记录发给客户的（是的没错，Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。\n    \n- 执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，**存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端**；\n    \n- 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息；\n    \n- 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。\n    \n\n至此，这个语句就执行完成了。\n\n## 索引下推\n\n在这部分非常适合讲索引下推（MySQL 5.6 推出的查询优化策略），这样大家能清楚的知道，「下推」这个动作，下推到了哪里。\n\n索引下推能够减少二级索引在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。\n\n举一个具体的例子，方便大家理解，这里一张用户表如下，我对 age 和 reward 字段建立了联合索引（age，reward）：\n\n![](statistic/asynccode-215.png)\n\n现在有下面这条查询语句：\n\n```SQL\nselect * from t_user  where age \u003e 20 and reward = 100000;\n```\n\n联合索引当遇到范围查询 (\u003e、\u003c) 就会停止匹配，也就是 age 字段能用到联合索引，但是 reward 字段则无法利用到索引。具体原因这里可以看这篇：[索引常见面试题(opens new window)](https://xiaolincoding.com/mysql/index/index_interview.html#%E6%8C%89%E5%AD%97%E6%AE%B5%E4%B8%AA%E6%95%B0%E5%88%86%E7%B1%BB)\n\n那么，**不使用索引下推（MySQL 5.6 之前的版本）**时，执行器与存储引擎的执行流程是这样的：\n\n- Server 层首先调用存储引擎的接口定位到满足查询条件的**第一条二级索引记录，也就是定位到 age \u003e 20 的第一条记录；**\n    \n- 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，**获取主键值，然后进行回表操作，将完整的记录返回给 Server 层；**\n    \n- **Server 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录**；\n    \n- 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层；\n    \n- 如此往复，直到存储引擎把表中的所有记录读完。\n    \n\n可以看到，**没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给 Server，接着 Server 再判断该记录的 reward 是否等于 100000。**\n\n而使用索引下推后，**判断记录的 reward 是否等于 100000 的工作交给了存储引擎层**，过程如下 ：\n\n- **Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age \u003e 20 的第一条记录；**\n    \n- 存储引擎定位到二级索引后，先不执行回表操作，**而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给 Server 层。**\n    \n- Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。\n    \n- 如此往复，直到存储引擎把表中的所有记录读完。\n    \n\n可以看到，使用了索引下推后，虽然 reward 列无法使用到联合索引，但是因为它包含在联合索引（age，reward）里，所以直接在存储引擎过滤出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。\n\n当你发现执行计划里的 Extr 部分显示了 “Using index condition”，说明使用了索引下推。\n\n![](statistic/asynccode-216.png)","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/1.%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E5%92%8C%E5%94%AF%E4%B8%80%E7%B4%A2%E5%BC%95":{"title":"1.普通索引和唯一索引","content":"# 查询过程\n\n  \n\n  \n\n![](statistic/asynccode-390.png)\n\nselect id from T where k=5\n\n这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。\n\n- 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。\n    \n- 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索\n    \n\n  \n\n这两种方式带来的**性能差异微乎其微**。**因为引擎是按页读写的**，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录。\n\n# 更新过程\n\n  \n\n## change buffer\n\n- 当需要更新一个数据页时，**如果数据页在内存中就直接更新**，\n    \n- 而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下**，InnoDB 会将这些更新操作缓存在 change buffer 中**，\n    \n- 在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。\n    \n- change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上\n    \n- change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge.\n    \n    - 除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。\n        \n    - 在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作.\n        \n\n  \n\n## 使用change buffer的条件\n\n对于唯一索引来说，不需要使用change buffer\n\n1. 所有的更新操作都要先判断这个操作是否违反唯一性约束。需要查询进内存\n    \n2. 如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。\n    \n\n  \n\n  \n\n实际上也只有普通索引可以使用change bufffer change buffer 用的是 buffer pool 里的内存，因此不能无限增大\n\n  \n\n如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。\n\n![](statistic/asynccode-6.jpg)\n\n  \n\n## change buffer 的使用场景\n\n  \n\n**change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。**\n\n- change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统\n    \n- 假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价\n    \n\n  \n\n在实际使用中，你会发现，普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。\n\n  \n\n  \n\n# change buffer 和redo log\n\n执行下列的语句\n\nmysql\u003e insert into t(id,k) values(id1,k1),(id2,k2);\n\n![](statistic/asynccode-389.png)\n\n这条语句，涉及了四个部分；\n\n- 内存\n    \n- redo","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/10.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%9F%A5%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E4%B9%9F%E6%89%A7%E8%A1%8C%E8%BF%99%E4%B9%88%E6%85%A2":{"title":"10.为什么我只查一行的语句，也执行这么慢？","content":"```Go\n\nmysql\u003e CREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB;\n\ndelimiter ;;\ncreate procedure idata()\nbegin\n  declare i int;\n  set i=1;\n  while(i\u003c=100000) do\n    insert into t values(i,i);\n    set i=i+1;\n  end while;\nend;;\ndelimiter ;\n\ncall idata();\n```\n\n# 第一类：查询长时间不返回\n\n```Go\n\nmysql\u003e select * from t where id=1;\n```\n\n**大概率是表 t 被锁住**了。接下来分析原因的时候，一般都是首先执行一下 show processlist 命令，看看当前语句处于什么状态\n\n## 等 MDL 锁\n\n  \n\n使用 show processlist 命令查看 Waiting for table metadata lock\n\n![](statistic/asynccode-378.png)\n\n这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。\n\n  \n\n这类问题的处理方式，就是找到谁持有 MDL 写锁，然后把它 kill 掉。\n\n- 通过查询 sys.schema_table_lock_waits 这张表，我们就可以直接找出造成阻塞的 process id，把这个连接用 kill 命令断开即可。\n    \n\n## 等 flush\n\nmysql\u003e select * from information_schema.processlist where id=1;\n\n线程的状态是 Waiting for table flush\n\n![](statistic/asynccode-380.png)\n\n  \n\n  \n\nMySQL 里面对表做 flush 操作的用法，一般有以下两个：\n\nflush tables t with read lock; flush tables with read lock;\n\n如果指定表 t 的话，代表的是只关闭表 t；如果没有指定具体的表名，则表示关闭 MySQL 里所有打开的表。\n\n  \n\n# 等行锁\n\n  \n\nmysql\u003e select * from t where id=1 lock in share mode;\n\n![](statistic/asynccode-379.png)\n\n个问题并不难分析，但问题是怎么查出是谁占着这个写锁。如果你用的是 MySQL 5.7 版本，可以通过 sys.innodb_lock_waits 表查到\n\nmysql\u003e select * from t sys.innodb_lock_waits where locked_table='`test`.`t`'\\G\n\n![](statistic/asynccode-381.png)\n\n4 号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是 KILL QUERY 4 或 KILL 4\n\n  \n\n# 第二类\n\n  \n\nmysql\u003e select * from t where c=50000 limit 1;\n\n**字段 c 上没有索引，这个语句只能走 id 主键顺序扫描**\n\n  \n\n  \n\n**一致性读导致一直等待这一行的读**\n\n![](statistic/asynccode-380.png)\n\n  \n\n  \n\n  \n\n执行“查一行”，可能会出现的被锁住和执行慢的例子。这其中涉及到了表锁、行锁和一致性读的概念。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/11.%E5%B9%BB%E8%AF%BB%E6%98%AF%E4%BB%80%E4%B9%88%E5%B9%BB%E8%AF%BB%E6%9C%89%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98":{"title":"11.幻读是什么，幻读有什么问题","content":"```Go\n\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `c` int(11) DEFAULT NULL,\n  `d` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB;\n\ninsert into t values(0,0,0),(5,5,5),\n(10,10,10),(15,15,15),(20,20,20),(25,25,25);\n```\n\n```Go\n\nbegin;\nselect * from t where d=5 for update;\ncommit;\n```\n\n比较好理解的是，这个语句会命中 d=5 的这一行，对应的主键 id=5，**因此在 select 语句执行完成后，id=5 这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行 commit 语句的时候释放。**由于字段 d 上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的 5 行记录上，会不会被加锁呢？\n\n  \n\n  \n\n# 幻读是什么？\n\n  \n\n![](statistic/asynccode-377.png)\n\n  \n\nsession A 里执行了三次查询，分别是 Q1、Q2 和 Q3。\n\n  \n\n都是 select * from t where d=5 for update。这个语句的意思你应该很清楚了，查所有 d=5 的行，而且使用的是当前读，并且加上写锁\n\n  \n\n  \n\n其中**，Q3 读到 id=1 这一行的现象，被称为“幻读**”。也就是说，**幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。**\n\n  \n\n这里，我需要对“幻读”做一个说明**：**\n\n- 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。\n    \n- 上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻**读仅专指“新插入的行”。**\n    \n\n  \n\n  \n\n# 幻读的问题\n\n  \n\n- 首先是语义上的。session A 在 T1 时刻就声明了，“我要把所有 d=5 的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。\n    \n- 其次，是数据一致性的问题。\n    \n\n  \n\n  \n\n  \n\n# 如何解决幻读\n\n  \n\n行锁只能锁住行，**但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。**\n\n  \n\n间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。\n\n![](statistic/asynccode-376.png)\n\n  \n\n当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 **6 个记录加上了行锁，还同时加了 7 个间隙锁**\n\n  \n\n  \n\n**在一行行扫描的过程中，不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁。**\n\n  \n\n比如行锁，分成读锁和写锁。下图就是这两种类型行锁的冲突关系\n\n![](statistic/asynccode-375.png)\n\n  \n\n但是间隙锁不一样，**跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作**。间隙锁之间都不存在冲突关系。\n\n  \n\n**间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间**。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。\n\n  \n\n  \n\n间隙锁和 next-key lock 的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”：**间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的**\n\n  \n\n  \n\n**隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row**。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/12.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E5%8F%AA%E6%94%B9%E4%B8%80%E8%A1%8C%E7%9A%84%E8%AF%AD%E5%8F%A5%E9%94%81%E8%BF%99%E4%B9%88%E5%A4%9A":{"title":"12.为什么我只改一行的语句，锁这么多","content":"加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。\n\n1. 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。\n    \n2. 原则 2：查找过程中访问到的对象才会加锁。\n    \n3. 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。\n    \n4. 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。\n    \n5. 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。\n    \n\n  \n\n  \n\nCREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25);\n\n  \n\n  \n\n# 等值查询间隙锁\n\n![](statistic/asynccode-370.png)\n\n用我们上面提到的加锁规则判断一下的话\n\n1. 根据原则 1，加锁单位是 next-key lock，session A 加锁范围就是 (5,10]；\n    \n2. 同时根据优化 2，这是一个等值查询 (id=7)，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)。\n    \n3. 所以，session B 要往这个间隙里面插入 id=8 的记录会被锁住，但是 session C 修改 id=10 这行是可以的。\n    \n\n  \n\n# 非唯一索引等值锁\n\n![](statistic/asynccode-371.png)\n\n  \n\n1. 根据原则 1，加锁单位是 next-key lock，因此会给 (0,5]加上 next-key lock。\n    \n2. 要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则 2，访问到的都要加锁，因此要给 (5,10]加 next-key lock。\n    \n3. 但是同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。\n    \n4. 根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。\n    \n5. 但 session C 要插入一个 (7,7,7) 的记录，就会被 session A 的间隙锁 (5,10) 锁住。**\\**\n    \n\n  \n\n# 主键索引范围锁\n\n你可以先思考一下这个问题：对于我们这个表 t，下面这两条查询语句，加锁范围相同吗？\n\nmysql\u003e select * from t where id=10 for update; mysql\u003e select * from t where id\u003e=10 and id\u003c11 for update;\n\n你可能会想，id 定义为 int 类型，这两个语句就是等价的吧？其实，它们并不完全等价。\n\n  \n\n现在，我们就让 session A 执行第二个查询语句，来看看加锁效果。\n\n![](statistic/asynccode-373.png)\n\n1. 开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock(5,10]。 根据优化 1， 主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。\n    \n2. 范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock(10,15]。\n    \n3. 所以，session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15]。这样，session B 和 session C 的结果你就能理解了。\n    \n\n  \n\n这里你需要注意一点，首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。\n\n  \n\n# 非唯一索引范围锁\n\n  \n\n![](statistic/asynccode-372.png)\n\n这次 session A 用字段 c 来判断，加锁规则跟案例三唯一的不同是：在第一次用 c=10 定位记录的时候，索引 c 上加了 (5,10]这个 next-key lock 后，由于索引 c 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-key lock。\n\n# 案例五：唯一索引范围锁 bug\n\n![](statistic/asynccode-374.png)\n\n  \n\nsession A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15]这个 next-key lock，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。\n\n但是实现上，InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20。而且由于这是个范围扫描，因此索引 id 上的 (15,20]这个 next-key lock 也会被锁上。\n\n# 未完~~~~","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/13.MySQL%E6%9C%89%E5%93%AA%E4%BA%9B%E9%A5%AE%E9%B8%A9%E6%AD%A2%E6%B8%B4%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%B9%E6%B3%95":{"title":"13.MySQL有哪些“饮鸩止渴”提高性能的方法？","content":"","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/14.MySQL%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E7%9A%84":{"title":"14.MySQL是怎么保证数据不丢的","content":"  \n\n  \n\n得到的结论是：只要 redo log 和 binlog 保证持久化到磁盘，就能确保 MySQL 异常重启后，数据可以恢复。\n\n  \n\nredo log 的写入流程是怎么样的，如何保证 redo log 真实地写入了磁盘。那么今天，我们就再一起看看 MySQL 写入 binlog 和 redo log 的流程\n\n  \n\n# binlog 的写入机制\n\n  \n\n其实，binlog 的写入逻辑比较简单：事务执行过程中，**先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。**\n\n  \n\n一个事务的 **binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入**。\n\n  \n\n这就涉及到了 binlog cache 的保存问题。\n\n- 系统给 binlog cache 分配了一片内存，每个线程一个，**参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。**\n    \n- 事务提交的时候，执行器把 **binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache**\n    \n\n![](statistic/asynccode-365.png)\n\n- 每个线程有自己 binlog cache，但是共用同一份 binlog 文件\n    \n    - 图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快\n        \n    - 图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。\n        \n\n  \n\n  \n\nwrite 和 fsync 的时机，是由参数 sync_binlog 控制的：\n\n- sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；\n    \n- sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；\n    \n- sync_binlog=N(N\u003e1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。\n    \n\n  \n\n# redo log 的写入机制\n\n  \n\n  \n\n事务在执行过程中，**生成的 redo log 是要先写到 redo log buffer 的。**\n\n  \n\nredo log buffer 里面的内容，是不是每次生成后都要直接持久化到磁盘呢？--**-答案是，不需要。**\n\n  \n\n- 如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失\n    \n- 那么，另外一个问题是，事务还没提交的时候，redo log buffer 中的部分日志有没有可能被持久化到磁盘呢**？答案是，确实会有**\n    \n\n**redo log 的三种状态（如下图）：**\n\n- 存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分；\n    \n- 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分；\n    \n- 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。\n    \n\n![](statistic/asynccode-367.png)\n\n  \n\n日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。\n\n  \n\n为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：\n\n- 设置为 0 的时候，表示每次事务提交时都只是把 **redo log 留在 redo log buffer** 中 ;\n    \n- 设置为 1 的时候，表示每次事务提交时都将 **redo log 直接持久化到磁盘**；\n    \n- 设置为 2 的时候，表示每次事务提交时都**只是把 redo log 写到 page cache**。\n    \n\n  \n\n  \n\nInnoDB 有一个后台线程**，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。**\n\n事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，**一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的**。\n\n  \n\n除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。\n\n- 一种是，**redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘**。注意，由于这个事务并没有提交，**所以这个写盘动作只是 write**，而没有调用 fsync，也就是只留在了文件系统的 page cache。\n    \n- 另一种是，**并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘**。\n    \n    - 如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。\n        \n\n  \n\n  \n\n介绍两阶段提交的时候说过，**时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。**\n\n如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。\n\n  \n\n**通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。**\n\n  \n\n  \n\n  \n\n这时候，你可能有一个疑问，这意味着我**从 MySQL 看到的 TPS 是每秒两万的话，每秒就会写四万次磁盘**。但是，磁盘能力也就两万左右，怎么能实现两万的 TPS？\n\n  \n\n就要用到**组提交（group commit）机制**\n\n- 这里，我需要先和你介绍日志逻辑序列号（log sequence number，LSN）的概念**。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length**。\n    \n\n  \n\n如图 3所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。\n\n![](statistic/asynccode-369.png)\n\n1. trx1 是第一个到达的，会被选为这组的 leader；\n    \n2. 等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160；\n    \n3. trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；\n    \n4. 这时候 trx2 和 trx3 就可以直接返回了。\n    \n\n  \n\n  \n\nMySQL 有一个很有趣的优化：**拖时间**。在介绍两阶段提交的时候，画了一个图。\n\n![](statistic/asynccode-366.png)\n\nMySQL 为了让组提交的效果更好， 。命令，实际上\n\n![](statistic/asynccode-368.png)","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/15.MySQL%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E4%B8%BB%E5%A4%87%E4%B8%80%E8%87%B4%E7%9A%84":{"title":"15.MySQL是怎么保证主备一致的","content":"我不止一次地和你提到了 binlog，大家知道 binlog 可以用来归档，也可以用来做主备同步，但它的内容是什么样的呢？为什么备库执行了 binlog 就可以跟主库保持一致了呢？\n\n  \n\n  \n\n# MySQL 主备的基本原理\n\n  \n\n![](statistic/asynccode-416.png)\n\n上图就是基本的主备切换流程：（M-S结构）\n\n- 在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。\n    \n- 当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。\n    \n\n  \n\n在状态 1 中，**虽然节点 B 没有被直接访问**，但是我依然建议你把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：\n\n1. 有时候一些运营类的**查询语句会被放到备库上去查，设置为只读可以防止误操作**；\n    \n2. 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致；\n    \n3. 可以用 readonly 状态，来判断节点的角色\n    \n4. 因为 readonly 设置对超级 (super) 权限用户是无效的，**而用于同步更新的线程，就拥有超级权限**。\n    \n\n  \n\n下图画出的就是一个 update 语句在节点 A 执行，然后同步到节点 B 的完整流程图，节点 A 到 B 这条线的内部流程是什么样的。\n\n![](statistic/asynccode-417.png)\n\n  \n\n**主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog**\n\n**备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接**。一个事务日志同步的完整过程是这样的：\n\n1. 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。\n    \n2. 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。\n    \n3. 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。\n    \n4. 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。sql_thread 读取中转日志，解析出日志里的命令，并执行。\n    \n\n  \n\n# binlog 的三种格式对比\n\n- binlog 有三种格式，\n    \n    - statement，\n        \n    - row\n        \n    - mixed，其实它就是前两种格式的混合。\n        \n\n  \n\n为了便于描述 binlog 的这三种格式间的区别，我创建了一个表，并初始化几行数据。\n\nmysql\u003e CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `t_modified`(`t_modified`) ) ENGINE=InnoDB; insert into t values(1,1,'2018-11-13'); insert into t values(2,2,'2018-11-12'); insert into t values(3,3,'2018-11-11'); insert into t values(4,4,'2018-11-10'); insert into t values(5,5,'2018-11-09');\n\n  \n\n如果要在表中删除一行数据的话，我们来看看这个 delete 语句的 binlog 是怎么记录的。\n\nmysql\u003e delete from t /*comment*/ where a\u003e=4 and t_modified\u003c='2018-11-10' limit 1;\n\n## statement\n\n当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文\n\n![](statistic/asynccode-415.png)\n\n这三行输出\n\n1. 第一行 SET @@SESSION.GTID_NEXT='ANONYMOUS’你可以先忽略，后面文章我们会在介绍主备切换的时候再提到；\n    \n2. 第二行是一个 BEGIN，跟第四行的 commit 对应，表示中间是一个事务；\n    \n3. 第三行就是真实执行的语句了。可以看到，在真实执行的 delete 命令之前，还有一个“use ‘test’”命令。这条命令不是我们主动执行的，而是 MySQL 根据当前要操作的表所在的数据库，自行添加的。这样做可以保证日志传到备库去执行的时候，不论当前的工作线程在哪个库里，都能够正确地更新到 test 库的表 t。use 'test’命令之后的 delete 语句，就是我们输入的 SQL 原文了。可以看到，binlog“忠实”地记录了 SQL 命令，甚至连注释也一并记录\n    \n4. 最后一行是一个 COMMIT。你可以看到里面写着 xid=61\n    \n\n  \n\n  \n\n运行这条 delete 命令产生了一个 warning，**原因是当前 binlog 设置的是 statement 格式，并且语句中有 limit，所以这个命令可能是 unsafe 的。**\n\n这是因为 delete 带 limit，很可能会出现主备数据不一致的情况\n\n- 如果 delete 语句使用的是索引 a，那么会根据索引 a 找到第一个满足条件的行，也就是说删除的是 a=4 这一行\n    \n- 但如果使用的是索引 t_modified，那么删除的就是 t_modified='2018-11-09’也就是 a=5 这一行。\n    \n\n  \n\n  \n\n**由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 t_modified**。因此，MySQL 认为这样写是有风险的\n\n## row\n\n如果我把 binlog 的格式改为 binlog_format=‘row’， 是不是就没有这个问题了呢？我们先来看看这时候 binog 中的内容吧。\n\n![](statistic/asynccode-418.png)\n\n  \n\n可以看到，与 statement 格式的 binlog 相比，前后的 BEGIN 和 COMMIT 是一样的。但是，row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Table_map 和 Delete_rows。\n\n- Table_map event，用于说明接下来要操作的表是 test 库的表 t;\n    \n- Delete_rows event，用于定义删除的行为。\n    \n\n  \n\n现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，我来给你举一个可以直接看出来的好处：**恢复数据**。\n\n  \n\n## mixed\n\n为什么会有 mixed 这种 binlog 格式的存在场景？推论过程是这样的：\n\n- 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式\n    \n- 但 row 格式的缺点是，很占空间。\n    \n- 所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。\n    \n\n  \n\n  \n\n# 循环复制问题\n\n  \n\n实际生产上使用比较多的是双 M 结构，就是下图所示的主备切换流程。\n\n![](statistic/asynccode-417.png)\n\n  \n\n你可以发现，双 M 结构和 M-S 结构，其实区别只是多了一条线，即：节点 A 和 B 之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。\n\n  \n\n有一个问题：\n\n业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。如果节点 **A 同时是节点 B 的备库**，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，**也就是循环复制**了。\n\n  \n\n  \n\n  \n\n我们可以用下面的逻辑，来解决两个节点间的循环复制的问题\n\n- 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系\n    \n- 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；\n    \n- 每个库在收到从自己的主库发过来的日志后，**先判断 server id，如果跟自己的相同**，表示这个日志是自己生成的，就直接丢弃这个日志。\n    \n\n按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样：\n\n1. 从节点 A 更新的事务，binlog 里面记的都是 A 的 server id；\n    \n2. 传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id；\n    \n3. 传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/16.MySQL%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E9%AB%98%E5%8F%AF%E7%94%A8%E7%9A%84":{"title":"16.MySQL是怎么保证高可用的","content":"正常情况下，只要主库执行更新生成的所有 binlog，都**可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性**\n\n  \n\n  \n\n**MySQL 要提供高可用能力，只有最终一致性是不够的**。\n\n  \n\n我再放一次上一篇文章中讲到的双 M 结构的主备切换流程图\n\n![](statistic/asynccode-412.png)\n\n  \n\n# 主备延迟\n\n  \n\n在介绍**主动切换流程**的详细步骤之前，我要先跟你说明一个概念，即**“同步延迟”。与数据同步有关的时间点主要包括以下三个：**\n\n1. 主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;\n    \n2. 之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;\n    \n3. 备库 B 执行完成这个事务，我们把这个时刻记为 T3。\n    \n\n  \n\n**主备延迟，就是同一个事务，在备库执行完成的时间和主库执行完成的时间之间的差值，也就是 T3-T1**。\n\n  \n\n可以在备库上执行 show slave status 命令，它的返回结果里面**会显示 seconds_behind_master，用于表示当前备库延迟了多少秒。**\n\n  \n\n在网络正常的时候，**日志从主库传给备库所需的时间是很短的，即 T2-T1 的值是非常小的**。也就是说，网络正常情况下，主备延迟的主要来源是备库接收完 binlog 和执行完这个事务之间的时间差。\n\n  \n\n  \n\n主备延迟最直接的表现是，**备库消费中转日志（relay log）的速度，比主库生产 binlog 的速度要慢**。\n\n  \n\n  \n\n# 主备延迟的来源\n\n- 首先，有些部署条件下，**备库所在机器的性能要比主库所在的机器性能差**。\n    \n    - 这种部署现在比较少了。因为主备可能发生切换，备库随时可能变成主库，所以主备库选用相同规格的机器，并且做对称部署，是现在比较常见的情况\n        \n- 第二种常见的可能了，即**备库的压力大**。一般的想法是，主库既然提供了写能力，那么备库可以提供一些读能力\n    \n    - 我们一般可以这么处理\n        \n        - 一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。\n            \n        - 通过 binlog 输出到外部系统，比如 Hadoop 这类系统，让外部系统提供统计类查询的能力.\n            \n- 这就是第三种可能了，即大事务。\n    \n    - 因为主库上必须等事务执行完成才会写入 binlog，再传给备库\n        \n    - 如果一个主库上的语句执行 10 分钟，那这个事务很可能就会导致从库延迟 10 分钟。\n        \n        - 不要一次性地用 delete 语句删除太多数据。其实，这就是一个典型的大事务场景\n            \n        - 另一种典型的大事务场景，就是大表 DDL\n            \n- 备库的并行复制能力\n    \n\n  \n\n  \n\n由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略\n\n# 可靠性优先策略\n\n可靠性优先策略:\n\n1. 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步；\n    \n2. 把主库 A 改成只读状态，即把 readonly 设置为 true\n    \n3. 断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止;\n    \n4. 备库 B 改成可读写状态，也就是把 readonly 设置为 false\n    \n5. 把业务请求切到备库 B\n    \n\n![](statistic/asynccode-411.png)\n\n  \n\n  \n\n图中的 SBM，是 seconds_behind_master 参数的简写\n\n  \n\n# 可用性优先策略\n\n  \n\n如果我强行把步骤 4、5 调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库 B 可以读写，那么系统几乎就没有不可用时间了\n\n- 可用性优先流程。这个切换流程的代价，就是可能**出现数据不一致**的情况。\n    \n\n  \n\n假设有一个表 t\n\nmysql\u003e CREATE TABLE `t` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `c` int(11) unsigned DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(c) values(1),(2),(3);\n\n  \n\n这个表定义了一个自增主键 id，初始化数据后，主库和备库上都是 3 行数据。接下来，业务人员要继续在表 t 上执行两条插入语句的命令，依次是：\n\ninsert into t(c) values(4); insert into t(c) values(5);\n\n在插入一条 c=4 的语句后，发起了主备切换。\n\n可用性优先策略，且 **binlog_format=mixed 时的切换流程和数据结果**。\n\n![](statistic/asynccode-414.png)\n\n现在，我们一起分析下这个切换流程：\n\n1. 步骤 2 中，主库 A 执行完 insert 语句，插入了一行数据（4,4），之后开始进行主备切换。\n    \n2. 步骤 3 中，由于主备之间有 5 秒的延迟，所以备库 B 还没来得及应用“插入 c=4”这个中转日志，就开始接收客户端“插入 c=5”的命令。\n    \n3. 步骤 4 中，备库 B 插入了一行数据（4,5），并且把这个 binlog 发给主库 A。\n    \n4. 步骤 5 中，备库 B 执行“插入 c=4”这个中转日志，插入了一行数据（5,4）。而直接在备库 B 执行的“插入 c=5”这个语句，传到主库 A，就插入了一行新数据（5,5）\n    \n\n  \n\n如果我还是用可用性优先策略，但设置 binlog_format=row\n\n  \n\n![](statistic/asynccode-413.png)\n\n为 row 格式在记录 binlog 的时候，会记录新插入的行的所有字段值，所以最**后只会有一行不一致**。而且，**两边的主备同步的应用线程会报错 duplicate key error 并停止。**\n\n  \n\n1. 使用 row 格式的 binlog 时，数据不一致的问题更容易被发现。而使用 mixed 或者 statement 格式的 binlog 时，数据很可能悄悄地就不一致了\n    \n2. 主备切换的可用性优先策略会导致数据不一致。因此，大多数情况下，我都建议你使用可靠性优先策略","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/17.%E5%A4%87%E5%BA%93%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%BB%B6%E8%BF%9F%E5%A5%BD%E5%87%A0%E4%B8%AA%E5%B0%8F%E6%97%B6":{"title":"17.备库为什么会延迟好几个小时","content":"你会发现，这些场景里，不论是偶发性的查询压力，还是备份，对备库延迟的影响一般是分钟级的，而且在备库恢复正常以后都能够追上来。但是，**如果备库执行日志的速度持续低于主库生成日志的速度，那这个延迟就有可能成了小时级别**。\n\n  \n\n  \n\n**备库并行复制能力**\n\n  \n\n![](statistic/asynccode-409.png)\n\n  \n\n谈到主备的并行复制能力，我们要关注的是图中黑色的两个箭头。\n\n- 一个箭头代表了客户端写入主库（更重要）\n    \n- 另一箭头代表的是备库上 sql_thread 执行中转日志（relay log）\n    \n\n  \n\n在主库上，影响并发度的原因就是各种锁了。\n\n而日志在备库上的执行，就是图中备库上 sql_thread 更新数据 (DATA) 的逻辑。如果是用单线程的话，就会导致备库应用日志不够快，造成主备延迟\n\n  \n\n  \n\n其实说到底，所有的多线程复制机制，都是要把图 1 中只有一个线程的 sql_thread，拆成多个线程，也就是都符合下面的这个模型\n\n![](statistic/asynccode-410.png)\n\nordinator 就是原来的 sql_thread, 不过现在它不再直接更新数据了，只负责读取中转日志和分发事务。真正更新日志的，变成了 worker 线程。而 work 线程的个数，就是由参数 slave_parallel_workers 决定的。根据我的经验，把这个值设置为 8~16 之间最好（32 核物理机的情况），\n\n  \n\n事务能不能按照轮询的方式分发给各个 worker，也就是第一个事务分给 worker_1，第二个事务发给 worker_2\n\n  \n\n- 其实是不行的。因为，事务被分发给 worker 以后，不同的 worker 就独立执行了。但**是，由于 CPU 的调度策略，很可能第二个事务最终比第一个事务先执行**。会导致主备不一致的问题。、\n    \n\n  \n\n请你再设想一下另外一个问题：同一个事务的多个更新语句，能不能分给不同的 worker 来执行呢？\n\n- 答案是，也不行。破坏了事务逻辑的隔离性。\n    \n\n  \n\n  \n\ncoordinator 在分发的时候，需要满足以下这两个基本要求\n\n- 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中。\n    \n- 同一个事务不能被拆开，必须放到同一个 worker 中。\n    \n\n  \n\n  \n\n# MySQL 5.6 版本的并行复制策略\n\n  \n\n官方 MySQL5.6 版本，支持了并行复制，只是支持的粒度是按库并行\n\n相比于按表和按行分发，这个策略有两个优势：\n\n- 构造 hash 值的时候很快，只需要库名；而且一个实例上 DB 数也不会很多，不会出现需要构造 100 万个项这种情况。\n    \n- 不要求 binlog 的格式。因为 statement 格式的 binlog 也可以很容易拿到库名\n    \n\n# MySQL 5.7 的并行复制策略\n\n  \n\n  \n\n# 未完~~~","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/18.%E4%B8%BB%E5%BA%93%E5%87%BA%E9%97%AE%E9%A2%98%E4%BA%86%E4%BB%8E%E5%BA%93%E6%80%8E%E4%B9%88%E5%8A%9E":{"title":"18.主库出问题了，从库怎么办？","content":"  \n\n大多数的互联网应用场景都是读多写少，因此你负责的业务，在发展过程中很可能先会遇到读性能的问题。而在数据库层解决读性能问题，所以需要一主多从。\n\n  \n\n![](statistic/asynccode-408.png)\n\n  \n\n图中，虚线箭头表示的是主备关系，也就是 **A 和 A’互为主备， 从库 B、C、D 指向的是主库 A**。一主多从的设置，一般用于读写分离，主库负责所有的写入和一部分读，其他的读请求则由从库分担。\n\n  \n\n  \n\n在一主多从架构下，主库故障后的主备切换问题。\n\n  \n\n下图就是主库发生故障，主备切换后的结果。相比于一主一备的切换流程，一主多从结构在切换完成后**，A’会成为新的主库，从库 B、C、D 也要改接到 A’。正是由于多了从库 B、C、D 重新指向的这个过程**，所以主备切换的复杂性也相应增加了。\n\n![](statistic/asynccode-407.png)\n\n  \n\n  \n\n  \n\n  \n\n# 基于位点的主备切换\n\n  \n\n当我们把节点 B 设置成节点 A’的从库的时候，需要执行一条 change master 命令\n\nCHANGE MASTER TO MASTER_HOST=$host_name MASTER_PORT=$port MASTER_USER=$user_name MASTER_PASSWORD=$password MASTER_LOG_FILE=$master_log_name MASTER_LOG_POS=$master_log_pos\n\n- MASTER_HOST、MASTER_PORT、MASTER_USER 和 MASTER_PASSWORD 四个参数，分别代表了主库 A’的 IP、端口、用户名和密码。\n    \n- 最后两个参数 MASTER_LOG_FILE 和 MASTER_LOG_POS 表示，要从主库的 master_log_name 文件的 master_log_pos 这个位置的日志继续同步。而这个位置就是我们所说的同步位点，也就是主库对应的文件名和日志偏移量。\n    \n\n  \n\n  \n\n  \n\n  \n\n# 未完~~~","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/19.%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E7%9A%84%E5%9D%91":{"title":"19.读写分离的坑","content":"","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/2.%E5%A6%82%E4%BD%95%E7%BB%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AD%97%E6%AE%B5%E5%8A%A0%E7%B4%A2%E5%BC%95":{"title":"2.如何给字符串字段加索引","content":"乎所有的系统都支持邮箱登录，如何在邮箱这样的字段上建立合理的索引\n\n  \n\n```Go\nmysql\u003e create table SUser(\nID bigint unsigned primary key,\nemail varchar(64), \n... \n)engine=innodb; \n```\n\nMySQL 是支持前缀索引的，也就是说，你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。\n\n```Go\nmysql\u003e alter table SUser add index index1(email);\n或\nmysql\u003e alter table SUser add index index2(email(6));\n```\n\n  \n\n第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串；\n\n而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。\n\n  \n\n如果使用的是 index1（即 email 整个字符串的索引结构），执行顺序是这样的：\n\n- 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值；\n    \n- 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；\n    \n- 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。\n    \n- 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行\n    \n\n  \n\n如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的：\n\n1. 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1；\n    \n2. 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃；\n    \n3. 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集；\n    \n4. 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。\n    \n\n在这个过程中，要回主键索引取 4 次数据，也就是扫描了 4 行。\n\n  \n\n  \n\n**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**\n\n  \n\n# 前缀索引对覆盖索引的影响\n\n- 使用**前缀索引就用不上覆盖索引对查询性能的优化了**，这也是你在选择是否使用前缀索引时需要考虑的一个因素\n    \n\n  \n\n  \n\n# 其他方式\n\n但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？\n\n- 第一种方式是使用倒序存储。如果你存储身份证号的时候把它倒过来存，每次查询的时候，你可以这么写：所以**最后这 6 位很可能就提供了足够的区分度**\n    \n\n```Go\nmysql\u003e select field_list from t where id_card = reverse('input_id_card_string');\n```\n\n- 第二种方式是使用 hash 字段。你可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段，**索引长度变小**\n    \n\n# 总结\n\n聊字符串字段创建索引的场景，你可以使用的方式\n\n- 直接创建完整索引，这样可能比较占用空间；\n    \n- 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；\n    \n- 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；\n    \n- 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。在实际应用中，你要根据","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/20.%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%95%B0%E6%8D%AE%E5%BA%93%E6%98%AF%E5%90%A6%E5%87%BA%E9%97%AE%E9%A2%98-%E4%BA%86":{"title":"20.如何判断数据库是否出问题 了","content":"","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/21.%E8%BF%98%E6%9C%89kill%E4%B8%8D%E6%8E%89%E7%9A%84%E8%AF%AD%E5%8F%A5":{"title":"21.还有kill不掉的语句","content":"","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/22.%E6%88%91%E6%9F%A5%E8%BF%99%E4%B9%88%E5%A4%9A%E6%95%B0%E6%8D%AE%E4%BC%9A%E4%B8%8D%E4%BC%9A%E6%8A%8A%E6%95%B0%E6%8D%AE%E5%BA%93%E5%86%85%E5%AD%98%E6%89%93%E7%88%86":{"title":"22.我查这么多数据，会不会把数据库内存打爆？","content":"","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/23.%E5%88%B0%E5%BA%95%E5%8F%AF%E4%B8%8D%E5%8F%AF%E4%BB%A5%E7%94%A8join":{"title":"23.到底可不可以用join","content":"","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/24.join%E8%AF%AD%E5%8F%A5%E6%80%8E%E4%B9%88%E4%BC%98%E5%8C%96":{"title":"24.join语句怎么优化","content":"介绍了 join 语句的两种算法，分别是 Index Nested-Loop Join(NLJ) 和 Block Nested-Loop Join(BNL)。\n\n  \n\n这两个算法都还有继续优化的空间，我们今天就来聊聊这个话题。\n\n  \n\n我还是创建两个表 t1、t2 来和你展开今天的问题\n\n```Go\n\ncreate table t1(id int primary key, a int, b int, index(a));\ncreate table t2 like t1;\ndrop procedure idata;\ndelimiter ;;\ncreate procedure idata()\nbegin\n  declare i int;\n  set i=1;\n  while(i\u003c=1000)do\n    insert into t1 values(i, 1001-i, i);\n    set i=i+1;\n  end while;\n  \n  set i=1;\n  while(i\u003c=1000000)do\n    insert into t2 values(i, i, i);\n    set i=i+1;\n  end while;\n\nend;;\ndelimiter ;\ncall idata();\n```\n\n# Multi-Range Read 优化\n\n  \n\n这个优化的主要目的是**尽量使用顺序读盘**。\n\n  \n\n回表过程是一行行地查数据，还是批量地查数据？\n\n```Go\nselect * from t1 where a\u003e=1 and a\u003c=100;\n```\n\n主键索引是一棵 B+ 树，在这棵树上，**每次只能根据一个主键 id 查到一行数据**。因此，回表肯定是一行行搜索主键索引的，基本流程如图 1 所示。\n\n![](statistic/asynccode-406.png)\n\n  \n\n如果随着 a 的值递增顺序查询的话，id 的值就变成随机的，**那么就会出现随机访问，性能相对较差**\n\n  \n\n因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，**如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。**\n\n这，就是 MRR 优化的设计思路。此时，语句的执行流程变成了这样:\n\n1. 根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ;\n    \n2. 将 read_rnd_buffer 中的 id 进行递增排序；\n    \n3. 排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。\n    \n\nread_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的\n\n![](statistic/asynccode-405.png)\n\n  \n\nMRR 能够提升性能的核心在于，这条查询语句在**索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势**。\n\n  \n\n# Batched Key Access\n\n  \n\n  \n\n这个 Batched Key Access(BKA) 算法，其实就是对 NLJ 算法的优化,\n\n  \n\n  \n\nNLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。**也就是说，对于表 t2 来说，每次都是匹配一个值。这时，MRR 的优势就用不上了**。\n\n  \n\n  \n\n既然如此，我们就把表 t1 的数据取出来一部分，先放到一个临时内存。这个临时内存不是别人，就是 **join_buffe**r。\n\n  \n\n**join_buffer 在 BNL 算法里的作用，是暂存驱动表的数据**。但是在 NLJ 算法里并没有用。那么，我们刚好就可以复用 join_buffer 到 BKA 算法中,\n\n  \n\n  \n\n如果要使用 BKA 优化算法的话，你需要在执行 SQL 语句之前，先设置\n\n```Go\nset optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';\n```\n\n  \n\n# BNL 算法的性能问题\n\n  \n\n使用 Block Nested-Loop Join(BNL) 算法时，可能会对被驱动表做多次扫描。如果这个被驱动表是一个大的冷数据表，**除了会导致 IO 压力大以外，还会对系统有什么影响呢**\n\n  \n\n由于 InnoDB 对 **Bufffer Pool** 的 LRU 算法做了优化，即：第一次从磁盘读入内存的数据页，**会先放在 old 区域**。如果 1 秒之后这个数据页不再被访问了，就不会被移动到 LRU 链表头部，这样对 Buffer Pool 的命中率影响就不大。\n\n  \n\n但是，如果一个使用 BNL 算法的 join 语句，多次扫描一个冷表，而且这个语句执行时间超过 1 秒，**就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部,**由于我们的 join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool 在这段时间内，young 区域的数据页没有被合理地淘汰。\n\n  \n\n  \n\n  \n\n**大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率**。\n\n  \n\n  \n\nBNL 算法对系统的影响主要包括三个方面：\n\n- 可能会多次扫描被驱动表，占用磁盘 IO 资源\n    \n- 判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源\n    \n- 可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率\n    \n\n  \n\n  \n\n我们执行语句之前，需要通过理论分析和查看 explain 结果的方式，确认是否要使用 BNL 算法。如果确认优化器会使用 BNL 算法，就需要做优化。**优化的常见做法是，给被驱动表的 join 字段加上索引，把 BNL 算法转成 BKA 算法**\n\n  \n\n  \n\n# BNL 转 BKA\n\n  \n\n一些情况下，我们可以直接在被驱动表上建索引，这时就可以直接转成 BKA 算法了\n\n但是，有时候你确实会碰到一些不适合在被驱动表上建索引的情况\n\nselect * from t1 join t2 on (t1.b=t2.b) where t2.b\u003e=1 and t2.b\u003c=2000;\n\n如果这条语句同时是一个低频的 SQL 语句，那么再为这个语句在表 t2 的字段 b 上创建一个索引就很浪费了。\n\n1. 把表 t1 的所有字段取出来，存入 join_buffer 中。这个表只有 1000 行，join_buffer_size 默认值是 256k，可以完全存入。\n    \n2. 扫描表 t2，取出每一行数据跟 join_buffer 中的数据进行对比，\n    \n    1. 如果不满足 t1.b=t2.b，则跳过；\n        \n    2. 如果满足 t1.b=t2.b, 再判断其他条件，也就是是否满足 t2.b 处于[1,2000]的条件，如果是，就作为结果集的一部分返回，\n        \n\n  \n\n  \n\n  \n\n在表 t2 的字段 b 上创建索引会浪费资源，但是不创建索引的话这个语句的等值条件要判断 10 亿次，想想也是浪费。那么，**有没有两全其美的办法呢**？\n\n这时候，**我们可以考虑使用临时表**。使用临时表的大致思路是：\n\n1. 把表 t2 中满足条件的数据放在临时表 tmp_t 中；\n    \n2. 为了让 join 使用 BKA 算法，给临时表 tmp_t 的字段 b 加上索引；\n    \n3. 让表 t1 和 tmp_t 做 join 操作。\n    \n\n```Go\ncreate temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;\ninsert into temp_t select * from t2 where b\u003e=1 and b\u003c=2000;\nselect * from t1 join temp_t on (t1.b=temp_t.b);\n```\n\n  \n\n  \n\n不论是在**原表上加索引，还是用有索引的临时表，我们的思路都是让 join 语句能够用上被驱动表上的索引**，来触发 BKA 算法，提升查询性能。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/25.%E4%B8%BA%E5%95%A5%E4%B8%B4%E6%97%B6%E8%A1%A8%E5%8F%AF%E4%BB%A5%E9%87%8D%E5%90%8D":{"title":"25.为啥临时表可以重名？","content":"我们在优化 join 查询的时候使用到了临时表。当时，我们是这么用的：\n\ncreate temporary table temp_t like t1; alter table temp_t add index(b); insert into temp_t select * from t2 where b\u003e=1 and b\u003c=2000; select * from t1 join temp_t on (t1.b=temp_t.b);\n\n临时表有哪些特征，为什么它适合这个场景？\n\n  \n\n我需要先帮你厘清一个容易误解的问题：**有的人可能会认为，临时表就是内存表。但是，这两个概念可是完全不同的。**\n\n  \n\n- **内存表**，指的是使用 Memory 引擎的表，建表语法是 create table … engine=memory。**这种表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在**。除了这两个特性看上去比较“奇怪”外，从其他的特征上看，它就是一个正常的表。\n    \n- 而临时表，**可以使用各种引擎类型** 。如果是使用 InnoDB 引擎或者 MyISAM 引擎的临时表，**写数据的时候是写到磁盘上的。当然，临时表也可以使用 Memory 引擎**\n    \n\n  \n\n# 临时表的特性\n\n  \n\n![](statistic/asynccode-403.png)\n\n临时表在使用上有以下几个特点：\n\n1. 建表语法是 create temporary table …。一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。\n    \n2. **临时表可以与普通表同名**。session A 内有同名的临时表和普通表的时候，**show create 语句，以及增删改查语句访问的是临时表**。\n    \n3. show tables 命令不显示临时表。\n    \n\n  \n\n**由于临时表只能被创建它的 session 访问，所以在这个 session 结束的时候，会自动删除临时表。**也正是由于这个特性，临时表就特别适合我们文章开头的 join 优化这种场景\n\n  \n\n  \n\n# 临时表的应用\n\n  \n\n由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。其中，**分库分表系统的跨库查询就是一个典型的使用场景**。\n\n  \n\n一般分库分表的场景，就是要把一个逻辑上的大表分散到不同的数据库实例上。比如。将一个大表 ht，按照字段 f，拆分成 1024 个分表，然后分布到 32 个数据库实例上。如下图所示：\n\n![](statistic/asynccode-404.png)\n\n  \n\n一般情况下，这种分库分表系统都有一个中间层 proxy。不过，也有一些方案会让客户端直接连接数据库，也就是没有 proxy 这一层。\n\n分区 key 的选择是以“**减少跨库和跨表查询**”为依据的。如果大部分的语句都会包含 f 的等值条件，那么就要用 f 做分区键，这样，在 proxy 这一层解析完 SQL 语句以后，就能确定将这条语句路由到哪个分表做查询。\n\n```Go\nselect v from ht where f=N;\n```\n\n这时，我们就可以通过分表规则（比如，N%1024) 来确认需要的数据被放在了哪个分表上。这种语句只需要访问一个分表，是分库分表方案最欢迎的语句形式了。\n\n是，如果这个表上还有另外一个索引 k，并且查询语句是这样的：\n\n```Go\nselect v from ht where k \u003e= M order by t_modified desc limit 100;\n```\n\n这时候，由于查询条件里面没有用到分区字段 f，只能到所有的分区中去查找满足条件的所有行，然后统一做 order by 的操作。这种情况下，有两种比较常用的思路。\n\n  \n\n**第一种思路是，在 proxy 层的进程代码中实现排序：**\n\n  \n\n这种方式的优势是处理速度快，拿到分库的数据以后，直接在内存中参与计算。不过，这个方案的缺点也比较明显\n\n1. 需要的开发工作量比较大。我们举例的这条语句还算是比较简单的，如果涉及到复杂的操作，比如 **group by，甚至 join 这样的操作，对中间层的开发能力要求比较高**；\n    \n2. 对 proxy 端的压力比较大，尤其是**很容易出现内存不够用和 CPU 瓶颈的问题**。\n    \n\n  \n\n**另一种思路就是，把各个分库拿到的数据，汇总到一个 MySQL 实例的一个表中**，然后在这个汇总实例上做逻辑操作。\n\n执行流程可以类似这样：\n\n1. 在汇总库上创建一个临时表 temp_ht，表里包含三个字段 v、k、t_modified；\n    \n2. 在各个分库上执行\n    \n\nselect v,k,t_modified from ht_x where k \u003e= M order by t_modified desc limit 100;\n\n1. 把分库执行的结果插入到 temp_ht 表中；\n    \n2. 执行\n    \n\n```Go\nselect v from temp_ht order by t_modified desc limit 100; \n```\n\n  \n\n![](statistic/asynccode-399.png)\n\n![](statistic/asynccode-399.png)\n\n![](statistic/asynccode-401.png)\n\n![](statistic/asynccode-398.png)\n\n![](statistic/asynccode-400.png)\n\n  \n\n  \n\n  \n\n  \n\n  \n\n# 为什么临时表可以重名？\n\n  \n\n不同线程可以创建同名的临时表，这是怎么做到的呢？\n\n  \n\n执行\n\n```Go\ncreate temporary table temp_t(id int primary key)engine=innodb;\n```\n\n  \n\n这个语句的时候，MySQL 要给这个 InnoDB 表创建一个 frm 文件**保存表结构定义**，还要有地方保存表数据。\n\n这个 frm 文件放在临时文件目录下，文件名的后缀是.frm，前缀是“**#sql{进程 id}_{线程 id}_ 序列号**”。\n\n  \n\n  \n\n而关于表中**数据的存放方式**，在不同的 MySQL 版本中有着不同的处理方式：\n\n1. 在 5.6 以及之前的版本里，**MySQL 会在临时文件目录下创建一个相同前缀、以.ibd 为后缀的文件，用来存放数据文件**；\n    \n2. 而从 5.7 版本开始**，MySQL 引入了一个临时文件表空间，专门用来存放临时文件的数据。因此，我们就不需要再创建 ibd 文件了**。\n    \n\n  \n\n  \n\n从文件名的前缀规则，我们可以看到，其实创建**一个叫作 t1 的 InnoDB 临时表，MySQL 在存储上认为我们创建的表名跟普通表 t1 是不同的，因此同一个库下面已经有普通表 t1 的情况下，还是可以再创建一个临时表 t1 的**\n\n  \n\n  \n\n先来举一个例子\n\n![](statistic/asynccode-402.png)\n\n  \n\n这个进程的进程号是 1234，session A 的线程 id 是 4，session B 的线程 id 是 5。所以你看到了，session A 和 session B 创建的临时表，在磁盘上的文件不会重名。\n\n  \n\nMySQL 维护数据表，除了物理上要有文件外，内存里面也有一套机制区别不同的表，每个表都对应一个 table_def_key。\n\n- **一个普通表的 table_def_key 的值是由“库名 + 表名”得到的**，所以如果你要在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现 table_def_key 已经存在了。\n    \n- **而对于临时表，table_def_key 在“库名 + 表名”基础上，又加入了“server_id+thread_id”**。\n    \n\n在实现上，每个线程都维护了自己的临时表链表。**这样每次 session 内操作表的时候，先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表，如果没有再操作普通表**；在 **session 结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE + 表名”操作**。\n\n  \n\n  \n\n# 临时表和主备复制\n\n既然写 binlog，就意味着备库需要。\n\n你可以设想一下，在主库上执行下面这个语句序列：\n\ncreate table t_normal(id int primary key, c int)engine=innodb;/*Q1*/ create temporary table temp_t like t_normal;/*Q2*/ insert into temp_t values(1,1);/*Q3*/ insert into t_normal select * from temp_t;/*Q4*/\n\n如果当前的 binlog_format=row，那么跟临时表有关的语句，就不会记录到 binlog 里。也就是说，只在 binlog_format=statment/mixed 的时候，binlog 中才会记录临时表的操作。\n\n  \n\n**主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的**。所以，这时候我们就需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行\n\n  \n\n  \n\n还有另外一个问题需要解决：主库上不同的线程创建同名的临时表是没关系的，但是传到备库执行是怎么处理的呢？\n\n![](statistic/asynccode-402.png)\n\n  \n\n主库 M 上的两个 session 创建了同名的临时表 t1，这两个 create temporary table t1 语句都会被传到备库 S 上。\n\n但是，**备库的应用日志线程是共用的，也就是说要在应用线程里面先后执行这个 create 语句两次**。\n\n  \n\nMySQL 在记录 binlog 的时候，会把主库执行这个语句的线程 id 写到 binlog 中。这样，在备库的应用线程就能够知道执行每个语句的主库线程 id，并利用这个线程 id 来构造临时表的 table_def_key：\n\n1. session A 的临时表 t1，在备库的 table_def_key 就是：库名 +t1+“M 的 serverid”+“session A 的 thread_id”;\n    \n2. session B 的临时表 t1，在备库的 table_def_key 就是 ：库名 +t1+“M 的 serverid”+“session B 的 thread_id”。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/26.%E8%BF%98%E8%A6%81%E4%B8%8D%E8%A6%81%E4%BD%BF%E7%94%A8Memory%E5%BC%95%E6%93%8E":{"title":"26.还要不要使用Memory引擎？","content":"内存表的数据组织结构\n\ncreate table t1(id int primary key, c int) engine=Memory; create table t2(id int primary key, c int) engine=innodb; insert into t1 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0); insert into t2 values(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(7,7),(8,8),(9,9),(0,0);\n\n我分别执行 select * from t1 和 select * from t2。\n\n![](statistic/asynccode-397.png)\n\n内存表 t1 的返回结果里面 0 在最后一行，而 InnoDB 表 t2 的返回结果里 0 在第一行。\n\n出现这个区别的原因，要从这两个引擎的主键索引的组织方式说起。\n\n***表 t2 用的是 InnoDB 引擎，它的主键索引 id 的组织方式，你已经很熟悉了：InnoDB 表的数据就放在主键索引树上**，**主键索引是 B+ 树**。所以表 t2 的数据组织方式如下图所示：\n\n![](statistic/asynccode-394.png)\n\n  \n\n  \n\n，Memory 引擎的数据和索引是分开的。我们来看一下表 t1 中的数据内容。\n\n![](statistic/asynccode-395.png)\n\n**内存表的数据部分以数组的方式单独存放，而主键 id 索引里，存的是每个数据的位置。主键 id 是 hash 索引**\n\n在内存表 t1 中，当我执行 select * 的时候，走的是全表扫描，也就是顺序扫描这个数组。因此，0 就是最后一个被读到，并放入结果集的数据。\n\n  \n\nnnoDB 和 Memory 引擎的数据组织方式是不同的：\n\n- InnoDB **引擎把数据放在主键索引上**，其他索引上保存的是主键 id。这种方式，我们称之为**索引组织表（Index Organizied Table）。**\n    \n- 而 Memory 引擎采用的是把数据单独存放，**索引上保存数据位置的数据组织形式**，我们称之为堆组织表（Heap Organizied Table）\n    \n\n  \n\n这两个引擎的一些典型不同：\n\n1. InnoDB 表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；\n    \n2. 当数据文件有空洞的时候，InnoDB 表在插入新数据的时候，为了保证数据有序性，**只能在固定的位置写入新值，而内存表找到空位就可以插入新值；**\n    \n3. 数据位置发生变化的时候，InnoDB 表只需要修改主键索引，而内存表需要修改所有索引；\n    \n4. **InnoDB 表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找**。而内存表没有这个区别，所有索引的“地位”都是相同的。\n    \n5. **InnoDB 支持变长数据类型，不同记录的长度可能不同**；**内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同**。\n    \n\n  \n\n# hash 索引和 B-Tree 索引\n\n内存表也是支 B-Tree 索引的。在 id 列上创建一个 B-Tree 索引，SQL 语句可以这么写：\n\nalter table t1 add index a_btree_index using btree (id);\n\n结构变为\n\n![](statistic/asynccode-396.png)\n\n一般在我们的印象中，**内存表的优势是速度快**，其中的一个原因就是 **Memory 引擎支持 hash 索引**\n\n  \n\n接下来我要跟你说明，为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：\n\n- 锁粒度问题；\n    \n- 数据持久化问题。\n    \n\n# 内存表的锁\n\n内存表不支持行锁，只支持表锁。因此，一张表只要有更新，就会堵住其他所有在这个表上的读写操作。\n\n  \n\n# 数据持久性问题\n\n数据放在内存中，是内存表的优势，但也是一个劣势。因为，数据库重启的时候，所有的内存表都会被清空。\n\n  \n\n  \n\n我建议你把**普通内存表都用 InnoDB 表来代替**。但是，有一个场景却是例外的。\n\n  \n\n在数据量可控，不会耗费过多内存的情况下，你可以考虑使用内存表。**内存临时表刚好可以无视内存表的两个不足**，主要是下面的三个原因\n\n- 临时表不会被其他线程访问，没有并发性的问题；\n    \n- 临时表重启后也是需要删除的，清空数据这个问题不存在；\n    \n- 备库的临时表也不会影响主库的用户线程。\n    \n\n  \n\ncreate temporary table temp_t(id int primary key, a int, b int, index (b))engine=memory; insert into temp_t select * from t2 where b\u003e=1 and b\u003c=2000; select * from t1 join temp_t on (t1.b=temp_t.b);\n\n其实这里使用内存临时表的效果更好，原因有三个：\n\n- 相比于 InnoDB 表，使用内存表不需要写磁盘，往表 temp_t 的写数据的速度更快\n    \n- 索引 b 使用 hash 索引，查找的速度比 B-Tree 索引快；\n    \n- 临时表数据只有 2000 行，占用的内存有限。","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/27.%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE%E4%B8%BA%E5%95%A5%E4%B8%8D%E8%BF%9E%E7%BB%AD":{"title":"27.自增主键为啥不连续","content":"由于自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂，因此索引更紧凑。\n\n自增主键不能保证连续递增\n\nCREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `c` (`c`) ) ENGINE=InnoDB;\n\n# 自增值保存在哪儿？\n\n**表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。**\n\n  \n\n  \n\n不同的引擎对于自增值的保存策略不同。\n\n- MyISAM 引擎的自增值保存在**数据文件**中。\n    \n- InnoDB 引擎的自增值，其实是保存在了**内存里**，并且到了 MySQL 8.0 版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为 MySQL 重启前的值”，具体情况是：\n    \n    - 在 MySQL 5.7 及之前的版本，**自增值保存在内存里**，并没有持久化。每次重启后，第一次打开表的时候，**都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值**。举例来说，如果一个表当前数据行里最大的 id 是 10，AUTO_INCREMENT=11。这时候，我们删除 id=10 的行，AUTO_INCREMENT 还是 11。但如果马上重启实例，重启后这个表的 AUTO_INCREMENT 就会变成 10。也就是说，MySQL 重启可能会修改一个表的 AUTO_INCREMENT 的值。\n        \n    - 在 MySQL 8.0 版本，将自增值的变更记录在了 **redo log** 中，重启的时候依靠 redo log 恢复重启之前的值。\n        \n\n  \n\n  \n\n# 自增值修改机制\n\n  \n\n在 MySQL 里面，如果字段 id 被定义为 AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下。\n\n- 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT 值填到自增字段；\n    \n- 如果插入数据时 id 字段指定了具体的值，就直接使用语句里指定的值。\n    \n\n根据要插入的值和当前自增值的大小关系，自增值的变更结果也会有所不同。假设，某次要插入的值是 X，当前的自增值是 Y。\n\n- 如果 X\u003cY，那么这个表的自增值不变；\n    \n- 如果 X≥Y，就需要把当前自增值修改为新的自增值。\n    \n\n**新的自增值生成算法是**：从 auto_increment_offset 开始，以 auto_increment_increment 为步长，持续叠加，直到找到第一个大于 X 的值，作为新的自增值。**其中，auto_increment_offset 和 auto_increment_increment 是两个系统参数，分别用来表示自增的初始值和步长，默认值都是 1。**\n\n比如，双 M 的主备结构里要求双写的时候，我们就可能会设置成 auto_increment_increment=2，让一个库的自增 id 都是奇数，另一个库的自增 id 都是偶数，避免两个库生成的主键发生冲突。\n\n当 auto_increment_offset 和 auto_increment_increment 都是 1 的时候，新的自增值生成逻辑很简单，就是：\n\n- 如果准备插入的值 \u003e= 当前自增值，新的自增值就是“准备插入的值 +1”；\n    \n- 否则，自增值不变。\n    \n\n  \n\n# 自增值的修改时机\n\n  \n\n假设，表 t 里面已经有了 (1,1,1) 这条记录，这时我再执行一条插入数据命令：\n\ninsert into t values(null, 1, 1);\n\n这个语句的执行过程为：\n\n1. 执行器调用 InnoDB 引擎接口写入一行，传入的这一行的值是 (0,1,1);\n    \n2. InnoDB 发现用户没有指定自增 id 的值，获取表 t 当前的自增值 2；\n    \n3. 将传入的行的值改成 (2,1,1);将表的自增值改成 3；\n    \n4. 继续执行插入数据操作，由于已经存在 c=1 的记录，所以报 Duplicate key error，\n    \n5. 语句返回。\n    \n\n  \n\n**这个表的自增值改成 3，是在真正执行插入数据的操作之前**。这个语句真正执行的时候，因为碰到唯一键 c 冲突，**所以 id=2 这一行并没有插入成功，但也没有将自增值再改回去**。\n\n  \n\n可见，**唯一键冲突是导致自增主键 id 不连续的第一种原因**。\n\n同样地**，事务回滚也会产生类似的现象，这就是第二种原因**\n\n  \n\n  \n\n自增值为什么不能回退？\n\n- 假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。\n    \n- 事务 B 正确提交了，但事务 A 出现了唯一键冲突。如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。\n    \n- 接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，**就会出现插入语句报错“主键冲突”。**\n    \n\n  \n\n  \n\n所以才只保证了自增 id 是递增的，但不保证是连续的。\n\n  \n\n  \n\n# 自增锁的优化\n\n  \n\n**自增 id 锁并不是一个事务锁，而是每次申请完就马上释放，以便允许别的事务再申请。**\n\n  \n\n- 在 MySQL 5.0 版本的时候，自增锁的范围是语句级别。也就是说，如果一个语句申请了一个表自增锁，这个锁会等语句执行结束以后才释放\n    \n- MySQL 5.1.22 版本引入了一个新策略，新增参数 innodb_autoinc_lock_mode，默认值是 1。\n    \n    - 这个参数的值被设置为 0 时，表示采用之前 MySQL 5.0 版本的策略，即**语句执行结束后才释放锁；**\n        \n    - 这个参数的值被设置为 1 时：\n        \n        - 普通 insert 语句，**自增锁在申请之后就马上释放**；\n            \n        - 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；\n            \n    - 这个参数的值被设置为 2 时，所有的申请自增主键的动作都是申请后就释放锁\n        \n\n  \n\n  \n\n在生产上，尤其是有 insert … select 这种批量插入数据的场景时，从并发插入数据性能的角度考虑，我建议你这样设置：**innodb_autoinc_lock_mode=2** ，并且 **binlog_format=row.** 这样做，**既能提升并发性，又不会出现数据一致性问题**。需要注意的是，我这里说的批量插入数据，包含的语句类型是 insert … select、replace … select 和 load data 语句。\n\n  \n\n  \n\n对于批量插入数据的语句，MySQL 有一个批量申请自增 id 的策略：\n\n1. 语句执行过程中，第一次申请自增 id，会分配 1 个；\n    \n2. 1 个用完以后，这个语句第二次申请自增 id，会分配 2 个；\n    \n3. 2 个用完以后，还是这个语句，第三次申请自增 id，会分配 4 个；\n    \n4. 依此类推，同一个语句去申请自增 id，每次申请到的自增 id 个数都是上一次的两倍。\n    \n\n  \n\n**申请后不回退，这是主键 id 出现自增 id 不连续的第三种原因。**\n\n  \n\n  \n\n# 总结\n\n自增见为啥不连续？\n\n**唯一键冲突**\n\n**事务回滚**\n\n**连续申请多个ID，但是没有用完**","lastmodified":"2023-07-31T03:20:22.321088373Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/28.insert%E4%B8%BA%E5%95%A5%E9%94%81%E9%82%A3%E4%B9%88%E5%A4%9A":{"title":"28.insert为啥锁那么多？","content":"MySQL 对自增主键锁做了优化，尽量在申请到自增 id 以后，就释放自增锁。\n\n  \n\ninsert 语句是一个很轻量的操作。不过，这个结论对于“普通的 insert 语句”才有效。也就是说，**还有些 insert 语句是属于“特殊情况”的，在执行过程中需要给其他资源加锁，或者无法在申请到自增 id 以后就立马释放自增锁**\n\n  \n\n```\n  \nCREATE TABLE t (\n  id int(11) NOT NULL AUTO_INCREMENT,\n  c int(11) DEFAULT NULL,\n  d int(11) DEFAULT NULL,\n  PRIMARY KEY (id),\n  UNIQUE KEY c (c)\n) ENGINE=InnoDB;\n\ninsert into t values(null, 1,1);\ninsert into t values(null, 2,2);\ninsert into t values(null, 3,3);\ninsert into t values(null, 4,4);\n\ncreate table t2 like t\n```\n\n\n# insert … select 语句\n\n在可重复读隔离级别下，binlog_format=statement 时执行：\n\ninsert into t2(c,d) select c,d from t;\n\n这个语句时，需要对表 t 的所有行和间隙加锁呢?\n\n其实，这个问题我们需要考虑的还是日志和数据的一致性。我们看下这个执行序列\n\n![](statistic/asynccode-393.png)\n\n实际的执行效果是，如果 session B 先执行，由于这个语句对表 t 主键索引加了 (-∞,1]这个 next-key lock，会在语句执行完成后，才允许 session A 的 insert 语句执行。\n\n但如果没有锁的话，就可能出现 session B 的 insert 语句先执行，但是后写入 binlog 的情况。于是，在 binlog_format=statement 的情况下，binlog 里面就记录了这样的语句序列：\n\ninsert into t values(-1,-1,-1); insert into t2(c,d) select c,d from t;\n\n# insert 循环写入\n\n  \n\n  \n\n# insert 唯一键冲突\n\n  \n\n  \n\n# insert into … on duplicate key update","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/29%E6%80%8E%E4%B9%88%E6%9C%80%E5%BF%AB%E5%9C%B0%E5%A4%8D%E5%88%B6%E4%B8%80%E5%BC%A0%E8%A1%A8":{"title":"29怎么最快地复制一张表","content":"**如果可以控制对源表的扫描行数和加锁范围很小**的话，我们简单地使用 **insert … select** 语句即可实现\n    \n\n  \n\n- **更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表**\n    \n\n```\ncreate database db1;\nuse db1;\n\ncreate table t(id int primary key, a int, b int, index(a))engine=innodb;\ndelimiter ;;\n  create procedure idata()\n  begin\n    declare i int;\n    set i=1;\n    while(i\u003c=1000)do\n      insert into t values(i,i,i);\n      set i=i+1;\n    end while;\n  end;;\ndelimiter ;\ncall idata();\n\ncreate database db2;\ncreate table db2.t like db1.t\n\n```\n\n\n假设，我们要把 db1.t 里面 a\u003e900 的数据行导出来，插入到 db2.t 中。\n\n# mysqldump 方法\n\n  \n\n一种方法是，使用 mysqldump 命令将数据导出成一组 INSERT 语句。你可以使用下面的命令：\n\nmysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction --set-gtid-purged=OFF db1 t --where=\"a\u003e900\" --result-file=/client_tmp/t.sql\n\n这条命令中，主要参数含义如下\n\n- –single-transaction 的作用是，在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法\n    \n- –add-locks 设置为 0，表示在输出的文件结果里，不增加\" LOCK TABLES t WRITE;\"\n    \n- –no-create-info 的意思是，不需要导出表结构\n    \n- –set-gtid-purged=off 表示的是，不输出跟 GTID 相关的信息\n    \n- –result-file 指定了输出文件的路径，其中 client 表示生成的文件是在客户端机器上的\n    \n\n生成的文件为\n\n![](statistic/asynccode-391.png)\n\n  \n\n如果你希望生成的文件中一条 INSERT 语句只插入一行数据的话，可以在执行 mysqldump 命令时，加上参数–skip-extended-insert\n\n  \n\n然后，你可以通过下面这条命令，将这些 INSERT 语句放到 db2 库里去执行\n\nmysql -h127.0.0.1 -P13000 -uroot db2 -e \"source /client_tmp/t.sql\"\n\n  \n\nsource 并不是一条 SQL 语句，而是一个客户端命令。mysql 客户端执行这个命令的流程是这样的：\n\n1. 打开文件，默认以分号为结尾读取一条条的 SQL 语句；\n    \n2. 将 SQL 语句发送到服务端执行。\n    \n\n  \n\n# 导出 CSV 文件\n\n  \n\n另一种方法是直接将结果导出成.csv 文件,\n\nmySQL 提供了下面的语法，用来将查询结果导出到服务端本地目录：\n\nselect * from db1.t where a\u003e900 into outfile '/server_tmp/t.csv';\n\n我们在使用这条语句时，需要注意如下几点:\n\n1. 这条语句会将结果保存在服务端。如果你执行命令的客户端和 MySQL 服务端不在同一个机器上，客户端机器的临时目录下是不会生成 t.csv 文件的\n    \n2. into outfile 指定了文件的生成位置（/server_tmp/），这个位置必须受参数 secure_file_priv 的限制。参数 secure_file_priv 的可选值和作用分别是:\n    \n    1. 如果设置为 empty，表示不限制文件生成的位置，这是不安全的设置\n        \n    2. 如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目\n        \n    3. 如果设置为 NULL，就表示禁止在这个 MySQL 实例上执行 select … into outfile 操作。\n        \n3. 这条命令不会帮你覆盖文件，因此你需要确保 /server_tmp/t.csv 这个文件不存在，否则执行语句时就会因为有同名文件的存在而报错\n    \n4. 这条命令生成的文本文件中，原则上一个数据行对应文本文件的一行。但是，如果字段中包含换行符，在生成的文本中也会有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开\n    \n\n得到.csv 导出文件后，你就可以用下面的 load data 命令将数据导入到目标表 db2.t 中\n\nload data infile '/server_tmp/t.csv' into table db2.t;\n\n这条语句的执行流程如下所示:\n\n1. 打开文件 /server_tmp/t.csv，以制表符 (\\t) 作为字段间的分隔符，以换行符（\\n）作为记录之间的分隔符，进行数据读取；\n    \n2. 启动事务。\n    \n3. 判断每一行的字段数与表 db2.t 是否相同：若不相同，则直接报错，事务回滚；若相同，则构造成一行，调用 InnoDB 引擎接口，写入到表中。\n    \n4. 重复步骤 3，直到 /server_tmp/t.csv 整个文件读入完成，提交事务。\n    \n\n  \n\n如果 binlog_format=statement，这个 load 语句记录到 binlog 里以后，怎么在备库重放呢?\n\n  \n\n由于 **/server_tmp/t.csv 文件只保存在主库所在的主机上**，如果只是把这条语句原文写到 binlog 中，**在备库执行的时候，备库的本地机器上没有这个文件，就会导致主备同步停止**。\n\n  \n\n所以，这条语句执行的完整流程，其实是下面这样的。:\n\n1. 主库执行完成后，将 /server_tmp/t.csv 文件的内容直接写到 binlog 文件中。\n    \n2. 往 binlog 文件中写入语句 load data local infile ‘/tmp/SQL_LOAD_MB-1-0’ INTO TABLE `db2`.`t`。\n    \n3. 把这个 binlog 日志传到备库\n    \n4. 备库的 apply 线程在执行这个事务日志时：\n    \n    1. 先将 binlog 中 t.csv 文件的内容读出来，写入到本地临时目录 /tmp/SQL_LOAD_MB-1-0 中；\n        \n    2. 再执行 load data 语句，往备库的 db2.t 表中插入跟主库相同的数据。\n        \n\n  \n\nload data 命令有两种用法\n\n1. 不加“local”，是读取服务端的文件，这个文件必须在 secure_file_priv 指定的目录或子目录下；\n    \n2. 加上“local”，读取的是客户端的文件，只要 mysql 客户端有访问这个文件的权限即可。这时候，MySQL 客户端会先把本地文件传给服务端，然后执行上述的 load data 流程。\n    \n\n  \n\n# 物理拷贝方法\n\n你可能会问，有物理导数据的方法吗？比如，**直接把 db1.t 表的.frm 文件和.ibd 文件拷贝到 db2 目录**下，是否可行呢？\n\n**答案是不行的**。因为，一个 InnoDB 表，除了包含这两个物理文件外，**还需要在数据字典中注册**。直接拷贝这两个文件的话，因为数据字典中**没有 db2.t 这个表**，系统是不会识别和接受它们的\n\n  \n\n  \n\n  \n\n在 MySQL 5.6 版本引入了**可传输表空间(transportable tablespace) 的方法**，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。\n\n假设我们现在的目标是在 db1 库下，复制一个跟表 t 相同的表 r，具体的执行步骤如下：\n\n1. 执行 create table r like t，创建一个相同表结构的空表；\n    \n2. 执行 alter table r discard tablespace，这时候 r.ibd 文件会被删除；\n    \n3. 执行 flush table t for export，这时候 db1 目录下会生成一个 t.cfg 文件；\n    \n4. 在 db1 目录下执行 cp t.cfg r.cfg; cp t.ibd r.ibd；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL 进程要有读写权限）；\n    \n5. 执行 unlock tables，这时候 t.cfg 文件会被删除；\n    \n6. 执行 alter table r import tablespace，将这个 r.ibd 文件作为表 r 的新的表空间，由于这个文件的数据内容和 t.ibd 是相同的，所以表 r 中就有了和表 t 相同的数据。\n    \n\n![](statistic/asynccode-392.png)\n\n有以下几个注意点：\n\n1. 在第 3 步执行完 flsuh table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁；\n    \n2. 在执行 import tablespace 的时候，为了让文件里的表空间 id 和数据字典中的一致，会修改 r.ibd 的表空间 id。而这个表空间 id 存在于每一个数据页中。因此，如果是一个很大的文件（比如 TB 级别），每个数据页都需要修改，所以你会看到这个 import 语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import 语句的耗时是非常短的。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/3.MySQL-%E9%94%99%E9%80%89":{"title":"3.MySQL 错选","content":"MySQL 中一张表其实是可以支持多个索引的。使用哪个索引是由 MySQL 来确定的。\n\n  \n\n一条本来可以执行得很快的语句，却由于 MySQL 选错了索引，而导致执行速度变得很慢\n\n  \n\n```Go\n\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `a` int(11) DEFAULT NULL,\n  `b` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `a` (`a`),\n  KEY `b` (`b`)\n) ENGINE=InnoDB；\n```\n\n使用存储过程来插入数据\n\n```Go\ndelimiter ;;\ncreate procedure idata()\nbegin\n  declare i int;\n  set i=1;\n  while(i\u003c=100000)do\n    insert into t values(i, i, i);\n    set i=i+1;\n  end while;\nend;;\ndelimiter ;\ncall idata();\n```\n\n这么一天的sql\n\n```Go\n\nmysql\u003e select * from t where a between 10000 and 20000;\n```\n\n这条查询语句的执行也确实符合预期，key 这个字段值是’a’，表示优化器选择了索引 a\n\n  \n\n不过别急，这个案例不会这么简单。在我们已经准备好的包含了 10 万行数据的表上，我们再做如下操作。\n\n![](statistic/asynccode-388.png)\n\n这里，session A 的操作你已经很熟悉了，它就是开启了一个事务。随后，session B 把数据都删除后，又调用了 idata 这个存储过程，插入了 10 万行数据。这时候，session B 的查询语句 select * from t where a between 10000 and 20000 就不会再选择索引 a 了","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/30.grant%E4%B9%8B%E5%90%8E%E8%A6%81%E8%B7%9F%E7%9D%80flush-privileges%E5%90%97":{"title":"30.grant之后要跟着flush privileges吗","content":"grant 语句是用来给用户赋权的。不知道你有没有见过一些操作文档里面提到，grant 之后要马上跟着执行一个 flush privileges 命令，才能使赋权语句生效\n\n  \n\n下来，我就先和你介绍一下 grant 语句和 flush privileges 语句分别做了什么事情\n\n  \n\n  \n\ncreate user 'ua'@'%' identified by 'pa';\n\n这条语句的逻辑是创建一个用户’ua’@’%’，密码是 pa。在 MySQL 里面，用户名 (user)+ 地址 (host) 才表示一个用户，因此 ua@ip1 和 ua@ip2 代表的是两个不同的用户。\n\n  \n\n这条命令做了两个动作：\n\n1. 磁盘上，往 mysql.user 表里插入一行，由于没有指定权限，所以这行数据上所有表示权限的字段的值都是 N；\n    \n2. 内存里，往数组 acl_users 里插入一个 acl_user 对象，这个对象的 access 字段值为0\n    \n\n  \n\n在 MySQL 中，用户权限是有不同的范围的。接下来，我就按照用户权限范围从大到小的顺序依次和你说明。\n\n  \n\n# 全局权限\n\n  \n\n全局权限，作用于整个 MySQL 实例，这些权限信息保存在 mysql 库的 user 表里。如果我要给用户 ua 赋一个最高权限的话，语句是这么写的\n\n  \n\ngrant all privileges on _._ to 'ua'@'%' with grant option\n\n这个 grant 命令做了两个动作:\n\n1. 磁盘上，将 mysql.user 表里，用户’ua’@’%'这一行的所有表示权限的字段的值都修改为‘Y’\n    \n2. 内存里，从数组 acl_users 中找到这个用户对应的对象，将 access 值（权限位）修改为二进制的“全 1”\n    \n\n在这个 grant 命令执行完成后，如果有新的客户端使用用户名 ua 登录成功，**MySQL 会为新连接维护一个线程对象，然后从 acl_users 数组里查到这个用户的权限，并将权限值拷贝到这个线程对象中**。之后在这个连接中执行的语句，所有关于全局权限的判断，都直接使用线程对象内部保存的权限位\n\n  \n\n- grant 命令对于全局权限，同时更新了磁盘和内存。命令完成后即时生效，接下来新创建的连接会使用新的权限。\n    \n- 对于一个已经存在的连接，它的全局权限不受 grant 命令的影响。\n    \n\n  \n\nrevoke all privileges on _._ from 'ua'@'%';\n\n这条 revoke 命令的用法与 grant 类似，做了如下两个动作：\n\n1. 磁盘上，将 mysql.user 表里，用户’ua’@’%'这一行的所有表示权限的字段的值都修改为“N”；\n    \n2. 内存里，从数组 acl_users 中找到这个用户对应的对象，将 access 的值修改为 0。\n    \n\n  \n\n# db 权限\n\n  \n\nMySQL 也支持库级别的权限定义。如果要让用户 ua 拥有库 db1 的所有权限，可以执行下面这条命令：\n\ngrant all privileges on db1.* to 'ua'@'%' with grant option;\n\n这条 grant 命令做了如下两个动作：\n\n1. 磁盘上，往 mysql.db 表中插入了一行记录，所有权限位字段设置为“Y”；\n    \n2. 内存里，增加一个对象到数组 acl_dbs 中，这个对象的权限位为“全 1”\n    \n\n  \n\n每次需要判断一个用户对一个数据库读写权限的时候，都需要遍历一次 acl_dbs 数组，根据 user、host 和 db 找到匹配的对象，然后根据对象的权限位来判断。\n\ngrant 修改 db 权限的时候，是同时对磁盘和内存生效的\n\n  \n\n  \n\n# 表权限和列权限\n\n  \n\n除了 db 级别的权限外，**MySQL 支持更细粒度的表权限和列权限**。其中，表权限定义存放在表 **mysql.tables_priv** 中，列权限定义存放在表 **mysql.columns_priv** 中。这两类权限，组合起来存放在内存的 **hash 结构 column_priv_hash** 中。\n\n  \n\n  \n\ncreate table db1.t1(id int, a int); grant all privileges on db1.t1 to 'ua'@'%' with grant option; GRANT SELECT(id), INSERT (id,a) ON mydb.mytbl TO 'ua'@'%' with grant option;\n\n跟 db 权限类似，这两个权限每次 grant 的时候都会修改数据表，也会同步修改内存中的 hash 结构。因此，对这两类权限的操作，也会马上影响到已经存在的连接。\n\n看到这里，你一定会问，看来 grant 语句都是即时生效的，那这么看应该就**不需要执行 flush privileges 语句了呀。答案也确实是这样的。**\n\n  \n\n**flush privileges 命令会清空 acl_users 数组，然后从 mysql.user 表中读取数据重新加载，重新构造一个 acl_users 数组。也就是说，以数据表中的数据为准，会将全局权限内存数组重新加载一遍。**\n\n  \n\n  \n\n**因此，正常情况下，grant 命令之后，没有必要跟着执行 flush privileges 命令**。\n\n  \n\n# flush privileges 使用场景\n\nflush privileges 是在什么时候使用呢？显然，当数据表中的权限数据跟内存中的权限数据不一致的时候，flush privileges 语句可以用来重建内存数据，达到一致状态","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/31-%E8%A6%81%E4%B8%8D%E8%A6%81%E4%BD%BF%E7%94%A8%E5%88%86%E5%8C%BA%E8%A1%A8":{"title":"31 要不要使用分区表","content":"分区表有什么问题，为什么公司规范不让使用分区表呢?\n\n  \n\n# 分区表是什么?\n\n  \n\n\n```\nCREATE TABLE t (\n  ftime datetime NOT NULL,\n  c int(11) DEFAULT NULL,\n  KEY (ftime)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\nPARTITION BY RANGE (YEAR(ftime))\n(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB,\n PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB,\n PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB,\nPARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);\ninsert into t values('2017-4-1',1),('2018-4-1',1);\n```\n\n\n我在表 t 中初始化插入了两行记录，按照定义的分区规则**，这两行记录分别落在 p_2018 和 p_2019 这两个分区**上。\n\n这个表包含了一个.frm 文件和 4 个.ibd 文件，每个分区对应一个.ibd 文件。也就是说：\n\n- 对于引擎层来说，这是 4 个表；\n    \n- 对于 Server 层来说，这是 1 个表。\n    \n\n# 未完~~~","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/32.%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E6%80%8E%E4%B9%88%E5%8A%9E":{"title":"32.自增id用完怎么办？","content":"既然自增 id 有上限，就有可能被用完。但是，自增 id 用完了会怎么样呢？今天这篇文章，我们就来看看 MySQL 里面的几种自增 id，一起分析一下它们的值达到上限以后，会出现什么情况。\n\n  \n\n# 表定义自增值 id\n\n  \n\n**表定义的自增值达到上限后的逻辑是：再申请下一个 id 时，得到的值保持不变**\n\n  \n\n```\ncreate table t(id int unsigned auto_increment primary key) auto_increment=4294967295;\ninsert into t values(null);\n//成功插入一行 4294967295\nshow create table t;\n/* CREATE TABLE t (\n  id int(10) unsigned NOT NULL AUTO_INCREMENT,\n  PRIMARY KEY (id)\n) ENGINE=InnoDB AUTO_INCREMENT=4294967295;\n*/\n\ninsert into t values(null);\n//Duplicate entry '4294967295' for key 'PRIMARY'\n```\n\n以看到，第一个 insert 语句插入数据成功后，**这个表的 AUTO_INCREMENT 没有改变（还是 4294967295**），就导致了第二个 insert 语句又拿到相同的自增 id 值，再试图执行插入语句，报主键冲突错误\n\n  \n\n2(32 )-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成 8 个字节的 bigint unsigned。\n\n  \n\n# InnoDB 系统自增 row_id\n\n如果你创建的 InnoDB 表**没有指定主键**，那么 InnoDB 会给你创建一个不可见的，**长度为 6 个字节的 row_id**。\n\n。**InnoDB 维护了一个全局的 dict_sys.row_id 值**，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1\n\n  \n\n实际上，在代码实现时 row_id 是一个长度为 8 字节的无符号长整型 (bigint unsigned)。但是，InnoDB 在设计时，给 row_id 留的只是 6 个字节的长度，这样写到数据表中时只放了最后 6 个字节，所以 row_id 能写到数据表中的值，就有两个特征：\n\n- row_id 写入表中的值范围，是从 0 到 2(48)-1；\n    \n- 当 dict_sys.row_id=2(48)时，如果再有插入数据的行为要来申请 row_id，拿到以后再取最后 6 个字节的话就是 0。也就是说，写入表的 row_id 是从 0 开始到 248-1。**达到上限后，下一个值就是 0，然后继续循环**。\n    \n\n# Xid\n\n和你介绍 redo log 和 binlog 相配合的时候，提到了它们有一个共同的字段叫作 Xid。它在 MySQL 中是用来对应事务的。\n\n  \n\nXid 只需要不在同一个 binlog 文件中出现重复值即可。虽然理论上会出现重复值，但是概率极小，可以忽略不计。\n\n  \n\n# Innodb trx_id\n\nXid 和 InnoDB 的 trx_id 是两个容易混淆的概念。\n\n  \n\nXid 是由 server 层维护的。InnoDB 内部使用 Xid，就是为了能够在 InnoDB 事务和 server 之间做关联。但是，InnoDB 自己的 trx_id，是另外维护的\n\n  \n\nInnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。\n\nInnoDB 数据可见性的核心思想是：**每一行数据都记录了更新它的 trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的 trx_id 做对比**。\n\n而对于只读事务，InnoDB 并不会分配 trx_id。\n\n  \n\n**I nnoDB 的 max_trx_id 递增值每次 MySQL 重启都会被保存起来，所以我们文章中提到的脏读的例子就是一个必现的 bug，好在留给我们的时间还很充裕**\n\n  \n\n# thread_id\n\n线程 id 才是 MySQL 中最常见的一种自增 id\n\n  \n\nthread_id 的逻辑很好理解：系统保存了一个全局变量 thread_id_counter，每新建一个连接，就将 thread_id_counter 赋值给这个新连接的线程变量。\n\nthread_id_counter 定义的大小是 4 个字节，**因此达到 232-1 后，它就会重置为 0，然后继续增加**","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/4.%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E7%9A%84MySQL%E4%BC%9A%E6%8A%96%E4%B8%80%E4%B8%8B-Flush":{"title":"4.为什么我的MySQL会“抖”一下 --Flush","content":"一条 SQL 语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。\n\n  \n\n不知道你有没有遇到过这样的场景，一条 SQL 语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。\n\n  \n\n# 你的 SQL 语句为什么变“慢”了\n\n  \n\n  \n\n我为你介绍了 WAL 机制。现在你知道了，InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志）\n\n  \n\n  \n\n**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。**\n\n  \n\n  \n\n做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存。\n\n  \n\n  \n\n掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就**是 flush**\n\n  \n\n  \n\n象，平时执行很快的更新操作，其实就是在写内存和日志，而 MySQL 偶尔“抖”一下的那个瞬间，可能就是在**刷脏页（flush）**\n\n  \n\n什么情况会引发数据库的 flush 过程呢？\n\n- **对应的就是 InnoDB 的 redo log 写满了**。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写\n    \n    - 这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住\n        \n- **对应的就是系统内存不足**。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。\n    \n    - 这种情况其实是常态，InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：第一种是，还没有使用的；第二种是，使用了并且是干净页；第三种是，使用了并且是脏页。\n        \n    - InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。\n        \n    - 而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。\n        \n    - 所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的\n        \n        - 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；\n            \n        - 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的\n            \n- 对应的就是 MySQL 认为系统“空闲”的时候\n    \n- 对应的就是 MySQL 正常关闭的情况\n    \n\n  \n\n# InnoDB 刷脏页的控制策略\n\n  \n\n- 首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。\n    \n    - 这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力\n        \n- InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。\n    \n    - 参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。InnoDB 会根据当前的**脏页比例**（假设为 M），算出一个范围在 0 到 100 之间的数字，**F1(N)**\n        \n    - InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N,InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 **F2(N)**(只要知道 N 越大，算出来的值越大就好了)\n        \n- 根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。\n    \n\n![](statistic/asynccode-387.png)\n\n  \n\n  \n\n  \n\n一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。\n\nMySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候**，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉**；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。\n\n  \n\n  \n\n# 总结\n\n利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。\n\n但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/5.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A1%A8%E6%95%B0%E6%8D%AE%E5%88%A0%E6%8E%89%E4%B8%80%E5%8D%8A%E8%A1%A8%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%9A%84%E7%A9%BA%E9%97%B4%E5%9B%9E%E6%94%B6":{"title":"5.为什么表数据删掉一半，表文件大小不变---数据库表的空间回收","content":"一个 InnoDB 表包含两部分，即：**表结构定义和数据**\n\n- 在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。\n    \n- 而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小，\n    \n\n  \n\n先和你说明为什么简单地删除表数据达不到表空间回收的效果，然后再和你介绍正确回收空间的方法。\n\n  \n\n# 参数 innodb_file_per_table\n\n  \n\n表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的\n\n- 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；\n    \n- 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。\n    \n\n  \n\n我建议你不论使用 MySQL 的哪个版本，**都将这个值设置为 ON**。因为，**一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令**，**系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的**\n\n  \n\n  \n\n# 数据的删除流程\n\n  \n\ninnoDB 里的数据都是用 B+ 树的结构组织.\n\n![](statistic/asynccode-385.png)\n\n  \n\n我们要删掉 R4 这个记录，InnoDB 引擎只会把 R4 这个记录标记为删除。**如果之后要再插入一个 ID 在 300 和 600 之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小**。\n\n  \n\n现在，你已经知道了 I**nnoDB 的数据是按页存储的**，那么如果我们删掉了一个数据页上的所有记录，会怎么样？答案**是，整个数据页就可以被复用了。**\n\n  \n\n  \n\n数据页的复用跟记录的复用是不同的?\n\n- 记录的复用，只限于符合范围条件的数据\n    \n- 而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。\n    \n    - 图 1 为例，如果将数据页 page A 上的所有记录删除以后，page A 会被标记为可复用。这时候如果要插入一条 ID=50 的记录需要使用新页的时候，page A 是可以被复用的\n        \n\n  \n\n  \n\n如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。\n\n  \n\n如果我们用 delete 命令把整个表的数据删除呢？结果就是，所**有的数据页都会被标记为可复用**。但是磁盘上，文件不会变小.\n\n  \n\n**delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的**\n\n  \n\n  \n\n- **delete 命令是不能回收表空间的**。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。\n    \n- 不止是删除数据会造成空洞，插入数据也会\n    \n    - 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂\n        \n\n  \n\n  \n\n  \n\n经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的.重建表，就可以达到这样的目的。\n\n  \n\n# 重建表\n\n**可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中**\n\n  \n\nMySQL 5.6 版本开始引入的 **Online DDL**，对这个操作流程做了优化\n\n  \n\nOnline DDL 之后，重建表的流程：（copy on write）\n\n1. 建立一个临时文件，扫描表 A 主键的所有数据页\n    \n2. ；用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；\n    \n3. 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；\n    \n4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；\n    \n5. 用临时文件替换表 A 的数据文件。\n    \n\n![](statistic/asynccode-386.png)\n\n  \n\n  \n\n# 总结\n\n  \n\n如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过 alter table 命令重建表，才能达到表文件变小的目的。我跟你介绍了重建表的两种实现方式**，Online DDL 的方式是可以考虑在业务低峰期使用的，**而 MySQL 5.5 及之前的版本，这个命令是会阻塞 DML 的，这个你需要特别小心。最后，又到了我们的课后问题时间。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/7.-order-by-%E6%98%AF%E6%80%8E%E4%B9%88%E5%B7%A5%E4%BD%9C%E7%9A%84":{"title":"7. order by 是怎么工作的？","content":"```Go\n\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `city` varchar(16) NOT NULL,\n  `name` varchar(16) NOT NULL,\n  `age` int(11) NOT NULL,\n  `addr` varchar(128) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `city` (`city`)\n) ENGINE=InnoDB;\n```\n\n```Go\n\nselect city,name,age from t where city='杭州' order by name limit 1000  ;\n```\n\n# 全字段排序\n\n  \n\n为避免全表扫描，我们需要在 city 字段加上索引。\n\n  \n\n**MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。**\n\n  \n\n这个语句执行流程如下所示 ：\n\n1. 初始化 sort_buffer，确定放入 name、city、age 这三个字段；\n    \n2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X；\n    \n3. 到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；\n    \n4. 从索引 city 取下一个记录的主键 id；\n    \n5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；\n    \n6. 对 sort_buffer 中的数据按照字段 name 做快速排序；\n    \n7. 按照排序结果取前 1000 行返回给客户端。\n    \n\n  \n\n按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。\n\n- 如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。\n    \n- 但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。\n    \n    - 部排序一般使用归并排序算法。可以这么简单理解，MySQL 将需要排序的数据分成 多份份，每一份单独排序后存在这些临时文件中。然后把这些有序文件再合并成一个有序的大文件。\n        \n\n  \n\n# rowid 排序\n\n上面的算法：**只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的**。但这个算法有一个问题，就是**如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件**，排序的性能会很差。所以如果单行很大，这个方法效率不够好。\n\n  \n\n  \n\n如果 MySQL 认为排序的单行长度太大会怎么做呢？\n\nmax_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。\n\n  \n\n  \n\n新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。\n\n整个执行流程就变成如下所示的样子：\n\n1. 初始化 sort_buffer，确定放入两个字段，即 name 和 id；\n    \n2. 从索引 city 找到第一个满足 city='杭州’条件的主键 id，也就是图中的 ID_X；\n    \n3. 到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；\n    \n4. 从索引 city 取下一个记录的主键 id；\n    \n5. 重复步骤 3、4 直到不满足 city='杭州’条件为止，也就是图中的 ID_Y；\n    \n6. 对 sort_buffer 中的数据按照字段 name 进行排序；\n    \n7. 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。\n    \n\n  \n\n![](statistic/asynccode-384.png)\n\n  \n\n全字段排序 VS rowid 排序\n\n- 如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。\n    \n- 如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据\n    \n\n  \n\n- 对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。\n    \n\n  \n\n  \n\n是不是所有的 order by 都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。\n\n**并不是所有的 order by 语句，都需要排序操作的**。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。\n\n  \n\n如果能够保证从 city 这个索引上取出来的行，**天然就是按照 name 递增排序的话**，就不用再排序了。\n\n```Go\nalter table t add index city_user(city, name);\n```\n\n  \n\n覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/8.%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BA%E9%9A%8F%E6%9C%BA%E6%B6%88%E6%81%AF":{"title":"8.如何正确显示随机消息","content":"英语学习 App 首页有一个随机显示单词的功能，也就是根据每个用户的级别有一个单词表，然后这个用户每次访问首页的时候，都会随机滚动显示三个单词。他们发现随着单词表变大，选单词这个逻辑变得越来越慢，甚至影响到了首页的打开速度。\n\n```Go\n\nmysql\u003e CREATE TABLE `words` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `word` varchar(64) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB;\n\ndelimiter ;;\ncreate procedure idata()\nbegin\n  declare i int;\n  set i=0;\n  while i\u003c10000 do\n    insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));\n    set i=i+1;\n  end while;\nend;;\ndelimiter ;\n\ncall idata();\n```\n\n接下来，我们就一起看看要随机选择 3 个单词，有什么方法实现，存在什么问题以及如何改进\n\n# 内存临时表\n\n首先，你会想到用 order by rand() 来实现这个逻辑\n\n```Go\n\nmysql\u003e select word from words order by rand() limit 3;\n```\n\n**对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。**优化器没有了这一层顾虑，那么它会优先考虑的，就是用于排序的行越小越好了，所以，MySQL 这时就会选择 rowid 排序。\n\n  \n\n这条语句的执行流程是这样的\n\n1. 创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。\n    \n2. 从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。\n    \n3. 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。\n    \n4. 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。\n    \n5. **从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位置信息”），分别存入 sort_buffer 中的两个字段里**。**这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000**。\n    \n6. 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。（**rowid 排序方法，I**nnoDB 会自己生成一个长度为 6 字节的 rowid 来作为主键）\n    \n7. 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。\n    \n\n  \n\n![](statistic/asynccode-382.png)\n\n  \n\n  \n\n**order by rand() 使用了内存临时表，内存临时表排序的时候使用了 rowid 排序方法。**\n\n# 磁盘临时表\n\n  \n\ntmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。**如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。**\n\n  \n\n磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。\n\n  \n\n当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程\n\n  \n\n这个 SQL 语句的排序确实没有用到临时文件，采用是 MySQL 5.6 版本引入的一个新的排序算法，即：**优先队列排序算法**\n\n  \n\n优先队列算法，就可以精确地只得到三个最小值，执行流程如下\n\n1. 对于这 10000 个准备排序的 (R,rowid)，先取前三行，构造成一个堆\n    \n2. 取下一个行 (R’,rowid’)，跟当前堆里面最大的 R 比较，如果 R’小于 R，把这个 (R,rowid) 从堆中去掉，换成 (R’,rowid’)；\n    \n3. 重复第 2 步，直到第 10000 个 (R’,rowid’) 完成比较。\n    \n\n  \n\n![](statistic/asynccode-383.png)\n\n  \n\n# 总结\n\n如果你直接使用 **order by rand()**，这个语句需要 **Using temporary 和 Using filesort**，查询的执行代价往往是比较大的。所以，在设计的时候你要尽量避开这种写法","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/MySQL/%E5%AE%9E%E8%B7%B5/9.%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%BA%9BSQL%E8%AF%AD%E5%8F%A5%E9%80%BB%E8%BE%91%E7%9B%B8%E5%90%8C%E6%80%A7%E8%83%BD%E5%8D%B4%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7%E6%A1%88%E4%BE%8B%E4%B8%80%E6%9D%A1%E4%BB%B6%E5%AD%97%E6%AE%B5%E5%87%BD%E6%95%B0%E6%93%8D%E4%BD%9C":{"title":"9.为什么这些SQL语句逻辑相同，性能却差异巨大？案例一：条件字段函数操作","content":"# 案例一：条件字段函数操作\n\n假设你现在维护了一个交易系统，其中交易记录表 tradelog 包含交易流水号（tradeid）、交易员 id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：\n\n  \n\n```Go\nmysql\u003e CREATE TABLE tradelog (\n  id int(11) NOT NULL,\n  tradeid varchar(32) DEFAULT NULL,\n  operator int(11) DEFAULT NULL,\n  t_modified datetime DEFAULT NULL,\n  PRIMARY KEY (id),\n  KEY tradeid (tradeid),\n  KEY t_modified (t_modified)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n```\n\n现在已经记录了从 2016 年初到 2018 年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中 7 月份的交易记录总数。\n\n  \n\n```Go\nmysql\u003e select count(*) from tradelog where month(t_modified)=7;\n```\n\n**如果对字段做了函数计算，就用不上索引了，这是 MySQL 的规定。**对**索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**\n\n可以如下写法\n\n  \n\n```Go\nmysql\u003e select count(*) from tradelog where\n    -\u003e (t_modified \u003e= '2016-7-1' and t_modified\u003c'2016-8-1') or\n    -\u003e (t_modified \u003e= '2017-7-1' and t_modified\u003c'2017-8-1') or \n    -\u003e (t_modified \u003e= '2018-7-1' and t_modified\u003c'2018-8-1');\n```\n\n  \n\n//\n\n# 案例二：隐式类型转换\n\n```Go\nmysql\u003e select * from tradelog where tradeid=110717;\n```\n\n交易编号 tradeid 这个字段上，本来就有索引，但是 explain 的结果却显示，这条语句需要走全表扫描。你可能也发现了，**tradeid 的字段类型是 varchar(32)，而输入的参数却是整型**，所以需要做类型转换。\n\n  \n\n那么，现在这里就有两个问题：\n\n1. 数据类型转换的规则是什么？\n    \n\n- **字符串和数字做比较的话，是将字符串转换成数字**\n    \n\n1. 为什么有数据类型转换，就需要走全索引扫描？\n    \n\n- 对于优化器来说，这个语句相当于\n    \n\n```Go\nmysql\u003e select * from tradelog where  CAST(tradid AS signed int) = 110717;\n```\n\n# 案例三：隐式字符编码转换\n\n  \n\n因为这两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。\n\n  \n\n字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。\n\n  \n\n# 总结\n\n  \n\n**对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。**","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Nginx%E4%B8%8EOpenResty/%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"面试题","content":"![[18道非常牛逼的Nginx面试题！.pdf]]\n\n\n\n\n\n\n![[Nginx面试题.pdf]]\n\n\n\n![[Nginx面试专题.pdf]]\n\n\n\n![[Nginx实战书籍.pdf]]\n\n\n\n","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Nginx%E4%B8%8EOpenResty/OpenResty/%E5%9F%BA%E7%A1%80":{"title":"基础","content":"OpenResty（也称为 ngx_openresty）是一个全功能的 Web 应用服务器。**它打包了标准的 Nginx 核心，很多的常用的第三方模块，以及它们的大多数依赖项**。\n\n通过揉和众多设计良好的 Nginx 模块，OpenResty 有效地把 Nginx 服务器转变为一个强大的 Web 应用服务器，基于它开发人员可以使用 Lua 编程语言对 Nginx 核心以及现有的各种 Nginx C 模块进行脚本编程，构建出可以处理一万以上并发请求的极端高性能的 Web 应用\n\nngx_openresty 目前有两大应用目标：\n\n1. **通用目的的 web 应用服务器。**在这个目标下，现有的 web 应用技术都可以算是和 OpenResty 或多或少有些类似，比如 Nodejs, PHP 等等。ngx_openresty 的性能（包括内存使用和 CPU 效率）算是最大的卖点之一。\n    \n2. Nginx 的脚本扩展编程，**用于构建灵活的 Web 应用网关和 Web 应用防火墙**。有些类似的是 NetScaler。其优势在于 Lua 编程带来的巨大灵活性。\n\n# 安装 OpenResty\n\nhttps://openresty.org/cn/installation.html\n\n  \n\n  \n\n# hello,world\n\n```Go\n\n$ resty -e \"ngx.say('hello world')\"\nhello world\n```\n\n```Go\n\nresty -e \"ngx.say('hello world'); ngx.sleep(10)\" \n```\n\n加了一行 sleep 休眠的代码，让 resty 运行的程序打印出字符串后，并不退出\n\n```Go\n\n$ ps -ef | grep nginx\n501 25468 25462   0  7:24下午 ttys000    0:00.01 /usr/local/Cellar/openresty/''1.13.6.2/nginx/sbin/nginx -p /tmp/resty_AfNwigQVOB/ -c conf/nginx.conf\n```\n\n终于看了熟悉的 NGINX 进程。看来，resty 本质上是启动了一个 NGINX 服务\n\n# OpenResty CLI\n\n安装完 OpenResty 后，默认就已经把 OpenResty 的 CLI：resty 安装好了。resty是个 1000 多行的 Perl 脚本\n\nresty 的功能很强大，想了解完整的列表，你可以查看resty -h或者 [官方文档](https://github.com/openresty/resty-cli)\n\n  \n\n# 更真是的helloworld\n\n  \n\n写出这样的 OpenResty 程序并不简单，你至少需要三步才能完成：\n\n- 创建工作目录；\n    \n- 修改 NGINX 的配置文件，把 Lua 代码嵌入其中；\n    \n- 启动 OpenResty 服务。\n    \n\n  \n\n```Go\n\nmkdir hello_world\ncd hello_worl\nmkdir logs/ conf/\n```\n\n下面是一个最简化的 nginx.conf，在根目录下新增 OpenResty 的content_by_lua指令，里面嵌入了ngx.say的代码：\n\n  \n\n```Go\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    server {\n        listen 8080;\n        location / {\n            content_by_lua '\n                ngx.say(\"hello, world\")\n            ';\n        }\n    }\n}\n```\n\n然后，启动 OpenResty 服务就可以了：\n\n```Go\n\nopenresty -p `pwd` -c conf/nginx.conf\n```\n\n使用curl 命令查看返回结果\n\n```Go\n\n$ curl -i 127.0.0.1:8080\nHTTP/1.1 200 OK\nServer: openresty/1.13.6.2\nContent-Type: text/plain\nTransfer-Encoding: chunked\nConnection: keep-alive\n\nhello, world\n```\n\n  \n\n# 将lua 从nginx.conf 中抽取除了\n\n```Go\n\n$ mkdir lua\n$ cat lua/hello.lua\nngx.say(\"hello, world\")\n```\n\n修改nginx.conf把 content_by_lua_block 改为 content_by_lua_file：\n\n```Go\n\npid logs/nginx.pid;\nevents {\n  worker_connections 1024;\n}\n\nhttp {\n  server {\n    listen 8080;\n    location / {\n      content_by_lua_file lua/hello.lua;\n      }\n    }\n  }\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Nginx%E4%B8%8EOpenResty/Untitled":{"title":"Untitled","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Nginx%E4%B8%8EOpenResty/ngixn/%E5%9F%BA%E7%A1%80":{"title":"基础","content":"Nginx (\"engine x\") 是一个高性能的 HTTP 和反向代理服务器，也是一个 IMAP/POP3/SMTP 代理服务器。\n\n由于 Nginx 使用基于事件驱动的架构，能够并发处理百万级别的 TCP 连接，高度模块化的设计和自由的许可证使得扩展 Nginx 功能的第三方模块层出不穷。\n\n因此其作为 Web 服务器被广泛应用到大流量的网站上，包括淘宝、腾讯、新浪、京东等访问量巨大的网站。\n\n  \n\n# 新手起步\n\n## 为啥选择nginx\n\n1. 处理响应请求很快\n    \n\n在正常的情况下，单次请求会得到更快的响应。在高峰期，Nginx 可以比其它的 Web 服务器更快的响应请求。\n\n2. 高并发连接\n    \n\n在互联网快速发展，互联网用户数量不断增加的今天，一些大公司、网站都需要面对高并发请求，如果有一个能够在峰值顶住 10 万以上并发请求的 Server，肯定会得到大家的青睐。理论上，Nginx 支持的并发连接上限取决于你的内存，10 万远未封顶。\n\n3. 低的内存消耗\n    \n\n在一般的情况下，10000 个非活跃的 HTTP Keep-Alive 连接在 Nginx 中仅消耗 2.5MB 的内存，这也是 Nginx 支持高并发连接的基础。\n\n4. 具有很高的可靠性：\n    \n\nNginx 是一个高可靠性的 Web 服务器，这也是我们为什么选择 Nginx 的基本条件，现在很多的网站都在使用 Nginx，足以说明 Nginx 的可靠性。高可靠性来自其核心框架代码的优秀设计、模块设计的简单性，并且这些模块都非常的稳定。\n\n5. 高扩展性\n    \n\nNginx 的设计极具扩展性，它完全是由多个不同功能、不同层次、不同类型且耦合度极低的模块组成。这种设计造就了 Nginx 庞大的第三方模块。\n\n6. 热部署\n    \n\nmaster 管理进程与 worker 工作进程的分离设计，使得 Nginx 具有热部署的功能，可以在 7 × 24 小时不间断服务的前提下，升级 Nginx 的可执行文件。也可以在不停止服务的情况下修改配置文件，更换日志文件等功能。\n\n7. 自由的 BSD 许可协议\n    \n\nBSD 许可协议不只是允许用户免费使用 Nginx，也允许用户修改 Nginx 源码，还允许用户用于商业用途。\n\n  \n\n  \n\n## Nginx使用\n\n1. 获取 Nginx，在 [http://nginx.org/en/download.html](http://nginx.org/en/download.html) 上可以获取当前最新的版本。\n    \n2. 解压缩 nginx-xx.tar.gz 包。\n    \n3. 进入解压缩目录，执行 ./configure\n    \n4. make \u0026 make install\n    \n\n若安装时找不到上述依赖模块，使用 --with-openssl=`\u003copenssl_dir\u003e`、--with-pcre=`\u003cpcre_dir\u003e`、--with-zlib=`\u003czlib_dir\u003e` 指定依赖的模块目录。如已安装过，此处的路径为安装目录；若未安装，则此路径为编译安装包路径，Nginx 将执行模块的默认编译安装。\n\n启动 Nginx 之后，浏览器中输入 [http://localhost](http://localhost/) 可以验证是否安装启动成功。\n\nNginx 配置示例:\n\n安装完成之后，配置目录 conf 下有以下配置文件，过滤掉了 xx.default 配置：\n\n```Plaintext\nubuntu: /opt/nginx-1.7.7/conf$ tree |grep -v default\n.\n├── fastcgi.conf\n├── fastcgi_params\n├── koi-utf\n├── koi-win\n├── mime.types\n├── nginx.conf\n├── scgi_params\n├── uwsgi_params\n└── win-utf\n```\n\n除了 `nginx.conf`，其余配置文件，一般只需要使用默认提供即可。\n\n`nginx.conf` 是主配置文件，默认配置去掉注释之后的内容如下图所示：\n\n```Plaintext\nworker_process      # 表示工作进程的数量，一般设置为cpu的核数\n\nworker_connections  # 表示每个工作进程的最大连接数\n\nserver{}            # 块定义了虚拟主机\n    \n    listen          # 监听端口\n\n    server_name     # 监听域名\n\n    location {}     # 是用来为匹配的 URI 进行配置，URI 即语法中的“/uri/”\n    \n    location /{}    # 匹配任何查询，因为所有请求都以 / 开头\n        \n        root        # 指定对应uri的资源查找路径，这里html为相对路径，完整路径为\n                    # /opt/nginx-1.7.7/html/\n\n        index       # 指定首页index文件的名称，可以配置多个，以空格分开。如有多# 个，按配置顺序查找。\n```\n\n![](statistic/asynccode-557.png)\n\n  \n\n# Localtion 匹配规则\n\n\u003e location [=|~|~*|^~] /uri/ { … }\n\n|   |   |\n|---|---|\n|模式|含义|\n|location = /uri|#NAME?|\n|location ^~ /uri|^~ 开头对URL路径进行前缀匹配，并且在正则之前。|\n|location ~ pattern|开头表示区分大小写的正则匹配|\n|location ~* pattern|开头表示不区分大小写的正则匹配|\n|location /uri|不带任何修饰符，也表示前缀匹配，但是在正则匹配之后|\n|location /|通用匹配，任何未匹配到其它location的请求都会匹配到，相当于switch中的default|\n\n多个 location 配置的情况下匹配顺序为\n\n- 首先精确匹配 `=`\n    \n- 其次前缀匹配 `^~`\n    \n- 其次是按文件中顺序的正则匹配\n    \n- 然后匹配不带任何修饰的前缀匹配。\n    \n- 最后是交给 `/` 通用匹配\n    \n- 当有匹配成功时候，停止匹配，按当前匹配规则处理请求\n    \n\n  \n\n```Plaintext\nlocation = / {\n   echo \"规则A\";\n}\nlocation = /login {\n   echo \"规则B\";\n}\nlocation ^~ /static/ {\n   echo \"规则C\";\n}\nlocation ^~ /static/files {\n    echo \"规则X\";\n}\nlocation ~ \\.(gif|jpg|png|js|css)$ {\n   echo \"规则D\";\n}\nlocation ~* \\.png$ {\n   echo \"规则E\";\n}\nlocation /img {\n    echo \"规则Y\";\n}\nlocation / {\n   echo \"规则F\";\n}\n```\n\n那么产生的效果如下：\n\n- 访问根目录 `/`，比如 `http://localhost/` 将匹配 `规则A`\n    \n- 访问 `http://localhost/login` 将匹配 `规则B`，`http://localhost/register` 则匹配 `规则F`\n    \n- 访问 `http://localhost/static/a.html` 将匹配 `规则C`\n    \n- 访问 `http://localhost/static/files/a.exe` 将匹配 `规则X`，虽然 `规则C` 也能匹配到，但因为最大匹配原则，最终选中了 `规则X`。你可以测试下，去掉规则 X ，则当前 URL 会匹配上 `规则C`。\n    \n- 访问 `http://localhost/a.gif`, `http://localhost/b.jpg` 将匹配 `规则D` 和 `规则 E` ，但是 `规则 D` 顺序优先，`规则 E` 不起作用，而 `http://localhost/static/c.png` 则优先匹配到 `规则 C`\n    \n- 访问 `http://localhost/a.PNG` 则匹配 `规则 E` ，而不会匹配 `规则 D` ，因为 `规则 E` 不区分大小写。\n    \n- 访问 `http://localhost/img/a.gif` 会匹配上 `规则D`,虽然 `规则Y` 也可以匹配上，但是因为正则匹配优先，而忽略了 `规则Y`。\n    \n- 访问 `http://localhost/img/a.tiff` 会匹配上 `规则Y`。\n    \n\n  \n\n在实际的使用中至少有3个匹配规则\n\n```Lua\n# 直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。\n# 这里是直接转发给后端应用服务器了，也可以是一个静态首页\n# 第一个必选规则\nlocation = / {\n    proxy_pass http://tomcat:8080/index\n}\n\n# 第二个必选规则是处理静态文件请求，这是 nginx 作为 http 服务器的强项\n# 有两种配置模式，目录匹配或后缀匹配，任选其一或搭配使用\nlocation ^~ /static/ {\n    root /webroot/static/;\n}\nlocation ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ {\n    root /webroot/res/;\n}\n\n# 第三个规则就是通用规则，用来转发动态请求到后端应用服务器\n# 非静态文件请求就默认是动态请求，自己根据实际把握\n# 毕竟目前的一些框架的流行，带.php、.jsp后缀的情况很少了\nlocation / {\n    proxy_pass http://tomcat:8080/\n}\n```\n\n#### **redirect 语法**\n\n```Plaintext\nserver {\n    listen 80;\n    server_name start.igrow.cn;\n    index index.html index.php;\n    root html;\n    if ($http_host !~ \"^star\\.igrow\\.cn$\") {\n        rewrite ^(.*) http://star.igrow.cn$1 redirect;\n    }\n}\n```\n\n#### **防盗链**\n\n```Plaintext\nlocation ~* \\.(gif|jpg|swf)$ {\n    valid_referers none blocked start.igrow.cn sta.igrow.cn;\n    if ($invalid_referer) {\n       rewrite ^/ http://$host/logo.png;\n    }\n}\n```\n\n#### **根据文件类型设置过期时间**\n\n```Plaintext\nlocation ~* \\.(js|css|jpg|jpeg|gif|png|swf)$ {\n    if (-f $request_filename) {\n        expires 1h;\n        break;\n    }\n}\n```\n\n#### **禁止访问某个目录**\n\n```Plaintext\nlocation ~* \\.(txt|doc)${\n    root /data/www/wwwroot/linuxtone/test;\n    deny all;\n}\n```\n\n一些可用的全局变量，可以参考[获取 Nginx 内置绑定变量](https://moonbingbing.gitbooks.io/openresty-best-practices/content/openresty/inline_var.html)章节","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Nginx%E4%B8%8EOpenResty/ngixn/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85":{"title":"编译安装","content":"Nginx 编译安装与配置使用\n\n1.查看系统版本/安装常用软件(系统最小化安装)\n\n```PHP\n#cat /etc/centos-release\nCentOS Linux release 7.6.1810 (Core) \n```\n\n### 1、安装编译环境\n\n```Lua\n yum -y install gcc gcc-c++\n```\n\n### 2、安装pcre软件包（使nginx支持http rewrite模块）\n\n```Lua\nyum install -y pcre pcre-devel\n```\n\n### 3、安装 openssl-devel（使 nginx 支持 ssl）\n\n```Lua\nyum install -y openssl openssl-devel \n```\n\n### 4、安装zlib\n\n```Lua\nyum install -y zlib zlib-devel gd gd-devel\n```\n\n### 5、创建用户 nginx\n\n```Lua\nuseradd -s /sbin/nologin nginx \n```\n\n### 6、编译安装 nginx\n\n### 1、下载安装包\n\n```Lua\nwget http://nginx.org/download/nginx-1.16.0.tar.gz\n# tar -zxvf nginx-1.16.0.tar.gz\n# cd nginx-1.16.0\n```\n\n### 2、编译配置\n\n```PHP\n[root@localhost.com nginx-1.16.0]# ./configure --prefix=/usr/local/nginx \\\n--user=nginx \\\n--group=nginx \\\n--with-pcre \\\n--with-http_ssl_module \\\n--with-http_v2_module \\\n--with-http_realip_module \\\n--with-http_addition_module \\\n--with-http_sub_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_mp4_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_random_index_module \\\n--with-http_secure_link_module \\\n--with-http_stub_status_module \\\n--with-http_auth_request_module \\\n--with-http_image_filter_module \\\n--with-http_slice_module \\\n--with-mail \\\n--with-threads \\\n--with-file-aio \\\n--with-stream \\\n--with-mail_ssl_module \\\n--with-stream_ssl_module \n    \n[root@localhost.com nginx-1.16.0]#make \u0026\u0026 make install\n[root@localhost.com nginx-1.16.0]# cd /usr/local/nginx/sbin\n[root@localhost.com sbin]# ./nginx              # 启动Nginx\n[root@localhost.com sbin]# ./nginx -t           # 验证配置文件是正确\n[root@localhost.com sbin]# ./nginx -s reload    # 重启Nginx\n[root@localhost.com sbin]# ./nginx -s stop      # 停止Nginx\n[root@localhost.com sbin]# ./nginx -v            # 查看是否安装成功\nnginx version: nginx/1.16.0[root@localhost.com sbin]# netstat -ntlp | grep nginx # 查看是否启动\ntcp   0    0 0.0.0.0:80     0.0.0.0:*     LISTEN    20949/nginx: master\n```\n\n### 3、配置 Nginx 命令和服务并开机启动\n\n### 1、创建服务配置文件\n\n```PHP\n[root@localhost.com sbin]# vim /usr/lib/systemd/system/nginx.service\n[Unit]Description=nginx - high performance web serverDocumentation=http://nginx.org/en/docs/After=network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/usr/local/nginx/logs/nginx.pidExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.confExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confExecReload= /usr/local/nginx/sbin/nginx -s reloadExecStop= /usr/local/nginx/sbin/nginx -s stopPrivateTmp=true[Install]WantedBy=multi-user.target\n```\n\n### 2、添加执行权限\n\n```PHP\n[root@localhost.com sbin]# chmod +x /usr/lib/systemd/system/nginx.service\n```\n\n### 3、启动服务(先停止nginx服务)\n\n```PHP\n[root@localhost.com sbin]## systemctl daemon-reload\n[root@localhost.com sbin]# systemctl start nginx.service   # 启动\n[root@localhost.com sbin]#systemctl stop nginx.service    # 停止\n[root@localhost.com sbin]# systemctl reload nginx.service  # 修改配置后重新加载生效 \n[root@localhost.com sbin]# systemctl restart nginx.service # 重启\n[root@localhost.com sbin]# systemctl status nginx   # 查看服务是否启动\n```\n\n### 4、添加开机启动\n\n```PHP\n[root@localhost.com sbin]# systemctl enable nginx.service\nCreated symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service\n```\n\n### 5、Nginx 编译参数\n\n```Lua\n# 查看 nginx 安装的模块\n[root@localhost.com sbin]# nginx -V\n\n# 模块参数具体功能 \n--with-cc-opt='-g -O2 -fPIE -fstack-protector'   # 设置额外的参数将被添加到CFLAGS变量。（FreeBSD或者ubuntu使用）\n--param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2' \n--with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' \n\n--prefix=/usr/share/nginx                        # 指向安装目录\n--conf-path=/etc/nginx/nginx.conf                # 指定配置文件\n--http-log-path=/var/log/nginx/access.log        # 指定访问日志\n--error-log-path=/var/log/nginx/error.log        # 指定错误日志\n--lock-path=/var/lock/nginx.lock                 # 指定lock文件\n--pid-path=/run/nginx.pid                        # 指定pid文件\n\n--http-client-body-temp-path=/var/lib/nginx/body    # 设定http客户端请求临时文件路径\n--http-fastcgi-temp-path=/var/lib/nginx/fastcgi     # 设定http fastcgi临时文件路径\n--http-proxy-temp-path=/var/lib/nginx/proxy         # 设定http代理临时文件路径\n--http-scgi-temp-path=/var/lib/nginx/scgi           # 设定http scgi临时文件路径\n--http-uwsgi-temp-path=/var/lib/nginx/uwsgi         # 设定http uwsgi临时文件路径\n\n--with-debug                                        # 启用debug日志\n--with-pcre-jit                                     # 编译PCRE包含“just-in-time compilation”\n--with-ipv6                                         # 启用ipv6支持\n--with-http_ssl_module                              # 启用ssl支持\n--with-http_stub_status_module                      # 获取nginx自上次启动以来的状态\n--with-http_realip_module                 # 允许从请求标头更改客户端的IP地址值，默认为关\n--with-http_auth_request_module           # 实现基于一个子请求的结果的客户端授权。如果该子请求返回的2xx响应代码，所述接入是允许的。如果它返回401或403中，访问被拒绝与相应的错误代码。由子请求返回的任何其他响应代码被认为是一个错误。\n--with-http_addition_module               # 作为一个输出过滤器，支持不完全缓冲，分部分响应请求\n--with-http_dav_module                    # 增加PUT,DELETE,MKCOL：创建集合,COPY和MOVE方法 默认关闭，需编译开启\n--with-http_geoip_module                  # 使用预编译的MaxMind数据库解析客户端IP地址，得到变量值\n--with-http_gunzip_module                 # 它为不支持“gzip”编码方法的客户端解压具有“Content-Encoding: gzip”头的响应。\n--with-http_gzip_static_module            # 在线实时压缩输出数据流\n--with-http_image_filter_module           # 传输JPEG/GIF/PNG 图片的一个过滤器）（默认为不启用。gd库要用到）\n--with-http_spdy_module                   # SPDY可以缩短网页的加载时间\n--with-http_sub_module                    # 允许用一些其他文本替换nginx响应中的一些文本\n--with-http_xslt_module                   # 过滤转换XML请求\n--with-mail                               # 启用POP3/IMAP4/SMTP代理模块支持\n--with-mail_ssl_module                    # 启用ngx_mail_ssl_module支持启用外部模块支持\n```\n\n### 6、Nginx 配置文件\n\n```Plain\n[root@localhost.com sbin]# vim /usr/local/nginx/conf/nginx.conf\n# 全局参数设置 \nworker_processes  1;          # 设置nginx启动进程的数量，一般设置成与逻辑cpu数量相同 \nerror_log  logs/error.log;    # 指定错误日志 \nworker_rlimit_nofile 102400;  # 设置一个nginx进程能打开的最大文件数 \npid        /var/run/nginx.pid; \nevents {                      # 事件配置\n    worker_connections  10240; # 设置一个进程的最大并发连接数\n    use epoll;                # 事件驱动类型\n} \n# http 服务相关设置 \nhttp {  \n    log_format  main  'remote_addr - remote_user [time_local] \"request\" '\n                      'status body_bytes_sent \"$http_referer\" '\n                      '\"http_user_agent\" \"http_x_forwarded_for\"'; \n    access_log  /var/log/nginx/access.log  main;    #设置访问日志的位置和格式 \n    sendfile          on;      # 用于开启文件高效传输模式，一般设置为on，若nginx是用来进行磁盘IO负载应用时，可以设置为off，降低系统负载\n    tcp_nopush        on;      # 减少网络报文段数量，当有数据时，先别着急发送, 确保数据包已经装满数据, 避免了网络拥塞\n    tcp_nodelay       on;      # 提高I/O性能，确保数据尽快发送, 提高可数据传输效率                           \n    gzip              on;      # 是否开启 gzip 压缩 \n    keepalive_timeout  65;     # 设置长连接的超时时间，请求完成之后还要保持连接多久，不是请求时间多久，目的是保持长连接，减少创建连接过程给系统带来的性能损                                    耗，类似于线程池，数据库连接池\n    types_hash_max_size 2048;  # 影响散列表的冲突率。types_hash_max_size 越大，就会消耗更多的内存，但散列key的冲突率会降低，检索速度就更快。                                            types_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能上升\n    include             /etc/nginx/mime.types;  # 关联mime类型，关联资源的媒体类型(不同的媒体类型的打开方式)\n    default_type        application/octet-stream;  # 根据文件的后缀来匹配相应的MIME类型，并写入Response header，导致浏览器播放文件而不是下载\n# 虚拟服务器的相关设置 \n    server { \n        listen      80;        # 设置监听的端口 \n        server_name  localhost;        # 设置绑定的主机名、域名或ip地址 \n        charset koi8-r;        # 设置编码字符 \n        location / { \n            root  /var/www/nginx;           # 设置服务器默认网站的根目录位置 \n            index  index.html index.htm;    # 设置默认打开的文档 \n            } \n        error_page  500 502 503 504  /50x.html; # 设置错误信息返回页面 \n            location = /50x.html { \n            root  html;        # 这里的绝对位置是/var/www/nginx/html \n        } \n    } \n }\n```\n\n### 7、检测 nginx 配置文件是否正确\n\n```Plain\n[root@localhost.com sbin]#usr/local/nginx/sbin/nginx -t\n```\n\n### 8、启动 nginx 服务\n\n```Plain\n[root@localhost.com sbin]# /usr/local/nginx/sbin/nginx\n```\n\n### 9、Nginx 命令控制\n\n```Plain\nnginx -c /path/to/nginx.conf     # 以特定目录下的配置文件启动nginx:\nnginx -s reload                  # 修改配置后重新加载生效\nnginx -s reopen                  # 重新打开日志文件\nnginx -s stop                    # 快速停止nginx\nnginx -s quit                    # 完整有序的停止nginx\nnginx -t                         # 测试当前配置文件是否正确\nnginx -t -c /path/to/nginx.conf  # 测试特定的nginx配置文件是否正确\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Obsidian/Obsidian-plugin":{"title":"Obsidian-plugin","content":"\n* [advanced-table](https://github.com/tgrosinger/advanced-tables-obsidian)\n* [banners](https://github.com/noatpad/obsidian-banners)\n* [calendar](https://github.com/liamcain/obsidian-calendar-plugin)\n* [commander](https://github.com/phibr0/obsidian-commander)\n* [dataview](https://github.com/blacksmithgu/obsidian-dataview)\n* [emoji-shortcodes](https://github.com/phibr0/obsidian-emoji-shortcodes)\n* [emoji-toolbar](https://github.com/oliveryh/obsidian-emoji-toolbar)\n* [excel-to-markdown-table](https://github.com/ganesshkumar/obsidian-excel-to-markdown-table)\n* [homepage](https://github.com/mirnovov/obsidian-homepage)\n* [hover-editor](https://github.com/nothingislost/obsidian-hover-editor)\n* [icon-folder)](https://github.com/FlorianWoelki/obsidian-icon-folder)\n* [icons](https://github.com/visini/obsidian-icons-plugin)\n* [image-toolkit](https://github.com/sissilab/obsidian-image-toolkit)\n* [minimal-settings](https://github.com/kepano/obsidian-minimal-settings)\n* [obsidian-git](https://github.com/denolehov/obsidian-git)\n* [recent-files](https://github.com/tgrosinger/recent-files-obsidian)\n* [settings-search](https://github.com/javalent/settings-search)\n* [style-settings](https://github.com/mgmeyers/obsidian-style-settings)\n* [tag-wrangler](https://github.com/pjeby/tag-wrangler)\n* [excalidraw](https://github.com/zsviczian/obsidian-excalidraw-plugin)","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Obsidian/dataview":{"title":"dataview","content":"#plugin \n\n# 官方地址\n\n* [代码仓库](https://github.com/blacksmithgu/obsidian-dataview)\n* [文档地址](https://blacksmithgu.github.io/obsidian-dataview/)\n\n# 其他教程\n*  [Obsidian DataView 入门保姆级引导手册](https://zhuanlan.zhihu.com/p/614881764)\n\n# 元数据\n\n元数据是一系列的键值对,可以给笔记，可以给note,list item ,task 添加元数据\n\n## 如何添加元数据\n\n### Frontmatter\n\n* frontmatter 是markdown的一种扩展，可以使用yaml 来添加元数据\n\n```\n --- \n alias: \"document\" \n last-reviewed: 2021-08-17 \n thoughts: \n\t rating: 8 \n\t reviewable: false \n ---\n```\n\n###  inline fields\n* 使用方法为在文件的任意位置添加\n```text\n\nBasic Field:: Some random Value \n**Bold Field**:: Nice!\n  \n```\n\n* 如果你需要标注list itme 或者 task 需要使用中括号\n```\n- [ ] Send an mail to David about the deadline [due:: 2022-04-05].\n```\n\n\n# 另外还有隐含的元数据\n\n## page 中的元数据\n\n\n[# Metadata on Pages](https://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-pages/)\n\n\n| Field Name       | Data Type      | Description                                                                                                                                                                   |\n|------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| file.name        | Text           | The file name as seen in Obsidians sidebar.                                                                                                                                   |\n| file.folder      | Text           | The path of the folder this file belongs to.                                                                                                                                  |\n| file.path        | Text           | The full file path, including the files name.                                                                                                                                 |\n| file.ext         | Text           | The extension of the file type; generally md.                                                                                                                                 |\n| file.link        | Link           | A link to the file.                                                                                                                                                           |\n| file.size        | Number         | The size (in bytes) of the file.                                                                                                                                              |\n| file.ctime       | Date with Time | The date that the file was created.                                                                                                                                           |\n| file.cday        | Date           | The date that the file was created.                                                                                                                                           |\n| file.mtime       | Date with Time | The date that the file was last modified.                                                                                                                                     |\n| file.mday        | Date           | The date that the file was last modified.                                                                                                                                     |\n| file.tags        | List           | A list of all unique tags in the note. Subtags are broken down by each level, so #Tag/1/A will be stored in the list as [#Tag, #Tag/1, #Tag/1/A].                             |\n| file.etags       | List           | A list of all explicit tags in the note; unlike file.tags, does not break subtags down, i.e. [#Tag/1/A]                                                                       |\n| file.inlinks     | List           | A list of all incoming links to this file, meaning all files that contain a link to this file.                                                                                |\n| file.outlinks    | List           | A list of all outgoing links from this file, meaning all links the file contains.                                                                                             |\n| file.aliases     | List           | A list of all aliases for the note as defined via the YAML frontmatter.                                                                                                       |\n| file.tasks       | List           | A list of all tasks (I.e., \\| [ ] some task) in this file.                                                                                                                    |\n| file.lists       | List           | A list of all list elements in the file (including tasks); these elements are effectively tasks and can be rendered in task views.                                            |\n| file.frontmatter | List           | Contains the raw values of all frontmatter in form of key \\| value text values; mainly useful for checking raw frontmatter values or for dynamically listing frontmatter keys. |\n| file.day         | Date           | Only available if the file has a date inside its file name (of form yyyy-mm-dd or yyyymmdd), or has a Date field/inline field.                                                |\n| file.starred     | Boolean        | if this file has been starred via the Obsidian Core Plugin \"Starred Files\".                                                                                                   |\n\n\n## 列表和任务中的元数据\n\n[# Metadata on Tasks and Lists](https://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-tasks/)\n\n| Field name     | Data Type | Description                                                                                                                                                                                                                                                                                               |\n|----------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| status         | Text      | The completion status of this task, as determined by the character inside the [ ] brackets. Generally a space \" \" for incomplete tasks and a \"x\" for complete tasks, but allows for plugins which support alternative task statuses.                                                                      |\n| checked        | Boolean   | Whether or not this task status is empty, meaning it has a space in its [ ] brackets                                                                                                                                                                                                                      |\n| completed      | Boolean   | Whether or not this specific task has been completed; this does not consider the completionnon-completion of any child tasks. A task is explicitly considered \"completed\" if it has been marked with an 'x'. If you use a custom status, i.e. [-], checked will be true, whereas completed will be false. |\n| fullyCompleted | Boolean   | Whether or not this task and all of its subtasks are completed.                                                                                                                                                                                                                                           |\n| text           | Text      | The plain text of this task, including any metadata field annotations.                                                                                                                                                                                                                                    |\n| visual         | Text      | The text of this task, which is rendered by Dataview. It can be modified to render arbitary text.                                                                                                                                                                                                         |\n| line           | Number    | The line of the file this task shows up on.                                                                                                                                                                                                                                                               |\n| lineCount      | Number    | The number of Markdown lines that this task takes up.                                                                                                                                                                                                                                                     |\n| path           | Text      | The full path of the file this task is in. Equals to file.path for pages                                                                                                                                                                                                                                  |\n| section        | Link      | link to the section this task is contained in.                                                                                                                                                                                                                                                            |\n| tags           | List      | Any tags inside of the text task.                                                                                                                                                                                                                                                                         |\n| outlinks       | List      | Any links defined in this task.                                                                                                                                                                                                                                                                           |\n| link           | Link      | link to the closest linkable block near this task; useful for making links which go to the task.                                                                                                                                                                                                          |\n| children       | List      | ny subtasks or sublists of this task.                                                                                                                                                                                                                                                                     |\n| task           | Boolean   | If true, this is a task; otherwise, it is a regular list element.                                                                                                                                                                                                                                         |\n| annotated      | Boolean   | True if the task text contains any metadata fields, false otherwise.                                                                                                                                                                                                                                      |\n| parent         | Number    | The line number of the task above this task, if present; will be null if this is a root-level task.                                                                                                                                                                                                       |\n| blockId        | Text      | The block ID of this task / list element, if one has been defined with the ^blockId syntax; otherwise null.                                                                                                                                                                                               |\n\n\n# DQL\n\n比较类似于sql, 可是实现以下的功能\n\n- Choosing an **output format** of your output (the [Query Type](https://blacksmithgu.github.io/obsidian-dataview/queries/query-types/))\n- Fetch pages **from a certain [source](https://blacksmithgu.github.io/obsidian-dataview/reference/sources/)**, i.e. a tag, folder or link\n- **Filtering pages/data** by simple operations on fields, like comparison, existence checks, and so on\n- **Transforming fields** for displaying, i.e. with calculations or splitting up multi-value fields\n- **Sorting** results based on fields\n- **Grouping** results based on fields\n- **Limiting** your result count\n\n## 查询语法\n\n```text\n```dataview \n\t\u003cQUERY-TYPE\u003e \u003cfields\u003e \n\tFROM \u003csource\u003e \n\t\u003cDATA-COMMAND\u003e \u003cexpression\u003e \n\t\u003cDATA-COMMAND\u003e \u003cexpression\u003e \n\t...\n```\n```\t\n```\n\n\n## 输出类型\n\n* **TABLE**: A table of results with one row per result and one or many columns of **field data**.\n* **LIST**: A bullet point list of **pages** which match the query. You can output one field for each page alongside their file links.\n* **TASK**: An interactive task list of **tasks** that match the given query.\n* **CALENDAR**: A calendar view displaying each hit via a dot on its referred date.\n\n\n```text\nLists all pages in your vault as a bullet point list\n\t```dataview \n\tLIST \n\t```\n\t\nLists all tasks (completed or not) in your vault \n\t```dataview \n\tTASK \n\t```\n\t\nRenders a Calendar view where each page is represented as a dot on its creation date. \n\t```dataview \n\tCALENDAR file.cday \n\t```\n\t\nShows a table with all pages of your vault, their field value of due, the files' tags and an average of the values of multi-value field working-hours \n\t```dataview \n\tTABLE due, file.tags AS \"tags\", average(working-hours)\n\t ```\n\n```\n\n\n## 数据来源\n\n* tags\n* folders\n* note\n* lint\n\n```\nLists all pages inside the folder Books and its sub folders \n\t```dataview \n\tLIST FROM \"Books\" \n\t``` \n\t\nLists all pages that include the tag #status/open or #status/wip \n\t```dataview \n\tLIST FROM #status/open OR #status/wip \n\t``` \n\t\nLists all pages that have either the tag #assignment and are inside folder \"30 School\" (or its sub folders), or are inside folder \"30 School/32 Homeworks\" and are linked on the page School Dashboard Current To Dos \n\n\t```dataview \n\tLIST FROM (#assignment AND \"30 School\") OR (\"30 School/32 Homeworks\" AND outgoing([[School Dashboard Current To Dos]])) \n\t```\n\n```\n\n\n## Filter, sort, group or limit results\n\n* ***FROM** like explained [above](https://blacksmithgu.github.io/obsidian-dataview/queries/structure/#choose-your-source).\n*  **WHERE**: Filter notes based on information **inside** notes, the meta data fields.\n*  **SORT**: Sorts your results depending on a field and a direction.\n*  **GROUP BY**: Bundles up several results into one result row per group.\n*  **LIMIT**: Limits the result count of your query to the given number.\n*  **FLATTEN**: Splits up one result into multiple results based on a field or calculation.\n\n\n```\nLists all pages that have a metadata field `due` and where `due` is before today \n\n\t```dataview \n\tLIST WHERE due AND due \u003c date(today) \n\t``` \nLists the 10 most recently created pages in your vault that have the tag #status/open \n\t```dataview \n\tLIST FROM #status/open SORT file.ctime DESC LIMIT 10 \n\t``` \nLists the 10 oldest and incompleted tasks of your vault as an interactive task list, grouped by their containing file and sorted from oldest to newest file. \n\t```dataview \n\tTASK WHERE !completed SORT created ASC LIMIT 10 GROUP BY file.link SORT rows.file.ctime ASC \n\t```\n\n\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Obsidian/excalidraw":{"title":"excalidraw","content":"#plugin\n\n*  [代码仓库](https://github.com/zsviczian/obsidian-excalidraw-plugin)\n* note 中插入excalidraw 语法\n\n```\n![[excalidraw]]\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Obsidian/obsidian-overview":{"title":"obsidian overview","content":"#overview\n\n# 主页内容\n\nobsidian 相关内容，包括插件\n\n\n# 结构\n\n","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Obsidian/template":{"title":"template","content":"#plugin \n\n* *[模板的使用方法](https://publish.obsidian.md/help-zh/%E6%8F%92%E4%BB%B6/%E6%A8%A1%E6%9D%BF)\n* 默认存放的文件夹 `/template`\n\n","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/1.Pipenv%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83":{"title":"1.Pipenv\u0026虚拟环境","content":"# 安装\n\n确保python和pip安装\n\n```Plain\npython --version\npip --version\n```\n\n安装\n\n```Plain\npip install --user pipenv\n```\n\n该安装是用户安装，需要将起添加到环境变量总\n\n  \n\n# 常用操作\n\n1. 在当前环境，初始化虚拟环境\n    \n\n```Plain\npipenv --two\npipenv --three\npipenv --python 3\npipenv --python 3.6\npipenv --python 2.7.1\n```\n\n2. 修改依赖镜像，想要安装依赖包速度快点，可以修改Pipfile文件里面的源(直接修改url地址)，常见的国内python加速镜像源：\n    \n\n```JSON\nhttp://mirrors.aliyun.com/pypi/simple/ //阿里\nhttps://pypi.tuna.tsinghua.edu.cn/simple/ //清华\nhttp://pypi.douban.com/ //豆瓣\nhttp://pypi.hustunique.com/ //华中理工大学\nhttp://pypi.sdutlinux.org/ //山东理工大学\nhttp://pypi.mirrors.ustc.edu.cn/ //中国科学技术大学\n```\n\n3. 启动当前目录下的虚拟环境，如果当前目录下无虚拟环境则会在当前目录下自动创建虚拟环境。启动了虚拟环境，执行python脚本才能正确找到依赖模块\n    \n\n```JSON\npipenv shell\n```\n\n4. 推出当前的虚拟环境的安装路径\n    \n\n```JSON\nexit\n```\n\n5. 安装或卸载依赖模块到虚拟环境中\n    \n\n```JSON\npipenv install XXX  # 安装XXX模块并加入到Pipfile\npipenv install XXX==1.11  # 安装固定版本的XXX模块并加入到Pipfile\npipenv install pytest --dev # 仅仅安装开发环境下的依赖包（项目部署上线不需要的包）\n     \npipenv uninstall XXX  # 卸载XXX模块并从Pipfile中移除\npipenv uninstall --all  # 卸载全部包并从Pipfile中移除\npipenv uninstall --all-dev  # 卸载全部开发包并从Pipfile中移除\n```\n\n6. pipenv graph 查看当前虚拟环境下的所有依赖\n    \n\n```JSON\npipenv graph\n```\n\n7. 升级包和依赖\n    \n\n```JSON\npipenv update --outdated  # 查看所有需要更新的依赖项\npipenv update  # 更新所有包的依赖项\npipenv update \u003c包名\u003e  # 更新指定的包的依赖项\n```\n\n8. 不激活虚拟环境下使用虚拟环境中的解释器执行 python 程序\n    \n\n```JSON\npipenv run python test.py \n```\n\n9. 通过requirements 安装依赖\n    \n\n```JSON\npipenv install -r requirements.txt\npipenv install -r --dev requirements.txt  # 只安装开发包\n```\n\n10. 初始化项目\n    \n\n```JSON\nmkdir project\ncd project\npipenv --python 3\n```\n\n11. 其他\n    \n\n```JSON\ngit clone project.git\n\ncd project\n\npip3 install pipenv\n\npipenv shell\n\npipenv sync # 如果要安装跟Pipfile.lock版本一致的包，则执行pipenv sync; \npipenv install # 如果不需要与Pipfile.lock版本一致，则执行pipenv install即可\n\npipenv run python main.py\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/10.%E6%B5%8B%E8%AF%95":{"title":"10.测试","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/11.%E6%97%A5%E5%BF%97":{"title":"11.日志","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/2.%E5%B7%A5%E7%A8%8B%E7%9A%84%E7%BB%93%E6%9E%84":{"title":"2.工程的结构","content":"在一个健康的开发周期中，代码风格，API 设计和自动化是非常关键的。同样的，对于工程的 [架构](http://www.amazon.com/gp/product/1257638017/ref=as_li_ss_tl?ie=UTF8\u0026tag=bookforkind-20\u0026linkCode=as2\u0026camp=1789\u0026creative=39095\u0026creativeASIN=1257638017) ,仓库的结构也是关键的一部分。\n\n当一个潜在的用户和贡献者登录到您的仓库页面时，他们会看到这些:\n\n- 工程的名字\n    \n- 工程的描述\n    \n- 一系列的文件\n    \n\n只有当他们滚动到目录下方时才会看到您工程的README。\n\n如果您的仓库的目录是一团糟，没有清晰的结构，他们可能要到处寻找才能找到您写的漂亮的文档。\n\n\u003e 为您的渴望的事业而奋斗，而不是仅仅只为您现在的工作而工作。\n\n当然，第一印象并不是一切。但是，您和您的同事会和这个仓库并肩战斗很长时间，会熟悉它的每一个角落和细节。拥有良好的布局，事半功倍。\n\n# 仓库样例\n\n**请看这里**: 这是 [Kenneth Reitz](http://kennethreitz.org/) 推荐的。\n\n这个仓库 [可以在GitHub上找到](https://github.com/kennethreitz/samplemod) 。\n\n```JSON\nREADME.rst\nLICENSE\nsetup.py\nrequirements.txt\nsample/__init__.py\nsample/core.py\nsample/helpers.py\ndocs/conf.py\ndocs/index.rst\ntests/test_basic.py\ntests/test_advanced.py\n```\n\n让我们看一下细节。\n\n## 真正的模块\n\n|   |   |\n|---|---|\n|布局|./sample/ or ./sample.py|\n|作用|核心代码|\n\n您的模块包是这个仓库的核心，它不应该隐藏起来:\n\n```JSON\n./sample/\n```\n\n如果您的模块只有一个文件，那么您可以直接将这个文件放在仓库的根目录下:\n\n```JSON\n./sample.py\n```\n\n这个模块文件不应该属于任何一个模棱两可的src或者python子目录。\n\n## License\n\n|   |   |\n|---|---|\n|布局|./LICENSE|\n|作用|许可证.|\n\n除了源代码本身以外，这个毫无疑问是您仓库最重要的一部分。在这个文件中要有完整的许可说明和授权。\n\n如果您不太清楚您应该使用哪种许可方式，请查看 [choosealicense.com](http://choosealicense.com/).\n\n当然，您也可以在发布您的代码时不做任何许可说明，但是这显然阻碍潜在的用户使用您的代码。\n\n## Setup.py\n\n|   |   |\n|---|---|\n|布局|./setup.py|\n|作用|打包和发布管理|\n\n如果您的模块包在您的根目录下，显然这个文件也应该在根目录下。\n\n## Requirements File\n\n|   |   |\n|---|---|\n|布局|./requirements.txt|\n|作用|开发依赖.|\n\n一个 [pip requirements file](https://pip.pypa.io/en/stable/user_guide/#requirements-files) 应该放在仓库的根目录。它应该指明完整工程的所有依赖包: 测试, 编译和文档生成。\n\n如果您的工程没有任何开发依赖，或者您喜欢通过 `setup.py` 来设置，那么这个文件不是必须的。\n\n## Documentation\n\n|   |   |\n|---|---|\n|布局|./docs/|\n|作用|包的参考文档|\n\n没有任何理由把这个放到别的地方。\n\n## Test Suite\n\n_想了解关于编写测试的建议，请查阅 :doc:`/writing/tests`。_\n\n|   |   |\n|---|---|\n|布局|./test_sample.py or ./tests|\n|作用|包的集合和单元测试|\n\n最开始，一组测试例子只是放在一个文件当中:\n\n```JSON\n./test_sample.py\n```\n\n当测试例子逐步增加时，您会把它放到一个目录里面，像下面这样:\n\n```JSON\ntests/test_basic.pytests/test_advanced.py\n```\n\n当然，这些测试例子需要导入您的包来进行测试，有几种方式来处理:\n\n- 将您的包安装到site-packages中。\n    \n- 通过简单直接的路径设置来解决导入的问题。\n    \n\n我极力推荐后者。如果使用 `setup.py develop` 来测试一个持续更新的代码库，需要为每一个版本的代码库设置一个独立的测试环境.太麻烦了。\n\n可以先创建一个包含上下文环境的文件 `tests/context.py`。 file:\n\n```JSON\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport sample\n```\n\n然后，在每一个测试文件中，导入:\n\n```JSON\nfrom .context import sample\n```\n\n这样就能够像期待的那样工作，而不用采用安装的方式。\n\n一些人会说应该把您的测试例子放到您的模块里面 -- 我不同意。这样会增加您用户使用的复杂度；而且添加测试模块将导致需要额外的依赖和运行环境。\n\n## Makefile\n\n|   |   |\n|---|---|\n|布局|./Makefile|\n|作用|常规的管理任务|\n\n如果您看看我的项目或者其他开源项目，您都会发现有一个Makefile。为什么？这些项目也不是用C写的啊。。。简而言之，make对于定义常规的管理任务是非常有用的工具。\n\n** 样例 Makefile:**\n\n```JSON\ninit:\n    pip install -r requirements.txt\n    \ntest:\n    py.test tests\n\nPHONY: \n    init test\n```\n\ninit**:**pip install -r requirements.txttest**:**py.test testsPHONY**:** init test\n\n一些其他的常规管理脚本（比如 `manage.py` 或者 `fabfile.py`），也放在仓库的根目录下。\n\n# 关于 Django Applications\n\n从Django 1.4开始，我发现有这样一个现象：很多开发者错误地使用Django自带的应用模板创建项目，导致他们的仓库结构非常糟糕。\n\n这是怎么回事儿? 是的, 他们在进入一个新的仓库后，通常都这样操作：\n\n```JSON\n$ django-admin.py startproject samplesite\n```\n\n这样的操作生成的仓库结构是这样的:\n\n```JSON\nREADME.rst\nsamplesite/manage.py\nsamplesite/samplesite/settings.py\nsamplesite/samplesite/wsgi.py\nsamplesite/samplesite/sampleapp/models.py\n```\n\n亲，不要这样做。\n\n相对路径会让您的工具和您的开发者都很疑惑。没有必要的嵌套对任何人都没有好处（除非您怀念庞大的SVN仓库）。\n\n让我们这样来做:\n\n```JSON\n$ django-admin.py startproject samplesite .\n```\n\n注意末尾的 \"`.`\"。\n\n生成的结构是这样的:\n\n```JSON\nREADME.rst\nmanage.py\nsamplesite/settings.py\nsamplesite/wsgi.py\nsamplesite/sampleapp/models.py\n```\n\n# 结构是一把钥匙\n\n得益于Python提供的导入与管理模块的方式，结构化Python项目变得相对简单。 这里说的简单，指的是结构化过程没有太多约束限制而且模块导入功能容易掌握。 因而您只剩下架构性的工作，包括设计、实现项目各个模块，并整理清他们之间 的交互关系。\n\n容易结构化的项目同样意味着它的结构化容易做得糟糕。糟糕结构的特征包括：\n\n- 多重且混乱的循环依赖关系：假如在 `furn.py` 内的Table与Chair类需要 导入 `workers.py` 中的Carpenter类以回答类似 `table.isdoneby()` 的问题，并且Carpenter类需要引入Table和Chair类以回答 `carpenter.whatdo()` 这类问题，这就是一种循环依赖的情况。在这种情况下,您得借助一些不怎么靠谱的 小技巧，比如在方法或函数内部使用import语句。\n    \n- 隐含耦合：Table类实现代码中每一个改变都会打破20个不相关的测试用例，由于它 影响了Carpenter类的代码，这要求谨慎地操作以适应改变。这样的情况意味着 Carpenter类代码中包含了太多关于Table类的假设关联（或相反）。\n    \n- 大量使用全局变量或上下文：如果Table和Carpenter类使用不仅能被修改而且能被 不同引用修改的全局变量，而不是明确地传递 `(height, width, type, wood)` 变量。您就需要彻底检查全局变量的所有入口，来理解到为什么一个长方形桌子变 成了正方形，最后发现远程的模板代码修改了这份上下文，弄错了桌子尺寸规格的 定义。\n    \n- 面条式代码 (Spaghetti code) ：多页嵌套的if语句与for循环，包含大量复制-粘贴 的过程代码，且没有合适的分割——这样的代码被称为面条式代码。Python中有意思 的缩进排版(最具争议的特性之一)使面条式代码很难维持。所以好消息是您也许不 会经常看到这种面条式代码。\n    \n- Python中更可能出现混沌代码：这类代码包含上百段相似的逻辑碎片，通常是缺乏 合适结构的类或对象，如果您始终弄不清手头上的任务应该使用FurnitureTable， AssetTable还是Table，甚至TableNew，也许您已经陷入了混沌代码中。\n    \n\n# 模块\n\nPython模块是最主要的抽象层之一，并且很可能是最自然的一个。\n\n抽象层允许将代码分为 不同部分，每个部分包含相关的数据与功能。\n\n  \n\n为遵守风格指南中的规定\n\n- **模块名称要短、使用小写，并避免使用特殊符号**，比如点(.) 和问号(?)。如 `my.spam.py` 这样的名字是必须不能用的！该方式命名将妨碍 Python的模块查找功能。就 my.spam.py 来说，Python 认为需要在 `my` 文件夹 中找到 `spam.py` 文件，实际并不是这样。这个例子 [example](http://docs.python.org/tutorial/modules.html#packages) 展示了点表示 法应该如何在Python文件中使用。如果愿意您可以将模块命名为 `my_spam.py`，\n    \n- **不过并不推荐在模块名中使用下划线。但是，在模块名称中使用其他字符（空格或连字号） 将阻止导入（-是减法运算符），因此请尽量保持模块名称简单，以无需分开单词**。 最重要的是，不要使用下划线命名空间，而是使用子模块。\n    \n\n```JSON\n# OK\nimport library.plugin.foo\n# not OK\nimport library.foo_plugin\n```\n\n除了以上的命名限制外，Python文件成为模块没有其他特殊的要求，但为了合理地使用这 个观念并避免问题，您需要理解import的原理机制。\n\n- `import modu` 语句将 寻找合适的文件，即调用目录下的 `modu.py` 文件（如果该文件存在）。\n    \n- 如果没有 找到这份文件，Python解释器递归地在 \"PYTHONPATH\" 环境变量中查找该文件，如果仍没 有找到，将抛出ImportError异常。\n    \n- 一旦找到 `modu.py`，**Python解释器将在隔离的作用域内执行这个模块。所有顶层 语句都会被执行，包括其他的引用。方法与类的定义将会存储到模块的字典中。**然后，这个 模块的变量、方法和类通过命名空间暴露给调用方，这是Python中特别有用和强大的核心概念。\n    \n- 在很多其他语言中，`include file` 指令被预处理器用来获取文件里的所有代码并‘复制’ 到调用方的代码中。Python则不一样：**include代码被独立放在模块命名空间里，这意味着您 一般不需要担心include的代码可能造成不好的影响，例如重载同名方法。**\n    \n\n也可以使用import语句的特殊形式 `from modu import *` 模拟更标准的行为。但 `import *` 通常 被认为是不好的做法。**使用** `from modu import *` **的代码较难阅读而且依赖独立性不足**。 **使用** **`from modu import func`** **能精确定位您想导入的方法并将其放到全局命名空间中**。 比 `from modu import *` 要好些，因为它明确地指明往全局命名空间中导入了什么方法，它和 `import modu` 相比唯一的优点是之后使用方法时可以少打点儿字。\n\n**差**\n\n```JSON\n[...]\n\nfrom modu import *\n[...]\n\nx = sqrt(4)  # sqrt是模块modu的一部分么？或是内建函数么？上文定义了么？\n```\n\n**稍好**\n\n```JSON\nfrom modu import sqrt\n[...]\nx = sqrt(4)  # 如果在import语句与这条语句之间，sqrt没有被重复定义，它也许是模块modu的一部分。\n```\n\n**最好的做法**\n\n```JSON\nimport modu\n[...]\nx = modu.sqrt(4)  # sqrt显然是属于模块modu的。\n```\n\n在 [代码风格](https://pythonguidecn.readthedocs.io/zh/latest/writing/style.html#code-style) 章节中提到，可读性是Python最主要的特性之一。可读性意味着避免 无用且重复的文本和混乱的结构，因而需要花费一些努力以实现一定程度的简洁。但不能 过份简洁而导致简短晦涩。除了简单的单文件项目外，**其他项目需要能够明确指出类和方法 的出处**，\n\n# 包\n\n**Python提供非常简单的包管理系统，即简单地将模块管理机制扩展到一个目录上(目录扩 展为包)。**\n\n**任意包含** **`__init__.py`** **文件的目录都被认为是一个Python包**。导入一个包里不同 模块的方式和普通的导入模块方式相似，特别的地方是 `__init__.py` 文件将集合 所有包范围内的定义。\n\n- `pack/` 目录下的 `modu.py` 文件通过 `import pack.modu` 语句导入。\n    \n    - 该语句会在 `pack` 目录下寻找 `__init__.py` 文件，并执行其中所有顶层 语句。\n        \n    - 以上操作之后，`modu.py` 内定义的所有变量、方法和类在pack.modu命名空 间中均可看到。\n        \n\n一个常见的问题是往 `__init__.py` 中加了过多代码，随着项目的复杂度增长， 目录结构越来越深，子包和更深嵌套的子包可能会出现。在这种情况下，导入多层嵌套 的子包中的某个部件需要执行所有通过路径里碰到的 `__init__.py` 文件。**如果 包内的模块和子包没有代码共享的需求，使用空白的** **`__init__.py`** **文件是正常 甚至好的做法**。\n\n最后，导入深层嵌套的包可用这个方便的语法：`import very.deep.module as mod`。 该语法允许使用 mod 替代冗长的 `very.deep.module`。\n\n# 面向对象编程\n\n**在Python中 一切都是对象，并且能按对象的方式处理**。这么说的意思是，例如函数是一等对象。 函数、类、字符串乃至类型都是Python对象：与其他对象一样，他们有类型，能作为 函数参数传递，并且还可能有自己的方法和属性。这样理解的话，Python是一种面向对象语言。\n\n然而，与Java不同的是，Python并没有将面向对象编程作为最主要的编程范式。非面向 对象的Python项目(比如，使用较少甚至不使用类定义，类继承，或其它面向对象编程的 机制)也是完全可行的。\n\n**此外在** **[模块](https://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html#id8)** **章节里曾提到，Python管理模块与命名空间的方式提供给开发者一个自然 的方式以实现抽象层的封装和分离**，这是使用面向对象最常见的原因。因而，如果业务逻辑 没有要求，Python开发者有更多自由去选择不使用面向对象。\n\n**在一些情况下，需要避免不必要的面向对象。当我们想要将状态与功能结合起来，使用 标准类定义是有效的**。但正如函数式编程所讨论的那个问题，函数式的“变量”状态与类的 状态并不相同。\n\n在某些架构中，典型代表是web应用，大量Python进程实例被产生以响应可能同时到达的 外部请求。在这种情况下，在实例化对象内保持某些状态，即保持某些环境静态信息， 容易出现并发问题或竞态条件。有时候在对象状态的初始化(通常通过 `__init__()` 方法实现)和在其方法中使用该状态之间，环境发生了变化，保留的状态可能已经过时。 举个例子，某个请求将对象加载到内存中并标记它为已读。如果同时另一个请求要删除 这个对象，删除操作可能刚好发生在第一个请求加载完该对象之后，结果就是第一个请 求标记了一个已经被删除的对象为已读。\n\n这些问题使我们产生一个想法：**使用无状态的函数是一种更好的编程范式**。**另一种建议 是尽量使用隐式上下文和副作用较小的函数与程序。函数的隐式上下文由函数内部访问 到的所有全局变量与持久层对象组成**。副作用即函数可能使其隐式上下文发生改变。如 果函数保存或删除全局变量或持久层中数据，这种行为称为副作用。\n\n把有隐式上下文和副作用的函数与仅包含逻辑的函数(纯函数)谨慎地区分开来，会带来 以下好处：\n\n- 纯函数的结果是确定的：给定一个输入，输出总是固定相同。\n    \n- 当需要重构或优化时，纯函数更易于更改或替换。\n    \n- 纯函数更容易做单元测试：很少需要复杂的上下文配置和之后的数据清除工作。\n    \n- 纯函数更容易操作、修饰和分发。\n    \n\n总之，对于某些架构而言，纯函数比类和对象在构建模块时更有效率，**因为他们没有任何 上下文和副作用。但显然在很多情况下，面向对象编程是有用甚至必要的**。例如图形桌面 应用或游戏的开发过程中，操作的元素(窗口、按钮、角色、车辆)在计算机内存里拥有相 对较长的生命周期。\n\n# 装饰器\n\nPython语言提供一个简单而强大的语法: '装饰器'。**装饰器是一个函数或类，它可以 包装(或装饰)一个函数或方法**。**被 '装饰' 的函数或方法会替换原来的函数或方法**。 **由于在Python中函数是一等对象，它也可以被 '手动操作'，但是使用@decorators 语法更清晰，因此首选这种方式**。\n\n```JSON\ndef foo():\n    # 实现语句\n    \ndef decorator(func):\n    # 操作func语句\n    return func\n    \nfoo = decorator(foo)  # 手动装饰\n\n@decorator\ndef bar():# 实现语句\n# bar()被装饰了\n```\n\n这个机制对于分离概念和避免外部不相关逻辑“污染”主要逻辑很有用处。 记忆化 \u003chttps://en.wikipedia.org/wiki/Memoization#Overview\u003e 或缓存就是一个很 好的使用装饰器的例子：您需要在table中储存一个耗时函数的结果，并且下次能直接 使用该结果，而不是再计算一次。这显然不属于函数的逻辑部分。\n\n# 上下文管理器\n\n上下文管理器是一个Python对象，为操作提供了额外的上下文信息。 这种额外的信息， 在使用 `with` 语句初始化上下文，以及完成 `with` 块中的所有代码时，采用可调用的形式。 这里展示了使用上下文管理器的为人熟知的示例，打开文件：\n\n```JSON\nwith open('file.txt') as f:\n    contents = f.read()\n```\n\n**任何熟悉这种模式的人都知道以这种形式调用** **`open`** **能确保** **```f` 的 ``close```** **方法会在某个时候被调用**。 这样可以减少开发人员的认知负担，并使代码更容易阅读。\n\n实现这个功能有两种简单的方法：使用类或使用生成器。 让我们自己实现上面的功能，\n\n- 以使用类方式开始：\n    \n\n```JSON\nclass CustomOpen(object):\n    def __init__(self, filename):\n        self.file = open(filename)\\\n        \n    def __enter__(self):\n        return self.file\n        \n    def __exit__(self, ctx_type, ctx_value, ctx_traceback):\n        self.file.close()\n \n \n with CustomOpen('file') as f:contents = f.read()\n```\n\n这只是一个常规的Python对象，它有两个由 `with` 语句使用的额外方法。 CustomOpen 首先被实例化，然后调用它的`__enter__`方法，而且 `__enter__` 的返回值在 `as f` 语句中被赋给 `f` 。 当 `with` 块中的内容执行完后，会调用 `__exit__` 方法。\n\n而生成器方式使用了Python自带的 [contextlib](https://docs.python.org/2/library/contextlib.html):\n\n```JSON\nfrom contextlib import contextmanager\n\n@contextmanager\ndef custom_open(filename):\n    f = open(filename)\n    try:\n        yield f\n    finally:\n        f.close()\n        \nwith custom_open('file') as f:contents = f.read()\n```\n\n这与上面的类示例道理相通，尽管它更简洁。**`custom_open`** **函数一直运行到** **`yield`** **语句。 然后它将控制权返回给** **`with`** **语句，然后在** **`as f`** **部分将yield的 f 赋值给f。** **`finally`** **确保不论** **`with`** **中是否发生异常，** **`close()`** **都会被调用。**\n\n  \n\n# 动态类型\n\n**Python是动态类型语言，这意味着变量并没有固定的类型**。实际上，Python 中的变量和其他 语言有很大的不同，特别是静态类型语言。变量并不是计算机内存中被写入的某个值，它们 只是指向内存的 ‘标签’ 或 ‘名称’ 。因此可能存在这样的情况，变量 'a' 先代表值1，然后变成 字符串 'a string' , 然后又变为指向一个函数。\n\nPython 的动态类型常被认为是它的缺点，的确这个特性会导致复杂度提升和难以调试的代码。 命名为 'a' 的变量可能是各种类型，开发人员或维护人员需要在代码中追踪命名，以保证它 没有被设置到毫不相关的对象上。\n\n这里有些避免发生类似问题的参考方法：\n\n- 避免对不同类型的对象使用同一个变量名\n    \n\n**差**\n\n```JSON\na = 1a = 'a string'def a():pass  # 实现代码\n```\n\n**好**\n\n```JSON\ncount = 1msg = 'a string'def func():pass  # 实现代码\n```\n\n- 使用简短的函数或方法能降低对不相关对象使用同一个名称的风险。即使是相关的不同 类型的对象，也更建议使用不同命名：\n    \n\n**差**\n\n```JSON\nitems = 'a b c d'  # 首先指向字符串...\nitems = items.split(' ')  # ...变为列表\nitems = set(items)  # ...再变为集合\n```\n\n重复使用命名对效率并没有提升：赋值时无论如何都要创建新的对象。然而随着复杂度的 提升，赋值语句被其他代码包括 'if' 分支和循环分开，使得更难查明指定变量的类型。 在某些代码的做法中，例如函数编程，推荐的是从不重复对同一个变量命名赋值。Java 内的实现方式是使用 'final' 关键字。Python并没有 'final' 关键字而且这与它的哲学 相悖。尽管如此，避免给同一个变量命名重复赋值仍是是个好的做法，并且有助于掌握 可变与不可变类型的概念。\n\n# 可变和不可变类型\n\nPython提供两种内置或用户定义的类型。\n\n- 可变类型允许内容的内部修改。典型的动态类型 包括列表与字典：列表都有可变方法，如 **`list.append()`** 和 **`list.pop()`**， 并且能就地修改。字典也是一样。\n    \n- 不可变类型没有修改自身内容的方法。比如，赋值为整数 6的变量 x 并没有 \"自增\" 方法，如果需要计算 x + 1，必须创建另一个整数变量并给其命名。\n    \n\n```JSON\nmy_list = [1, 2, 3]\nmy_list[0] = 4\nprint my_list  # [4, 2, 3] \u003c- 原列表改变了\n\n\nx = 6\nx = x + 1  # x 变量是一个新的变量\n```\n\n**这种差异导致的一个后果就是，可变类型是不 '稳定 '的，因而不能作为字典的键使用**。合理地 使用可变类型与不可变类型有助于阐明代码的意图。例如与列表相似的不可变类型是元组， **创建方式为** **`(1, 2)`****。元组是不可修改的，并能作为字典的键使用。**\n\nPython 中一个可能会让初学者惊讶的特性是：**字符串是不可变类型。这意味着当需要组合一个 字符串时，将每一部分放到一个可变列表里，使用字符串时再组合 ('join') 起来的做法更高效**。 值得注意的是，使用列表推导的构造方式比在循环中调用 `append()` 来构造列表更好也更快。\n\n**差**\n\n```JSON\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = \"\"\nfor n in range(20):\n    nums += str(n)   # 慢且低效\nprint nums\n```\n\n**好**\n\n```JSON\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = []\nfor n in range(20):\n    nums.append(str(n))\nprint \"\".join(nums)  # 更高效\n```\n\n**更好**\n\n```JSON\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = [str(n) for n in range(20)]\nprint \"\".join(nums)\n```\n\n**最好Best**\n\n```JSON\n# 创建将0到19连接起来的字符串 (例 \"012..1819\")\nnums = map(str, range(20))\nprint \"\".join(nums)\n```\n\n最后关于字符串的说明的一点是，使用 `join()` 并不总是最好的选择。比如当用预先 确定数量的字符串创建一个新的字符串时，使用加法操作符确实更快，但在上文提到的情况 下或添加到已存在字符串的情况下，使用 `join()` 是更好的选择。\n\n```JSON\nfoo = 'foo'\nbar = 'bar'\nfoobar = foo + bar  # 好的做法\nfoo += 'ooo'  # 不好的做法, 应该这么做:\nfoo = ''.join([foo, 'ooo'])\n```\n\n```JSON\nfoo = 'foo' \nbar = 'bar'\nfoobar = '%s%s' % (foo, bar) # 可行\nfoobar = '{0}{1}'.format(foo, bar) # 更好\nfoobar = '{foo}{bar}'.format(foo=foo, bar=bar) # 最好\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/3.%E7%B1%BB%E5%9E%8B":{"title":"3.类型","content":"# 整数\n\nPython可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：`1`，`100`，`-8080`，`0`，等等。\n\n计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用`0x`前缀和0-9，a-f表示，例如：`0xff00`，`0xa5b4c3d2`，等等。\n\n对于很大的数，例如`10000000000`，很难数清楚0的个数。Python允许在数字中间以`_`分隔，因此，写成`10_000_000_000`和`10000000000`是完全一样的。十六进制数也可以写成`0xa1b2_c3d4`。\n\n  \n\n# 浮点数\n\n  \n\n浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如，1.23x10(9)和12.3x10(8)是完全相等的。浮点数可以用数学写法，如`1.23`，`3.14`，`-9.01`，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x10(9)就是`1.23e9`，或者`12.3e8`，0.000012可以写成`1.2e-5`，等等。\n\n整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！），而浮点数运算则可能会有四舍五入的误差。\n\n  \n\n除法运算的结果为小数，即浮点数\n\n如果要舍弃小数部分可以使用`//`\n\n```JSON\n1//2     //0\n\n5.0//2.5   //2.0 \n```\n\n# 布尔值\n\n- True\n    \n- False\n    \n\n# None\n\n# 字符串\n\n字符串是以单引号`'`或双引号`\"`三引号`'''`括起来的任意文本.比如`'abc'`，`\"xyz\"`等等。请注意，`''`或`\"\"`本身只是一种表示方式，不是字符串的一部分，因此，字符串`'abc'`只有`a`，`b`，`c`这3个字符。如果`'`本身也是一个字符，那就可以用`\"\"`括起来，比如`\"I'm OK\"`包含的字符是`I`，`'`，`m`，空格，`O`，`K`这6个字符。\n\n单引号`'`或双引号`\"`可以很好的处理字符串本身有单引号的情况\n\n```JSON\n'\"hello world\" he said'\n```\n\n## 长字符串\n\n使用三个双引号，如`\"\"\"like this\"\"\"` 。请注意，这让解释器能够识别表示字符串开始和结束位置的引号，因此字符串本身可包含单引号和双引号，无需使用反斜杠进行转义长字符串可以跨越多行\n\n\u003e 常规字符串也可横跨多行。只要在行尾加上反斜杠，反斜杠和换行符将被转义，即被忽略\n\u003e \n\u003e ```JSON\n\u003e print(\"hello,\\\n\u003e world\")\n\u003e ```\n\n## 编码\n\nPython字符串使用Unicode编码来表示文本\n\n## 格式化\n\n### %\n\n在Python中，采用的格式化方式和C语言是一致的，用`%`实现，举例如下：\n\n```Plaintext\n\u003e\u003e\u003e 'Hello, %s' % 'world'\n'Hello, world'\n\u003e\u003e\u003e 'Hi, %s, you have $%d.' % ('Michael', 1000000)\n'Hi, Michael, you have $1000000.'\n```\n\n常见的占位符有：\n\n|   |   |\n|---|---|\n|占位符|替换内容|\n|%d|整数|\n|%f|浮点数|\n|%s|字符串|\n|%x|十六进制整数|\n\n### format()\n\n```Plaintext\nfoo = 'foo' \nbar = 'bar'\nfoobar = '%s%s' % (foo, bar) # 可行\nfoobar = '{0}{1}'.format(foo, bar) # 更好\nfoobar = '{foo}{bar}'.format(foo=foo, bar=bar) # 最好\n```\n\n### f-string\n\n```JSON\nr = 2.5\ns = 3.14 * r ** 2\nprint(f'The area of a circle with radius {r} is {s:.2f}')\n```","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/4.%E5%AE%B9%E5%99%A8":{"title":"4.容器","content":"python 支持一种数据结构的基本概念，名为容器（container）。容器基本上就是可包含其他对象的对象。两种主要的容器是序列（如列表和元组）和映射（如字典）。在序列中，每个元素都有编号，而在映射中，每个元素都有名称（也叫键）。有一种既不是序列也不是映射的容器，它就是集合（set）\n\n![[Pasted image 20230727232959.png]]\n\n\n\n# 序列\n\npython有多种序列，本章重点讨论其中最常用的两种：列表和元组。字符串也具有和列表和元组相同的特性。列表和元组的主要不同在于，列表是可以修改的，而元组不可以。这意味着列表适用于需要中途添加元素的情形，而元组适用于出于某种考虑需要禁止修改序列的情形。禁止修改序列通常出于技术方面的考虑，与Python的内部工作原理相关，这也是有些内置函数返回元组的原因所在。在你自己编写程序时，几乎在所有情况下都可使用列表来代替元组。一种例外情况是将元组用作字典键\n\n  \n\n## 索引\n\n### 索引可以是正数\n\n用索引来访问list中每一个位置的元素，记得索引是从`0`开始的：\n\n```Plaintext\n\u003e\u003e\u003e classmates[0]\n'Michael'\n\u003e\u003e\u003e classmates[1]\n'Bob'\n\u003e\u003e\u003e classmates[2]\n'Tracy'\n\u003e\u003e\u003e classmates[3]\nTraceback (most recent call last):\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\nIndexError: list index out of range\n```\n\n当索引超出了范围时，Python会报一个`IndexError`错误，所以，要确保索引不要越界，记得最后一个元素的索引是`len(classmates) - 1`。\n\n### 索引也可以是负数\n\n如果要取最后一个元素，除了计算索引位置外，还可以用`-1`做索引，直接获取最后一个元素：\n\n```Plaintext\n\u003e\u003e\u003e classmates[-1]\n'Tracy'\n```\n\n以此类推，可以获取倒数第2个、倒数第3个：\n\n```Plaintext\n\u003e\u003e\u003e classmates[-2]\n'Bob'\n\u003e\u003e\u003e classmates[-3]\n'Michael'\n\u003e\u003e\u003e classmates[-4]\nTraceback (most recent call last):\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\nIndexError: list index out of range\n```\n\n当然，倒数第4个就越界了。列表\n\nPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。\n\n比如，列出班里所有同学的名字，就可以用一个list表示：\n\n```Plaintext\n\u003e\u003e\u003e classmates = ['Michael', 'Bob', 'Tracy']\n\u003e\u003e\u003e classmates\n['Michael', 'Bob', 'Tracy']\n```\n\n## 切片\n\n除使用索引来访问单个元素外，还可使用切片（slicing）来访问特定范围内的元素，可使用两个索引，并用冒号分隔\n\n### 可以使用正数\n\n```JSON\n\u003e\u003e\u003e tag = '\u003ca href=\"http://www.python.org\"\u003ePython web site\u003c/a\u003e'\n\u003e\u003e\u003e tag[9:30]\n'http://www.python.org'\n```\n\n### 可以使用负数\n\n```JSON\n\u003e\u003e\u003e tag[32:-4]\n'Python web site'\n```\n\n### 可以忽略索引\n\n```JSON\n\u003e\u003e\u003e numbers[-3:]\n[8, 9, 10]\n\u003e\u003e\u003e numbers[:3]\n[1, 2, 3]\n\u003e\u003e\u003e numbers[:]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n\n### 可以设置步长\n\n执行切片操作时，你显式或隐式地指定起点和终点，但通常省略另一个参数，即步长。在普通切片中，步长为1。这意味着从一个元素移到下一个元素，因此切片包含起点和终点之间的所有元素\\\n\n```JSON\n\u003e\u003e\u003e numbers[0:10:2]\n[1, 3, 5, 7, 9]\n\u003e\u003e\u003enumbers[3:6:3]\n[4]\n```\n\n## 序列相加\n\n```JSON\n\u003e\u003e\u003e [1, 2, 3] + [4, 5, 6]\n[1, 2, 3, 4, 5, 6]\n\u003e\u003e\u003e 'Hello,' + 'world!'\n'Hello, world!'\n\u003e\u003e\u003e [1, 2, 3] + 'world!'\nTraceback (innermost last):\n File \"\u003cpyshell\u003e\", line 1, in ?\n    [1, 2, 3] + 'world!'\nTypeError: can only concatenate list (not \"string\") to list\n```\n\n## 序列乘法\n\n```JSON\n\u003e\u003e\u003e 'python' * 5\n'pythonpythonpythonpythonpython'\n\u003e\u003e\u003e [42] * 10\n[42, 42, 42, 42, 42, 42, 42, 42, 42, 42]\n```\n\n## in\n\n```JSON\n\u003e\u003e\u003e permissions = 'rw'\n\u003e\u003e\u003e 'w' in permissions\nTrue\n\u003e\u003e\u003e 'x' in permissions\nFalse\n\u003e\u003e\u003e users = ['mlh', 'foo', 'bar']\n\u003e\u003e\u003e input('Enter your user name: ') in users\nEnter your user name: mlh\nTrue\n\u003e\u003e\u003e subject = '$$$ Get rich now!!! $$$'\n\u003e\u003e\u003e '$$$' in subject\nTrue\n```\n\n## 长度、最小值和最大值\n\n```JSON\n\u003e\u003e\u003e numbers = [100, 34, 678]\n\u003e\u003e\u003e len(numbers)\n3\n\u003e\u003e\u003e max(numbers)\n678\n\u003e\u003e\u003e min(numbers)\n```\n\n# 列表\n\n表是可变的，即可修改其内容。另外，列表有很多特有的方法\n\n## 元素内的类型\n\nlist里面的元素的数据类型也可以不同，比如：\n\n```Plaintext\n\u003e\u003e\u003e L = ['Apple', 123, True]\n```\n\nlist元素也可以是另一个list，比如：\n\n```Plaintext\n\u003e\u003e\u003e s = ['python', 'java', ['asp', 'php'], 'scheme']\n\u003e\u003e\u003e len(s)\n4\n```\n\n要注意`s`只有4个元素，其中`s[2]`又是一个list，如果拆开写就更容易理解了：\n\n```Plaintext\n\u003e\u003e\u003e p = ['asp', 'php']\n\u003e\u003e\u003e s = ['python', 'java', p, 'scheme']\n```\n\n要拿到`'php'`可以写`p[1]`或者`s[2][1]`，因此`s`可以看成是一个二维数组，类似的还有三维、四维……数组，不过很少用到。\n\n## 函数list\n\n鉴于不能像修改列表那样修改字符串，因此在有些情况下使用字符串来创建列表很有帮助。为此，可使用函数list\n\n```JSON\n\u003e\u003e\u003e list('Hello')\n['H', 'e', 'l', 'l', 'o']\n```\n\n字符列表（如前述代码中的字符列表）转换为字符串，可使用下面的表达式\n\n```JSON\n''.join(somelist)\n```\n\n## 新增元素\n\nlist是一个可变的有序表，所以，可以往list中追加元素到末尾：\n\n```Plaintext\n\u003e\u003e\u003e classmates.append('Adam')\n\u003e\u003e\u003e classmates\n['Michael', 'Bob', 'Tracy', 'Adam']\n```\n\n也可以把元素插入到指定的位置，比如索引号为`1`的位置：\n\n```Plaintext\n\u003e\u003e\u003e classmates.insert(1, 'Jack')\n\u003e\u003e\u003e classmates\n['Michael', 'Jack', 'Bob', 'Tracy', 'Adam']\n```\n\n批量的新增\n\n```JSON\n\u003e\u003e\u003e a = [1, 2, 3]\n\u003e\u003e\u003e b = [4, 5, 6]\n\u003e\u003e\u003e a[len(a):] = b\n\u003e\u003e\u003e a\n[1, 2, 3, 4, 5, 6]\n```\n\n## 删除元素\n\n要删除list末尾的元素，用`pop()`方法：\n\n```Plaintext\n\u003e\u003e\u003e classmates.pop()\n'Adam'\u003e\u003e\u003e classmates\n['Michael', 'Jack', 'Bob', 'Tracy']\n```\n\n要删除指定位置的元素，用`pop(i)`方法，其中`i`是索引位置：\n\n```Plaintext\n\u003e\u003e\u003e classmates.pop(1)\n'Jack'\u003e\u003e\u003e classmates\n['Michael', 'Bob', 'Tracy']\n```\n\n删除指定元素\n\n```JSON\n\u003e\u003e\u003e knights = ['We', 'are', 'the', 'knights', 'who', 'say', 'ni']\n\u003e\u003e\u003e knights.index('who')\n4\n\u003e\u003e\u003e knights.index('herring')\nTraceback (innermost last):\n  File \"\u003cpyshell\u003e\", line 1, in ?\n    knights.index('herring')\nValueError: list.index(x): x not in list\n```\n\n## 替换某个位置元素\n\n要把某个元素替换成别的元素，可以直接赋值给对应的索引位置：\n\n```Plaintext\n\u003e\u003e\u003e classmates[1] = 'Sarah'\u003e\u003e\u003e classmates\n['Michael', 'Sarah', 'Tracy']\n```\n\n## 清空列表\n\n```JSON\n\u003e\u003e\u003e lst = [1, 2, 3]\n\u003e\u003e\u003e lst.clear()\n\u003e\u003e\u003e lst\n[]\n```\n\n## 复制列表\n\n方法copy复制列表。前面说过，常规复制只是将另一个名称关联到列表。\n\n```JSON\n\u003e\u003e\u003e a = [1, 2, 3]\n\u003e\u003e\u003e b = a\n\u003e\u003e\u003e b[1] = 4\n\u003e\u003e\u003e a\n[1, 4, 3]\n```\n\n复制\n\n```JSON\n\u003e\u003e\u003e a = [1, 2, 3]\n\u003e\u003e\u003e b = a.copy()\n\u003e\u003e\u003e b[1] = 4\n\u003e\u003e\u003e a\n[1, 2, 3]\n```\n\n这类似于使用a[:]或list(a)，它们也都复制a\n\n## 统计出现的次数\n\n```JSON\n\u003e\u003e\u003e ['to', 'be', 'or', 'not', 'to', 'be'].count('to')\n2\n\u003e\u003e\u003e x = [[1, 2], 1, 1, [2, 1, [1, 2]]]\n\u003e\u003e\u003e x.count(1)\n2\n\u003e\u003e\u003e x.count([1, 2])\n1\n```\n\n## 第一次出现的索引\n\n```JSON\n\u003e\u003e\u003e knights = ['We', 'are', 'the', 'knights', 'who', 'say', 'ni']\n\u003e\u003e\u003e knights.index('who')\n4\n\u003e\u003e\u003e knights.index('herring')\nTraceback (innermost last):\nFile \"\u003cpyshell\u003e\", line 1, in ?\nknights.index('herring')\nValueError: list.index(x): x not in list\n```\n\n## 排序\n\n```JSON\n\u003e\u003e\u003e x = [4, 6, 2, 1, 7, 9]\n\u003e\u003e\u003e x.sort()\n\u003e\u003e\u003e x\n[1, 2, 4, 6, 7, 9]\n```\n\n如果需要新创建个列表\n\n```JSON\n\u003e\u003e\u003e x = [4, 6, 2, 1, 7, 9]\n\u003e\u003e\u003e y = sorted(x)\n\u003e\u003e\u003e x\n[4, 6, 2, 1, 7, 9]\n\u003e\u003e\u003e y\n[1, 2, 4, 6, 7, 9]\n```\n\n反序\n\n```JSON\n\u003e\u003e\u003e x = [1, 2, 3]\n\u003e\u003e\u003e x.reverse()\n\u003e\u003e\u003e x\n[3, 2, 1]\n```\n\n需要新创建个列表\n\n```JSON\n\u003e\u003e\u003e x = [1, 2, 3]\n\u003e\u003e\u003e list(reversed(x))\n[3, 2, 1]\n```\n\n  \n\nsort() 可以字段排序字段，和反序\n\n```JSON\n\u003e\u003e\u003e x = ['aardvark', 'abalone', 'acme', 'add', 'aerate']\n\u003e\u003e\u003e x.sort(key=len)\n\u003e\u003e\u003e x\n['add', 'acme', 'aerate', 'abalone', 'aardvark']\n\n    \u003e\u003e\u003e x = [4, 6, 2, 1, 7, 9]\n\u003e\u003e\u003e x.sort(reverse=True)\n\u003e\u003e\u003e x\n[9, 7, 6, 4, 2, 1]\n```\n\n  \n\n# 元组\n\n组也是序列，唯一的差别在于元组是不能修改的（你可能注意到了，字符串也不能修改）。\n\n元组语法很简单，只要将一些值用逗号分隔，就能自动创建一个元组，也可以使用括号\n\n```JSON\n\u003e\u003e\u003e 1, 2, 3\n(1, 2, 3)\n\n\u003e\u003e\u003e (1, 2, 3)\n(1, 2, 3)\n\n\u003e\u003e\u003e ()\n()\n\n//只有一个值的元组，必须在它后面加上逗号\n\u003e\u003e\u003e 42\n42\n\u003e\u003e\u003e 42,\n(42,)\n\u003e\u003e\u003e (42,)\n(42,)\n```\n\n- 它们用作映射中的键（以及集合的成员），而列表不行。映射将在第4章详细介绍。\n    \n- 有些内置函数和方法返回元组，这意味着必须跟它们打交道。只要不尝试修改元组，与元组“打交道”通常意味着像处理列表一样处理它们（需要使用元组没有的index和count等方法时例外）。\n    \n\n## 函数tuple\n\n函数tuple的工作原理与list很像：它将一个序列作为参数，并将其转换为元组[插图]。如果参数已经是元组，就原封不动地返回它。\n\n```JSON\n\u003e\u003e\u003e tuple([1, 2, 3])\n(1, 2, 3)\n\u003e\u003e\u003e tuple('abc')\n('a', 'b', 'c')\n\u003e\u003e\u003e tuple((1, 2, 3))\n(1, 2, 3)\n```\n\n  \n\n# 映射和字典\n\n过名称来访问其各个值的数据结构。这种数据结构称为映射（mapping）。字典是Python中唯一的内置映射类型，其中的值不按顺序排列，而是存储在键下。键可能是数、字符串或元组。\n\n```JSON\nphonebook = {'Alice': '2341', 'Beth': '9102', 'Cecil': '3258'}\n\n//空字典（没有任何项）用两个花括号表示，类似于下面这样：\n{}\n```\n\n操作`\n\n- len(d)返回字典d包含的项（键-值对）数。\n    \n- d[k]返回与键k相关联的值。\n    \n- d[k] = v将值v关联到键k。\n    \n- del d[k]删除键为k的项。\n    \n- k in d检查字典d是否包含键为k的项。\n    \n\n字典列表的比较\n\n- 键的类型：字典中的键可以是整数，但并非必须是整数。字典中的键可以是任何不可变的类型，如浮点数（实数）、字符串或元组。\n    \n- 自动添加：即便是字典中原本没有的键，也可以给它赋值，这将在字典中创建一个新项。然而，如果不使用append或其他类似的方法，就不能给列表中没有的元素赋值。\n    \n- 成员资格：表达式k in d（其中d是一个字典）查找的是键而不是值，而表达式v in l（其中l是一个列表）查找的是值而不是索引。这看似不太一致，但你习惯后就会觉得相当自然。毕竟如果字典包含指定的键，检查相应的值就很容易。\n    \n\n### 函数dict\n\n可使用函数dict从其他映射（如其他字典）或键-值对序列创建字典\n\n```JSON\n\u003e\u003e\u003e items = [('name', 'Gumby'), ('age', 42)]\n\u003e\u003e\u003e d = dict(items)\n\u003e\u003e\u003e d\n{'age': 42, 'name': 'Gumby'}\n\u003e\u003e\u003e d['name']\n'Gumby'\n\u003e\u003e\u003e d = dict(name='Gumby', age=42)\n\u003e\u003e\u003e d\n{'age': 42, 'name': 'Gumby'}\n```\n\n### 清空字典\n\n```JSON\n\u003e\u003e\u003e d = {}\n\u003e\u003e\u003e d['name'] = 'Gumby'\n\u003e\u003e\u003e d['age'] = 42\n\u003e\u003e\u003e d\n{'age': 42, 'name': 'Gumby'}\n\u003e\u003e\u003e returned_value = d.clear()\n\u003e\u003e\u003e d\n{}\n\u003e\u003e\u003e print(returned_value)\nNone\n```\n\n### 复制字典","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/4.%E6%B5%81%E7%A8%8B":{"title":"4.流程","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/5.%E5%87%BD%E6%95%B0":{"title":"5.函数","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/6.%E6%A8%A1%E5%9D%97%E5%8C%85":{"title":"6.模块、包","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/7.%E7%B1%BB":{"title":"7.类","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/8.%E5%BC%82%E5%B8%B8":{"title":"8.异常","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/9.%E6%A0%87%E5%87%86%E5%BA%93":{"title":"9.标准库","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Python/python-%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99":{"title":"python 相关资料","content":"![[python编程快速上手-让繁琐工作自动化.pdf]]","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E5%86%85%E5%AD%98%E5%BF%AB%E7%85%A7":{"title":"内存快照","content":"**AOF 正因为记录的是操作命令，而不是实际的数据**，所以，用 AOF 方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。\n\n  \n\n另一种持久化方法：**内存快照。所谓内存快照，就是指内存中的数据在某一个时刻的状态记录。**\n\n**对 Redis 来说，它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照**。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。这个快照文件就称为 RDB 文件，其中，RDB 就是 **Redis DataBase 的缩写。**\n\n  \n\n有两个关键问题\n\n- 对哪些数据做快照？这关系到快照的执行效率问题；\n    \n- 做快照时，数据还能被增删改吗？这关系到 Redis 是否被阻塞，能否同时正常处理请求。\n    \n\n# 给哪些内存数据做快照\n\n  \n\nRedis 的数据都在内存中，为了提供所有数据的可靠性保证，**它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中。**\n\n  \n\n对于 Redis 而言，它的单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作，所以，针对任何操作，我们都会提一个灵魂之问：“它会阻塞主线程吗?”**RDB 文件的生成是否会阻塞主线程，这就关系到是否会降低 Redis 的性能。**\n\n  \n\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。\n\n- save：在主线程中执行，会导致阻塞；\n    \n- bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。\n    \n\n  \n\n  \n\n# 快照时数据能修改吗?\n\n  \n\n  \n\n  \n\n**避免阻塞和正常处理写操作并不是一回事**。此时，主线程的确没有阻塞，可以正常接收请求，但是，**为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。**\n\n  \n\n**为了快照而暂停写操作，肯定是不能接受的**。所以这个时候，Redis 就会借助操作系统提供的**写时复制技术（Copy-On-Write, COW）**，在执行快照的同时，正常处理写操作。\n\n  \n\nCopy on Write 过程\n\n如图\n\n![](statistic/asynccode-430.png)\n\n  \n\n简单来说，**bgsave 子进程是由主线程 fork 生成的**，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。\n\n如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块**数据就会被复制一份，生成该数据的副本**。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。\n\n  \n\n  \n\n# 多久做一次快照？\n\n  \n\n  \n\n如果频繁地执行全量快照，也会带来两方面的开销。\n\n- 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环\n    \n- 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，**子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长**。\n    \n\n  \n\n  \n\n此时，我们可以做增量快照，所谓**增量快照，**做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销？\n\n  \n\n增量快照的问题？\n\n在第一次做完全量快照后，T1 和 T2 时刻如果再做快照，我们只需要将被修改的数据写入快照文件就行。但是，这么做的前提是，我们需要记住哪些数据被修改了。你可不要小瞧这个“记住”功能**，它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题**\n\n![](statistic/asynccode-431.png)\n\n  \n\n  \n\n虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销。\n\n  \n\nRedis 4.0 中提出了一个**混合使用 AOF 日志和内存快照的方法**。简单来说，**内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作**","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E5%88%87%E7%89%87%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%A4%9A%E4%BA%86%E6%98%AF%E8%AF%A5%E5%8A%A0%E5%86%85%E5%AD%98%E8%BF%98%E6%98%AF%E5%8A%A0%E5%AE%9E%E4%BE%8B":{"title":"切片集群：数据增多了，是该加内存还是加实例","content":"在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 fork 在执行时会阻塞主线程。**数据量越大，fork 操作造成的主线程阻塞的时间越长**。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致 Redis 响应变慢了。\n\n  \n\n这个时候，我们注意到了 Redis 的切片集群。虽然组建切片集群比较麻烦，但是**它可以保存大量数据，而且对 Redis 主线程的阻塞影响较小。**\n\n  \n\n  \n\n切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。回到我们刚刚的场景中，如果把 25GB 的数据平均分成 5 份（当然，也可以不做均分），使用 5 个实例来保存，每个实例只需要保存 5GB 数据。\n\n  \n\n  \n\n![](statistic/asynccode-419.png)\n\n  \n\n实例在为 5GB 数据生成 RDB 时，数据量就小了很多，fork 子进程一般不会给主线程带来较长时间的阻塞。采用多个实例保存数据切片后，**我们既能保存 25GB 数据，又避免了 fork 子进程阻塞主线程而导致的响应突然变慢**\n\n  \n\n  \n\n  \n\n# 如何保存更多的数据\n\n  \n\n为了保存大量数据，我们使用**了大内存云主机和切片集群两种方法**。实际上，这两种方法分别对应着 Redis 应对数据量增多的两种方案：纵向扩展（scale up）和横向扩展（scale out）。\n\n- 纵向扩展：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。\n    \n- 横向扩展：横向增加当前 Redis 实例的个数\n    \n\n  \n\n**纵向扩展会受到硬件和成本的限制**\n\n  \n\n在面向百万、千万级别的用户规模时，横向扩展的 Redis 切片集群会是一个非常好的选择。\n\n  \n\n  \n\n要想把切片集群用起来，我们就需要解决两大问题：\n\n- 数据切片后，在多个实例之间如何分布？\n    \n- 客户端怎么确定想要访问的数据在哪个实例上？\n    \n\n# 数据切片和实例的对应分布关系\n\n  \n\n数据需要分布在不同实例上，那么，**数据和实例之间如何对应呢**？这就和接下来我要讲的 Redis Cluster 方案有关了不过，我们要先弄明白切片集群和 Redis Cluster 的联系与区别。\n\n  \n\n**Redis Cluster 方案采用哈希槽**（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系.\n\n在 Redis Cluster 方案中，**一个切片集群共有 16384 个哈希槽**，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中\n\n- 首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值\n    \n- 然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽\n    \n\n  \n\n可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，**如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个**\n\n  \n\n我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用 cluster addslots 命令，指定每个实例上的哈希槽个数。\n\n  \n\n在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。\n\n  \n\n  \n\n# 客户端如何定位数据？\n\n  \n\n在**定位键值对数据时，它所处的哈希槽是可以通过计算得到的**，这个计算可以在客户端发送请求时来执行。但是，要进一步定位到实例，还需要知道哈希槽分布在哪个实例上\n\n  \n\n**一般来说，客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端.**\n\n但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。\n\n  \n\n客户端为什么**可以在访问任何一个实例时，都能获得所有的哈希槽信息**呢？\n\n这是因为，**Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散**。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了.\n\n  \n\n  \n\n**客户端收到哈希槽信息后，会把哈希槽信息缓存在本地**。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了.\n\n  \n\n  \n\n在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个\n\n- 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽\n    \n- 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍\n    \n\n  \n\n**此时，实例之间还可以通过相互传递消息，获得最新的哈希槽分配信息，但是，客户端是无法主动感知这些变化的**。这就会导致，它缓存的分配信息和最新的分配信息就不一致了，那该怎么办呢？\n\nRedis Cluster 方案提供了一种**重定向机制**，所谓的“重定向”，就是指，**客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令**。\n\n  \n\n那客户端又是怎么知道重定向时的新实例的访问地址呢？\n\n- 当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。\n    \n\nGET hello:key (error) MOVED 13320 172.16.19.5:6379\n\nMOVED 命令表示，客户端请求的键值对所在的哈希槽 13320，实际是在 172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请求了。\n\n  \n\n可以看到，由于负载均衡，Slot 2 中的数据已经从实例 2 迁移到了实例 3，但是，客户端缓存仍然记录着“Slot 2 在实例 2”的信息，所以会给实例 2 发送命令。\n\n- 实例 2 给客户端返回一条 MOVED 命令，把 Slot 2 的最新位置（也就是在实例 3 上），\n    \n- 返回给客户端，客户端就会再次向实例 3 发送请求，同时还会更新本地缓存，把 Slot 2 与实例的对应关系更新过来\n    \n\n![](statistic/asynccode-421.png)\n\n在实际应用时，如**果 Slot 2 中的数据比较多，就可能会出现一种情况：客户端向实例 2 发送请求，但此时，Slot 2 中的数据只有一部分迁移到了实例 3，还有部分数据没有迁移。**在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息\n\nGET hello:key (error) ASK 13320 172.16.19.5:6379\n\n客户端请求的键值对所在的**哈希槽 13320，在 172.16.19.5 这个实例上，但是这个哈希槽正在迁移**。此时，客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。**这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据**。\n\n  \n\nASK 命令表示两层含义：\n\n- 第一，表明 Slot 数据还在迁移中；\n    \n- 第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。\n    \n\n![](statistic/asynccode-420.png)","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6%E4%B8%BB%E5%BA%93%E6%8C%82%E4%BA%86%E5%A6%82%E4%BD%95%E4%B8%8D%E9%97%B4%E6%96%AD%E6%9C%8D%E5%8A%A1":{"title":"哨兵机制：主库挂了，如何不间断服务","content":"  \n\n但是，一旦有写操作请求了，按照主从库模式下的读写分离要求，需要由主库来完成写操作。如果主库挂了，如何不间断服务\n\n![](statistic/asynccode-429.png)\n\n如果主库挂了，我们就需要运行一个新主库，比如说把一个从库切换为主库，把它当成主库。\n\n这就涉及到三个问题：\n\n1. 主库真的挂了吗？\n    \n2. 该选择哪个从库作为主库？\n    \n3. 怎么把新主库的相关信息通知给从库和客户端呢？\n    \n\n在 Redis 主从集群中，哨兵机制是实现主从库自动切换的关键机制，它有效地解决了主从复制模式下故障转移的这三个问题\n\n  \n\n# 哨兵机制的基本流程\n\n哨兵主要负责的就是三个任务：\n\n- 监控\n    \n- 选主（选择主库）\n    \n- 通知\n    \n\n![](statistic/asynccode-427.png)\n\n## 监控\n\n监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行\n\n- 如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”\n    \n- 如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程\n    \n\n  \n\n### 主观下线和客观下线\n\n哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态。\n\n- 如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“**主观下线**”\n    \n    - 如果检测的是从库，那么，哨兵简单地把它标记为“**主观下线**”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断\n        \n    - 如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障。误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下\n        \n        - 通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。\n            \n        - 只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”\n            \n        \n        ![](statistic/asynccode-426.png)\n        \n\n# 选主\n\n主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库\n\n  \n\n### 如何选定新主库\n\n一般来说，我把哨兵选择新主库的过程称为“筛选 + 打分”\n\n![](statistic/asynccode-428.png)\n\n**首先来看筛选的条件**\n\n- 一般情况下，我们肯定要先保证所选的从库仍然在线运行\n    \n- 判断它之前的网络连接状态\n    \n    - 你使用配置项 down-after-milliseconds * 10如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库\n        \n\n  \n\n**打分过程**\n\n- 第一轮：优先级最高的从库得分高。\n    \n    - 用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级\n        \n- 第二轮：和旧主库同步程度最接近的从库得分高\n    \n    - 库会用 master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。此时，我们想要找的从库，它的 slave_repl_offset 需要最接近 master_repl_offset\n        \n- 第三轮：ID 号小的从库得分高\n    \n\n## 通知\n\n**哨兵会把新主库的连接信息发给其他从库**，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，**哨兵会把新主库的连接信息通知给客户端**，让它们把请求操作发到新主库上","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4%E5%93%A8%E5%85%B5%E6%8C%82%E4%BA%86%E4%B8%BB%E4%BB%8E%E5%BA%93%E5%88%87%E6%8D%A2":{"title":"哨兵集群：哨兵挂了，主从库切换","content":"实际上，一旦多个实例组成了哨兵集群，即使有哨兵实例出现故障挂掉了，其他哨兵还能继续协作完成主从库切换的工作，包括监控、选主、通知。\n\n  \n\n在配置哨兵的信息时，我们只需要用到下面的这个配置项，**设置主库的 IP 和端口，并没有配置其他哨兵的连接信息**\n\n\n```\nsentinel monitor \u003cmaster-name\u003e \u003cip\u003e \u003credis-port\u003e \u003cquorum\u003e\n```\n\n\n  \n\n# 基于 pub/sub 机制的哨兵集群组成\n\n  \n\n哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制.\n\n  \n\n**哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息,只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换**\n\n  \n\n在下图中，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到“__sentinel__:hello”频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。\n\n然后，哨兵 2、3 可以和哨兵 1 建立网络连接。通过这个方式，哨兵 2 和 3 也可以建立网络连接，这样一来，哨兵集群就形成了\n\n![](statistic/asynccode-425.png)\n\n  \n\n## 哨兵是如何知道从库的 IP 地址和端口的呢\n\n这是由哨兵向主库发送 INFO 命令来完成的。就像下图所示，\n\n- **哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵**。\n    \n- 接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接\n    \n\n![](statistic/asynccode-422.png)\n\n  \n\n  \n\n  \n\n**通过 pub/sub 机制，哨兵之间可以组成集群，同时，哨兵又通过 INFO 命令，获得了从库连接信息，也能和从库建立连接，并进行监控了。**\n\n  \n\n  \n\n# 基于 pub/sub 机制的客户端事件通知\n\n客户端能够获取到哨兵集群在监控、选主、切换这个过程中发生的各种事件\n\n  \n\n哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息，哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。\n\n  \n\n  \n\n几个关键事件：\n\n  \n\n![](statistic/asynccode-424.png)\n\n知道了这些频道之后，你就可以让客户端从哨兵这里订阅消息了。具体的操作步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，我们可以在客户端执行订阅命令，来获取不同的事件消息。\n\n你可以执行如下命令，来订阅“所有实例进入客观下线状态的事件”：\n\n你也可以执行如下命令，订阅所有的事件：\n\n\n```\nSUBSCRIBE +odown\n```\n\n\n你也可以执行如下命令，订阅所有的事件\n\n\n```\nPSUBSCRIBE *\n```\n\n\n哨兵把新主库选择出来后，客户端就会看到下面的 switch-master 事件。这个事件表示主库已经切换了，新主库的 IP 地址和端口信息已经有了\n\n```\n\nswitch-master \u003cmaster name\u003e \u003coldip\u003e \u003coldport\u003e \u003cnewip\u003e \u003cnewport\u003e\n```\n\n\n  \n\n# 由哪个哨兵执行主从切换\n\n确定由哪个哨兵执行主从切换的过程，和主库“客观下线”的判断过程类似，也是一个“投票仲裁”的过程。\n\n  \n\n在具体了解这个过程前，我们再来看下，判断“客观下线”的仲裁过程\n\n  \n\n- 兵集群要判定主库“客观下线”，需要有一定数量的实例都认为该主库已经“主观下线”了.\n    \n\n  \n\n- 任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票\n    \n- 一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”,这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的\n    \n\n  \n\n此时，这个哨兵就可以再给其他哨兵发送命令，**表明希望由自己来执行主从切换**，并让所有其他哨兵进行投票.这个投票过程称为“**Leader 选举**”。因为最终执行主从切换的哨兵称为 Leader\n\n任何一个想成为 Leader 的哨兵，要满足两个条件：\n\n- 第一，拿到半数以上的赞成票；\n    \n- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值\n    \n\n  \n\n  \n\n过程举例：\n\n![](statistic/asynccode-423.png)\n\n  \n\n- 在 T1 时刻，S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。\n    \n- 在 T2 时刻，S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。\n    \n- 在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意。同时，S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。\n    \n- 在 T4 时刻，S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。\n    \n- 最后，在 T5 时刻，S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了 Leader。接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E4%B8%80%E4%B8%AA%E9%94%AE%E5%80%BC%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E7%BB%84%E6%88%90":{"title":"基本架构：一个键值数据库的组成","content":"如果自己设计一个键值数据库 SimpleKV\n\n开始构造 SimpleKV 时，首先就要**考虑里面可以存什么样的数据，对数据可以做什么样的操作，也就是数据模型和操作接口**。它们看似简单，实际上却是**我们理解 Redis 经常被用于缓存、秒杀、分布式锁等场景**的重要基础。\n\n  \n\n  \n\n  \n\n![](statistic/asynccode-14.jpg)\n\n  \n\n# 可以存哪些数据\n\n  \n\n对于键值数据库而言，基本的数据模型是 key-value 模型.\n\n我们在对键值数据库进行选型时，一个重要的考虑因素是它支持的 value 类型.\n\n- Memcached 支持的 value 类型仅为 String 类型.\n    \n- 而 Redis 支持的 value 类型包括了 String、哈希表、列表、集合、有序集合。\n    \n\n  \n\n# 可以对数据的操作\n\n先来了解下 SimpleKV 需要支持的 3 种基本操作，即 PUT、GET 和 DELETE\n\n- PUT：新写入或更新一个 key-value 对；\n    \n- GET：根据一个 key 读取相应的 value 值；\n    \n- DELETE：根据一个 key 删除整个 key-value 对。\n    \n\n  \n\n有些键值数据库的**新写 / 更新操作叫 SET**。新写入和更新虽然是用一个操作接口，但在实际执行时，**会根据 key 是否存在而执行相应的新写或更新流程**\n\n  \n\n查询一个用户在一段时间内的访问记录。这种操作在键值数据库中属于 SCAN 操作，即根据一段 key 的范围返回相应的 value 值。**因此，PUT/GET/DELETE/SCAN 是一个键值数据库的基本操作集合.**\n\n  \n\n  \n\n# 值对保存在内存还是外存\n\n- 保存在内存的好处是读写很快，毕竟内存的访问速度一般都在百 ns 级别。但是，潜在的风险是一旦掉电，所有的数据都会丢失\n    \n\n  \n\n- 保存在外存，虽然可以避免数据丢失，但是受限于磁盘的慢速读写（通常在几 ms 级别），键值数据库的整体性能会被拉低\n    \n\n  \n\n  \n\n# 总体架构\n\n个键值数据库包括**了访问框架、索引模块、操作模块和存储模块**四部分\n\n![](statistic/asynccode-15.jpg)\n\n  \n\n![](statistic/asynccode-443.png)\n\n  \n\n## 采用什么访问模式---访问框架\n\n访问模式通常有两种：\n\n- 一种是通过函数库调用的方式供外部应用使用，，上图中的 libsimplekv.so，就是以动态链接库的形式链接到我们自己的程序中\n    \n- 另一种是通过网络框架以 Socket 通信的形式对外提供键值对操作，网络框架中包括 Socket Server 和协议解析\n    \n\n实际的键值数据库也基本采用上述两种方式，例如，RocksDB 以动态链接库的形式使用，而 Memcached 和 Redis 则是通过网络框架访问\n\n  \n\n  \n\n使用网络框架访问，我们会遇到一个系统设计上的问题，简单来说**，就是网络连接的处理、网络请求的解析，以及数据存取的处理，是用一个线程、多个线程**，**还是多个进程来交互处理呢**？该如何进行设计和取舍呢？我们一般把这个问题称为 **I/O 模型设计。**\n\n  \n\n可能经常听说 **Redis 是单线程**，那么，Redis 又是如何做到“单线程，高性能”。\n\n  \n\n  \n\n## 如何定位键值对的位置？---索引模块\n\n  \n\n索引的作用是**让键值数据库根据 key 找到相应 value 的存储位置**，进而执行操作。\n\n索引的类型有很多，常见的有**哈希表、B+ 树、字典树**等\n\n  \n\n不同的索引结构在**性能、空间消耗、并发控制**等方面具有不同的特征。如果你看过其他键值数据库，就会发现，不同键值数据库采用的索引并不相同\n\n  \n\n- Memcached 和 Redis 采用哈希表作为 key-value 索引，\n    \n    - 内存键值数据库（例如 Redis）采用哈希表作为索引，很大一部分原因在于，其**键值数据基本都是保存在内存中**的，而内存的**高性能随机访问特性**可以很好地与哈希表 O(1) 的操作复杂度相匹配。\n        \n- 而 RocksDB 则采用跳表作为内存中 key-value 的索引\n    \n\n对于 **Redis 而言**，很有意思的一点是，它的 value 支持多种类型，当我们通过索引找到一个 key 所对应的 value 后，仍然需要从 **value 的复杂结构（例如集合和列表**）中进一步找到我们实际需要的数据。\n\n  \n\n## 不同操作的具体逻辑是怎样的？---操作模块\n\n操作模块就实现了不同操作的具体逻辑\n\n- 对于 GET/SCAN 操作而言，此时根据 value 的存储位置返回 value 值即可；\n    \n- 对于 PUT 一个新的键值对数据而言，SimpleKV 需要为该键值对分配内存空间；\n    \n- 对于 DELETE 操作，SimpleKV 需要删除键值对，并释放相应的内存空间，这个过程由分配器完成。\n    \n\n  \n\n## 如何实现重启后快速提供服务？--存储模块\n\n- SimpleKV 采用了常用的内存分配器 glibc 的 malloc 和 free，libc 的分配器在处理随机的大小内存块分配时，表现并不好。**分配器是键值数据库中的一个关键因素**。\n    \n\n  \n\n- 我也希望 SimpleKV 重启后能快速重新提供服务，所以，我在 SimpleKV 的存储模块中增加了持久化功能\n    \n    - 一种方式是，对于每一个键值对，**都对其进行落盘保存**，这虽然让 SimpleKV 的数据更加可靠，但是，因为每次都要写盘，SimpleKV 的性能会受到很大影响。\n        \n    - 另一种方式是，**只是周期性地把内存中的键值数据保存到文件中**，这样可以避免频繁写盘操作的性能影响。但是，一个潜在的代价是 SimpleKV 的数据仍然有丢失的风险。\n        \n    - 就直接采用了文件形式\n        \n\n  \n\n  \n\n  \n\n# 实际Redis架构\n\n![](statistic/asynccode-442.png)\n\n  \n\n- Redis 主要通过网络框架进行访问，而不再是动态库了，这也使得 Redis 可以作为一个基础性的网络服务进行访问，扩大了 Redis 的应用范围。\n    \n- Redis 数据模型中的 value 类型很丰富，因此也带来了更多的操作接口，例如面向列表的 LPUSH/LPOP，面向集合的 SADD/SREM 等。在下节课，我将和你聊聊这些 value 模型背后的数据结构和操作效率，以及它们对 Redis 性能的影响。\n    \n- Redis 的持久化模块能支持两种方式：**日志（AOF）和快照（RDB）**，这两种持久化方式具有不同的优劣势，影响到 Redis 的访问性能和可靠性。\n    \n- SimpleKV 是个简单的单机键值数据库，但是，**Redis 支持高可靠集群和高可扩展集群**，因此，Redis 中包含了相应的集群功能支撑模块。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E4%B8%BB%E4%BB%8E%E5%BA%93":{"title":"数据同步：主从库","content":"AOF 和 RDB，如果 Redis 发生了宕机，它们可以分别通过回放日志和重新读入 RDB 文件的方式恢复数据，从而保证尽量少丢失数据，提升可靠性。\n\n不过，即使用了这两种方法，也依然存在服务不可用的问题。比如说，我们在**实际使用时只运行了一个 Redis 实例，那么，如果这个实例宕机了，它在恢复期间，是无法服务新来的数据存取请求的**\n\n  \n\nRedis 具有高可靠性，又是什么意思呢？其实，这里有两层含义：\n\n- 一是数据尽量少丢失，\n    \n- 二是服务尽量少中断\n    \n\nAOF 和 RDB 保证了前者，而对于后者，Redis 的做法就是**增加副本冗余量，将一份数据同时保存在多个实例上**\n\n- Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。\n    \n    - 读操作：主库、从库都可以接收；\n        \n    - 写操作：首先到主库执行，然后，主库将写操作同步给从库\n        \n\n为什么要采用读写分离的方式呢\n\n  \n\n- 不管是主库还是从库，都能接收客户端的写操作，那么，一个直接的问题就是：如果客户端对同一个数据（例如 k1）前后修改了三次，每一次的修改请求都发送到不同的实例上，在不同的实例上执行，那么，这个数据在这三个实例上的副本就不一致了（分别是 v1、v2 和 v3）\n    \n- 如果我们非要保持这个数据在三个实例上一致，就要涉及到加锁、实例间协商是否完成修改等一系列操作，但这会带来巨额的开销，当然是不太能接受的\n    \n- 主从库模式一旦采用了读写分离，所有数据的修改只会在主库上进行，不用协调三个实例。主库有了最新的数据后，会同步给从库，这样，主从库的数据就是一致的\n    \n\n  \n\n# 主从库间如何进行第一次同步\n\n当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系\n\n现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据\n\nreplicaof 172.16.19.3 6379\n\n主从库间数据第一次同步的三个阶段:\n\n- 第一阶段是**主从库间建立连接、协商同步的过程**，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了\n    \n    - 从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数\n        \n        - runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。\n            \n        - offset，此时设为 -1，表示第一次复制\n            \n    - 主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库\n        \n    - FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库\n        \n- 在第二阶段，**主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载**。这个过程依赖于**内存快照生成的 RDB 文件**\n    \n    - 主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件\n        \n    - 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 **replication buffer**，记录 RDB 文件生成后收到的所有写操作。\n        \n- 最后，也就是第三个阶段，主库会把**第二阶段执行过程中新收到的写命令，再发送给从库**。具体的操作是，当主库完成 RDB 文件发送后，就会**把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步**了\n    \n\n  \n\n  \n\n# 主从级联模式分担全量复制时的主库压力\n\n  \n\n一次全量复制中，对于主库来说，需要完成两个耗时的操作：**生成 RDB 文件和传输 RDB 文件**\n\n  \n\n- 如果从库数量很多，而且都要和主库进行全量复制的话，就会导致**主库忙于 fork 子进程生成 RDB 文件**，进行数据全量同步。**fork 这个操作会阻塞主线程处理正常请求**，从而导致主库响应应用程序的请求速度变慢\n    \n\n  \n\n  \n\n那么，有没有好的解决方法可以分担主库压力呢？\n\n这就是“主 - 从 - 从”模式\n\n**我们可以通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上**\n\n  \n\n我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令。\n\n  \n\n  \n\n  \n\n# 基于长连接的命令传播\n\n  \n\n一旦主从库完成了全量复制**，它们之间就会一直维护一个网络连接**，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为**基于长连接的命令传播**，可以避免频繁建立连接的开销。\n\n  \n\n# 主从库间网络断了怎么办\n\n  \n\n  \n\n从 Redis 2.8 开始，网络断了之后，**主从库会采用增量复制的方式继续同步**\n\n- 全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库\n    \n- 主从库之间具体是怎么保持同步的呢？这里的奥妙就在于 **repl_backlog_buffer 这个缓冲区**。我们先来看下它是如何用于增量命令的同步的\n    \n\n  \n\n  \n\n主库会把断连期间收到的写操作命令，写入 **replication buffer**，同时也会把这些操作命令也写入 **repl_backlog_buffer** 这个缓冲区\n\n  \n\n**repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置**\n\n刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 **master_repl_offset**。主库接收的新写操作越多，这个值就会越大.\n\n同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 **slave_repl_offset** 也在不断增加。**正常情况下，这两个偏移量基本相等**\n\n  \n\n  \n\n主从库的连接恢复之后，从库首先会给主库发送 psync 命令，**并把自己当前的 slave_repl_offset 发给主库**\n\n  \n\n  \n\n在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset 会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset 之间的命令操作同步给从库就行.\n\n  \n\n因为 repl_backlog_buffer 是**一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作**。如果从库的读取速度比较慢，**就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致**。\n\n一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：\n\n缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小\n\n  \n\n在实际应用中，考虑到可能存在一些突发的请求压力.我们通常需要把这个缓冲空间扩大一倍，即\n\nrepl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值\n\n  \n\n针对这种情况，\n\n- 一方面，你可以根据 Redis 所在服务器的内存资源再适当增加 repl_backlog_size 值，比如说设置成缓冲空间大小的 4 倍，\n    \n- 另一方面，你可以考虑使用切片集群来分担单个主库的请求压力","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/%E9%AB%98%E6%80%A7%E8%83%BDIO%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8D%95%E7%BA%BF%E7%A8%8BRedis%E8%83%BD%E9%82%A3%E4%B9%88%E5%BF%AB":{"title":"高性能IO模型：为什么单线程Redis能那么快","content":"\n- Redis 是单线程，主要是指 **Redis 的网络 IO 和键值对读写是由一个线程来完成的**，这也是 Redis 对外提供键值存储服务的主要流程。\n    \n- 但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。\n    \n\n  \n\n  \n\n  \n\n深入地学习下 Redis 的单线程设计机制以及多路复用机制\n\n  \n\n  \n\n# Redis 为什么用单线程？\n\n## 多线程的开销\n\n  \n\n使用多线程，可以增加系统吞吐率，或是可以增加系统扩展性。于一个多线程的系统来说，在有合理的资源分配的情况下，可**以增加系统中处理请求操作的资源实体**，**进而提升系统能够同时处理的请求数，即吞吐率**\n\n  \n\n如果没有良好的系统设计，实际得到的结果，其实是右图所展示的那样。我们刚开始增加线程数时，系统吞吐率会增加，但是，再进一步增加线程时，系统吞吐率就增长迟缓了，有时甚至还会出现下降的情况\n\n![](statistic/asynccode-435.png)\n\n为什么会出现这种情况呢？\n\n- 一个关键的瓶颈在于，系统中**通常会存在被多线程同时访问的共享资源**，当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个**额外的机制，就会带来额外的开销**\n    \n- 并发访问控制一直是多线程开发中的一个难点问题，如果没有精细的设计，比如说，只是简单地采用一个粗粒度互斥锁，就会出现不理想的结果：即使增加了线程，大部分线程也在**等待获取访问共享资源的互斥锁，并行变串行**\n    \n\n  \n\n## Redis 为啥那么快\n\nRedis 却能使用单线程模型达到每秒数十万级别的处理能力，这是为什么呢？\n\n  \n\n- 一方面，**Redis 的大部分操作在内存上完成**，\n    \n- 再加上它采用了**高效的数据结构**，例如哈希表和跳表，这是它实现高性能的一个重要原因。\n    \n- 另一方面，就是 Redis 采用了**多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率**\n    \n\n  \n\n首先，我们要弄明白网络操作的基本 IO 模型和潜在的阻塞点\n\n  \n\n### 基本IO模型和潜在的阻塞点\n\n  \n\n以SimpleKV为例，SimpleKV 为了处理一个 Get 请求，\n\n1. 需要监听客户端请求（bind/listen）\n    \n2. 和客户端建立连接（accept）\n    \n3. 从 socket 中读取请求（recv）\n    \n4. 解析客户端发送请求（parse）\n    \n5. 根据请求类型读取键值数据（get）\n    \n6. 最后给客户端返回结果，即向 socket 中写回数据（send）\n    \n\n  \n\n即使一下的六步，既然 Redis 是单线程，那么，最基本的一种实现是在一个线程中依次执行这些操作\n\n![](statistic/asynccode-437.png)\n\n在这里的网络 IO 操作中，有潜在的阻塞点，分别是 **accept() 和 recv()**\n\n- Redis 监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这里，导致其他客户端无法和 Redis 建立连接\n    \n- 类似的，当 Redis 通过 recv() 从一个客户端读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()\n    \n\n### 非阻塞模型\n\nSocket 网络模型的非阻塞模式设置，**主要体现在三个关键的函数调用上**，如果想要使用 socket 非阻塞模式，就必须要了解这三个函数的调用返回类型和设置模式\n\nsocket() 方法会返回主动套接字，**然后调用 listen() 方法，将主动套接字转化为监听套接字**，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回**已连接套接字**\n\n![](statistic/asynccode-436.png)\n\n- 针对监听套接字，我们可以设置非阻塞模式：当 **Redis 调用 accept() 但一直未有连接请求到达时，Redis 线程可以返回处理其他操作，而不用一直等待**\n    \n    - 虽然 Redis 线程可以不用继续等待，但是总得有机制继续在监听套接字上等待后续连接请求，并在有请求时通知 Redis。\n        \n- 类似的，我们也可以针对已连接套接字设置非阻塞模式：**Redis 调用 recv() 后，如果已连接套接字上一直没有数据到达，Redis 线程同样可以返回处理其他操作。**、\n    \n    - 我们也需要有机制继续监听该已连接套接字，并在有数据达到时通知 Redis。\n        \n\n  \n\n### 基于多路复用的高性能IO模型\n\nLinux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 **select/epoll 机制**\n\n  \n\n简单来说，在 Redis 只运行单线程的情况下，**该机制允许内核中，同时存在多个监听套接字和已连接套接字。**\n\n内核会**一直监听这些套接字上的连接请求或数据请求**。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。\n\n  \n\n下图就是基于多路复用的 Redis IO 模型，图中的**多个 FD 就是刚才所说的多个套接字**。Redis 网络框架调用 e**poll 机制，让内核监听这些套接字，**Redis 线程**不会阻塞在某一个特定的监听或已连接套接字上**，也就是说，不会阻塞在某一个特定的客户端请求处理上。Redis 可以同时和多个客户端连接并处理请求，从而提升并发性\n\n  \n\n![](statistic/asynccode-437.png)\n\n为了在请求到达时能通知到 Redis 线程，**select/epoll 提供了基于事件的回调机制**，即针对不同事件的发生，调用相应的处理函数\n\n  \n\n**回调机制如何工作？**\n\n- select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。\n    \n- 这些事件会被放进一个**事件队列**，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。\n    \n- 同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能\n    \n\n  \n\n因为这个机制的实现有很多种，既有基于 **Linux 系统下的 select 和 epoll 实现**，也有基于 **FreeBSD 的 kqueue 实现**，以及**基于 Solaris 的 evport 实现**，这样，你可以根据 Redis 实际运行的操作系统，选择相应的多路复用实现。\n\n  \n\n  \n\n# 潜在的性能瓶颈\n\nRedis单线程处理IO请求性能瓶颈主要包括2个方面\n\n1. 任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种：\n    \n    1. 操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；\n        \n    2. 使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；\n        \n    3. 大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；\n        \n    4. 淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；\n        \n    5. AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；\n        \n    6. 主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；\n        \n2. 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。\n    \n\n  \n\n解决方案：\n\n1. 针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了**lazy-free机制**，**把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响**。\n    \n\n  \n\n1. 针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/AOF%E6%97%A5%E5%BF%97%E5%AE%95%E6%9C%BA%E4%BA%86Redis%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1":{"title":"AOF日志：宕机了，Redis如何避免数据丢失","content":"Redis 的持久化主要有两大机制，即 **AOF（Append Only File）日志和 RDB 快照**。\n\n# AOF日志如何实现\n\n  \n\n- 数据库是写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复\n    \n- AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志，如下图所示\n    \n    ![](statistic/asynccode-432.png)\n    \n\n  \n\n## AOF 为什么要先执行命令再记日志呢\n\n- 传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，\n    \n- 而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的\n    \n    - 以 Redis 收到“set testkey testvalue”命令后记录的日志为例\n        \n    - “*3”表示当前命令有三个部分，每部分都是由“$+数字”开头，后面紧跟着具体的命令、键或值，\n        \n    - “数字”表示这部分中的命令、键或值一共有多少字节\n        \n        ![](statistic/asynccode-432.png)\n        \n- 但是，为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，**并不会先去对这些命令进行语法检查**。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错\n    \n- 而写后日志这种方式，就是先让系统执行命令，**只有命令能执行成功，才会被记录到日志中**，否则，系统就会直接向客户端报错\n    \n- AOF 还有一个好处：**它是在命令执行后才记录日志，所以不会阻塞当前的写操作**\n    \n\n  \n\n## AOF潜在的风险\n\n- 首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。\n    \n- 其次，AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，**AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢**，进而导致后续的操作也无法执行了\n    \n\n  \n\n这两个风险都是和 AOF 写回磁盘的时机相关的，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除了\n\n## 三种写回策略\n\nAOF 机制给我们提供了三个选择，也就是 AOF 配置项 appendfsync 的三个可选值，\n\n- Always，**同步写回**：每个写命令执行完，立马同步地将日志写回磁盘；\n    \n- Everysec，**每秒写回**：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；\n    \n- No，**操作系统控制的写回**：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，**由操作系统决定何时将缓冲区内容写回磁盘**\n    \n\n  \n\n  \n\n这三种写回策略都无法做到两全其美\n\n![](statistic/asynccode-433.png)\n\n  \n\n总结一下就是：\n\n- 想要获得高性能，就选择 No 策略；\n    \n- 如果想要得到高可靠性保证，就选择 Always 策略；\n    \n- 如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。\n    \n\n  \n\n## 日志文件太大了怎么办？\n\n随着接收的写命令越来越多，AOF 文件会越来越大。这也就意味着，我们一定要**小心 AOF 文件过大带来的性能问题**\n\n**主要在于三个方面：**\n\n- 一是，**文件系统本身对文件大小有限制**，无法保存过大的文件；\n    \n- 二是，如果文件太大，**之后再往里面追加命令记录的话，效率也会变低**；\n    \n- 三是，如果发生宕机，AOF 中记录的命令要一个被重新执行，用于故障恢复，**如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用**\n    \n\n  \n\n  \n\n  \n\nAOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读**取数据库中的所有键值对**，**然后对每一个键值对用一条命令记录它的写入**\n\n  \n\n实际上，重写机制具有“多变一”功能。所谓的“多变一”，也就是说，旧日志文件中的多条命令，在重写后的新日志中变成了一条命令。\n\n  \n\nAOF 文件是以追加的方式，逐一记录接收到的写命令的。当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令\n\n  \n\n## AOF 重写会阻塞吗?\n\n  \n\n和 AOF 日志由主线程写回不同，重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。\n\n  \n\n  \n\n我把重写的过程总结为“一个拷贝，两处日志”。\n\n- “一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，**这里面就包含了数据库的最新数据**，grewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。\n    \n- “两处日志”\n    \n    - 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 **AOF 日志**，Redis 会把这个**操作写到它的缓冲区**。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复\n        \n    - 而第二处日志，就是指新**的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区**。这样，重写日志也不**会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录**\n        \n\n![](statistic/asynccode-434.png)\n\n  \n\n  \n\nAOF 日志重写的时候，是由 bgrewriteaof 子进程来完成的，不用主线程参与，前面说的非阻塞也是指子进程的执行不阻塞主线程。**这个重写过程有其他潜在的阻塞风险。**\n\n  \n\nRedis采用fork子进程重写AOF文件时，潜在的阻塞风险包括：fork子进程 和 AOF重写过程中父进程产生写入的场景，下面依次介绍\n\n- fork子进程，**fork这个瞬间一定是会阻塞主线程的，fork时并不会一次性拷贝所有内存数据给子进程。fork采用操作系统提供的写实复制(Copy On Write)机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题**，但fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间越久。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？**“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险**，就是下面介绍的场景。\n    \n    - fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。\n        \n\n  \n\nAOF 重写也有一个重写日志，为什么它不共享使用 AOF 本身的日志呢？\n\n- **AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。**所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%9F%BA%E7%A1%80/redis-%E7%9A%84%E6%85%A2%E6%93%8D%E4%BD%9C":{"title":"redis 的慢操作","content":"Redis 的快，到底是快在哪里呢？实际上，这里有一个重要的表现：**它接收到一个键值对操作后，能以微秒级别的速度找到数据，并快速完成操作。**\n\n  \n\n  \n\n为啥 Redis 能有这么突出的表现呢？\n\n- 一方面，这是因为它是**内存数据库**，所有操作都在内存上完成，内存的访问速度本身就很快。\n    \n- 另一方面，这要归功于它的**数据结构**。这是因为，键值对是按一定的数据结构来组织的，操作键值对最终就是对数据结构进行增删改查操作，所以高效的数据结构是 Redis 快速处理数据的基础。\n    \n    - 一方面是因为 O(1) 复杂度**的哈希表被广泛使用**，包括 String、Hash 和 Set，它们的操作复杂度基本由哈希表决定，\n        \n    - 另一方面，Sorted Set 也采用了 O(logN) 复杂度的跳表.\n        \n    - 我们不能忘了复杂度较高的 List 类型，它的两种底层实现结构：双向链表和压缩列表的操作复杂度都是 O(N)。因此，我的建议是：\n        \n        - 因地制宜地使用 List 类型。例如，既然它的 POP/PUSH 效率很高，那么就将它主要用于 FIFO 队列场景，而不是作为一个可以随机读写的集合。\n            \n\n  \n\n  \n\n  \n\nRedis值的数据类型\n\n- String（字符串）\n    \n- List（列表）\n    \n- Hash（哈希）\n    \n- Set（集合）\n    \n- Sorted Set（有序集合）\n    \n\n  \n\nRedis值的底层数据结构\n\n- 动态字符串\n    \n- 双向链表\n    \n- 压缩列表\n    \n- 哈希表\n    \n- 跳表\n    \n- 整数数组\n    \n\n  \n\n![](statistic/asynccode-438.png)\n\n- String 类型的底层实现只有一种数据结构，也就是简单动态字符串。\n    \n- List、Hash、Set 和 Sorted Set 这四种数据类型，都有两种底层实现结构。通常情况下，我们会把这四种类型称为集合类型，它们的特点是**一个键对应了一个集合的数据**\n    \n\n  \n\n于是可以引出一下的问题\n\n1. 这些数据结构都是值的底层实现，键和值本身之间用什么结构组织？\n    \n2. 为什么集合类型有那么多的底层结构，它们都是怎么组织数据的，都很快吗？\n    \n3. 什么是简单动态字符串，和常用的字符串是一回事吗？\n    \n\n  \n\n  \n\n# 键和值用什么结构组织？\n\n  \n\n为了实现从键到值的快速访问，Redis 使用了**一个哈希表来保存所有键值对。**\n\n一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。\n\n如果值是集合类型的话，作为数组元素的哈希桶怎么来保存呢？”其实，哈**希桶中的元素保存的并不是值本身，而是指向具体值的指针**\n\n  \n\n如果值是集合类型的话，作为数组元素的哈希桶怎么来保存呢？”其实，哈希桶中的元素保存的并不是值本身，而是指向具体值的指针\n\n  \n\n![](statistic/asynccode-438.png)\n\n哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，**就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素**\n\n  \n\n  \n\n只是了解了哈希表的 O(1) 复杂度和快速查找特性，那么，当你往 Redis 中写入大量数据后，就可能发现操作有时候会突然变慢了。这其实是因为你忽略了一个潜在的风险点，那就是**哈希表的冲突问题和 rehash 可能带来的操作阻塞。**\n\n  \n\n# 为什么哈希表操作变慢了？\n\n## 哈希冲突\n\n哈希冲突，也就是指，**两个 key 的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中**。\n\n  \n\nredis 解决哈希冲突的方式，就是**链式哈希**。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接.\n\n![](statistic/asynccode-441.png)\n\n  \n\n  \n\n这里依然存在一个问题，哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多。，Redis 会对哈希表做 **rehash 操作**\n\n  \n\n## rehash\n\nrehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。\n\n  \n\n为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash，这个过程分为三步：\n\n- 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍\n    \n- 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中\n    \n- 释放哈希表 1 的空间\n    \n\n哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而**原来的哈希表 1 留作下一次 rehash 扩容备用。**\n\n**第二步涉及大量的数据拷贝**，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞\n\n  \n\n### 渐进式 rehash\n\n- 第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中\n    \n- 等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries\n    \n\n![](statistic/asynccode-439.png)\n\n  \n\n# 集合数据的操作效率\n\n和 String 类型不同，一个集合类型的值，\n\n- 第一步是通过全局哈希表找到对应的哈希桶位置，\n    \n- 第二步是在集合中再增删改查\n    \n\n  \n\n集合的操作效率和哪些因素相关呢？\n\n- 首先，与集合的底层数据结构有关，使用哈希表实现的集合，要比使用链表实现的集合访问效率更高。\n    \n- 其次，操作效率和这些操作本身的执行特点有关，比如读写一个元素的操作要比读写所有元素的效率高\n    \n\n  \n\n## 底层数据结构\n\n集合类型的底层数据结构主要有 5 种：\n\n- 整数数组\n    \n- 双向链表\n    \n- 哈希表\n    \n- 压缩列表\n    \n- 跳表\n    \n\n  \n\n**整数数组和双向链表**也很常见，它们的操作特征都是顺序读写，也就是通过数组下标或者链表的指针逐个元素，操作复杂度基本是 O(N)，操作效率比较低；\n\n### **压缩列表**\n\n**压缩列表**实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，**压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数**；压缩列表在表尾还有一个 zlend，表示列表结束\n\n|   |   |\n|---|---|\n|zlbytes|列表长度|\n|zltail|列表尾的偏移量|\n|zllen|列表中的 entry 个数|\n|zlend|列表结束|\n\n- 如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。\n    \n- 而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了\n    \n\n  \n\n### **跳表**\n\n有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表\n\n**跳表**在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。\n\n![](statistic/asynccode-440.png)\n\n  \n\n这个查找过程就是在多级索引上跳来跳去，最后定位到元素。这也正好符合“跳”表的叫法，当数据量很大时，跳表的查找复杂度就是 O(logN)\n\n![](statistic/asynccode-440.png)\n\n  \n\n## 不同操作的复杂度\n\n结了一个“四句口诀”，希望能帮助你快速记住集合常见操作的复杂度。这样你在使用过程中，就可以提前规避高复杂度操作了。\n\n单元素操作是基础；\n\n范围操作非常耗时；\n\n统计操作通常高效；\n\n例外情况只有几个。\n\n  \n\n1. 第一，单元素操作，是**指每一种集合类型对单个数据实现的增删改查操作**。例如，Hash 类型的 HGET、HSET 和 HDEL，Set 类型的 SADD、SREM、SRANDMEMBER 等,这些操作的**复杂度由集合采用的数据结构**决定,\n    \n2. 第二，范围操作，是指集合类型中的遍历操作，可以返回集合中的所有数据,这类操作的复杂度一般是 O(N)，比较耗时，我们应该尽量避免\n    \n3. 第三，统计操作，是指集合类型对集合中所有元素个数的记录,集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计,这类操作复杂度只有 O(1)\n    \n4. 第四，例外情况，是指某些数据结构的特殊记录，例如压缩列表和双向链表都会记录表头和表尾的偏移量。这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂度也只有 O(1)，可以实现快速操作。\n    \n\n  \n\n  \n\n  \n\n  \n\n  \n\nRedis 之所以能快速操作键值对\n\n- 一方面是因为 O(1) 复杂度的哈希表被广泛使用，包括 String、Hash 和 Set，它们的操作复杂度基本由哈希表决定，\n    \n- 另一方面，Sorted Set 也采用了 O(logN) 复杂度的跳表.\n    \n- 我们不能忘了复杂度较高的 List 类型，它的两种底层实现结构：双向链表和压缩列表的操作复杂度都是 O(N)。因此，我的建议是：因地制宜地使用 List 类型。例如，既然它的 POP/PUSH 效率很高，那么就将它主要用于 FIFO 队列场景，而不是作为一个可以随机读写的集合。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E4%BA%8B%E5%8A%A1%E6%9C%BA%E5%88%B6":{"title":"事务机制","content":"事务是数据库的一个重要功能。所谓的事务，就是指对数据进行读写的一系列操作。事务在执行时，会提供专门的属性保证，包括原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），也就是 ACID 属性。\n\n# 事务 ACID 属性的要求\n\n  \n\n- 原子性。原子性的要求很明确，就是一个事务中的多个操作必须都完成，或者都不完成\n    \n- 一致性。指数据库中的数据在事务执行前后是一致的\n    \n- 隔离性。它要求数据库在执行一个事务时，其它操作无法存取到正在执行事务访问的数据。\n    \n- 持久性。数据库执行事务后，数据的修改要被持久化保存下来。当数据库重启后，数据的值需要是被修改后的值。\n    \n\n  \n\n# Redis 如何实现事务\n\n事务的执行过程包含三个步骤，Redis 提供了 MULTI、EXEC 两个命令来完成这三个步骤。\n\n- 第一步，客户端要使用一个命令显式地表示一个事务的开启。在 Redis 中，**这个命令就是 MULTI**。\n    \n- 第二步，客户端把事务中本身要执行的具体操作（例如增删改数据）发送给服务器端。**这些命令虽然被客户端发送到了服务器端，但 Redis 实例只是把这些命令暂存到一个命令队列中，并不会立即执行**。\n    \n- 第三步，客户端向服务器端发送提交事务的命令，让数据库实际执行第二步中发送的具体操作。Redis 提供的 EXEC 命令就是执行事务提交的。**当服务器端收到 EXEC 命令后，才会实际执行命令队列中的所有命令。**\n    \n\n  \n\n  \n\n# Redis 事务机制能保证那些属性\n\n## 原子性------不一定能保证\n\n但是，如果事务执行发生错误了，原子性还能保证吗？我们需要分三种情况来看。\n\n- 第一种情况是，在执行 EXEC 命令前，客户端发送的操作命令本身就有错误（比如语法错误，使用了不存在的命令），在命令入队时就被 Redis 实例判断出来了。\n    \n    - 对于这种情况，在命令入队时，**Redis 就会报错并且记录下这个错误**。此时，我们还能继续提交命令操作。等**到执行了 EXEC 命令之后**，Redis 就**会拒绝执行所有提交的命令操作**，返回事务失败的结果。这样一来，事务中的所有命令都不会再被执行了，保证了原子性。\n        \n- 第二种，事务操作入队时，命令和操作的数据类型不匹配，但 Redis 实例没有检查出错误\n    \n    - 在执行完 EXEC 命令以后，Redis 实际执行这些事务操作时，就会报错。不过，需要注意的是，虽然 Redis 会对错误命令报错，但还是会把正确的命令执行完。在这种情况下，事务的原子性就无法得到保证了\n        \n- 第三种，**在执行事务的 EXEC 命令时**，Redis 实例发生了故障，导致事务执行失败。\n    \n    - 如果 Redis 开启了 AOF 日志，那么，只会有部分的事务操作被记录到 AOF 日志中。我们需要使用 redis-check-aof 工具检查 AOF 日志文件，**这个工具可以把未完成的事务操作从 AOF 文件中去除。这样一来，我们使用 AOF 恢复实例后，事务操作不会再被执行，从而保证了原子性。**\n        \n\n  \n\n**小结**\n\n- 命令入队时就报错，会放弃事务执行，保证原子性；\n    \n- 命令入队时没报错，实际执行时报错，不保证原子性；\n    \n- EXEC 命令执行时实例故障，如果开启了 AOF 日志，可以保证原子性。\n    \n\n  \n\n## 一致性-----能保证\n\n我们按照命令出错和实例故障的发生时机，分成三种情况来看。\n\n- 情况一：命令入队时就报错\n    \n    - 在这种情况下，事务本身就会被放弃执行，所以可以保证数据库的一致性。\n        \n- 情况二：命令入队时没报错，实际执行时报错\n    \n    - 在这种情况下，有错误的命令不会被执行，正确的命令可以正常执行，也不会改变数据库的一致性。\n        \n- 情况三：EXEC 命令执行时实例发生故障\n    \n    - 如果我们没有开启 RDB 或 AOF，那么，实例故障重启后，数据都没有了，数据库是一致的。\n        \n    - 如果我们使用了 AOF 日志，而事务操作还没有被记录到 AOF 日志时，实例就发生了故障，那么，使用 AOF 日志恢复的数据库数据是一致的。如果只有部分操作被记录到了 AOF 日志，我们可以使用 redis-check-aof 清除事务中已经完成的操作，数据库恢复后也是一致的。\n        \n\n  \n\n## 隔离性-----可以保证（ WATCH 机制）\n\n事务的隔离性保证，会受到和事务一起执行的并发操作的影响。而事务执行又可以分成命令入队（EXEC 命令执行前）和命令实际执行（EXEC 命令执行后）两个阶段，所以，我们就针对这两个阶段，分成两种情况来分析：\n\n- 并发操作在 EXEC 命令前执行，此时，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证；\n    \n    - WATCH 机制的作用是，**在事务执行前，监控一个或多个键的值变化情况，当事务调用 EXEC 命令执行时，WATCH 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏**。然后，客户端可以再次执行事务，此时，如果没有并发修改事务数据的操作了，事务就能正常执行，隔离性也得到了保证\n        \n    \n    ![](statistic/asynccode-449.png)\n    \n- 并发操作在 EXEC 命令后执行，此时，隔离性可以保证。\n    \n\n  \n\n## 持久性\n\n数据是否持久化保存完全取决于 Redis 的持久化配置模式。\n\n- 如果 Redis 没有使用 RDB 或 AOF，那么事务的持久化属性肯定得不到保证\n    \n- 如果 Redis 使用了 RDB 模式，那么，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发生了实例宕机，这种情况下，事务修改的数据也是不能保证持久化的。\n    \n- 如果 Redis 采用了 AOF 模式，因为 AOF 模式的三种配置选项 no、everysec 和 always 都会存在数据丢失的情况，所以，事务的持久性属性也还是得不到保证。\n    \n\n管 Redis 采用什么持久化模式，**事务的持久性属性是得不到保证的**\n\n  \n\n# 总结\n\n![](statistic/asynccode-450.png)\n\n  \n\n  \n\nredis 的事务机制可以保证一致性和隔离性，但是无法保证持久性。\n\n原子性的情况比较复杂，只有当事务中使用的命令语法有误时，原子性得不到保证，在其它情况下，事务都可以原子性执行","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E4%BF%9D%E5%AD%98%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97":{"title":"保存时间序列","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%E5%90%8E%E4%B8%BA%E5%95%A5%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E7%8E%87%E8%BF%98%E6%98%AF%E5%BE%88%E9%AB%98":{"title":"删除数据后，为啥内存占用率还是很高","content":"明明做了数据删除，数据量已经不大了，为什么使用 top 命令查看时，还会发现 Redis 占用了很多内存呢？\n\n  \n\n这是因为，当数据删除后，Redis 释放的内存空间会由内存分配器管理，并不会立即返回给操作系统。所以，操作系统仍然会记录着给 Redis 分配了大量内存\n\n  \n\n- 潜在的风险点：**Redis 释放的内存空间可能并不是连续的，那么，这些不连续的内存空间很有可能处于一种闲置的状态**。\n    \n- 这就会导致一个问题：**虽然有空闲空间，Redis 却无法用来保存数据，不仅会减少 Redis 能够实际保存的数据量，还会降低 Redis 运行机器的成本回报率**。\n    \n\n  \n\n# 内存碎片是如何形成的？\n\n  \n\n内存碎片的形成有内因和外因两个层面的原因。简单来说**，内因是操作系统的内存分配机制，外因是 Redis 的负载特征。**\n\n## 内因：内存分配器的分配策略\n\n  \n\n内存分配器的分配策略就决定了操作系统无法做到“按需分配”。这是因为**，内存分配器一般是按固定大小来分配内存**，而不是完全按照应用程序申请的内存空间大小给程序分配。\n\nRedis 可以使用 libc、jemalloc、tcmalloc 多种内存分配器来分配内存，**默认使用 jemalloc。**\n\n  \n\njemalloc 的分配策略之一，**是按照一系列固定的大小划分内存空间**，例如 8 字节、16 字节、32 字节、48 字节，…, 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间。\n\n  \n\n**这样的分配方式本身是为了减少分配次数** ，例如，Redis 申请一个 20 字节的空间保存数据，jemalloc 就会分配 32 字节，此时，如果应用还要写入 10 字节的数据，Redis 就不用再向操作系统申请空间了，因为刚才分配的 32 字节已经够用了，这就避免了一次分配操作。\n\n  \n\n  \n\n## 外因：键值对大小不一样和删改操作\n\n  \n\n- Redis 通常作为共用的缓存系统或键值数据库对外提供服务，所以，不同业务应用的数据都可能保存在 Redis 中，这就会带来不同大小的键值对。这样一来，Redis 申请内存空间分配时，本身就会有大小不一的空间需求。这是第一个外因\n    \n- 第二个外因是，这些键值对会被修改和删除，这会导致空间的扩容和释放。具体来说，一方面，如果修改后的键值对变大或变小了，就需要占用额外的空间或者释放不用的空间。另一方面，删除的键值对就不再需要内存空间了，此时，就会把空间释放出来，形成空闲空间\n    \n\n  \n\n  \n\n# 如何判断是否有内存碎片\n\nRedis 是内存数据库，内存利用率的高低直接关系到 Redis 运行效率的高低。为了让用户能监控到实时的内存使用情况，Redis 自身提供了 INFO 命令\n\nINFO memory # Memory used_memory:1073741736 used_memory_human:1024.00M used_memory_rss:1997159792 used_memory_rss_human:1.86G … mem_fragmentation_ratio:1.86\n\n这里有一个 mem_fragmentation_ratio 的指标\n\n- **mem_fragmentation_ratio 大于 1 但小于 1.5。这种情况是合理的。**这是因为，刚才我介绍的那些因素是难以避免的。毕竟，内因的内存分配器是一定要使用的，分配策略都是通用的，不会轻易修改；而外因由 Redis 负载决定，也无法限制。所以，存在内存碎片也是正常的。\n    \n- **mem_fragmentation_ratio 大于 1.5** 。这表明内存碎片率已经超过了 50%。一般情况下，这个时候，我们就**需要采取一些措施来降低内存碎片率了。**\n    \n\n  \n\n# 如何清理内存碎片\n\n  \n\n**一个“简单粗暴”的方法就是重启 Redis 实例**。当然，这并不是一个“优雅”的方法，毕竟，重启 Redis 会带来两个后果：\n\n- 如果 Redis 中的数据没有持久化，那么，数据就会丢失；\n    \n- 即使 Redis 数据持久化了，我们还需要通过 AOF 或 RDB 进行恢复，恢复时长取决于 AOF 或 RDB 的大小，如果只有一个 Redis 实例，恢复阶段无法提供服务。\n    \n\n  \n\n还有什么其他好办法吗?\n\nRedis 自身提供了一种内存碎片自动清理的方法，我们先来看这个方法的基本机制：\n\n当有数据把一块连续的内存空间分割成好几块不连续的空间时，**操作系统就会把数据拷贝到别处**。此时，数据拷贝需要能把这些数据原来占用的空间都空出来，把原本不连续的内存空间变成连续的空间。否则，如果数据拷贝后，并没有形成连续的内存空间，这就不能算是清理了。\n\n![](statistic/asynccode-463.png)\n\n  \n\n碎片清理是有代价的，操作系统需要把多份数据拷贝到新位置，把原有空间释放出来，这会带来时间开销。因为 Redis 是单线程，在数据拷贝时，Redis 只能等着，这就导致 Redis 无法及时处理请求，性能就会降低。\n\n  \n\n有什么办法可以尽量缓解这个问题吗？\n\n这就要提到，Redis 专门为自动内存碎片清理功机制设置的参数了。我们可以通过设置参数，来控制碎片清理的开始和结束时机，以及占用的 CPU 比例，从而减少碎片清理对 Redis 本身请求处理的性能影响。\n\n- active-defrag-ignore-bytes 100mb：表示内存碎片的字节数达到 100MB 时，开始清理；\n    \n- active-defrag-threshold-lower 10：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81":{"title":"如何使用Redis实现分布式锁","content":"上节课，我提到，在应对并发问题时，除了**原子操作，Redis 客户端还可以通过加锁的方式，来控制并发写操作对共享数据的修改，从而保证数据的正确性。**\n\n  \n\n  \n\n但是，Redis 属于分布式系统，当有多个客户端需要争抢锁时，我们必须要保证，**这把锁不能是某个客户端本地的锁**。\n\n  \n\n**在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是保存在一个共享存储系统中的，可以被多个客户端共享访问和获取。**\n\n  \n\n  \n\n# 单机上的锁和分布式锁的联系与区别\n\n  \n\n  \n\n对于在单机上运行的多线程程序来说，锁本身可以用一个变量表示。\n\n- 变量值为 0 时，表示没有线程获取锁；\n    \n- 变量值为 1 时，表示已经有线程获取到锁了\n    \n\n  \n\n  \n\n和单机上的锁类似，分布式锁同样可以用一个变量来实现，客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为 0，表明客户端不再持有锁，\n\n  \n\n在分布式场景下，**锁变量需要由一个共享存储系统来维护**，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，**加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值**。\n\n  \n\n  \n\n实现分布式锁的要求：\n\n- 要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，**在实现分布式锁时，我们需要保证这些锁操作的原子性；**\n    \n- 要求二：**共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了**。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。\n    \n\n  \n\n  \n\n  \n\n# 基于单个节点实现分布式锁\n\n  \n\n  \n\nRedis 可以用哪些单命令操作实现加锁操作。\n\n  \n\n- **首先是 SETNX 命令，它用于设置键值对的值**。具体来说，**就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。**\n    \n\nSETNX key value\n\n- 总结来说，我们就可以用 SETNX 和 DEL 命令组合来实现加锁和释放锁操作。下面的伪代码示例显示了锁操作的过程，你可以看下。\n    \n\n// 加锁 SETNX lock_key 1 // 业务逻辑 DO THINGS // 释放锁 DEL lock_key\n\n- 第一个风险是，假如某个客户端在执行了 SETNX 命令、加锁之后，**紧接着却在操作共享数据时发生了异常，结果一直没有执行最后的 DEL 命令释放锁 ----**一个有效的解决方法是，**给锁变量设置一个过期时间**\n    \n- 我们再来看第二个风险。**如果客户端 A 执行了 SETNX 命令加锁后，假设客户端 B 执行了 DEL 命令释放锁，此时，客户端 A 的锁就被误释放了。**如果客户端 C 正好也在申请加锁，就可以成功获得锁，进而开始操作共享数据。这样一来，客户端 A 和 C 同时在对共享数据进行操作，数据就会被修改错误，这也是业务层不能接受的。\n    \n    - 需要能区分来自不同客户端的锁操作\n        \n        - 所以，我们在加锁操作时，**可以让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端。**\n            \n\n// 加锁, unique_value作为客户端唯一性的标识 SET lock_key unique_value NX PX 10000\n\n  \n\n# 基于多个 Redis 节点实现高可靠的分布式锁\n\n  \n\n  \n\n为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了**分布式锁算法 Redlock**。\n\n  \n\nRedlock 算法的基本思路，是**让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败**。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失\n\n  \n\nRedlock 算法的实现需要有 N 个独立的 Redis 实例。接下来，我们可以分成 3 步来完成加锁操作。\n\n- 第一步是，客户端获取当前时间。\n    \n- 第二步是，客户端按顺序依次向 N 个 Redis 实例执行加锁操作。\n    \n- 第三步是，一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。\n    \n    - 客户端只有在满足下面的这两个条件时，才能认为是加锁成功。\n        \n        - 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁；\n            \n        - 条件二：客户端获取锁的总耗时没有超过锁的有效时间。\n            \n\n  \n\n  \n\n# 总结\n\n  \n\n在基于单个 Redis 实例实现分布式锁时，对于加锁操作，我们需要满足三个条件。\n\n- 加锁包括了**读取锁变量、检查锁变量值和设置锁变量值三个操作**，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；\n    \n- 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，**我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；**\n    \n- **锁变量的值需要能区分来自不同客户端的加锁操作**，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E5%BC%82%E6%AD%A5%E6%9C%BA%E5%88%B6%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%98%BB%E5%A1%9E":{"title":"异步机制：如何避免单线程模型的阻塞","content":"影响 Redis 性能的 5 大方面的潜在因素：\n\n1. Redis 内部的阻塞式操作；\n    \n2. CPU 核和 NUMA 架构的影响；\n    \n3. Redis 关键系统配置；\n    \n4. Redis 内存碎片；\n    \n5. Redis 缓冲区\n    \n\n  \n\n  \n\n这一节介绍Redis 内部的阻塞式操作 相关内容\n\n  \n\n**Redis 的网络 IO 和键值对读写是由主线程完成的**。那么，如果在主线程上执行的操作消耗的时间太长，就会引起主线程阻塞。\n\nRedis 既有服务客户端请求的键值对增删改查操作，也有保证可靠性的持久化操作，还有进行主从复制时的数据同步操作，等等。操作这么多，究竟哪些会引起阻塞呢\n\n  \n\n  \n\n# Redis实例的阻塞点？\n\n下面我们来看看和 Redis 实例交互的对象，以及交互时会发生的操作\n\n- 客户端：网络 IO，键值对增删改查操作，数据库操作；\n    \n- 磁盘：生成 RDB 快照，记录 AOF 日志，AOF 日志重写；\n    \n- 主从节点：主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB 文件；\n    \n- 切片集群实例：向其他实例传输哈希槽信息，数据迁移。\n    \n\n![](statistic/asynccode-476.png)\n\n## 与客户端交互的阻塞点\n\n  \n\n- 网络 IO 有时候会比较慢，但是 **Redis 使用了 IO 多路复用机制**，避免了主线程一直处在等待网络连接或请求到来的状态，所以，网络 IO 不是导致 Redis 阻塞的因素\n    \n- 键值对的增删改查操作是 Redis 和客户端交互的主要部分，也是 Redis 主线程执行的主要任务。所以，**复杂度高的增删改查操作肯定会阻塞 Redis**\n    \n    - 怎么判断操作复杂度是不是高呢？这里有一个最基本的标准，就是看操作的复杂度是否为 O(N)\n        \n    - **第一个阻塞点：集合全量查询和聚合操作**\n        \n    - **集合自身的删除操作同样也有潜在的阻塞风险 ，bigkey 删除操作就是 Redis 的第二个阻塞点**\n        \n        - 删除操作的本质是要释放键值对占用的内存空间。在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序，所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞\n            \n        - 什么时候会释放大量内存呢？其实就是在删除大量键值对数据的时候，**最典型的就是删除包含了大量元素的集合，也称为 bigkey 删除**。\n            \n- Redis 的第三个阻塞点：清空数据库\n    \n\n## 和磁盘交互时的阻塞点\n\nredis 开发者早已认识到磁盘 IO 会带来阻塞，所以就把 **Redis 进一步设计为采用子进程的方式生成 RDB 快照文件，以及执行 AOF 日志重写操作**。这样一来，这两个操作由子进程负责执行，慢速的磁盘 IO 就不会阻塞主线程了.\n\n  \n\nRedis 直接记录 AOF 日志时，会根据不同的写回策略对数据做落盘保存。一个同步写磁盘的操作的耗时大约是 1～2ms，如果有大量的写操作需要记录在 AOF 日志中，并同步写回的话，就会阻塞主线程了**.Redis 的第四个阻塞点了：AOF 日志同步写**。\n\n## 主从节点交互时的阻塞点\n\n主库需要生成 RDB 文件，并传输给从库。主库在复制的过程中，创建和传输 RDB 文件都是由子进程来完成的，不会阻塞主线程。但是，对于从库来说，**它在接收了 RDB 文件后，需要使用 FLUSHDB 命令清空当前数据库**，**这就正好撞上了刚才我们分析的第三个阻塞点**\n\n  \n\n此外，从库在清空当前数据库后，还需要把 RDB 文件加载到内存，这个过程的快慢和 RDB 文件的大小密切相关，RDB 文件越大，加载过程越慢，所以，**加载 RDB 文件就成为了 Redis 的第五个阻塞点**。\n\n## 切片集群实例交互时的阻塞点\n\n如果你使用了 Redis Cluster 方案，而且同时正好迁移的是 bigkey 的话，就会造成主线程的阻塞，因为 Redis Cluster 使用了同步迁移.**这里你只需要知道，当没有 bigkey 时，切片集群的各实例在进行交互时不会阻塞主线程**\n\n  \n\n# 合计五个阻塞点\n\n- 集合全量查询和聚合操作；\n    \n- bigkey 删除；\n    \n- 清空数据库；\n    \n- AOF 日志同步写；\n    \n- 从库加载 RDB 文件。\n    \n\n  \n\n# 哪些阻塞点可以异步执行\n\n先来了解下异步执行对操作的要求;\n\n- 如果一个操作能被异步执行，就意味着，它并不是 Redis 主线程的关键路径上的操作\u003e\n    \n- 我再解释下关键路径上的操作是啥。这就是说，客户端把请求发送给 Redis 后，等着 Redis 返回数据结果的操作\n    \n\n  \n\n![](statistic/asynccode-475.png)\n\n操作 1 就不算关键路径上的操作，因为它不用给客户端返回具体数据，所以可以由后台子线程异步执行。而操作 2 需要把结果返回给客户端，它就是关键路径上的操作，所以主线程必须立即把这个操作执行完\n\n  \n\n  \n\n- 对于 Redis 来说，**读操作是典型的关键路径操作**，因为客户端发送了读操作之后，就会等待读取的数据返回，以便进行后续的数据处理\n    \n\n  \n\n- 删除操作并不需要给客户端返回具体的数据结果，所以不算是关键路径操作。而我们刚才总结的**第二个阻塞点“bigkey 删除”，和第三个阻塞点“清空数据库”，都是对数据做删除，并不在关键路径上**。因此，我们可以使用后台子线程来异步执行删除操作。\n    \n- 对于第四个阻塞点“AOF 日志同步写”来说，为了保证数据可靠性，Redis 实例需要保证 AOF 日志中的操作记录已经落盘，这个操作虽然需要实例等待，**但它并不会返回具体的数据结果给实例**。所以，我们也**可以启动一个子线程来执行 AOF 日志的同步写**，而不用让主线程等待 AOF 日志的写完成\n    \n\n  \n\n- 从库加载 RDB 文件”这个阻塞点。从库要想对客户端提供数据存取服务，就必须把 RDB 文件加载完成。所以，这个操作也属于关键路径上的操作，我们必须让从库的主线程来执行。\n    \n\n  \n\n对于 Redis 的五大阻塞点来说，**除了“集合全量查询和聚合操作”和“从库加载 RDB 文件”，其他三个阻塞点涉及的操作都不在关键路径上，所以，我们可以使用 Redis 的异步子线程机制来实现 bigkey 删除，清空数据库，以及 AOF 日志同步写。**\n\n  \n\n  \n\n  \n\n  \n\n# 异步的子线程机制\n\n  \n\nRedis 主线程启动后，**会使用操作系统提供的 pthread_create 函数创建 3 个子线程**，分别由它们负责 A**OF 日志写操作、键值对删除以及文件关闭的异步**执行。\n\n  \n\n- 主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把**这个操作封装成一个任务，放入到任务队列中**，然后给客户端返回一个完成信息，表明删除已经完成。\n    \n- 这个时候删除还没有执行，等到后台**子线程从任务队列中读取任务后，才开始实际删除键值对，并释放相应的内存空间。**我们把这种异步删除也称为**惰性删除**（lazy free）\n    \n- 和惰性删除类似，当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这样主线程就不用一直等待 AOF 日志写完了。\n    \n\n  \n\n  \n\n![](statistic/asynccode-474.png)","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E6%97%81%E8%B7%AF%E7%BC%93%E5%AD%98Redis%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84":{"title":"旁路缓存：Redis是如何工作的？","content":"Redis 提供了高性能的数据存取功能，所以广泛应用在缓存场景中，既能有效地提升业务应用的响应速度，还可以避免把高并发大压力的请求发送到数据库层。\n\n们需要系统地掌握缓存的一系列内容：\n\n- 工作原理\n    \n- 替换策略\n    \n- 异常处理\n    \n- 扩展机制\n    \n\n具体来说，我们需要解决四个关键问题：\n\n- Redis 缓存具体是怎么工作的？\n    \n- Redis 缓存如果满了，该怎么办？\n    \n- 为什么会有缓存一致性、缓存穿透、缓存雪崩、缓存击穿等异常，该如何应对？\n    \n- Redis 的内存毕竟有限，如果用快速的固态硬盘来保存数据，可以增加缓存的数据量，那么，Redis 缓存可以使用快速固态硬盘吗？\n    \n\n  \n\n# 缓存的特征\n\n首先，你要知道，一个系统中的不同层之间的访问速度不一样，所以我们才需要缓存，这样就可以把一些需要频繁访问的数据放在缓存中，以加快它们的访问速度。\n\n- 缓存的第一个特征：在一个层次化的系统中，缓存一定是一个快速子系统，数据存在缓存中时，能避免每次从慢速子系统中存取数据。对应到互联网应用来说，Redis 就是快速子系统，而数据库就是慢速子系统了。\n    \n- 这其实包含了缓存的第二个特征：缓存系统的容量大小总是小于后端慢速系统的，我们不可能把所有数据都放在缓存系统中\n    \n\n  \n\n缓存和后端慢速系统之间，必然存在数据写回和再读取的交互过程。简单来说，缓存中的数据需要按一定规则淘汰出去，写回后端系统，而新的数据又要从后端系统中读取进来，写入缓存\n\n  \n\n# Redis 缓存处理请求的两种情况\n\n把 Redis 用作缓存时，我们会把 Redis 部署在数据库的前端，业务应用在访问数据时，会先查询 Redis 中是否保存了相应的数据。此时，根据数据是否存在缓存中，会有两种情况。\n\n- 缓存命中：Redis 中有相应数据，就直接读取 Redis，性能非常快。\n    \n- 缓存缺失：**Redis 中没有保存相应数据，就从后端数据库中读取数据，性能就会变慢。**而且，一**旦发生缓存缺失，为了让后续请求能从缓存中读取到数据，我们需要把缺失的数据写入 Redis**，这个过程叫作缓存更新。缓**存更新操作会涉及到保证缓存和数据库之间的数据一致性问题。**\n    \n\n![](statistic/asynccode-459.png)\n\n  \n\n用 Redis 缓存时，我们基本有三个操作\n\n- 应用读取数据时，需要先读取 Redis；\n    \n- 发生缓存缺失时，需要从数据库读取数据；\n    \n- 发生缓存缺失时，还需要更新缓存。\n    \n\n  \n\n  \n\n# Redis 作为旁路缓存的使用操作\n\n  \n\n我们也把 Redis 称为旁路缓存，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。\n\n使用 Redis 缓存时，具体来说，我们需要在应用程序中增加三方面的代码\n\n  \n\n那么，使用 Redis 缓存时，具体来说，我们需要在应用程序中增加三方面的代码：\n\n- 当应用程序需要读取数据时，我们需要在代码中显式调用 Redis 的 GET 操作接口，进行查询；\n    \n- 如果缓存缺失了，应用程序需要再和数据库连接，从数据库中读取数据；\n    \n- 当缓存中的数据需要更新时，我们也需要在应用程序中显式地调用 SET 操作接口，把更新的数据写入缓存。\n    \n\nString cacheKey = “productid_11010003”; String cacheValue = redisCache.get(cacheKey)； //缓存命中 if ( cacheValue != NULL) return cacheValue; //缓存缺失 else cacheValue = getProductFromDB(); redisCache.put(cacheValue) //缓存更新\n\n  \n\n# 缓存的类型\n\n按照 Redis 缓存是否接受写请求，我们可以把它分成只读缓存和读写缓存。先来了解下只读缓存。\n\n## 只读缓存\n\n  \n\n当 Redis 用作只读缓存时，\n\n- 读取数据的话，**会先调用 Redis GET 接口，查询数据是否存在**。\n    \n- **而所有的数据写请求，会直接发往后端的数据库，在数据库中增删改。**\n    \n- **对于删改的数据来说，如果 Redis 已经缓存了相应的数据，应用需要把这些缓存的数据删除，Redis 中就没有这些数据了**。\n    \n\n  \n\n  \n\n假设业务应用要修改数据 A，此时，数据 A 在 Redis 中也缓存了，那么，应用会先直接在数据库里修改 A，并把 Redis 中的 A 删除。等到应用需要读取数据 A 时，会发生缓存缺失，此时，应用从数据库中读取 A，并写入 Redis，以便后续请求从缓存中直接读取，如下图所示\n\n  \n\n![](statistic/asynccode-458.png)\n\n只读缓存直接在数据库中更新数据的好处是，所有最新的数据都在数据库中，而数据库是提供数据可靠性保障的，这些数据不会有丢失的风险。\n\n  \n\n## 读写缓存\n\n对于读写缓存来说，**除了读请求会发送到缓存进行处理（直接在缓存中查询数据是否存在)**，**所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。**\n\n  \n\n和只读缓存不一样的是，**在使用读写缓存时，最新的数据是在 Redis 中**，而 Redis 是内存数据库，一旦出现掉电或宕机，内存中的数据就会丢失。这也就是说，应用的最新数据可能会丢失，给应用业务带来风险。\n\n  \n\n所以，根据业务应用对数据可靠性和缓存性能的不同要求，我们会有**同步直写和异步写回两种策略**\n\n  \n\n### 同步直写\n\n  \n\n同步直写是指，**写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据**，才给客户端返回。\n\n- 这样，即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证。\n    \n- 不过，同步直写会降低缓存的访问性能。这是因为缓存中处理写请求的速度是很快的，而数据库处理写请求的速度较慢。即使缓存很快地处理了写请求，也需要等待数据库处理完所有的写请求，才能给应用返回结果，这就增加了缓存的响应延迟。\n    \n\n### 异步写回\n\n异步写回策略，则是优先考虑了响应延迟。此时，所有写请求都先在缓存中处理。**等到这些增改的数据要被从缓存中淘汰出来时，缓存将它们写回后端数据库**。\n\n- 处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。\n    \n\n![](statistic/asynccode-458.png)\n\n  \n\n# 总结\n\nRedis 做缓存时，还有两种模式，分别是只读缓存和读写缓存。其中，读写缓存还提供了同步直写和异步写回这两种模式，**同步直写模式侧重于保证数据可靠性**，而异步写回模式则侧重于提供低延迟访问，我们要根据实际的业务场景需求来进行选择\n\n选择只读缓存，还是读写缓存，主要看我们对写请求是否有加速的需求\n\n  \n\n- 如果需要对写请求进行加速，我们选择读写缓存；\n    \n- 如果写请求很少，或者是只需要提升读请求的响应速度的话，我们选择只读缓存。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E6%97%A0%E9%94%81%E7%9A%84%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9CRedis%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E5%B9%B6%E5%8F%91%E8%AE%BF%E9%97%AE":{"title":"无锁的原子操作：Redis如何应对并发访问","content":"为了保证并发访问的正确性，Redis 提供了两种方法，分别是加锁和原子操作。\n\n  \n\n- 加锁是一种常用的方法，在读取数据前，客户端需要先获得锁，否则就无法进行操作。当一个客户端获得锁后，就会一直持有这把锁，直到客户端完成数据更新，才释放这把锁。\n    \n    - 一个是，如果加锁操作多，会降低系统的并发访问性能；\n        \n    - 第二个是，Redis 客户端要加锁时，需要用到分布式锁，而分布式锁实现复杂，需要用额外的存储系统来提供加解锁操作\n        \n- 原子操作是另一种提供并发访问控制的方法\n    \n    - 原子操作是指执行过程保持原子性的操作，而且原子操作执行时并不需要再加锁，实现了无锁操作\n        \n\n  \n\n  \n\n  \n\n# 并发访问中需要对啥进行控制？\n\n我们说的并发访问控制，是指对多个客户端访问**操作同一份数据的过程进行控制**，以保证任何一个客户端发送的操作在 Redis 实例上执行时具有互斥性？**客户端 A 的访问操作在执行时，客户端 B 的操作不能执行，需要等到 A 的操作结束后，才能执行。**\n\n  \n\n  \n\n并发访问控制对应的操作主要是数据修改操作。当客户端需要修改数据时，基本流程分成两步：\n\n- 客户端先把数据读取到本地，在本地进行修改；\n    \n- 客户端修改完数据后，再写回 Redis。\n    \n\n我们把这个流程叫做“读取 - 修改 - 写回”操作（Read-Modify-Write，简称为 RMW 操作）\n\n当有多个客户端对同一份数据执行 RMW 操作的话，**我们就需要让 RMW 操作涉及的代码以原子性方式执行**。访问同一份数据的 RMW 操作代码，就叫做临界区代码\n\n  \n\n**如果我们对临界区代码的执行没有控制机制，就会出现数据更新错误**\n\n  \n\n![](statistic/asynccode-451.png)\n\n  \n\n# Redis 的两张原子操作方法\n\n  \n\n为了实现并发控制要求的临界区代码互斥执行，Redis 的原子操作采用了两种方法：\n\n1. 把多个操作在 Redis 中实现成一个操作，也就是单命令操作；\n    \n2. 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。\n    \n\n  \n\n  \n\nRedis 是使用单线程来串行处理客户端的请求操作命令的，所以，当 Redis 执行某个命令操作时，其他命令是无法执行的，这相当于命令操作是互斥执行的\n\n  \n\n  \n\n# 总结\n\nRedis 提供了两种原子操作的方法来实现并发控制，分别是单命令操作和 Lua 脚本。因为原子操作本身不会对太多的资源限制访问，可以维持较高的系统并发性能。\n\n  \n\n而 Redis 的 Lua 脚本可以包含多个操作，这些操作都会以原子性的方式执行，绕开了单命令操作的限制。**不过，如果把很多操作都放在 Lua 脚本中原子执行，会导致 Redis 执行脚本的时间增加，**同样也会降低 Redis 的并发性能。\n\n**在编写 Lua 脚本时，你要避免把不需要做并发控制的操作写入脚本中。**","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E6%9B%BF%E6%8D%A2%E7%AD%96%E7%95%A5%E7%BC%93%E5%AD%98%E6%BB%A1%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E":{"title":"替换策略：缓存满了怎么办","content":"MySQL 中有 1TB 的数据，如果我们使用 Redis 把这 1TB 的数据都缓存起来，虽然应用都能在内存中访问数据了，但是，这样配置并不合理，因为性价比很低\n\n解决这个问题就涉及到缓存系统的一个重要机制，即**缓存数据的淘汰机制。**简单来说，数据淘汰机制包括两步：\n\n- 第一，根据一定的策略，筛选出对应用访问来说“不重要”的数据；\n    \n- 第二，将这些数据从缓存中删除，为新来的数据腾出空间，\n    \n\n  \n\n# 设置多大的缓存合适\n\n按照“八二原理”来设置缓存空间容量，也就是把缓存空间容量设置为总数据量的 20% 的话，就有可能拦截到 80% 的访问。\n\n- 在实践过程中，我看到过的**缓存容量占总数据量的比例，从 5% 到 40% 的都有**。这个容量规划不能一概而论，是需要结合应用数据实际访问特征和成本开销来综合考虑的。\n    \n- 我**会建议把缓存容量设置为总数据量的 15% 到 30%**，兼顾访问性能和内存空间开销。\n    \n\n  \n\n不过，**缓存被写满是不可避免的**。即\n\n  \n\n# Redis 缓存有哪些淘汰策略\n\n  \n\nRedis 4.0 之前一共实现了 6 种内存淘汰策略，在 4.0 之后，又增加了 2 种策略\n\n  \n\n我们可以按照是否会进行数据淘汰把它们分成两类：\n\n- 不进行数据淘汰的策略，只有 noeviction 这一种。\n    \n- 会进行淘汰的 7 种其他策略。\n    \n\n会进行淘汰的 7 种策略，我们可以再进一步根据淘汰候选数据集的范围把它们分成两类：\n\n- 在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis 4.0 后新增）四种。\n    \n- 在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种\n    \n\n  \n\n![](statistic/asynccode-16.jpg)\n\n  \n\n  \n\n  \n\n  \n\n  \n\n## 不进行数据淘汰的策略-- noebiction\n\n默认情况下，Redis 在使用的内存空间超过 maxmemory 值时，并不会淘汰数据，也就是设定的 noeviction 策略。对应到 Redis 缓存，也就是指，**一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误**。\n\n  \n\n  \n\n  \n\n## 在设置了过期时间的数据中进行淘汰\n\nvolatile-random、volatile-ttl、volatile-lru 和 volatile-lfu 它们筛选的候选数据范围，被限制在已经设置了过期时间的键值对上。也正因为此，即使缓存没有写满，这些数据如果过期了，也会被删除\n\n  \n\n我们使用 EXPIRE 命令对一批键值对设置了过期时间后，无论是这些键值对的过期时间是快到了，还是 Redis 的内存使用量达到了 maxmemory 阈值，具体筛选规则进行淘汰。\n\n  \n\n  \n\n- volatile-ttl 在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。\n    \n- volatile-random 就像它的名称一样，在设置了过期时间的键值对中，进行随机删除。\n    \n- volatile-lru 会使用 LRU 算法筛选设置了过期时间的键值对。\n    \n- volatile-lfu 会使用 LFU 算法选择设置了过期时间的键值对。\n    \n\n  \n\nLRU 算法的全称是 Least Recently Used，这是**按照最近最少使用的原则来筛选数据，最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中**\n\n- LRU 会把所有的数据组织成一个链表，链表的头和尾分别表示 MRU 端和 LRU 端，分别代表最近最常使用的数据和最近最不常用的数据。\n    \n\n![](statistic/asynccode-456.png)\n\n- 我们现在有数据 6、3、9、20、5。如果数据 20 和 3 被先后访问，它们都会从现有的链表位置移到 MRU 端，而链表中在它们之前的数据则相应地往后移一位。因为，LRU 算法选择删除数据时，都是从 LRU 端开始，所以把刚刚被访问的数据移到 MRU 端，就可以让它们尽可能地留在缓存中。\n    \n- Redis 默认会**记录每个数据的最近一次访问的时间戳**（由键值对数据结构 **RedisObject 中的 lru 字段记录**）。然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去\n    \n- 当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这儿的挑选标准是：**能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值**\n    \n\n  \n\nLFU算法的全称是Least FrequentlyUsed，LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。\n\n  \n\n  \n\n  \n\n## 所有数据范围内进行淘汰\n\nallkeys-lru、allkeys-random、allkeys-lfu 这三种淘汰策略的备选淘汰数据范围，就扩大到了所有键值对，无论这些键值对是否设置了过期时间。它们筛选数据进行淘汰的规则是：\n\n  \n\n  \n\n- allkeys-random 策略，从所有键值对中随机选择并删除数据；\n    \n- allkeys-lru 策略，使用 LRU 算法在所有数据中进行筛选。\n    \n- allkeys-lfu 策略，使用 LFU 算法在所有数据中进行筛选。\n    \n\n、\n\n  \n\n  \n\n使用缓存策略的建议\n\n- 优先使用 allkeys-lru 策略。\n    \n- 如果业务应用中的数据访问频率相差不大，没有明显的冷热数据区分，建议使用 allkeys-random 策略，随机选择淘汰的数据就行。\n    \n- 如果你的业务中有置顶的需求，比如置顶新闻、置顶视频，那么，可以使用 volatile-lru 策略，同时不给这些置顶数据设置过期时间。这样一来，这些需要置顶的数据一直不会被删除，而其他数据会在过期时根据 LRU 规则进行筛选\n    \n\n  \n\n# 如何处理淘汰的数据\n\n  \n\n  \n\n一般来说，一旦被淘汰的数据选定后，如果这个数据是干净数据，那么我们就直接删除；如果这个数据是脏数据，我们需要把它写回数据库\n\n![](statistic/asynccode-457.png)\n\n  \n\n- 干净数据和脏数据的区别就在于，和最初从后端数据库里读取时的值相比，有没有被修改过。干净数据一直没有被修改，所以后端数据库里的数据也是最新值。在替换时，它可以被直接删除\n    \n- 而脏数据就是曾经被修改过的，已经和后端数据库中保存的数据不一致了。此时，如果不把脏数据写回到数据库中，这个数据的最新值就丢失了，就会影响应用的正常使用。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E6%B3%A2%E5%8A%A8%E7%9A%84%E5%93%8D%E5%BA%94%E5%BB%B6%E8%BF%9F%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E5%8F%98%E6%85%A2%E7%9A%84Redis":{"title":"波动的响应延迟：如何应对变慢的Redis？","content":"  \n\n  \n\n# Redis真的变慢了吗？\n\n- 一个最直接的方法，就是**查看 Redis 的响应延迟**。\n    \n    - 大部分时候，Redis 延迟很低，但是在某些时刻，有些 Redis 实例会出现很高的响应延迟，甚至能达到几秒到十几秒，不过持续时间不长，这也叫**延迟“毛刺”。**当你发现 Redis 命令的执行时间突然就增长到了几秒，基本就可以认定 Redis 变慢了\n        \n- 第二个方法了，也就是基于当前环境下的 Redis 基线性能做判断\n    \n    - 所谓的基线性能呢，也就是一个系统在低压力、无干扰下的基本性能，这个性能只由当前的软硬件配置决定。\n        \n    - redis-cli 命令提供了–intrinsic-latency 选项，可以用来监测和统计测试期间内的最大延迟，这个延迟可以作为 Redis 的基线性能。\n        \n    - 如果你观察到的 Redis 运行时延迟是其基线性能的 2 倍及以上，就可以认定 Redis 变慢了\n        \n- 如果你想了解网络对 Redis 性能的影响，一个简单的方法是用 iPerf 这样的工具，测量从 Redis 客户端到服务器端的网络延迟。如果这个延迟有几十毫秒甚至是几百毫秒，就说明，Redis 运行的网络环境中很可能有大流量的其他应用程序在运行，导致网络拥塞了。这个时候，你就需要协调网络运维，调整网络的流量分配了.\n    \n\n  \n\n# 如何应对Redis变慢\n\n![](statistic/asynccode-464.png)\n\n如Redis 架构图。你可以重点关注下我在图上新增的红色模块，它们是影响 Redis 性能的三大要素：\n\n- Redis 自身的操作特性\n    \n- 文件系统\n    \n- 操作系统\n    \n\n  \n\n  \n\n  \n\n1. Redis 自身的操作特性会对Redis的效率产生影响\n    \n2. Redis 会持久化保存数据到磁盘，这个过程要依赖文件系统来完成，所以，**文件系统将数据写回磁盘的机制，会直接影响到 Redis 持久化的效率**。而且，在持久化的过程中，Redis 也还在接收其他请求，持久化的效率高低又会影响到 Redis 处理请求的性能。\n    \n3. 另一方面，Redis 是内存数据库，内存操作非常频繁，所以**，操作系统的内存机制会直接影响到 Redis 的处理效率**。比如说，**如果 Redis 的内存不够用了，操作系统会启动 swap 机制，这就会直接拖慢 Redis**\n    \n\n  \n\n# Redis 自身操作特性的影响\n\n  \n\nRedis 提供的键值对命令操作对延迟性能的影响\n\n## 慢查询命令\n\n查询命令，就是指在 Redis 中执行速度慢的命令，这会导致 Redis 延迟增加**。**Redis 提供的命令操作很多，并不是所有命令都慢，这和命令操作的复杂度有关。\n\n  \n\n当你发现 Redis 性能变慢时，可以通过 Redis 日志，或者是 latency monitor 工具，查询变慢的请求，根据请求对应的具体命令以及官方文档，确认下是否采用了复杂度高的慢查询命令\n\n  \n\n如果的确有大量的慢查询命令，有两种处理方式：\n\n1. 用其他高效命令代替\n    \n2. 当**你需要执行排序、交集、并集操作时，可以在客户端完成**，而不要用 SORT、SUNION、SINTER 这些命令，以免拖慢 Redis 实例。\n    \n    1. 如果业务逻辑就是要求使用慢查询命令，那你得考虑采用性能更好的 CPU，更快地完成查询命令，避免慢查询的影响\n        \n    2. 还有一个比较容易忽略的慢查询命令，就是 KEYS。它用于返回和输入模式匹配的所有 key。因为 KEYS 命令需要遍历存储的键值对，所以操作延时高。KEYS 命令一般不被建议用于生产环境中\n        \n\n  \n\n## 过期key操作\n\n  \n\n过期 key 的自动删除机制。**它是 Redis 用来回收内存空间的常用机制，应用广泛，本身就会引起 Redis 操作阻塞，导致性能变慢，所以，你必须要知道该机制对性能的影响。**\n\n  \n\nRedis 键值对的 key 可以设置过期时间。默认情况下，Redis 每 100 毫秒会删除一些过期 key，具体的算法如下：\n\n- 采样 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 个数的 key，并将其中过期的 key 全部删除；（ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 是 Redis 的一个参数，默认是 20）\n    \n- 如果超过 25% 的 key 过期了，则重复删除的过程，直到过期 key 的比例降至 25% 以下。\n    \n\n  \n\n上述算法：一秒内基本有 200 个过期 key 会被删除。**如果触发了上面这个算法的第二条，Redis 就会一直删除以释放内存空间。**注意，删除操作是阻塞的（**Redis 4.0 后可以用异步线程机制来减少阻塞影响**）。所以，一旦该条件触发，Redis 的线程就会一直执行删除，这样一来，就没办法正常服务其他的键值操作了，就会进一步引起其他键值操作的延迟增加，Redis 就会变慢。\n\n  \n\n  \n\n上述算法的第二天是如何触发的？其中一个重要来源**，就是频繁使用带有相同时间参数的 EXPIREAT 命令设置过期 key**，这就会导致，在同一秒内有大量的 key 同时过期\n\n  \n\n# 文件系统的影响\n\n## AOF 模式的影响\n\n  \n\nRedis 会采用 AOF 日志或 RDB 快照。其中，AOF 日志提供了三种日志写回策略：no、everysec、always。**这三种写回策略依赖文件系统的两个系统调用完成，也就是 write 和 fsync**\n\n- write 只要把日志记录写到内核缓冲区，就可以返回了，并不需要等待日志实际写回到磁盘\n    \n- 而 fsync 需要把日志记录写回到磁盘后才能返回，时间较长\n    \n\n  \n\n![](statistic/asynccode-465.png)\n\n  \n\n- 当写回策略配置为 everysec 和 always 时，Redis 需要调用 fsync 把日志写回磁盘。但是，这两种写回策略的具体执行情况还不太一样。\n    \n    - 在使用 everysec 时，Redis 允许丢失一秒的操作记录，所以，Redis 主线程并不需要确保每个操作记录日志都写回磁盘。而且，fsync 的执行时间很长，如果是在 Redis 主线程中执行 fsync，就容易阻塞主线程。所以，当写回策略配置为 everysec 时，Redis 会使用后台的子线程异步完成 fsync 的操作。\n        \n    - 而对于 always 策略来说，Redis 需要确保每个操作记录日志都写回磁盘，如果用后台子线程异步完成，主线程就无法及时地知道每个操作是否已经完成了，这就不符合 always 策略的要求了。所以，always 策略并不使用后台子线程来执行\n        \n\n  \n\n  \n\n  \n\n另外，在使用 AOF 日志时，为了避免日志文件不断增大，Redis 会执行 AOF 重写，生成体量缩小的新的 AOF 日志文件。AOF 重写本身需要的时间很长，也容易阻塞 Redis 主线程，所以，Redis 使用子进程来进行 AOF 重写\n\n  \n\nAOF 重写会对磁盘进行大量 IO 操作，同时，fsync 又需要等到数据写到磁盘后才能返回，所以，当 AOF 重写的压力比较大时，就会导致 fsync 被阻塞。虽然 fsync 是由后台子线程负责执行的，但是，主线程会监控 fsync 的执行进度。\n\n  \n\n当主线程使用后台子线程执行了一次 fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的 fsync 还没有执行完，那么它就会阻塞。所以，如果后台子线程执行的 fsync 频繁阻塞的话（比如 AOF 重写占用了大量的磁盘 IO 带宽），主线程也会阻塞，导致 Redis 性能变慢。\n\n  \n\n![](statistic/asynccode-467.png)\n\n  \n\n  \n\n你可以检查下 Redis 配置文件中的 appendfsync 配置项，该配置项的取值表明了 Redis 实例使用的是哪种 AOF 日志写回策略\n\n  \n\n![](statistic/asynccode-466.png)\n\n  \n\n- 如果 AOF 写回策略使用了 everysec 或 always 配置，请先确认下业务方对数据可靠性的要求，明确是否需要每一秒或每一个操作都记日志\n    \n- 如果业务应用对延迟非常敏感，但同时允许一定量的数据丢失，那么，可以把配置项 no-appendfsync-on-rewrite 设置为 yes\n    \n- 如果的确需要高性能，同时也需要高可靠数据保证，我建议你考虑采用高速的固态硬盘作为 AOF 日志的写入设备。\n    \n\n  \n\n# 操作系统的影响\n\n## swap 的影响\n\n  \n\n  \n\n如果 Redis 的 AOF 日志配置只是 no，或者就没有采用 AOF 模式，那么，还会有什么问题导致性能变慢吗？ ----一个潜在的瓶颈：操作系统的内存 swap。\n\n内存 swap 是操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，涉及到磁盘的读写，所以，一旦触发 swap，无论是被换入数据的进程，还是被换出数据的进程，其性能都会受到慢速磁盘读写的影响。\n\n  \n\n正常情况下，Redis 的操作是直接通过访问内存就能完成，一旦 swap 被触发了，Redis 的请求操作需要等到磁盘数据读写完成才行。而且，和我刚才说的 AOF 日志文件读写使用 fsync 线程不同，swap 触发后影响的是 Redis 主 IO 线程，这会极大地增加 Redis 的响应时间。\n\n  \n\n通常，触发 swap 的原因主要是物理机器内存不足：\n\n1. Redis 实例自身使用了大量的内存，导致物理机器的可用内存不足；\n    \n2. 和 Redis 实例在同一台机器上运行的其他进程，在进行大量的文件读写操作。文件读写本身会占用系统内存，这会导致分配给 Redis 实例的内存量变少，进而触发 Redis 发生 swap。\n    \n\n  \n\n我也给你提供一个解决思路：**增加机器的内存或者使用 Redis 集群**\n\n  \n\n  \n\n## 内存大页的影响\n\n  \n\n了内存 swap，还有一个和内存相关的因素，即内存大页机制（Transparent Huge Page, THP），也会影响 Redis 性能。\n\n  \n\n  \n\n**如果采用了内存大页，那么，即使客户端请求只修改 100B 的数据，Redis 也需要拷贝 2MB 的大页。当客户端请求修改或新写入数据较多时，内存大页机制将导致大量的拷贝，这就会影响 Redis 正常的访存操作，最终导致性能变慢。**\n\n  \n\n# 总结\n\n应对Redis变慢的方法\n\n1. 获取 Redis 实例在当前环境下的基线性能。\n    \n2. 是否用了慢查询命令？如果是的话，就使用其他命令替代慢查询命令，或者把聚合计算命令放在客户端做。\n    \n3. 是否对过期 key 设置了相同的过期时间？对于批量删除的 key，可以在每个 key 的过期时间上加一个随机数，避免同时删除。\n    \n4. 是否存在 bigkey？ 对于 bigkey 的删除操作，如果你的 Redis 是 4.0 及以上的版本，可以直接利用异步线程机制减少主线程阻塞；如果是 Redis 4.0 以前的版本，可以使用 SCAN 命令迭代删除；对于 bigkey 的集合查询和聚合操作，可以使用 SCAN 命令在客户端完成。\n    \n5. Redis AOF 配置级别是什么？业务层面是否的确需要这一可靠性级别？如果我们需要高性能，同时也允许数据丢失，可以将配置项 no-appendfsync-on-rewrite 设置为 yes，避免 AOF 重写和 fsync 竞争磁盘 IO 资源，导致 Redis 延迟增加。当然， 如果既需要高性能又需要高可靠性，最好使用高速固态盘作为 AOF 日志的写入盘。\n    \n6. Redis 实例的内存使用是否过大？发生 swap 了吗？如果是的话，就增加机器内存，或者是使用 Redis 集群，分摊单机 Redis 的键值对数量和内存压力。同时，要避免出现 Redis 和其他内存需求大的应用共享机器的情况。\n    \n7. 在 Redis 实例的运行环境中，是否启用了透明大页机制？如果是的话，直接关闭内存大页机制就行了。\n    \n8. 是否运行了 Redis 主从集群？如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞。\n    \n9. 是否使用了多核 CPU 或 NUMA 架构的机器运行 Redis 实例？使用多核 CPU 时，可以给 Redis 实例绑定物理核；使用 NUMA 架构时，注意把 Redis 实例和网络中断处理程序运行在同一个 CPU Socket 上。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97Redis%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88":{"title":"消息队列，Redis的解决方案","content":"Redis 适合做消息队列？这个问题的背后，隐含着两方面的核心问题\n\n- 消息队列的消息存取需求是什么？\n    \n- Redis 如何实现消息队列的需求？\n    \n\n# 消息队列的消息存取需求\n\n## 消息队列存取消息的过程\n\n在分布式系统中，当两个组件要基于消息队列进行通信时，\n\n- 一个组件会把要处理的数据以消息的形式传递给消息队列，然后，这个组件就可以继续执行其他操作了；\n    \n- 远端的另一个组件从消息队列中把消息读取出来，再在本地进行处理。\n    \n\n![](statistic/asynccode-477.png)\n\n  \n\n一般把消息队列中发送消息的组件称为生产者（例子中的组件 1），把接收消息的组件称为消费者（例子中的组件 2），下图展示了一个通用的消息队列的架构模型\n\n![](statistic/asynccode-478.png)\n\n  \n\n不过，消息队列在存取消息时，必须要满足三个需求，分别是**消息保序、处理重复的消息和保证消息可靠性**\n\n  \n\n## 消息保序\n\n虽然消费者是异步处理消息，但是，消费者仍然需要按照生产者发送消息的顺序来处理消息，避免后发送的消息被先处理了。对于要求消息保序的场景来说，一旦出现这种消息被乱序处理的情况，就可能会导致业务逻辑被错误执行，从而给业务方造成损失\n\n## 重复消息处理\n\n消费者从消息队列读取消息时，有时会因为网络堵塞而出现消息重传的情况。此时，消费者可能会收到多条重复的消息。对于重复的消息，消费者如果多次处理的话，就可能造成一个业务逻辑被多次执行，如果业务逻辑正好是要修改数据，那就会出现数据被多次修改的问题了。\n\n## 消息可靠性保证\n\n还可能出现因为**故障或宕机导致消息没有处理完成的情况**。此时，**消息队列需要能提供消息可靠性的保证，也就是说，当消费者重启后，可以重新读取消息再次进行处理**，否则，就会出现消息漏处理的问题了。\n\n  \n\n# Redis消息队列解决方案\n\n  \n\n**Redis 的 List 和 Streams 两种数据类型，就可以满足消息队列的这三个需求。**\n\n## 基于 List 的消息队列解决方案\n\n  \n\nlist 本身就是按先进先出的顺序对数据进行存取的.\n\n- 生产者可以使用 LPUSH 命令把要发送的消息依次写入 List，\n    \n- 而消费者则可以使用 RPOP 命令，从 List 的另一端按照消息的写入顺序，依次读取消息并进行处理\n    \n\n  \n\n![](statistic/asynccode-480.png)\n\n有一个潜在的风险点：\n\n**在生产者往 List 中写入数据时，List 并不会主动地通知消费者有新消息写入**，如果消费者想要及时处理消息，**就需要在程序中不停地调用 RPOP 命令（比如使用一个 while(1) 循环）**。如果有新消息写入，RPOP 命令就会返回结果，否则，RPOP 命令返回空值，再继续循环。\n\n  \n\n- 为了解决这个问题，Redis 提供了 BRPOP 命令 ，BRPOP 命令也称为阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据\n    \n\n  \n\n我们还需要考虑解决重复消息处理的问题，这里其实有一个要求：**消费者程序本身能对重复消息进行判断**\n\n- 一方面，消息队列要能给每一个消息提供全局唯一的 ID 号；\n    \n- 另一方面，消费者程序要把已经处理过的消息的 ID 号记录下来。\n    \n\n当收到一条消息后，消费者程序就可以对比收到的消息 ID 和记录的已处理过的消息 ID，来判断当前收到的消息有没有经过处理\n\n不过，List 本身是不会为每个消息生成 ID 号的，所以，消息的全局唯一 ID 号就需要生产者程序在发送消息前自行生成。生成之后，我们在用 LPUSH 命令把消息插入 List 时，需要在消息中包含这个全局唯一 ID。\n\n  \n\n消费者程序从 List 中读取一条消息后，List 就不会再留存这条消息了。所以，如果消费者程序在处理消息的过程出现了故障或宕机，就会导致消息没有处理完成，那么，消费者程序再次启动后，就没法再次从 List 中读取消息了。\n\n为了留存消息，List 类型提供了 BRPOPLPUSH 命令，这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存。\n\n  \n\n  \n\n# 基于Stream 的消息队列解决方案\n\n  \n\n**Streams 是 Redis 专门为消息队列设计的数据类型**，它提供了丰富的消息队列操作命令\n\n- XADD：插入消息，保证有序，可以自动生成全局唯一 ID；\n    \n- XREAD：用于读取消息，可以按 ID 读取数据；、\n    \n- XREADGROUP：按消费组形式读取消息；\n    \n- XPENDING 和 XACK：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。\n    \n\nXADD 命令可以往消息队列中插入新消息，消息的格式是键 - 值对形式。就可以往名称为 mqstream 的消息队列中插入一条消息，消息的键是 repo，值是 5。其中，消息队列名称后面的*，表示让 Redis 为插入的数据自动生成一个全局唯一的 ID，例如“1599203861727-0”\n\n\n```\nXADD mqstream * repo 5​\n\n\"1599203861727-0\"\n```\n\nXREAD 在读取消息时，可以指定一个消息 ID，并从这个消息 ID 的下一条消息开始进行读取。\n\n我们可以执行下面的命令，从 ID 号为 1599203861727-0 的消息开始，消费者也可以在调用 XRAED 时设定 block 配置项，实现类似于 BRPOP 的阻塞读取操作。一旦设置了 block 配置项，XREAD 就会阻塞，阻塞的时长可以在 block 配置项进行设置 如 block 100\n\n\n```\nXREAD BLOCK 100 STREAMS  mqstream 1599203861727-0\n1) 1) \"mqstream\"\n   2) 1) 1) \"1599274912765-0\"\n         2) 1) \"repo\"\n            2) \"3\"\n      2) 1) \"1599274925823-0\"\n         2) 1) \"repo\"\n            2) \"2\"\n      3) 1) \"1599274927910-0\"\n         2) 1) \"repo\"\n            2) \"1\"\n```\n\n\n**Streams 本身可以使用 XGROUP 创建消费组，创建消费组之后**，Streams 可以使用 XREADGROUP 命令让消费组内的消费者读取消息，\n\n  \n\n创建一个名为 group1 的消费组，这个消费组消费的消息队列是 mqstream\n\nXGROUP create mqstream group1 0 OK\n\n然后，我们再执行一段命令，让 group1 消费组里的消费者 consumer1 从 mqstream 中读取所有消息，其中，命令最后的参数“\u003e”，表示从第一条尚未被消费的消息开始读取。因为在 consumer1 读取消息前，group1 中没有其他消费者读取过消息，所以，consumer1 就得到 mqstream 消息队列中的所有消息了（一共 4 条）\n\n```\nXREADGROUP group group1 consumer1 streams mqstream \u003e\n1) 1) \"mqstream\"\n   2) 1) 1) \"1599203861727-0\"\n         2) 1) \"repo\"\n            2) \"5\"\n      2) 1) \"1599274912765-0\"\n         2) 1) \"repo\"\n            2) \"3\"\n      3) 1) \"1599274925823-0\"\n         2) 1) \"repo\"\n            2) \"2\"\n      4) 1) \"1599274927910-0\"\n         2) 1) \"repo\"\n            2) \"1\"\n```\n\n\n**消息队列中的消息一旦被消费组里的一个消费者读取了，就不能再被该消费组内的其他消费者读取了。**\n\n  \n\n  \n\n![](statistic/asynccode-479.png)","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E7%BC%93%E5%86%B2%E5%8C%BA%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%83%BD%E5%BC%95%E5%8F%91%E6%83%A8%E6%A1%88%E7%9A%84%E5%9C%B0%E6%96%B9":{"title":"缓冲区：一个可能引发“惨案”的地方","content":"  \n\n缓冲区的功能其实很简单，主要就是用**一块内存空间来暂时存放命令数据，以免出现因为数据和命令的处理速度慢于发送速度而导致的数据丢失和性能问题，**\n\n但因为缓冲区的内存空间有限，如果往里面写入数据的速度持续地大于从里面读取数据的速度。当缓冲区占用的内存超出了设定的上限阈值时，就会出现**缓冲区溢出**。\n\n  \n\n  \n\n- 缓冲区在 Redis 中的一个主要应用场景，就是在客户端和服务器端之间进行通信时，\n    \n    - 用来暂存客户端发送的命令数据，\n        \n    - 暂存服务器端返回给客户端的数据结果。\n        \n- 缓冲区的另一个主要应用场景，是在主从节点间进行数据同步时，用来暂存主节点接收的写命令和数据。\n    \n\n# 客户端输入和输出缓冲区\n\n  \n\n为了避免客户端和服务器端的请求发送和处理速度不匹配，**服务器端给每个连接的客户端都设置了一个输入缓冲区和输出缓冲区，我们称之为客户端输入缓冲区和输出缓冲区，**输入缓冲区会先把客户端发送过来的命令暂存起来，Redis 主线程再从输入缓冲区中读取命令，进行处理。当 Redis 主线程处理完数据后，会把结果写入到输出缓冲区，再通过输出缓冲区返回给客户端，如下图所示\n\n![](statistic/asynccode-462.png)\n\n## 如何应对输入缓冲区溢出\n\n输入缓冲区就是用来暂存客户端发送的请求命令的：\n\n- 写入了 bigkey，比如一下子写入了多个百万级别的集合类型数据；\n    \n- 服务器端处理请求的速度过慢，例如，Redis 主线程出现了间歇性阻塞，无法及时处理正常发送的请求，导致客户端发送的请求在缓冲区越积越多。\n    \n\n我们可以从两个角度去考虑如何避免，\n\n- 一是把缓冲区调大，（**Redis 并没有提供参数让我们调节客户端输入缓冲区的大小**）\n    \n- 二是从数据命令的发送和处理速度入手\n    \n    - 避免客户端写入 bigkey，以及避免 Redis 主线程阻塞\n        \n\n  \n\n## 如何应对输出缓冲区溢出\n\nRedis 的输出缓冲区暂存的是 Redis 主线程要返回给客户端的数据。\n\nRedis 为每个客户端设置的输出缓冲区也包括两部分：\n\n- 一部分，是一个大小为 16KB 的固定缓冲空间，用来暂存 OK 响应和出错信息；\n    \n- 另一部分，是一个可以动态增加的缓冲空间，用来暂存大小可变的响应结果。\n    \n\n  \n\n那什么情况下会发生输出缓冲区溢出呢？\n\n- 服务器端返回 bigkey 的大量结果；\n    \n- 执行了 MONITOR 命令；（MONITOR 命令主要用在调试环境中，不要在线上生产环境中持续使用 MONITOR）\n    \n- 缓冲区大小设置得不合理。\n    \n\n  \n\n我们看下输出缓冲区大小设置的问题。和输入缓冲区不同，我们可以通过 client-output-buffer-limit 配置项，来设置缓冲区的大小。具体设置的内容包括两方面：\n\n- 设置缓冲区大小的上限阈值；\n    \n- 设置输出缓冲区持续写入数据的数量上限阈值，和持续写入数据的时间的上限阈值。\n    \n\n  \n\n总结下如何应对输出缓冲区溢出：\n\n- 避免 bigkey 操作返回大量数据结果；\n    \n- 避免在线上环境中持续使用 MONITOR 命令。\n    \n- 使用 client-output-buffer-limit 设置合理的缓冲区大小上限，或是缓冲区连续写入时间和写入量上限。\n    \n\n  \n\n# 主从集群中的缓冲区\n\n主从集群间的数据复制包括**全量复制和增量复制**两种。\n\n- **全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库**。无论在哪种形式的复制中，为了保证主从节点的数据一致，都会用到缓冲区。\n    \n- *这两种复制场景下的缓冲区，在溢出影响和大小设置方面并不一样\n    \n\n  \n\n## 复制缓冲区的溢出问题 ---全量复制过程中\n\n  \n\n在全量复制过程中，**主节点在向从节点传输 RDB 文件的同时**，会继续接收客户端发送的写命令请求。这些写命令就**会先保存在复制缓冲区中**，等 RDB 文件传输完成后，再发送给从节点去执行。**主节点上会为每个从节点都维护一个复制缓冲区，来保证主从节点间的数据同步。**\n\n![](statistic/asynccode-461.png)\n\n  \n\n所以，如果在全量复制时，从节点接收和加载 RDB 较慢，同时主节点接收到了大量的写命令，写命令在复制缓冲区中就会越积越多，最终导致溢出。\n\n其实，主节点上的复制缓冲区，本质上也是一个用于和从节点连接的客户端（我们称之为从节点客户端），使用的输出缓冲区。**复制缓冲区一旦发生溢出，主节点也会直接关闭和从节点进行复制操作的连接**，导致全量复制失败。\n\n那如何避免复制缓冲区发生溢出呢？\n\n- 一方面，我们可以控制主节点保存的数据量大小。按通常的使用经验，我们会把主节点的数据量控制在 2~4GB。\n    \n- 另一方面，我们可以使用 client-output-buffer-limit 配置项，来设置合理的复制缓冲区大小。设置的依据，就是主节点的数据量大小、主节点的写负载压力和主节点本身的内存大小。\n    \n\n  \n\n主节点上复制缓冲区的内存开销，会是每个从节点客户端输出缓冲区占用内存的总和。\n\n  \n\n## 复制积压缓冲区的溢出问题----增量复制过程中\n\n我们再来看下增量复制时使用的缓冲区，这个缓冲区称为复制积压缓冲区，**主节点在把接收到的写命令同步给从节点时，同时会把这些写命令写入复制积压缓冲区。一旦从节点发生网络闪断，再次和主节点恢复连接后，从节点就会从复制积压缓冲区中，读取断连期间主节点接收到的写命令，进而进行增量同步**\n\n![](statistic/asynccode-460.png)\n\n学过复制积压缓冲区了，只不过我当时告诉你的是它的英文名字 repl_backlog_buffer\n\n  \n\n从缓冲区溢出的角度再来回顾下两个重点：\n\n- 首先，复制积压缓冲区是一个大小有限的环形缓冲区。当主节点把复制积压缓冲区写满后，会覆盖缓冲区中的旧命令数据。如果从节点还没有同步这些旧命令数据，就会造成主从节点间重新开始执行全量复制\n    \n- 其次，为了应对复制积压缓冲区的溢出问题，我们可以调整复制积压缓冲区的大小，也就是设置 repl_backlog_size 这个参数的值。\n    \n\n  \n\n# 总结\n\n按照缓冲区的用途，例如是用于客户端通信还是用于主从节点复制，我把缓冲区分成了客户端的输入和输出缓冲区，以及主从集群中主节点上的复制缓冲区和复制积压缓冲区。\n\n  \n\n![](statistic/asynccode-17.jpg)\n\n从缓冲区溢出对 Redis 的影响的角度，我再把这四个缓冲区分成两类做个总结\n\n- 缓冲区溢出导致网络连接关闭：普通客户端、订阅客户端，以及从节点客户端，它们使用的缓冲区，本质上都是 Redis 客户端和服务器端之间，或是**主从节点之间为了传输命令数据而维护的**。这些缓冲区一旦发生溢出，处理机制都是直接把客户端和服务器端的连接，或是主从节点间的连接关闭。网络连接关闭造成的直接影响，就是业务程序无法读写 Redis，或者是主从节点全量同步失败，需要重新执行。\n    \n- 缓冲区溢出导致命令数据丢失：主节点上的复制积压缓冲区属于环形缓冲区，一旦发生溢出，新写入的命令数据就会覆盖旧的命令数据，导致旧命令数据的丢失，进而导致主从节点重新进行全量复制\n    \n\n  \n\n  \n\n本质上看，缓冲区溢出，无非就是三个原因：命令数据发送过快过大；命令数据处理较慢；缓冲区空间过小。明白了这个，我们就可以有针对性地拿出应对策略了。\n\n  \n\n- 针对命令数据发送过快过大的问题，对于普通客户端来说可以避免 bigkey，而对于复制缓冲区来说，就是避免过大的 RDB 文件。\n    \n- 针对命令数据处理较慢的问题，解决方案就是减少 Redis 主线程上的阻塞操作，例如使用异步的删除操作\n    \n- 针对缓冲区空间过小的问题，解决方案就是使用 client-output-buffer-limit 配置项设置合理的输出缓冲区、复制缓冲区和复制积压缓冲区大小。当然，**我们不要忘了，输入缓冲区的大小默认是固定的**，我们无法通过配置来修改它，除非直接去修改 Redis 源码。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E7%BC%93%E5%AD%98%E5%BC%82%E5%B8%B8%E7%BC%93%E5%AD%98%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98":{"title":"缓存异常：缓存和数据库的一致性问题","content":"我们经常会遇到一些异常问题，概括来说有 4 个方面：\n\n- 缓存中的数据和数据库中的不一致；\n    \n- 缓存雪崩；\n    \n- 缓存击穿\n    \n- 缓存穿透\n    \n\n# 缓存和数据库的数据不一致是如何发生的？\n\n  \n\n其实，这里的“一致性”包含了两种情况：\n\n- 缓存中有数据，那么，缓存的数据值需要和数据库中的值相同；\n    \n- 缓存中本身没有数据，那么，数据库中的值必须是最新值。\n    \n\n  \n\n不符合这两种情况的，就属于缓存和数据库的数据不一致问题了\n\n  \n\n我们可以把缓存分成读写缓存和只读缓存。\n\n## **读写缓存**\n\n对于**读写缓存来说**，如果要对数据进行增删改，就需要在缓存中进行，同时还要根据采取的写回策略，决定是否同步写回到数据库中：\n\n- 同步直写策略：写缓存时，也同步写数据库，缓存和数据库中的数据一致；\n    \n- 异步写回策略：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障，那么，此时，数据库就没有最新的数据了。\n    \n\n  \n\n对于读写缓存来说，要想保证缓存和数据库中的数据一致，就要采用同步直写策略：\n\n我们要在业务应用中**使用事务机制，来保证缓存和数据库的更新具有原子性，也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试**。**否则，我们就无法实现同步直写**\n\n  \n\n## 只读缓存\n\n对于只读缓存，如果有数据新增，会直接写入数据库；而有数据删改时，就需要把只读缓存中的数据标记为无效。\n\nTomcat 上运行的应用，无论是新增（Insert 操作）、修改（Update 操作）、还是删除（Delete 操作）数据 X，都会直接在数据库中增改删。当然，如果应用执行的是修改或删除操作，还会删除缓存的数据 X。\n\n这个过程中会不会出现数据不一致的情况呢？考虑到新增数据和删改数据的情况不一样，所以我们分开来看。\n\n  \n\n### 新增数据\n\n**如果是新增数据，数据会直接写到数据库中，不用对缓存做任何操作，此时，缓存中本身就没有新增数据，而数据库中是最新值，这种情况符合我们刚刚所说的一致性的第 2 种情况**，所以，此时，缓存和数据库的数据是一致的\n\n### 删改数据\n\n如果发生删改操作，应用既要更新数据库，也要在缓存中删除数据。这两个操作如果无法保证原子性，就会出现数据不一致问题了。\n\n  \n\n  \n\n在更新数据库和删除缓存值的过程中，无论这两个操作的执行顺序谁先谁后，只要有一个操作失败了，就会导致客户端读取到旧值。\n\n  \n\n  \n\n# 如何解决数据不一致\n\n重试机制\n\n具体来说，**可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中（例如使用 Kafka 消息队列）。当应用没有能够成功地删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或更新**。\n\n  \n\n**如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作**，此时，我们也可以保证数据库和缓存的数据一致了。否则的话，我们还需要再次进行重试**。如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了**\n\n  \n\n  \n\n实际上，即使**这两个操作第一次执行时都没有失败，当有大量并发请求时，应用还是有可能读到不一致的数据。**\n\n同样，我们按照不同的删除和更新顺序，分成两种情况来看\n\n## 情况一：先删除缓存，再更新数据库\n\n  \n\n假设线程 A 删除缓存值后，还没有来得及更新数据库（比如说有网络延迟），线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取。这会带来两个问题：\n\n1. 线程 B 读取到了旧值；\n    \n2. 线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值。\n    \n\n  \n\n解决方法：\n\n- 在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作。----**延迟双删**\n    \n    - 要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存，然后，线程 A 再进行删除。所以，线程 A sleep 的时间，就需要大于线程 B 读取数据再写入缓存的时间\n        \n\n  \n\n## 情况二：先更新数据库值，再删除缓存值。\n\n  \n\n如果线程 A 删除了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。\n\n  \n\n解决方案：\n\n在这种情况下，如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。而且，线程 A 一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存缺失，进而从数据库中读取最新值\n\n  \n\n  \n\n# 总结\n\n  \n\n**在大多数业务场景下，我们会把 Redis 作为只读缓存使用，**我们既可以先删除缓存值再更新数据库，也可以先更新数据库再删除缓存。我的建议是，**优先使用先更新数据库再删除缓存的方法**，原因主要有两个\n\n  \n\n- 先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力；\n    \n- 如果业务应用中读取数据库和写缓存的时间不好估算，那么，延迟双删中的等待时间就不好设置。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E7%BC%93%E5%AD%98%E5%BC%82%E5%B8%B8%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%87%BB%E7%A9%BF%E7%A9%BF%E9%80%8F":{"title":"缓存异常：缓存雪崩、击穿、穿透","content":"# 缓存雪崩\n\n缓存雪崩是指**大量的应用请求无法在 Redis 缓存中进行处理**，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。\n\n  \n\n存雪崩一般是由两个原因导致的，应对方案也有所不同：\n\n- 第一个原因是：缓存中有大量数据同时过期，导致大量请求无法得到处理。\n    \n    ![](statistic/asynccode-455.png)\n    \n    - 针对大量数据同时失效带来的缓存雪崩问题，我给你提供两种解决方案。\n        \n        - 首先，我们可以避免给大量的数据设置相同的过期时间。\n            \n        - 我们还可以通过服务降级，来应对缓存雪崩\n            \n            - 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息\n                \n            - 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取\n                \n- 第二个原因：Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。\n    \n    - 解决方案\n        \n        - 第一个建议，是在业务系统中实现服务熔断或请求限流机制。\n            \n        - 我给你的第二个建议就是事前预防。\n            \n            - 主从节点的方式构建 Redis 缓存高可靠集群\n                \n\n  \n\n  \n\n# 缓存击穿\n\n缓存击穿是指，针对某个**访问非常频繁的热点数据的请求，无法在缓存中进行处理**，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。**缓存击穿的情况，经常发生在热点数据过期失效时。**\n\n![](statistic/asynccode-454.png)\n\n解决方案\n\n- 对于访问特别频繁的热点数据，我们就不设置过期时间了\n    \n\n# 缓存穿透\n\n缓存穿透是指要访问的**数据既不在 Redis 缓存中，也不在数据库中**，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。\n\n应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，**如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力**\n\n![](statistic/asynccode-452.png)\n\n  \n\n发生缓存穿透的原因：\n\n- 业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据；\n    \n- 恶意攻击：专门访问数据库中没有的数据。\n    \n\n  \n\n  \n\n三种解决方案：\n\n- 第一种方案是，缓存空值或缺省值。\n    \n- 第二种方案是，使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。\n    \n- 最后一种方案是，**在请求入口的前端进行请求检测**。缓存穿透的一个原因是有大量的恶意请求访问不存在的数据，所以，一个有效的应对方案是在请求入口前端，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库\n    \n\n  \n\n  \n\n# 总结\n\n![](statistic/asynccode-453.png)","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E7%BC%93%E5%AD%98%E8%A2%AB%E6%B1%A1%E6%9F%93%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86-LFU":{"title":"缓存被污染如何处理）—— LFU","content":"那什么是缓存污染呢？在一些场景下，有些**数据被访问的次数非常少**，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存空间。\n\n  \n\n# 如何解决缓存污染问题\n\n我们也能很容易想到解决方案，那就是得把不会再被访问的数据筛选出来并淘汰掉。\n\nLFU 缓存策略的优化。\n\n  \n\nLFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数。当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。\n\n  \n\n  \n\n为了避免操作链表的开销，Redis 在实现 LRU 策略时使用了两个近似方法：\n\n- Redis 是用 RedisObject 结构来保存数据的，RedisObject 结构中设置了一个 lru 字段，用来记录数据的访问时间戳；\n    \n- Redis 并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量（例如 10 个）的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选。\n    \n\n  \n\n在此基础上，Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 lru 字段，又进一步拆分成了两部分。\n\n- ldt 值：lru 字段的前 16bit，表示数据的访问时间戳；\n    \n- counter 值：lru 字段的后 8bit，表示数据的访问次数。\n    \n\n  \n\n在实现 LFU 策略时，Redis 并没有采用数据每被访问一次，就给对应的 counter 值加 1 的计数规则，而是采用了一个更优化的计数规则。\n\n- 每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E8%BF%98%E5%8F%AF%E4%BB%A5%E5%AE%9A%E4%B9%89%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%90%97":{"title":"还可以定义新的数据类型吗","content":"RedisObject 包括元数据和指针。其中，**元数据的一个功能就是用来区分不同的数据类型**，**指针用来指向具体的数据类型的值**\n\n# Redis 的基本对象结构\n\nRedisObject 的内部组成包括了 type、encoding、lru 和 refcount 4 个元数据，以及 1 个*ptr指针\n\n- type：表示值的类型，涵盖了我们前面学习的五大基本类型；\n    \n- encoding：是值的编码方式，用来表示 Redis 中实现各个基本类型的底层数据结构，例如 SDS、压缩列表、哈希表、跳表等；\n    \n- lru：记录了这个对象最后一次被访问的时间，用于淘汰过期的键值对；\n    \n- refcount：记录了对象的引用计数；\n    \n- *ptr：是指向数据的指针。\n    \n\n![](statistic/asynccode-481.png)\n\n所以，我们在定义了新的数据类型后，也只要在 RedisObject 中设置好新类型的 type 和 encoding，再用*ptr指向新类型的实现，就行了","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/%E9%9B%86%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%85%B6%E9%9B%86%E5%90%88%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F":{"title":"集合数据类型和其集合统计模式","content":"通常情况下，我们面临的用户数量以及访问量都是巨大的，所以，我们必须要选择能够非常高效地统计大量数据（例如亿级）的集合类型，要想选择合适的集合，我们就得了解常用的集合统计模式。四种统计模式\n\n- 聚合统计\n    \n- 排序统计\n    \n- 二值状态统计\n    \n- 基数统计\n    \n\n  \n\n# 聚合统计\n\n所谓的**聚合统计**，就是指统计多个集合元素的聚合结果，包括：\n\n- 统计多个集合的共有元素（交集统计）；\n    \n- 把两个集合相比，统计其中一个集合独有的元素（差集统计）；\n    \n- 统计多个集合的所有元素（并集统计）\n    \n\n  \n\n当你需要对多个集合进行聚合计算时，Set 类型会是一个非常不错的选择。不过，我要提醒你一下，这里有一个潜在的风险。\n\nSet 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议：\n\n- 你可以从主从集群中选择一个从库，让它专门负责聚合计算，\n    \n- 或者是把数据读取到客户端，在客户端来完成聚合统计\n    \n\n  \n\n# 排序统计\n\n这就要求集合类型能**对元素保序**，也就是说，**集合中的元素可以按序排列**，这种对元素保序的集合类型叫作有序集合。\n\n在 Redis 常用的 4 个集合类型中（List、Hash、Set、Sorted Set），**List 和 Sorted Set 就属于有序集合**\n\n- List 是按照元素进入 List 的顺序进行排序的，\n    \n- 而 Sorted Set 可以根据元素的权重来排序\n    \n\n在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set\n\n  \n\n# 二值状态排序\n\n  \n\n二值状态统计。这里的**二值状态就是指集合元素的取值就只有 0 和 1 两种**，在签到打卡的场景中，我们只用记录签到（1）或未签到（0），所以它就是非常典型的二值状态。\n\n这个时候，我们就可以选择 **Bitmap**。这是 Redis 提供的扩展数据类型。我来给你解释一下它的实现原理：\n\n- Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。\n    \n- String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的**每个 bit 位利用起来，用来表示一个元素的二值状态**。你可以把 Bitmap 看作是一个 bit 数组\n    \n    - 不过，需要注意的是，**Bitmap 的偏移量是从 0 开始算**的，也就是说 offset 的最小值是 0。当使用 SETBIT 对一个 bit 位进行写操作时，**这个 bit 位会被设置为 1**。Bitmap 还提供了 BITCOUNT 操作，用来统计这个 bit 数组中所有“1”的个数\n        \n\n  \n\n  \n\n# 基数统计\n\n基数统计。基数统计就是指统计一个集合中不重复的元素个数；\n\n**Set** 类型默认支持去重，所以看到有去重需求时，我们可能第一时间就会想到用 Set 类型。\n\n**HyperLogLog** 是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小\n\n  \n\n  \n\n  \n\n# 总结\n\n![](statistic/asynccode-484.png)","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/CPU%E7%BB%93%E6%9E%84%E4%B9%9F%E4%BC%9A%E5%BD%B1%E5%93%8DRedis%E7%9A%84%E6%80%A7%E8%83%BD":{"title":"CPU结构也会影响Redis的性能","content":"**目前主流服务器的 CPU 架构，基于 CPU 多核架构多 CPU 架构优化 Redis 性能的方法**\n\n  \n\n# 主流的CPU架构\n\n一个 CPU 处理器中**一般有多个运行核心**，我们把**一个运行核心称为一个物理核**，每个物理核都可以运行应用程序。每个物理核都拥有私有的一级缓存（Level 1 cache，简称 L1 cache），包括一级指令缓存和一级数据缓存，以及私有的二级缓存（Level 2 cache，简称 L2 cache）。\n\n  \n\n  \n\n- 因为 L1 和 L2 缓存是每个物理核私有的，所以，当数据或指令保存在 L1、L2 缓存时，物理核访问它们的延迟不超过 10 纳秒，速度非常快。\n    \n    ![](statistic/asynccode-473.png)\n    \n- 但是，这些 L1 和 L2 缓存的大小受限于处理器的制造技术，一般只有 KB 级别，存不下太多的数据。\n    \n- 所以，**不同的物理核还会共享一个共同的三级缓存（Level 3 cache，简称为 L3 cache）**。L3 缓存能够使用的存储资源比较多，所以一般比较大，能达到几 MB 到几十 MB，这就能让应用程序缓存更多的数据\n    \n\n![](statistic/asynccode-468.png)\n\n- 在主流的服务器上，一个 CPU 处理器会有 10 到 20 多个物理核。同时，为了提升服务器的处理能力，服务器上通常还会有多个 CPU 处理器（也称为多 CPU Socket），每个处理器有自己的物理核（包括 L1、L2 缓存），L3 缓存，以及连接的内存，同时，**不同处理器间通过总线连接**\n    \n    ![](statistic/asynccode-470.png)\n    \n\n  \n\n在多 CPU 架构上，应用程序可以在不同的处理器上运行。在刚才的图中，Redis 可以先在 Socket 1 上运行一段时间，然后再被调度到 Socket 2 上运行。\n\n- **如果应用程序先在一个 Socket 上运行，并且把数据保存到了内存，然后被调度到另一个 Socket 上运行，此时，应用程序再进行内存访问时，就需要访问之前 Socket 上连接的内存，**\n    \n- **这种访问属于远端内存访问。和访问 Socket 直接连接的内存相比，远端内存访问会增加应用程序的延迟**\n    \n\n  \n\n在多 CPU 架构下，**一个应用程序访问所在 Socket 的本地内存和访问远端内存的延迟并不一致，所以，我们也把这个架构称为非统一内存访问架构（Non-Uniform Memory Access，NUMA 架构）**。\n\n  \n\n总结CPU架构对应用程序运行的影响\n\n- L1、L2 缓存中的指令和数据的访问速度很快，所以，充分利用 L1、L2 缓存，可以有效缩短应用程序的执行时间\n    \n- 在 NUMA 架构下，如果应用程序从一个 Socket 上调度到另一个 Socket 上，就可能会出现远端内存访问的情况，这会直接增加应用程序的执行时间\n    \n\n  \n\n  \n\n# CPU 多核对 Redis 性能的影响\n\n- 在一个 CPU 核上运行时，应用程序需要记录自身使用的软硬件资源信息（例如栈指针、CPU 核的寄存器值等），我们把这些信息称为**运行时信息，**同时，应用程序访问最频繁的指令和数据还会被缓存到 L1、L2 缓存上，以便提升执行速度\n    \n- 在多核 CPU 的场景下，一旦应用程序需要在一个新的 CPU 核上运行，那么，**运行时信息就需要重新加载到新的 CPU 核上**。而且，**新的 CPU 核的 L1、L2 缓存也需要重新加载数据和指令，这会导致程序的运行时间增加**\n    \n\n  \n\n如果在 CPU 多核场景下，**Redis 实例被频繁调度到不同 CPU 核上运行的话，那么，对 Redis 实例的请求处理时间影响就更大了**。每调度一次，一**些请求就会受到运行时信息、指令和数据重新加载过程的影响**，这就会导致某些请求的延迟明显高于其他请求。\n\n我们可以使用 taskset 命令把一个程序绑定在一个核上运行。\n\n  \n\n  \n\n# CPU 的 NUMA 架构对 Redis 性能的影响\n\n  \n\n我们先来看下 Redis 实例和网络中断程序的数据交互：网络中断处理程序从网卡硬件中读取数据，并把数据写入到操作系统内核维护的一块内存缓冲区。**内核会通过 epoll 机制触发事件**，通知 Redis 实例，Redis 实例再把数据从内核的内存缓冲区拷贝到自己的内存空间.\n\n![](statistic/asynccode-472.png)\n\n  \n\n有一个潜在的风险：**如果网络中断处理程序和 Redis 实例各自所绑的 CPU 核不在同一个 CPU Socket 上，那么，Redis 实例读取网络数据时，就需要跨 CPU Socket 访问内存，这个过程会花费较多时间**\n\n  \n\n  \n\n  \n\n如下图：\n\n![](statistic/asynccode-469.png)\n\n  \n\n图中的网络中断处理程序被绑在了 CPU Socket 1 的某个核上，而 Redis 实例则被绑在了 CPU Socket 2 上。此时，网络中断处理程序读取到的网络数据，被保存在 CPU Socket 1 的本地内存中，当 Redis 实例要访问网络数据时，就需要 Socket 2 通过总线把内存访问命令发送到 Socket 1 上，进行远程访问，时间开销比较大\n\n  \n\n  \n\n所以，为了避免 Redis 跨 CPU Socket 访问网络数据，我们**最好把网络中断程序和 Redis 实例绑在同一个 CPU Socket** 上，这样一来，Redis 实例就可以直接从本地内存读取网络数据了。\n\n![](statistic/asynccode-471.png)\n\n  \n\n**在 CPU 的 NUMA 架构下，对 CPU 核的编号规则，并不是先把一个 CPU Socket 中的所有逻辑核编完，再对下一个 CPU Socket 中的逻辑核编码，而是先给每个 CPU Socket 中每个物理核的第一个逻辑核依次编号，再给每个 CPU Socket 中的物理核的第二个逻辑核依次编号**。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/Codis-vs-Redis-Cluster":{"title":"Codis vs Redis Cluster","content":"要开发一个图片存储系统，要求这个系统能快速地记录**图片 ID** 和图片在存储系统中保存时的 ID（可以直接叫作**图片存储对象 ID**）。同时，还要能够根据图片 ID 快速查找到图片存储对象 ID。因为图片数量巨大，所以我们就用 10 位数来表示图片 ID 和图片存储对象 ID。\n\n  \n\n  \n\n图片 ID 和图片存储对象 ID 正好一一对应，是典型的“键 - 单值”模式，String 类型提供的“一个键对应一个值的数据”的保存形式刚好契合。\n\n  \n\n- 我们的第一个方案就是用 String 保存数据。我们把图片 ID 和图片存储对象 ID 分别作为键值对的 key 和 value 来保存，其中，图片存储对象 ID 用了 String 类型。\n    \n    - 我们保存了 1 亿张图片，大约用了 6.4GB 的内存。但是，随着图片数据量的不断增加，我们的 Redis 内存使用量也在增加，结果就遇到了大内存 Redis 实例因为生成 RDB 而响应变慢的问题\n        \n    - **String 类型并不是适用于所有场合的**，它有一个明显的短板，就是**它保存数据时所消耗的内存空间较多**。\n        \n\n  \n\n- String 类型的内存空间消耗在哪儿了\n    \n- 用什么数据结构可以节省内存\n    \n- 以及如何用集合类型保存单值键值对。\n    \n\n  \n\n# 为什么 String 类型内存开销大\n\n  \n\n其实，除了记录实际数据，**String 类型还需要额外的内存空间记录数据长度、空间使用等信息，这些信息也叫作元数据。**\n\n  \n\nString 类型具体是怎么保存数据的呢？\n\n  \n\n- 当你保存 64 位有符号整数时，String 类型会把它保存为一个 8 字节的 Long 类型整数，这种保存方式通常也叫作 int 编码方式\n    \n- 当你保存的数据中包含字符时，String 类型就会用**简单动态字符串（**Simple Dynamic String，**SDS**）结构体来保存。\n    \n    - buf：字节数组，保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加一个“\\0”，这就会额外占用 1 个字节的开销。\n        \n    - len：占 4 个字节，表示 buf 的已用长度\n        \n    - alloc：也占个 4 字节，表示 buf 的实际分配长度，一般大于 len\n        \n\n![](statistic/asynccode-444.png)\n\n  \n\n对于 String 类型来说，除了 SDS 的额外开销，还有一个来自于 RedisObject 结构体的开销。\n\n因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，R**edis 会用一个 RedisObject 结构体来统一记录这些元数据**，**同时指向实际数据。**一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针，这个指针再进一步指向具体数据类型的实际数据所在，例如指向 String 类型的 SDS 结构所在的内存地址，可以看一下下面的示意图\n\n![](statistic/asynccode-448.png)\n\n  \n\nRedis 还对 Long 类型整数和 SDS 的内存布局做了专门的设计。\n\n- 一方面，当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了，节省了指针的空间开销\n    \n- 另一方面，当保存的是字符串数据，\n    \n    - 并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片、\n        \n    - 字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构\n        \n\n![](statistic/asynccode-447.png)\n\n  \n\n  \n\n**Redis 会使用一个全局哈希表保存所有键值对**，哈希表的每一项是一个 **dictEntry** 的结构体，用来指向一个键值对。\n\ndictEntry 结构中有三个 8 字节的指针，分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节。\n\n![](statistic/asynccode-445.png)\n\njemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间；如果你申请 24 字节空间，jemalloc 则会分配 32 字节。\n\n  \n\n# 用什么数据结构可以节省内存？\n\nRedis 有一种底层数据结构，叫**压缩列表（ziplist）**，这是一种非常节省内存的结构。\n\n- zlbytes：列表长度\n    \n- zltail：列表尾的偏移量\n    \n- zllen：列表中的 entry 个数\n    \n- zlend:表示列表结束。\n    \n\n  \n\n![](statistic/asynccode-446.png)\n\n每个 entry 的元数据包括下面几部分:\n\n- prev_len，表示前一个 entry 的长度。prev_len 有两种取值情况：1 字节或 5 字节。取值 1 字节时，表示上一个 entry 的长度小于 254 字节。否则，就取值为 5 字节。\n    \n- len：表示自身长度，4 字节；\n    \n- encoding：表示编码方式，1 字节；\n    \n- content：保存实际数据。.\n    \n\n这些 entry 会挨个儿放置在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间\n\n  \n\n  \n\n**Redis 基于压缩列表实现了 List、Hash 和 Sorted Set 这样的集合类型，这样做的最大好处就是节省了 dictEntry 的开销。**当你用 String 类型时，一个键值对就有一个 dictEntry，要用 32 字节空间。但采用集合类型时，一个 key 就对应一个集合的数据，能保存的数据多了很多，但也只用了一个 dictEntry，\n\n  \n\n  \n\n  \n\n# 如何用集合类型保存单值的键值对\n\n  \n\n在保存单值的键值对时，**可以采用基于 Hash 类型的二级编码方法**。这里说的二级编码，就是把一个单值的数据拆分成两部分，**前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value**，这样一来，我们就可以把单值数据保存到 Hash 集合中","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/GEO%E6%98%AF%E4%BB%80%E4%B9%88":{"title":"GEO是什么？","content":"我们学习了 Redis 的 5 大基本数据类型：String、List、Hash、Set 和 Sorted Set，它们可以满足大多数的数据存储需求，但是在面对海量数据统计时，它们的内存开销很大，而且对于一些特殊的场景，它们是无法支持的。所以，Redis 还提供了 3 种扩展数据类型，分别是 Bitmap、HyperLogLog 和 GEO。\n\n  \n\n# 面向 LBS 应用的 GEO 数据类型\n\n位置信息服务（Location-Based Service，LBS）的应用访问的数据是和人或物关联的**一组经纬度信息**，而且要能查询相邻的经纬度范围，GEO 就非常适合应用在 LBS服务的场景中.\n\n## GEO的底层结构\n\n一般来说，在设计一个数据类型的底层结构时，我们首先需要知道，要处理的数据有什么访问特点。\n\n以叫车服务为例，来分析下 LBS 应用中经纬度的存取特点：\n\n1. 每一辆网约车都有一个编号（例如 33），网约车需要将自己的经度信息（例如 116.034579）和纬度信息（例如 39.000452 ）发给叫车应用。\n    \n2. 用户在叫车的时候，叫车应用会根据用户的经纬度位置（例如经度 116.054579，纬度 39.030452），查找用户的附近车辆，并进行匹配。\n    \n3. 等把位置相近的用户和车辆匹配上以后，叫车应用就会根据车辆的编号，获取车辆的信息，并返回给用户\n    \n\n  \n\n这种数据记录模式**属于一个 key（例如车 ID）对应一个 value（一组经纬度**）。当有很多车辆信息要保存时，就需要有一个集合来保存一系列的 key 和 value。\n\n- Hash 集合类型可以快速存取一系列的 key 和 value。Hash 类型看起来是一个不错的选择。但问题是，对于一个 LBS 应用来说，除了记录经纬度信息，还需要根据用户的经纬度信息在车辆的 Hash 集合中进行范围查询。一旦涉及到范围查询，就意味着集合中的元素需要有序，但 Hash 类型的元素是无序的，显然不能满足我们的要求\n    \n- Sorted Set 类型。也支持一个 key 对应一个 value 的记录模式，其中，**key 就是 Sorted Set 中的元素，而 value 则是元素的权重分数**。更重要的是，S**orted Set 可以根据元素的权重分数排序**，支持范围查询。这就能满足 LBS 服务中查找相邻位置的需求了\n    \n\n  \n\n实际上，GEO 类型的底层数据结构就是用 Sorted Set 来实现的。\n\n用 Sorted Set 来保存车辆的经纬度信息时，Sorted Set 的元素是车辆 ID，元素的权重分数是经纬度信息。\n\n![](statistic/asynccode-482.png)\n\n## GeoHash 的编码方法\n\nRedis 采用了业界广泛使用的 GeoHash 编码方法，这个方法的基本原理就是“二分区间，区间编码“，\n\n当我们要对一组经纬度进行 GeoHash 编码时，我们要先对经度和纬度分别编码，然后再把经纬度各自的编码组合成一个最终编码。\n\n- 首先，我们来看下经度和纬度的单独编码过程。\n    \n    - 对于一个地理位置信息来说，它的经度范围是[-180,180]。GeoHash 编码会把一个经度值编码成一个 N 位的二进制值，我们来对经度范围[-180,180]做 N 次的二分区操作，其中 N 可以自定义。\n        \n    - 在进行第一次二分区时，经度范围[-180,180]会被分成两个子区间：[-180,0) 和[0,180]（我称之为左、右分区）。此时，我们可以查看一下要编码的经度值落在了左分区还是右分区。如果是落在左分区，我们就用 0 表示；如果落在右分区，就用 1 表示\n        \n    - 们再对经度值所属的分区再做一次二分区，同时再次查看经度值落在了二分区后的左分区还是右分区，按照刚才的规则再做 1 位编码。当做完 N 次的二分区后，经度值就可以用一个 N bit 的数来表示了\n        \n- 对纬度的编码方式，和对经度的一样，只是纬度的范围是[-90，90]，\n    \n- 当一组经纬度值都编完码后，我们再把它们的各自编码值组合在一起，组合的规则是：最终编码值的偶数位上依次是经度的编码值，奇数位上依次是纬度的编码值，其中，偶数位从 0 开始，奇数位从 1 开始\n    \n    - 我们刚刚计算的经纬度（116.37，39.86）的各自编码值是 11010 和 10111，组合之后，就能得到最终编码值 1110011101\n        \n\n  \n\n使用 GeoHash 编码后，**我们相当于把整个地理空间划分成了一个个方格**，**每个方格对应了 GeoHash 中的一个分区**\n\n**我们把经度区间[-180,180]做一次二分区，把纬度区间[-90,90]做一次二分区，就会得到 4 个分区**\n\n- 分区一：[-180,0) 和[-90,0)，编码 00；\n    \n- 分区二：[-180,0) 和[0,90]，编码 01；\n    \n- 分区三：[0,180]和[-90,0)，编码 10；\n    \n- 分区四：[0,180]和[0,90]，编码 11\n    \n\n  \n\n**有的编码值虽然在大小上接近，但实际对应的方格却距离比较远，**我们用 4 位来做 GeoHash 编码，把经度区间[-180,180]和纬度区间[-90,90]各分成了 4 个分区，一共 16 个分区，对应了 16 个方格。编码值为 0111 和 1000 的两个方格就离得比较远，如下图所示\n\n![](statistic/asynccode-483.png)\n\n  \n\n## 如何操作GEO类型\n\n- GEOADD 命令：用于把一组经纬度信息和相对应的一个 ID 记录到 GEO 类型集合中；\n    \n- GEORADIUS 命令：会根据输入的经纬度位置，查找以这个经纬度为中心的一定范围内的其他元素。当然，我们可以自己定义这个范围。\n    \n\n  \n\n假设车辆 ID 是 33，经纬度位置是（116.034579，39.030452），我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations。执行下面的这个命令，就可以把 ID 号为 33 的车辆的当前经纬度位置存入 GEO 集合中\n\nGEOADD cars:locations 116.034579 39.030452 33\n\n当用户想要寻找自己附近的网约车时，LBS 应用就可以使用 GEORADIUS 命令。\n\nGEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10\n\n  \n\n# R","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/Pika%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8ESSD%E5%AE%9E%E7%8E%B0%E5%A4%A7%E5%AE%B9%E9%87%8FRedis":{"title":"Pika：如何基于SSD实现大容量Redis","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/Redis-%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E4%B8%8E%E6%95%85%E9%9A%9C%E5%88%87%E6%8D%A2%E6%9C%89%E5%93%AA%E4%BA%9B%E5%9D%91":{"title":"Redis 主从同步与故障切换，有哪些坑？","content":"# 主从数据不一致\n\n那为啥会出现这个坑呢？其实这是因为主从库间的命令复制是异步进行的。\n\n- 在主从库命令传播阶段，主库收到新的写命令后，会发送给从库。但是，主库并不会等到从库实际执行完命令后，再把结果返回给客户端，而是主库自己在本地执行完命令后，就会向客户端返回结果了。如果从库还没有执行主库同步过来的命令，主从库间的数据就不一致了。\n    \n\n  \n\n那在什么情况下，从库会滞后执行同步命令呢？其实，这里主要有两个原因\n\n- 一方面，主从库间的网络可能会有传输延迟，所以从库不能及时地收到主库发送的命令，从库上执行同步命令的时间就会被延后。\n    \n- 另一方面，即使从库及时收到了主库的命令，但是，也可能会因为正在处理其它复杂度高的命令（例如集合操作命令）而阻塞\n    \n\n  \n\n解决方法：\n\n- 首先，在硬件环境配置方面，我们要尽量保证主从库间的网络连接状况良好。\n    \n- 我们还可以开发一个外部程序来监控主从库间的复制进度。\n    \n\n  \n\n  \n\n# 读取过期数据","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%AE%9E%E8%B7%B5/String-%E7%9A%84%E5%B1%80%E9%99%90":{"title":"String 的局限","content":"","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/Redis/%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"常见面试题","content":"![](statistic/asynccode-499.png)\n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E8%AE%A4%E8%AF%86-redis)认识 Redis\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E4%BB%80%E4%B9%88%E6%98%AF-redis)什么是 Redis？\n\n我们直接看 Redis 官方是怎么介绍自己的。\n\n![](statistic/asynccode-19.jpg)\n\nRedis 官方的介绍原版是英文的，我翻译成了中文后截图的，所以有些文字读起来会比较拗口，没关系，我会把里面比较重要的特性抽出来讲一下。\n\nRedis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此读写速度非常快，常用于缓存，消息队列、分布式锁等场景。\n\nRedis 提供了多种数据类型来支持不同的业务场景，比如 String(字符串)、Hash(哈希)、 List (列表)、Set(集合)、Zset(有序集合)、Bitmaps（位图）、HyperLogLog（基数统计）、GEO（地理信息）、Stream（流），并且对数据类型的操作都是原子性的，因为执行命令由单线程负责的，不存在并发竞争的问题。\n\n除此之外，Redis 还支持事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、哨兵模式、切片机群模式）、发布/订阅模式，内存淘汰机制、过期删除机制等等。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%92%8C-memcached-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB)Redis 和 Memcached 有什么区别？\n\n很多人都说用 Redis 作为缓存，但是 Memcached 也是基于内存的数据库，为什么不选择它作为缓存呢？要解答这个问题，我们就要弄清楚 Redis 和 Memcached 的区别。 Redis 与 Memcached 共同点：\n\n1. 都是基于内存的数据库，一般都用来当做缓存使用。\n    \n2. 都有过期策略。\n    \n3. 两者的性能都非常高。\n    \n\nRedis 与 Memcached 区别：\n\n- Redis 支持的数据类型更丰富（String、Hash、List、Set、ZSet），而 Memcached 只支持最简单的 key-value 数据类型；\n    \n- Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用，而 Memcached 没有持久化功能，数据全部存在内存之中，Memcached 重启或者挂掉后，数据就没了；\n    \n- Redis 原生支持集群模式，Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；\n    \n- Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持；\n    \n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8-redis-%E4%BD%9C%E4%B8%BA-mysql-%E7%9A%84%E7%BC%93%E5%AD%98)为什么用 Redis 作为 MySQL 的缓存？\n\n主要是因为 Redis 具备「高性能」和「高并发」两种特性。\n\n_1、Redis 具备高性能_\n\n假如用户第一次访问 MySQL 中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据缓存在 Redis 中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了，操作 Redis 缓存就是直接操作内存，所以速度相当快。\n\n![](statistic/asynccode-495.png)\n\n如果 MySQL 中的对应数据改变的之后，同步改变 Redis 缓存中相应的数据即可，不过这里会有 Redis 和 MySQL 双写一致性的问题，后面我们会提到。\n\n_2、 Redis 具备高并发_\n\n单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍，Redis 单机的 QPS 能轻松破 10w，而 MySQL 单机的 QPS 很难破 1w。\n\n所以，直接访问 Redis 能够承受的请求是远远大于直接访问 MySQL 的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。\n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84)Redis 数据结构\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%86%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88)Redis 数据类型以及使用场景分别是什么？\n\nRedis 提供了丰富的数据类型，常见的有五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合）。\n\n![](statistic/asynccode-497.png)\n\n![](statistic/asynccode-490.png)\n\n随着 Redis 版本的更新，后面又支持了四种数据类型： BitMap（2.2 版新增）、HyperLogLog（2.8 版新增）、GEO（3.2 版新增）、Stream（5.0 版新增）。 Redis 五种数据类型的应用场景：\n\n- String 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。\n    \n- List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。\n    \n- Hash 类型：缓存对象、购物车等。\n    \n- Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。\n    \n- Zset 类型：排序场景，比如排行榜、电话和姓名排序等。\n    \n\nRedis 后续版本又支持四种数据类型，它们的应用场景如下：\n\n- BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等；\n    \n- HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等；\n    \n- GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车；\n    \n- Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。\n    \n\nTIP\n\n想深入了解这 9 种数据类型，可以看这篇：[2万字 + 20 张图 ｜ 细说 Redis 常见数据类型和应用场景(opens new window)](https://xiaolincoding.com/redis/data_struct/command.html)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E4%BA%94%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84-redis-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0)五种常见的 Redis 数据类型是怎么实现？\n\n我画了一张 Redis 数据类型和底层数据结构的对应关图，左边是 Redis 3.0版本的，也就是《Redis 设计与实现》这本书讲解的版本，现在看还是有点过时了，右边是现在 Redis 7.0 版本的。\n\n![](statistic/asynccode-499.png)\n\n\u003e String 类型内部实现\n\nString 类型的底层的数据结构实现主要是 SDS（简单动态字符串）。 SDS 和我们认识的 C 字符串不太一样，之所以没有使用 C 语言的字符串表示，因为 SDS 相比于 C 的原生字符串：\n\n- SDS 不仅可以保存文本数据，还可以保存二进制数据。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。\n    \n- SDS 获取字符串长度的时间复杂度是 O(1)。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。\n    \n- Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。\n    \n\n\u003e List 类型内部实现\n\nList 类型的底层数据结构是由双向链表或压缩列表实现的：\n\n- 如果列表的元素个数**小于 512 个**（默认值，可由 list-max-ziplist-entries 配置），**列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置）**，Redis 会使用**压缩列表**作为 List 类型的底层数据结构；\n    \n- 如果列表的元素不满足上面的条件，Redis 会使用**双向链表**作为 List 类型的底层数据结构；\n    \n\n但是在 Redis 3.2 版本之后，**List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表**。\n\n\u003e Hash 类型内部实现\n\nHash 类型的底层数据结构是由压缩列表或哈希表实现的：\n\n- 如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用**压缩列表作为 Hash 类型的底层数据结构**；\n    \n- 如果哈希类型元素不满足上面条件，Redis **会使用哈希表**作为 Hash 类型的底层数据结构。\n    \n\n在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。\n\n\u003e Set 类型内部实现\n\nSet 类型的底层数据结构是由哈希表或整数集合实现的：\n\n- 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Re**dis 会使用整数集合作为 Set 类型的底层数据结构**；\n    \n- 如果集合中的元素不满足上面条件，则 Redis 使**用哈希表**作为 Set 类型的底层数据结构。\n    \n\n\u003e ZSet 类型内部实现\n\nZset 类型的底层数据结构是由压缩列表或跳表实现的：\n\n- 如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构；\n    \n- 如果有序集合的元素不满足上面的条件，Redis 会使用**跳表作为 Zset 类型的底层数据结构**；\n    \n\n在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。\n\nTIP\n\n想深入了解这 9 种数据结构，可以看这篇：[2万字 + 40 张图 ｜ 细说 Redis 数据结构(opens new window)](https://xiaolincoding.com/redis/data_struct/data_struct.html)\n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B)Redis 线程模型\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E6%98%AF%E5%8D%95%E7%BA%BF%E7%A8%8B%E5%90%97)Redis 是单线程吗？\n\nRedis 单线程指的是「**接收客户端请求-\u003e解析请求 -\u003e进行数据读写等操作-\u003e发送数据给客户端**」这个过程是由一个线程（主线程）来完成的，这也是我们常说 Redis 是单线程的原因。\n\n但是，Redis 程序并不是单线程的，Redis 在启动的时候，是会启动后台线程（BIO）的：\n\n- Redis 在 2.6 版本，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务；\n    \n- Redis 在 4.0 版本之后，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。例如执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当我们要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，**因此我们应该使用 unlink 命令来异步删除大key**。\n    \n\n之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」**这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞**，这样就无法处理后续的请求了。\n\n后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，**消费者（BIO）不停轮询这个队列，拿出任务就去执行对应的方法即可**。\n\n![](statistic/asynccode-492.png)\n\n关闭文件、AOF 刷盘、释放内存这三个任务都有各自的任务队列：\n\n- BIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭；\n    \n- BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘，\n    \n- BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象；\n    \n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%BC%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84)Redis 单线程模式是怎样的？\n\nRedis 6.0 版本之前的单线模式如下图：\n\n![](statistic/asynccode-498.png)\n\n图中的蓝色部分是一个事件循环，是由主线程负责的，可以看到网络 I/O 和命令处理都是单线程。 Redis 初始化的时候，会做下面这几件事情：\n\n- 首先，调用 epoll_create() 创建一个 epoll 对象和调用 socket() 创建一个服务端 socket\n    \n- 然后，调用 bind() 绑定端口和调用 listen() 监听该 socket；\n    \n- 然后，将调用 epoll_ctl() 将 listen socket 加入到 epoll，同时注册「连接事件」处理函数。\n    \n\n初始化完后，主线程就进入到一个事件循环函数，主要会做以下事情：\n\n- 首先，先调用处理发送队列函数，看是发送队列里是否有任务，如果有发送任务，则通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。\n    \n- 接着，调用 epoll_wait 函数等待事件的到来：\n    \n    - 如果是连接事件到来，则会调用连接事件处理函数，该函数会做这些事情：调用 accpet 获取已连接的 socket -\u003e 调用 epoll_ctl 将已连接的 socket 加入到 epoll -\u003e 注册「读事件」处理函数；\n        \n    - 如果是读事件到来，则会调用读事件处理函数，该函数会做这些事情：调用 read 获取客户端发送的数据 -\u003e 解析命令 -\u003e 处理命令 -\u003e 将客户端对象添加到发送队列 -\u003e 将执行结果写到发送缓存区等待发送；\n        \n    - 如果是写事件到来，则会调用写事件处理函数，该函数会做这些事情：通过 write 函数将客户端发送缓存区里的数据发送出去，如果这一轮数据没有发送完，就会继续注册写事件处理函数，等待 epoll_wait 发现可写后再处理 。\n        \n\n以上就是 Redis 单线模式的工作方式，如果你想看源码解析，可以参考这一篇：[为什么单线程的 Redis 如何做到每秒数万 QPS ？(opens new window)](https://mp.weixin.qq.com/s/oeOfsgF-9IOoT5eQt5ieyw)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E9%87%87%E7%94%A8%E5%8D%95%E7%BA%BF%E7%A8%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%BF%99%E4%B9%88%E5%BF%AB)Redis 采用单线程为什么还这么快？\n\n官方使用基准测试的结果是，单线程的 Redis 吞吐量可以达到 10W/每秒，如下图所示：\n\n![](statistic/asynccode-486.png)\n\n之所以 Redis 采用单线程（网络 I/O 和执行命令）那么快，有如下几个原因：\n\n- **Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构**，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了；\n    \n- **Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销**，而且也不会导致死锁问题。\n    \n- **Redis 采用了 I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制**。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。\n    \n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-6-0-%E4%B9%8B%E5%89%8D%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E5%8D%95%E7%BA%BF%E7%A8%8B)Redis 6.0 之前为什么使用单线程？\n\n我们都知道单线程的程序是无法利用服务器的多核 CPU 的，那么早期 Redis 版本的主要工作（网络 I/O 和执行命令）为什么还要使用单线程呢？我们不妨先看一下Redis官方给出的[FAQ (opens new window)](https://link.juejin.cn/?target=https%3A%2F%2Fredis.io%2Ftopics%2Ffaq)。\n\n![](statistic/asynccode-493.png)\n\n核心意思是：CPU 并不是制约 Redis 性能表现的瓶颈所在，**更多情况下是受到内存大小和网络I/O的限制，所以 Redis 核心网络模型使用单线程并没有什么问题，**如果你想要使用服务的多核CPU，**可以在一台服务器上启动多个节点或者采用分片集群的方式。**\n\n除了上面的官方回答，选择单线程的原因也有下面的考虑。\n\n使用了单线程后，可维护性高，多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-6-0-%E4%B9%8B%E5%90%8E%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BC%95%E5%85%A5%E4%BA%86%E5%A4%9A%E7%BA%BF%E7%A8%8B)Redis 6.0 之后为什么引入了多线程？\n\n虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，**Redis 的性能瓶颈有时会出现在网络 I/O 的处理上**。\n\n所以为了提高网络 I/O 的并行度，**Redis 6.0 对于网络 I/O 采用多线程来处理。但是对于命令的执行，Redis 仍然使用单线程来处理，所以大家不要误解 Redis 有多线程同时执行命令**。\n\nRedis 官方表示，Redis 6.0 版本引入的多线程 I/O 特性对性能提升至少是一倍以上。\n\nRedis 6.0 版本支持的 I/O 多线程特性，**默认情况下 I/O 多线程只针对发送响应数据（write client socket），并不会以多线程的方式处理读请求（read client socket）**。要想开启多线程处理客户端读请求，就需要把 Redis.conf 配置文件中的 io-threads-do-reads 配置项设为 yes。\n\n```C\n//读请求也使用io多线程\nio-threads-do-reads yes \n```\n\n同时， Redis.conf 配置文件中提供了 IO 多线程个数的配置项。\n\n```C\n// io-threads N，表示启用 N-1 个 I/O 多线程（主线程也算一个 I/O 线程）\nio-threads 4 \n```\n\n关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。\n\n因此， Redis 6.0 版本之后，Redis 在启动的时候，默认情况下会额外创建 6 个线程（_这里的线程数不包括主线程_）：\n\n- Redis-server ： Redis的主线程，主要负责执行命令；\n    \n- bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务；\n    \n- io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力。\n    \n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E6%8C%81%E4%B9%85%E5%8C%96)Redis 持久化\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1)Redis 如何实现数据不丢失？\n\nRedis 的读写操作都是在内存中，所以 Redis 性能才会高，但是当 Redis 重启后，内存中的数据就会丢失，那为了保证内存中的数据不会丢失，Redis 实现了数据持久化的机制，这个机制会把数据存储到磁盘，这样在 Redis 重启就能够从磁盘中恢复原有的数据。\n\nRedis 共有三种数据持久化的方式：\n\n- AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里；\n    \n- RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘；\n    \n- 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点；\n    \n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#aof-%E6%97%A5%E5%BF%97%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84)AOF 日志是如何实现的？\n\nRedis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。\n\n![](statistic/asynccode-485.png)\n\n我这里以「_set name xiaolin_」命令作为例子，Redis 执行了这条命令后，记录在 AOF 日志里的内容如下图：\n\n![](statistic/asynccode-488.png)\n\n我这里给大家解释下。\n\n「*3」表示当前命令有三个部分，每部分都是以「$+数字」开头，后面紧跟着具体的命令、键或值。然后，这里的「数字」表示这部分中的命令、键或值一共有多少字节。例如，「$3 set」表示这部分有 3 个字节，也就是「set」命令这个字符串的长度。\n\n\u003e 为什么先执行命令，再把数据写入日志呢？\n\nReids 是先执行写操作命令后，才将该命令记录到 AOF 日志里的，这么做其实有两个好处。\n\n- 避免额外的检查开销：因为如果先将写操作命令记录到 AOF 日志里，再执行该命令的话，如果当前的命令语法有问题，那么如果不进行命令语法检查，该错误的命令记录到 AOF 日志里后，Redis 在使用日志恢复数据时，就可能会出错。\n    \n- 不会阻塞当前写操作命令的执行：因为当写操作命令执行成功后，才会将命令记录到 AOF 日志。\n    \n\n当然，这样做也会带来风险：\n\n- 数据可能会丢失： 执行写操作命令和记录日志是两个过程，那当 Redis 在还没来得及将命令写入到硬盘时，服务器发生宕机了，这个数据就会有丢失的风险。\n    \n- 可能阻塞其他操作： 由于写操作命令执行成功后才记录到 AOF 日志，所以不会阻塞当前命令的执行，但因为 AOF 日志也是在主线程中执行，所以当 Redis 把日志文件写入磁盘的时候，还是会阻塞后续的操作无法执行。\n    \n\n\u003e AOF 写回策略有几种？\n\n先来看看，Redis 写入 AOF 日志的过程，如下图：\n\n![](statistic/asynccode-488.png)\n\n具体说说：\n\n1. Redis 执行完写操作命令后，会将命令追加到 server.aof_buf 缓冲区；\n    \n2. 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区 page cache，等待内核将数据写入硬盘；\n    \n3. 具体内核缓冲区的数据什么时候写入到硬盘，由内核决定。\n    \n\nRedis 提供了 3 种写回硬盘的策略，控制的就是上面说的第三步的过程。 在 Redis.conf 配置文件中的 appendfsync 配置项可以有以下 3 种参数可填：\n\n- Always，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；\n    \n- Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘；\n    \n- No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。\n    \n\n我也把这 3 个写回策略的优缺点总结成了一张表格：\n\n![](statistic/asynccode-502.png)\n\n\u003e AOF 日志过大，会触发什么机制？\n\nAOF 日志是一个文件，随着执行的写操作命令越来越多，文件的大小会越来越大。 如果当 AOF 日志文件过大就会带来性能问题，比如重启 Redis 后，需要读 AOF 文件的内容以恢复数据，如果文件过大，整个恢复的过程就会很慢。\n\n所以，Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。\n\nAOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。\n\n举个例子，在没有使用重写机制前，假设前后执行了「_set name xiaolin_」和「_set name xiaolincoding_」这两个命令的话，就会将这两个命令记录到 AOF 文件。\n\n![](statistic/asynccode-489.png)\n\n但是在使用重写机制后，就会读取 name 最新的 value（键值对） ，然后用一条 「set name xiaolincoding」命令记录到新的 AOF 文件，之前的第一个命令就没有必要记录了，因为它属于「历史」命令，没有作用了。这样一来，一个键值对在重写日志中只用一条命令就行了。\n\n重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，这就相当于压缩了 AOF 文件，使得 AOF 文件体积变小了。\n\n\u003e 重写 AOF 日志的过程是怎样的？\n\nRedis 的重写 AOF 过程是由后台子进程 _bgrewriteaof_ 来完成的，这么做可以达到两个好处：\n\n- 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程；\n    \n- 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。\n    \n\n触发重写机制后，主进程就会创建重写 AOF 的子进程，此时父子进程共享物理内存，重写子进程只会对这个内存进行只读，重写 AOF 子进程会读取数据库里的所有数据，并逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志（新的 AOF 文件）。\n\n但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？\n\n为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。\n\n在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。\n\n![](statistic/asynccode-497.png)\n\n也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作:\n\n- 执行客户端发来的命令；\n    \n- 将执行后的写命令追加到 「AOF 缓冲区」；\n    \n- 将执行后的写命令追加到 「AOF 重写缓冲区」；\n    \n\n当子进程完成 AOF 重写工作（_扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志_）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的。\n\n主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作：\n\n- 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致；\n    \n- 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。\n    \n\n信号函数执行完后，主进程就可以继续像往常一样处理命令了。\n\nTIP\n\nAOF 日志的内容就暂时提这些，想更详细了解 AOF 日志的工作原理，可以详细看这篇：[AOF 持久化是怎么实现的(opens new window)](https://xiaolincoding.com/redis/storage/aof.html)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#rdb-%E5%BF%AB%E7%85%A7%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%91%A2)RDB 快照是如何实现的呢？\n\n因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。\n\n为了解决这个问题，Redis 增加了 RDB 快照。所谓的快照，就是记录某一个瞬间东西，比如当我们给风景拍照时，那一个瞬间的画面和信息就记录到了一张照片。\n\n所以，RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。\n\n因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。\n\n\u003e RDB 做快照时会阻塞线程吗？\n\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行：\n\n- 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程；\n    \n- 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞；\n    \n\nRedis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置：\n\n```C\nsave 900 1\nsave 300 10\nsave 60 10000\n```\n\n别看选项名叫 save，实际上执行的是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件。 只要满足上面条件的任意一个，就会执行 bgsave，它们的意思分别是：\n\n- 900 秒之内，对数据库进行了至少 1 次修改；\n    \n- 300 秒之内，对数据库进行了至少 10 次修改；\n    \n- 60 秒之内，对数据库进行了至少 10000 次修改。\n    \n\n这里提一点，Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。\n\n\u003e RDB 在执行快照的时候，数据能修改吗？\n\n可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。\n\n执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。\n\n![](statistic/asynccode-487.png)\n\n如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。\n\n![](statistic/asynccode-503.png)\n\nTIP\n\nRDB 快照的内容就暂时提这些，想更详细了解 RDB 快照的工作原理，可以详细看这篇：[RDB 快照是怎么实现的？(opens new window)](https://xiaolincoding.com/redis/storage/rdb.html)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E6%B7%B7%E5%90%88%E6%8C%81%E4%B9%85%E5%8C%96)为什么会有混合持久化？\n\nRDB 优点是数据恢复速度快，但是快照的频率不好把握。频率太低，丢失的数据就会比较多，频率太高，就会影响性能。\n\nAOF 优点是丢失数据少，但是数据恢复不快。\n\n为了集成了两者的优点， Redis 4.0 提出了混合使用 AOF 日志和内存快照，也叫混合持久化，既保证了 Redis 重启速度，又降低数据丢失风险。\n\n混合持久化工作在 AOF 日志重写过程，当开启了混合持久化时，在 AOF 重写日志时，fork 出来的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，然后主线程处理的操作命令会被记录在重写缓冲区里，重写缓冲区里的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。\n\n也就是说，使用了混合持久化，AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。\n\n![](statistic/asynccode-18.jpg)\n\n这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。\n\n加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。\n\n混合持久化优点：\n\n- 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。\n    \n\n混合持久化缺点：\n\n- AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差；\n    \n- 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。\n    \n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E9%9B%86%E7%BE%A4)Redis 集群\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E9%AB%98%E5%8F%AF%E7%94%A8)Redis 如何实现服务高可用？\n\n要想设计一个高可用的 Redis 服务，一定要从 Redis 的多服务节点来考虑，比如 Redis 的主从复制、哨兵模式、切片集群。\n\n\u003e 主从复制\n\n主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是「读写分离」的方式。\n\n主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。\n\n![](statistic/asynccode-494.png)\n\n也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。\n\n注意，主从服务器之间的命令复制是异步进行的。\n\n具体来说，在主从服务器命令传播阶段，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。\n\n所以，无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的。\n\nTIP\n\n想更详细了解 Redis 主从复制的工作原理，可以详细看这篇：[主从复制是怎么实现的？(opens new window)](https://xiaolincoding.com/redis/cluster/master_slave_replication.html)\n\n\u003e 哨兵模式\n\n在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。\n\n为了解决这个问题，Redis 增加了哨兵模式（Redis Sentinel），因为哨兵模式做到了可以监控主从服务器，并且提供主从节点故障转移的功能。\n\n![](statistic/asynccode-497.png)\n\nTIP\n\n想更详细了解 Redis 哨兵的工作原理，可以详细看这篇：[哨兵是怎么实现的？(opens new window)](https://xiaolincoding.com/redis/cluster/sentinel.html)\n\n\u003e 切片集群模式\n\n当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。\n\nRedis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步：\n\n- 根据键值对的 key，按照 [CRC16 算法 (opens new window)](https://en.wikipedia.org/wiki/Cyclic_redundancy_check)计算一个 16 bit 的值。\n    \n- 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。\n    \n\n接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案：\n\n- 平均分配： 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。\n    \n- 手动分配： 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。\n    \n\n为了方便你的理解，我通过一张图来解释数据、哈希槽，以及节点三者的映射分布关系。\n\n![](statistic/asynccode-491.png)\n\n上图中的切片集群一共有 2 个节点，假设有 4 个哈希槽（Slot 0～Slot 3）时，我们就可以通过命令手动分配哈希槽，比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3。\n\n```C\nredis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1\nredis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3\n```\n\n然后在集群运行的过程中，key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 4 进行取模，再根据各自的模数结果，就可以被映射到哈希槽 1（对应节点1） 和 哈希槽 2（对应节点2）。\n\n需要注意的是，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E5%AF%BC%E8%87%B4%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E6%80%8E%E4%B9%88%E5%8A%9E)集群脑裂导致数据丢失怎么办？\n\n\u003e 什么是脑裂？\n\n先来理解集群的脑裂现象，这就好比一个人有两个大脑，那么到底受谁控制呢？\n\n那么在 Redis 中，集群脑裂产生数据丢失的现象是怎样的呢？\n\n在 Redis 主从架构中，部署方式一般是「一主多从」，主节点提供写操作，从节点提供读操作。 如果主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道 Redis 内部已经出现了问题，还在照样的向这个失联的主节点写数据（过程A），此时这些数据被旧主节点缓存到了缓冲区里，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。\n\n这时，哨兵也发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出问题了），于是哨兵就会在「从节点」中选举出一个 leader 作为主节点，这时集群就有两个主节点了 —— 脑裂出现了。\n\n然后，网络突然好了，哨兵因为之前已经选举出一个新主节点了，它就会把旧主节点降级为从节点（A），然后从节点（A）会向新主节点请求数据同步，因为第一次同步是全量同步的方式，此时的从节点（A）会清空掉自己本地的数据，然后再做全量同步。所以，之前客户端在过程 A 写入的数据就会丢失了，也就是集群产生脑裂数据丢失的问题。\n\n总结一句话就是：由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。\n\n\u003e 解决方案\n\n当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。\n\n在 Redis 的配置文件中有两个参数我们可以设置：\n\n- min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。\n    \n- min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。\n    \n\n我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。\n\n这两个配置项组合后的要求是，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的写请求了。\n\n即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了。\n\n等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。\n\n再来举个例子。\n\n假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵的 down-after-milliseconds 设置为 10s，主库因为某些原因卡住了 15s，导致哨兵判断主库客观下线，开始进行主从切换。\n\n同时，因为原主库卡住了 15s，没有一个从库能和原主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。\n\n这样一来，主从切换完成后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了。\n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E4%B8%8E%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0)Redis 过期删除与内存淘汰\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E4%BD%BF%E7%94%A8%E7%9A%84%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%E6%98%AF%E4%BB%80%E4%B9%88)Redis 使用的过期删除策略是什么？\n\nRedis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。\n\n每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个过期字典（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。\n\n当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：\n\n- 如果不在，则正常读取键值；\n    \n- 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。\n    \n\nRedis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。\n\n\u003e 什么是惰性删除策略？\n\n惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。\n\n惰性删除的流程图如下：\n\n![](statistic/asynccode-497.png)\n\n惰性删除策略的优点：\n\n- 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。\n    \n\n惰性删除策略的缺点：\n\n- 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。\n    \n\n\u003e 什么是定期删除策略？\n\n定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。\n\nRedis 的定期删除的流程：\n\n1. 从过期字典中随机抽取 20 个 key；\n    \n2. 检查这 20 个 key 是否过期，并删除已过期的 key；\n    \n3. 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。\n    \n\n可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。\n\n定期删除的流程如下：\n\n![](statistic/asynccode-504.png)\n\n定期删除策略的优点：\n\n- 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。\n    \n\n定期删除策略的缺点：\n\n- 难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。\n    \n\n可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。\n\nTIP\n\nRedis 的过期删除的内容就暂时提这些，想更详细了解的，可以详细看这篇：[Redis 过期删除策略和内存淘汰策略有什么区别？(opens new window)](https://xiaolincoding.com/redis/module/strategy.html)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E6%8C%81%E4%B9%85%E5%8C%96%E6%97%B6-%E5%AF%B9%E8%BF%87%E6%9C%9F%E9%94%AE%E4%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%9A%84)Redis 持久化时，对过期键会如何处理的？\n\nRedis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），下面我们分别来看过期键在这两种格式中的呈现状态。\n\nRDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。\n\n- RDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。\n    \n- RDB 加载阶段：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况：\n    \n    - 如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响；\n        \n    - 如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。\n        \n\nAOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。\n\n- AOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值。\n    \n- AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。\n    \n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E4%B8%BB%E4%BB%8E%E6%A8%A1%E5%BC%8F%E4%B8%AD-%E5%AF%B9%E8%BF%87%E6%9C%9F%E9%94%AE%E4%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86)Redis 主从模式中，对过期键会如何处理？\n\n当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。\n\n从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%86%85%E5%AD%98%E6%BB%A1%E4%BA%86-%E4%BC%9A%E5%8F%91%E7%94%9F%E4%BB%80%E4%B9%88)Redis 内存满了，会发生什么？\n\n在 Redis 的运行内存达到了某个阀值，就会触发内存淘汰机制，这个阀值就是我们设置的最大运行内存，此值在 Redis 的配置文件中可以找到，配置项为 maxmemory。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5%E6%9C%89%E5%93%AA%E4%BA%9B)Redis 内存淘汰策略有哪些？\n\nRedis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。\n\n_1、不进行数据淘汰的策略_\n\nnoeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。\n\n_2、进行数据淘汰的策略_\n\n针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰：\n\n- volatile-random：随机淘汰设置了过期时间的任意键值；\n    \n- volatile-ttl：优先淘汰更早过期的键值。\n    \n- volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值；\n    \n- volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值；\n    \n\n在所有数据范围内进行淘汰：\n\n- allkeys-random：随机淘汰任意键值;\n    \n- allkeys-lru：淘汰整个键值中最久未使用的键值；\n    \n- allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。\n    \n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#lru-%E7%AE%97%E6%B3%95%E5%92%8C-lfu-%E7%AE%97%E6%B3%95%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB)LRU 算法和 LFU 算法有什么区别？\n\n\u003e 什么是 LRU 算法？\n\nLRU 全称是 Least Recently Used 翻译为最近最少使用，会选择淘汰最近最少使用的数据。\n\n传统 LRU 算法的实现是基于「链表」结构，链表中的元素按照操作顺序从前往后排列，最新操作的键会被移动到表头，当需要内存淘汰时，只需要删除链表尾部的元素即可，因为链表尾部的元素就代表最久未被使用的元素。\n\nRedis 并没有使用这样的方式实现 LRU 算法，因为传统的 LRU 算法存在两个问题：\n\n- 需要用链表管理所有的缓存数据，这会带来额外的空间开销；\n    \n- 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。\n    \n\n\u003e Redis 是如何实现 LRU 算法的？\n\nRedis 实现的是一种近似 LRU 算法，目的是为了更好的节约内存，它的实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间。\n\n当 Redis 进行内存淘汰时，会使用随机采样的方式来淘汰数据，它是随机取 5 个值（此值可配置），然后淘汰最久没有使用的那个。\n\nRedis 实现的 LRU 算法的优点：\n\n- 不用为所有的数据维护一个大链表，节省了空间占用；\n    \n- 不用在每次数据访问时都移动链表项，提升了缓存的性能；\n    \n\n但是 LRU 算法有一个问题，无法解决缓存污染问题，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。\n\n因此，在 Redis 4.0 之后引入了 LFU 算法来解决这个问题。\n\n\u003e 什么是 LFU 算法？\n\nLFU 全称是 Least Frequently Used 翻译为最近最不常用的，LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。\n\n所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。\n\n\u003e Redis 是如何实现 LFU 算法的？\n\nLFU 算法相比于 LRU 算法的实现，多记录了「数据的访问频次」的信息。Redis 对象的结构如下：\n\n```C\ntypedef struct redisObject {...// 24 bits，用于记录对象的访问信息unsigned lru:24;  \n    ...} robj;\n```\n\nRedis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。\n\n在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。\n\n在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，用来记录 key 的访问时间戳；低 8bit 存储 logc(Logistic Counter)，用来记录 key 的访问频次。\n\n![](statistic/asynccode-495.png)\n\nTIP\n\nRedis 的内存淘汰的内容就暂时提这些，想更详细了解的，可以详细看这篇：[Redis 过期删除策略和内存淘汰策略有什么区别？(opens new window)](https://xiaolincoding.com/redis/module/strategy.html)\n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1)Redis 缓存设计\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E3%80%81%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F)如何避免缓存雪崩、缓存击穿、缓存穿透？\n\n\u003e 如何避免缓存雪崩？\n\n通常我们为了保证缓存中的数据与数据库中的数据一致性，会给 Redis 里的数据设置过期时间，当缓存数据过期后，用户访问的数据如果不在缓存里，业务系统需要重新生成缓存，因此就会访问数据库，并将数据更新到 Redis 里，这样后续请求都可以直接命中缓存。\n\n![](statistic/asynccode-503.png)\n\n那么，当大量缓存数据在同一时间过期（失效）时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是缓存雪崩的问题。\n\n对于缓存雪崩问题，我们可以采用两种方案解决。\n\n- 将缓存失效时间随机打散： 我们可以在原有的失效时间基础上增加一个随机值（比如 1 到 10 分钟）这样每个缓存的过期时间都不重复了，也就降低了缓存集体失效的概率。\n    \n- 设置缓存不过期： 我们可以通过后台服务来更新缓存数据，从而避免因为缓存失效造成的缓存雪崩，也可以在一定程度上避免缓存并发问题。\n    \n\n\u003e 如何避免缓存击穿？\n\n我们的业务通常会有几个数据会被频繁地访问，比如秒杀活动，这类被频地访问的数据被称为热点数据。\n\n如果缓存中的某个热点数据过期了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是缓存击穿的问题。\n\n![](statistic/asynccode-496.png)\n\n可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。 应对缓存击穿可以采取前面说到两种方案：\n\n- 互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。\n    \n- 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；\n    \n\n\u003e 如何避免缓存穿透？\n\n当发生缓存雪崩或击穿时，数据库中还是保存了应用要访问的数据，一旦缓存恢复相对应的数据，就可以减轻数据库的压力，而缓存穿透就不一样了。\n\n当用户访问的数据，既不在缓存中，也不在数据库中，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是缓存穿透的问题。\n\n![](statistic/asynccode-500.png)\n\n缓存穿透的发生一般有这两种情况：\n\n- 业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；\n    \n- 黑客恶意攻击，故意大量访问某些读取不存在数据的业务；\n    \n\n应对缓存穿透的方案，常见的方案有三种。\n\n- 非法请求的限制：当有大量恶意请求访问不存在的数据的时候，也会发生缓存穿透，因此在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。\n    \n- 设置空值或者默认值：当我们线上业务发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。\n    \n- 使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在：我们可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在，即使发生了缓存穿透，大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库，保证了数据库能正常运行，Redis 自身也是支持布隆过滤器的。\n    \n\nTIP\n\n推荐阅读：[什么是缓存雪崩、击穿、穿透？(opens new window)](https://xiaolincoding.com/redis/cluster/cache_problem.html)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%BC%93%E5%AD%98%E7%AD%96%E7%95%A5-%E5%8F%AF%E4%BB%A5%E5%8A%A8%E6%80%81%E7%BC%93%E5%AD%98%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE%E5%91%A2)如何设计一个缓存策略，可以动态缓存热点数据呢？\n\n由于数据存储受限，系统并不是将所有数据都需要存放到缓存中的，而只是将其中一部分热点数据缓存起来，所以我们要设计一个热点数据动态缓存的策略。\n\n热点数据动态缓存的策略总体思路：通过数据最新访问时间来做排名，并过滤掉不常访问的数据，只留下经常访问的数据。\n\n以电商平台场景中的例子，现在要求只缓存用户经常访问的 Top 1000 的商品。具体细节如下：\n\n- 先通过缓存系统做一个排序队列（比如存放 1000 个商品），系统会根据商品的访问时间，更新队列信息，越是最近访问的商品排名越靠前；\n    \n- 同时系统会定期过滤掉队列中排名最后的 200 个商品，然后再从数据库中随机读取出 200 个商品加入队列中；\n    \n- 这样当请求每次到达的时候，会先从队列中获取商品 ID，如果命中，就根据 ID 再从另一个缓存数据结构中读取实际的商品信息，并返回。\n    \n\n在 Redis 中可以用 zadd 方法和 zrange 方法来完成排序队列和获取 200 个商品的操作。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E8%AF%B4%E8%AF%B4%E5%B8%B8%E8%A7%81%E7%9A%84%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5)说说常见的缓存更新策略？\n\n常见的缓存更新策略共有3种：\n\n- Cache Aside（旁路缓存）策略；\n    \n- Read/Write Through（读穿 / 写穿）策略；\n    \n- Write Back（写回）策略；\n    \n\n实际开发中，Redis 和 MySQL 的更新策略用的是 Cache Aside，另外两种策略应用不了。\n\n\u003e Cache Aside（旁路缓存）策略\n\nCache Aside（旁路缓存）策略是最常用的，应用程序直接与「数据库、缓存」交互，并负责对缓存的维护，该策略又可以细分为「读策略」和「写策略」。\n\n![](statistic/asynccode-497.png)\n\n写策略的步骤：\n\n- 先更新数据库中的数据，再删除缓存中的数据。\n    \n\n读策略的步骤：\n\n- 如果读取的数据命中了缓存，则直接返回数据；\n    \n- 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。\n    \n\n注意，写策略的步骤的顺序不能倒过来，即不能先删除缓存再更新数据库，原因是在「读+写」并发的时候，会出现缓存和数据库的数据不一致性的问题。\n\n举个例子，假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21。\n\n![](statistic/asynccode-493.png)\n\n最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库的数据不一致。\n\n为什么「先更新数据库再删除缓存」不会有数据不一致的问题？\n\n继续用「读 + 写」请求的并发的场景来分析。\n\n假如某个用户数据在缓存中不存在，请求 A 读取数据时从数据库中查询到年龄为 20，在未写入缓存中时另一个请求 B 更新数据。它更新数据库中的年龄为 21，并且清空缓存。这时请求 A 把从数据库中读到的年龄为 20 的数据写入到缓存中。\n\n![](statistic/asynccode-493.png)\n\n最终，该用户年龄在缓存中是 20（旧值），在数据库中是 21（新值），缓存和数据库数据不一致。 从上面的理论上分析，先更新数据库，再删除缓存也是会出现数据不一致性的问题，但是在实际中，这个问题出现的概率并不高。\n\n因为缓存的写入通常要远远快于数据库的写入，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况。而一旦请求 A 早于请求 B 删除缓存之前更新了缓存，那么接下来的请求就会因为缓存不命中而从数据库中重新读取数据，所以不会出现这种不一致的情况。\n\nCache Aside 策略适合读多写少的场景，不适合写多的场景，因为当写入比较频繁时，缓存中的数据会被频繁地清理，这样会对缓存的命中率有一些影响。如果业务对缓存命中率有严格的要求，那么可以考虑两种解决方案：\n\n- 一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响；\n    \n- 另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受。\n    \n\n\u003e Read/Write Through（读穿 / 写穿）策略\n\nRead/Write Through（读穿 / 写穿）策略原则是应用程序只和缓存交互，不再和数据库交互，而是由缓存和数据库交互，相当于更新数据库的操作由缓存自己代理了。\n\n_1、Read Through 策略_\n\n先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库查询数据，并将结果写入到缓存组件，最后缓存组件将数据返回给应用。\n\n_2、Write Through 策略_\n\n当有数据更新的时候，先查询要写入的数据在缓存中是否已经存在：\n\n- 如果缓存中数据已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，然后缓存组件告知应用程序更新完成。\n    \n- 如果缓存中数据不存在，直接更新数据库，然后返回；\n    \n\n下面是 Read Through/Write Through 策略的示意图：\n\n![](statistic/asynccode-500.png)\n\nRead Through/Write Through 策略的特点是由缓存节点而非应用程序来和数据库打交道，在我们开发过程中相比 Cache Aside 策略要少见一些，原因是我们经常使用的分布式缓存组件，无论是 Memcached 还是 Redis 都不提供写入数据库和自动加载数据库中的数据的功能。而我们在使用本地缓存的时候可以考虑使用这种策略。\n\n\u003e Write Back（写回）策略\n\nWrite Back（写回）策略在更新数据的时候，只更新缓存，同时将缓存数据设置为脏的，然后立马返回，并不会更新数据库。对于数据库的更新，会通过批量异步更新的方式进行。\n\n实际上，Write Back（写回）策略也不能应用到我们常用的数据库和缓存的场景中，因为 Redis 并没有异步更新数据库的功能。\n\nWrite Back 是计算机体系结构中的设计，比如 CPU 的缓存、操作系统中文件系统的缓存都采用了 Write Back（写回）策略。\n\nWrite Back 策略特别适合写多的场景，因为发生写操作的时候， 只需要更新缓存，就立马返回了。比如，写文件的时候，实际上是写入到文件系统的缓存就返回了，并不会写磁盘。\n\n但是带来的问题是，数据不是强一致性的，而且会有数据丢失的风险，因为缓存一般使用内存，而内存是非持久化的，所以一旦缓存机器掉电，就会造成原本缓存中的脏数据丢失。所以你会发现系统在掉电之后，之前写入的文件会有部分丢失，就是因为 Page Cache 还没有来得及刷盘造成的。\n\n这里贴一张 CPU 缓存与内存使用 Write Back 策略的流程图：\n\n![](statistic/asynccode-506.png)\n\n有没有觉得这个流程很熟悉？因为我在写 [CPU 缓存文章 (opens new window)](https://xiaolincoding.com/os/1_hardware/cpu_mesi.html#%E5%86%99%E5%9B%9E)的时候提到过。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E7%BC%93%E5%AD%98%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7)如何保证缓存和数据库数据的一致性？\n\nTIP\n\n推荐阅读：[数据库和缓存如何保证一致性？(opens new window)](https://xiaolincoding.com/redis/architecture/mysql_redis_consistency.html)\n\n## [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%AE%9E%E6%88%98)Redis 实战\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97)Redis 如何实现延迟队列？\n\n延迟队列是指把当前要做的事情，往后推迟一段时间再做。延迟队列的常见使用场景有以下几种：\n\n- 在淘宝、京东等购物平台上下单，超过一定时间未付款，订单会自动取消；\n    \n- 打车的时候，在规定时间没有车主接单，平台会取消你的单并提醒你暂时没有车主接单；\n    \n- 点外卖的时候，如果商家在10分钟还没接单，就会自动取消订单；\n    \n\n在 Redis 可以使用有序集合（ZSet）的方式来实现延迟消息队列的，ZSet 有一个 Score 属性可以用来存储延迟执行的时间。\n\n使用 zadd score1 value1 命令就可以一直往内存中生产消息。再利用 zrangebysocre 查询符合条件的所有待处理的任务， 通过循环执行队列任务即可。\n\n![](statistic/asynccode-505.png)\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E7%9A%84%E5%A4%A7-key-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86)Redis 的大 key 如何处理？\n\n\u003e 什么是 Redis 大 key？\n\n大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。\n\n一般而言，下面这两种情况被称为大 key：\n\n- String 类型的值大于 10 KB；\n    \n- Hash、List、Set、ZSet 类型的元素的个数超过 5000个；\n    \n\n\u003e 大 key 会造成什么问题？\n\n大 key 会带来以下四种影响：\n\n- 客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。\n    \n- 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。\n    \n- 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。\n    \n- 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。\n    \n\n\u003e 如何找到大 key ？\n\n_1、redis-cli --bigkeys 查找大key_\n\n可以通过 redis-cli --bigkeys 命令查找大 key：\n\n```Shell\nredis-cli -h 127.0.0.1 -p6379 -a \"password\" -- bigkeys\n```\n\n使用的时候注意事项：\n\n- 最好选择在从节点上执行该命令。因为主节点上执行时，会阻塞主节点；\n    \n- 如果没有从节点，那么可以选择在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；或者可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能。\n    \n\n该方式的不足之处：\n\n- 这个方法只能返回每种类型中最大的那个 bigkey，无法得到大小排在前 N 位的 bigkey；\n    \n- 对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。但是，一个集合中的元素个数多，并不一定占用的内存就多。因为，有可能每个元素占用的内存很小，这样的话，即使元素个数有很多，总内存开销也不大；\n    \n\n_2、使用 SCAN 命令查找大 key_\n\n使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。\n\n对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。\n\n对于集合类型来说，有两种方法可以获得它占用的内存大小：\n\n- 如果能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。List 类型：`LLEN` 命令；Hash 类型：`HLEN` 命令；Set 类型：`SCARD` 命令；Sorted Set 类型：`ZCARD` 命令；\n    \n- 如果不能提前知道写入集合的元素大小，可以使用 `MEMORY USAGE` 命令（需要 Redis 4.0 及以上版本），查询一个键值对占用的内存空间。\n    \n\n_3、使用 RdbTools 工具查找大 key_\n\n使用 RdbTools 第三方开源工具，可以用来解析 Redis 快照（RDB）文件，找到其中的大 key。\n\n比如，下面这条命令，将大于 10 kb 的 key 输出到一个表格文件。\n\n```Shell\nrdb dump.rdb -c memory --bytes 10240 -f redis.csv\n```\n\n\u003e 如何删除大 key？\n\n删除操作的本质是要释放键值对占用的内存空间，不要小瞧内存的释放过程。\n\n释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序。\n\n所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞，如果主线程发生了阻塞，其他所有请求可能都会超时，超时越来越多，会造成 Redis 连接耗尽，产生各种异常。\n\n因此，删除大 key 这一个动作，我们要小心。具体要怎么做呢？这里给出两种方法：\n\n- 分批次删除\n    \n- 异步删除（Redis 4.0版本以上）\n    \n\n_1、分批次删除_\n\n对于删除大 Hash，使用 `hscan` 命令，每次获取 100 个字段，再用 `hdel` 命令，每次删除 1 个字段。\n\nPython代码：\n\n```Python\ndef del_large_hash():\n  r = redis.StrictRedis(host='redis-host1', port=6379)\n    large_hash_key =\"xxx\" #要删除的大hash键名\n    cursor = '0'while cursor != 0:# 使用 hscan 命令，每次获取 100 个字段\n        cursor, data = r.hscan(large_hash_key, cursor=cursor, count=100)for item in data.items():# 再用 hdel 命令，每次删除1个字段\n                r.hdel(large_hash_key, item[0])\n```\n\n对于删除大 List，通过 `ltrim` 命令，每次删除少量元素。\n\nPython代码：\n\n```Python\ndef del_large_list():\n  r = redis.StrictRedis(host='redis-host1', port=6379)\n  large_list_key = 'xxx'  #要删除的大list的键名while r.llen(large_list_key)\u003e0:#每次只删除最右100个元素\n      r.ltrim(large_list_key, 0, -101) \n```\n\n对于删除大 Set，使用 `sscan` 命令，每次扫描集合中 100 个元素，再用 `srem` 命令每次删除一个键。\n\nPython代码：\n\n```Python\ndef del_large_set():\n  r = redis.StrictRedis(host='redis-host1', port=6379)\n  large_set_key = 'xxx'   # 要删除的大set的键名\n  cursor = '0'while cursor != 0:# 使用 sscan 命令，每次扫描集合中 100 个元素\n    cursor, data = r.sscan(large_set_key, cursor=cursor, count=100)for item in data:# 再用 srem 命令每次删除一个键\n      r.srem(large_size_key, item)\n```\n\n对于删除大 ZSet，使用 `zremrangebyrank` 命令，每次删除 top 100个元素。\n\nPython代码：\n\n```Python\ndef del_large_sortedset():\n  r = redis.StrictRedis(host='large_sortedset_key', port=6379)\n  large_sortedset_key='xxx'while r.zcard(large_sortedset_key)\u003e0:# 使用 zremrangebyrank 命令，每次删除 top 100个元素\n    r.zremrangebyrank(large_sortedset_key,0,99) \n```\n\n_2、异步删除_\n\n从 Redis 4.0 版本开始，可以采用异步删除法，用 unlink 命令代替 del 来删除。\n\n这样 Redis 会将这个 key 放入到一个异步线程中进行删除，这样不会阻塞主线程。\n\n除了主动调用 unlink 命令实现异步删除之外，我们还可以通过配置参数，达到某些条件的时候自动进行异步删除。\n\n主要有 4 种场景，默认都是关闭的：\n\n```Plain\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del\nnoslave-lazy-flush no\n```\n\n它们代表的含义如下：\n\n- lazyfree-lazy-eviction：表示当 Redis 运行内存超过 maxmeory 时，是否开启 lazy free 机制删除；\n    \n- lazyfree-lazy-expire：表示设置了过期时间的键值，当过期之后是否开启 lazy free 机制删除；\n    \n- lazyfree-lazy-server-del：有些指令在处理已存在的键时，会带有一个隐式的 del 键的操作，比如 rename 命令，当目标键已存在，Redis 会先删除目标键，如果这些目标键是一个 big key，就会造成阻塞删除的问题，此配置表示在这种场景中是否开启 lazy free 机制删除；\n    \n- slave-lazy-flush：针对 slave (从节点) 进行全量数据同步，slave 在加载 master 的 RDB 文件前，会运行 flushall 来清理自己的数据，它表示此时是否开启 lazy free 机制删除。\n    \n\n建议开启其中的 lazyfree-lazy-eviction、lazyfree-lazy-expire、lazyfree-lazy-server-del 等配置，这样就可以有效的提高主线程的执行效率。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E7%AE%A1%E9%81%93%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8)Redis 管道有什么用？\n\n管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能。\n\n普通命令模式，如下图所示：\n\n![](statistic/asynccode-496.png)\n\n管道模式，如下图所示：\n\n![](statistic/asynccode-497.png)\n\n使用管道技术可以解决多个命令执行时的网络等待，它是把多个命令整合到一起发送给服务器端处理之后统一返回给客户端，这样就免去了每条命令执行后都要等待的情况，从而有效地提高了程序的执行效率。\n\n但使用管道技术也要注意避免发送的命令过大，或管道内的数据太多而导致的网络阻塞。\n\n要注意的是，管道技术本质上是客户端提供的功能，而非 Redis 服务器端的功能。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E4%BA%8B%E5%8A%A1%E6%94%AF%E6%8C%81%E5%9B%9E%E6%BB%9A%E5%90%97)Redis 事务支持回滚吗？\n\nMySQL 在执行事务时，会提供回滚机制，当事务执行发生错误时，事务中的所有操作都会撤销，已经修改的数据也会被恢复到事务执行前的状态。\n\nRedis 中并没有提供回滚机制，虽然 Redis 提供了 DISCARD 命令，但是这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。\n\n下面是 DISCARD 命令用法：\n\n```C\n#读取 count 的值4127.0.0.1:6379\u003e GET count\n\"1\"\n#开启事务\n127.0.0.1:6379\u003e MULTI \nOK\n#发送事务的第一个操作，对count减1127.0.0.1:6379\u003e DECR count\nQUEUED\n#执行DISCARD命令，主动放弃事务\n127.0.0.1:6379\u003e DISCARD\nOK\n#再次读取a:stock的值，值没有被修改\n127.0.0.1:6379\u003e GET count\n\"1\"\n```\n\n事务执行过程中，如果命令入队时没报错，而事务提交后，实际执行时报错了，正确的命令依然可以正常执行，所以这可以看出 Redis 并不一定保证原子性（原子性：事务中的命令要不全部成功，要不全部失败）。\n\n比如下面这个例子：\n\n```C\n#获取name原本的值\n127.0.0.1:6379\u003e GET name\n\"xiaolin\"\n#开启事务\n127.0.0.1:6379\u003e MULTI\nOK\n#设置新值\n127.0.0.1:6379(TX)\u003e SET name xialincoding\nQUEUED\n#注意，这条命令是错误的\n# expire 过期时间正确来说是数字，并不是‘10s’字符串，但是还是入队成功了127.0.0.1:6379(TX)\u003e EXPIRE name 10s\nQUEUED\n#提交事务，执行报错\n#可以看到 set 执行成功，而 expire 执行错误。\n127.0.0.1:6379(TX)\u003e EXEC\n1) OK\n2) (error) ERR value is not an integer or out of range\n#可以看到，name 还是被设置为新值了\n127.0.0.1:6379\u003e GET name\n\"xialincoding\"\n```\n\n\u003e 为什么Redis 不支持事务回滚？\n\nRedis [官方文档 (opens new window)](https://redis.io/topics/transactions)的解释如下：\n\n![](statistic/asynccode-501.png)\n\n大概的意思是，作者不支持事务回滚的原因有以下两个：\n\n- 他认为 Redis 事务的执行时，错误通常都是编程错误造成的，这种错误通常只会出现在开发环境中，而很少会在实际的生产环境中出现，所以他认为没有必要为 Redis 开发事务回滚功能；\n    \n- 不支持事务回滚是因为这种复杂的功能和 Redis 追求的简单高效的设计主旨不符合。\n    \n\n这里不支持事务回滚，指的是不支持事务运行时错误的事务回滚。\n\n### [#](https://xiaolincoding.com/redis/base/redis_interview.html#%E5%A6%82%E4%BD%95%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84)如何用 Redis 实现分布式锁的？\n\n分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用。如下图所示：\n\n![](statistic/asynccode-499.png)\n\nRedis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁，而且 Redis 的读写性能高，可以应对高并发的锁操作场景。\n\nRedis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁：\n\n- 如果 key 不存在，则显示插入成功，可以用来表示加锁成功；\n    \n- 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。\n    \n\n基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。\n\n- 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；\n    \n- 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；\n    \n- 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端；\n    \n\n满足这三个条件的分布式命令如下：\n\n```C\nSET lock_key unique_value NX PX 10000 \n```\n\n- lock_key 就是 key 键；\n    \n- unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；\n    \n- NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；\n    \n- PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。\n    \n\n而解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除。\n\n可以看到，解锁是有两个操作，这时就需要 Lua 脚本来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。\n\n```C\n// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放if redis.call(\"get\",KEYS[1]) == ARGV[1] then\n    return redis.call(\"del\",KEYS[1])elsereturn 0\nend\n```\n\n这样一来，就通过使用 SET 命令和 Lua 脚本在 Redis 单节点上完成了分布式锁的加锁和解锁。\n\n\u003e 基于 Redis 实现分布式锁有什么优缺点？\n\n基于 Redis 实现分布式锁的优点：\n\n1. 性能高效（这是选择缓存实现分布式锁最核心的出发点）。\n    \n2. 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。\n    \n3. 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。\n    \n\n基于 Redis 实现分布式锁的缺点：\n\n- 超时时间不好设置。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。\n    \n    - 那么如何合理设置超时时间呢？ 我们可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。\n        \n- Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。\n    \n\n\u003e Redis 如何解决集群情况下分布式锁的可靠性？\n\n为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。\n\n它是基于多个 Redis 节点的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点。\n\nRedlock 算法的基本思路，是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。\n\n这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。\n\nRedlock 算法加锁三个过程：\n\n- 第一步是，客户端获取当前时间（t1）。\n    \n- 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作：\n    \n    - 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。\n        \n    - 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。\n        \n- 第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 \u003c 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。\n    \n\n可以看到，加锁成功要同时满足两个条件（_简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功_）：\n\n- 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁；\n    \n- 条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间。\n    \n\n加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁最初设置的过期时间」减去「客户端从大多数节点获取锁的总耗时（t2-t1）」。如果计算的结果已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。\n\n加锁失败后，客户端向所有 Redis 节点发起释放锁的操作，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。","lastmodified":"2023-07-31T03:20:22.325088367Z","tags":[]},"/docker/%E4%B8%8E%E5%A4%96%E7%95%8C%E8%B5%84%E6%BA%90%E7%9A%84%E5%85%B1%E4%BA%AB":{"title":"与外界资源的共享","content":"# 如何拷贝容器内的数据\n\ncp 命令，它可以在宿主机和容器之间拷贝文件，是最基本的一种数据交换功能。\n\ndocker cp 的用法很简单，很类似 Linux 的“cp”“scp”，指定源路径（src path）和目标路径（dest path）就可以了。如果源路径是宿主机那么就是把文件拷贝进容器，如果源路径是容器那么就是把文件拷贝出容器，**注意需要用容器名或者容器 ID 来指明是哪个容器的路径**。\n\n```Go\n\ndocker cp a.txt 062:/tmp\n```\n\n# 如何共享主机上的文件\n\n只需要在 docker run 命令启动容器的时候使用 **-v 参数就行，具体的格式是“宿主机路径: 容器内路径”。**\n\n我还是以 Redis 为例，启动容器，使用 -v 参数把本机的“/tmp”目录挂载到容器里的“/tmp”目录，也就是说让容器共享宿主机的“/tmp”目录\n\n```Go\n\ndocker run -d --rm -v /tmp:/tmp redis\n```\n\n# 如何实现网络互通\n\nDocker 提供了三种网络模式，分别是 null、host 和 bridge。\n\n- null 是最简单的模式，也就是没有网络，但允许其他的网络插件来自定义网络连接，这里就不多做介绍了。\n    \n- host 的意思是直接使用宿主机网络，相当于去掉了容器的网络隔离（其他隔离依然保留），**所有的容器会共享宿主机的 IP 地址和网卡**。这种模式没有中间层，自然通信效率高，**但缺少了隔离**，运行太多的容器也容易导致端口冲突。host 模式需要在 docker run 时使用 --net=host 参数，下面我就用这个参数启动 Nginx：\n    \n\n```Go\n\ndocker run -d --rm --net=host nginx:alpine\n```\n\n- 第三种 bridge，也就是桥接模式，它有点类似现实世界里的交换机、路由器，只不过是由软件虚拟出来的，容器和宿主机再通过虚拟网卡接入这个网桥（图中的 docker0），那么它们之间也就可以正常的收发网络数据包了\n    \n\n# 如何分配服务端口号\n\n使用 host 模式或者 bridge 模式，我们的容器就有了 IP 地址，建立了与外部世界的网络连接，接下来要解决的就是网络服务的端口号问题。\n\n一台主机上的端口号数量是有限的，**而且多个服务之间还不能够冲突，但我们打包镜像应用的时候通常都使用的是默认端口**，容器实际运行起来就很容易因为**端口号被占用而无法启动**。\n\n解决这个问题的方法就是加入一个“中间层”，由容器环境例如 Docker 来统一管理分配端口号，在本机端口和容器端口之间做一个“映射”操作\n\n**端口号映射需要使用 bridge 模式**，并且在 docker run 启动容器时使用 -p 参数，形式和共享目录的 -v 参数很类似，用 : 分隔本机端口和容器端口\n\n```Go\n\ndocker run -d -p 80:80 --rm nginx:alpine\ndocker run -d -p 8080:80 --rm nginx:alpine\n\n# 这样就把本机的 80 和 8080 端口分别“映射”到了两个容器里的 80 端口\n```","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/docker/%E5%A6%82%E4%BD%95%E5%AE%B9%E5%99%A8%E5%8C%96%E5%BA%94%E7%94%A8":{"title":"如何容器化应用","content":"  \n\n镜像是容器的静态形式，它打包了应用程序的所有运行依赖项，方便保存和传输。使用容器技术运行镜像，就形成了动态的容器，由于镜像只读不可修改，所以应用程序的运行环境总是一致的。\n\n  \n\n容器化的应用就是指以镜像的形式打包应用程序，然后在容器环境里从镜像启动容器。\n\n  \n\n我这里就对今天的镜像操作和容器操作做个小结：\n\n- 常用的镜像操作有 docker pull、docker images、docker rmi，分别是拉取镜像、查看镜像和删除镜像。\n    \n- 用来启动容器的 docker run 是最常用的命令，它有很多参数用来调整容器的运行状态，对于后台服务来说应该使用 -d。\n    \n- docker exec 命令可以在容器内部执行任意程序，对于调试排错特别有用。\n    \n- 其他常用的容器操作还有 docker ps、docker stop、docker rm，用来查看容器、停止容器和删除容器。","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/docker/Docker-%E5%88%9D%E5%AD%A6%E8%80%85%E5%91%BD%E4%BB%A4":{"title":"Docker 初学者命令","content":"https://docs.docker.com/reference/\n\n# **一、基础命令**\n\n1. ## **帮助命令**\n    \n\n```JavaScript\ndocker --version  # 显示docker的版本信息\ndocker info    # 显示docker的系统信息\ndocker 命令 --help    # 显示帮助命令\n```\n\n2. ## **镜像命令**\n    \n\n1. ### **搜索镜像**\n    \n\n```Go\n# 搜索镜像\ndocker search mysql\n# 条件过滤搜索结果\ndocker search --filter=STARS=5000\n```\n\n![](statistic/asynccode-569.png)\n\n「列表解释」\n\n- 「NAME: 镜像名称」\n    \n- 「DESCRIPTION: 镜像介绍」\n    \n- 「STARS: 镜像的stars」\n    \n- 「OFFICIAL: 是否是官方提供的」\n    \n- 「AUTOMATED: 是不是自动化的」\n    \n\n### **2.2 拉取镜像**\n\n```Go\n# 默认拉取最新的镜像\ndocker pull mysql\n# 指定版本下载\ndocker pull mysql:5.7\n```\n\n#### **2.3 查看所有镜像**\n\n```Go\n# 查看所有镜像信息\ndocker images -a\n# 查看所有的镜像id\ndocker images -aq\n```\n\n![](statistic/asynccode-566.png)\n\n「列表解释」\n\n- 「REPOSITORY: 镜像的仓库源」\n    \n- 「TAG: 镜像的标签」\n    \n- 「IMAGE ID: 镜像的id」\n    \n- 「CREATE: 镜像的创建时间」\n    \n- 「SIZE: 镜像的大小」\n    \n\n### 2.4 删除镜像\n\n```JavaScript\n#删除指定id的镜像\ndocker rmi 镜像id\ndocker rmi 镜像id 镜像id 镜像id 镜像id\n#删除指定名称的镜像\ndocker rmi mysql:5.7\n#迭代删除所有的镜像\ndocker rmi -f $(docker images  -aq)\n```\n\n3. ## **[容器](https://cloud.tencent.com/product/tke?from=10680)****命令**\n    \n\n### **3.1 运行镜像**\n\n```JavaScript\ndocker run [可选参数] image\n# 运行实例\ndocker run --name=mycat -d -p 8080:8080 tomcat\n# 用完即删\ndocker run -it --rm tomcat\n# 指定环境变量（实例）\ndocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms64m -Xmx512m\"  elasticsearch:7.6.2\n```\n\n「参数说明」\n\n- 「--name=\"Name\"：容器名字 tomacat01、tomacat02区分容器」\n    \n- -「e」: 「指定环境变量」\n    \n- 「-d：后台守护进程运行」\n    \n- 「-it：使用交互方式运行，进入容器查看内容」\n    \n- 「-p：指定容器的端口 -p 8080:8080」\n    \n    - 「-p ：主机端口：容器端口」\n        \n    - 「-p ：容器端口」\n        \n- 「-P：随机指定端口」\n    \n- 「-v: 指定数据卷」\n    \n    - 「-v 容器文件位置:[宿主机](https://cloud.tencent.com/product/cdh?from=10680)文件位置」\n        \n- 「--volumes-from: 指定容器的数据卷共享（指定谁，就同步谁的数据！继承！）」\n    \n    - 「--volumes-from:继承自那个容器」（父容器删除不影响已存在数据）\n        \n- 「--net: 缺省 bridge」\n    \n\n### **3.2 进入容器**\n\n```Go\n# 运行一个centos并进入到容器里面\ndocker run -it centos /bin/bash\n# 退出容器\nexit\n```\n\n#### **3.3 查看容器**\n\n```Go\n# 查看正在运行中的容器\ndocker ps\n# 查看所有容器\ndocker ps -a\n```\n\n#### **3.4 退出容器**\n\n```JavaScript\nexit   # 直接容器停止并退出\nCtrl + P + Q  # 容器退出不停止\n```\n\n#### **3.5 删除容器**\n\n```Go\n# 删除指定容器\ndocker rm bde00bc086cf\n# 强制删除运行中的容器\ndocker rm -f bde00bc086cf\n# 迭代删除全部的容器\ndocker rm -f $(docker ps -aq)\n```\n\n#### **3.6 容器的启动与停止**\n\n```Go\n# 启动容器\ndocker start 容器id\n# 重启容器\ndocker restart 容器id\n# 停止容器\ndocker stop 容器id\n# 强制杀死容器\ndocker kill 容器id\n```\n\n#### **3.7 进入当前在正在运行中的命令**\n\n```Go\n# 进入到指定容器内部进行修改  开启一个新的终端\ndocker exec -it 0cd4d9d94de2 /bin/bash\n# 进入到正在执行中的终端\ndocker attach 容器id\n```\n\n#### **3.8 将文件从容器拷贝到宿主机上**\n\n```JavaScript\ndocker cp 容器id:容器内文件的路径 宿主机路径\n#实例\ndocker cp 0cd4d9d94de2:/Test.java /Test.java\n```\n\n#### **3.9 其他常用命令**\n\n**「查看日志命令」**\n\n```Go\n# 查看容器运行产生的日志\ndocker logs -ft --tail 10 容器id\n```\n\n复制\n\n「参数解析：」\n\n- 「f: 格式化日志」\n    \n- 「t: 携带日志时间戳」\n    \n\n**「查看进程」**\n\n```Go\n# 查看cpu等信息\ndocker top 0cd4d9d94de2\n# 查看容器元信息\ndocker inspect 容器id\n```\n\n## **二、可视化面板**\n\n1. ## **安装**\n    \n\n```Go\n# 安装可视化面板 portainer （数据卷路径不可改变）\ndocker run -d -p 8088:9000 --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainer\n```\n\n![](statistic/asynccode-568.png)\n\nimage-20200906161505532\n\n## **三、提交容器为一个镜像**\n\n1. ## **提交容器**\n    \n\n```Go\n# 提交一个容器为一个镜像（将容器打包）\ndocker commit [可选参数] 服务id 自定义镜像名称[:版本标签]\n# 示例代码提交\ndocker commit  -a=\"huangfu\" -m=\"增加了主页\" 19329ae6df90  diytomcat:1.0\n```\n\n「参数解释：」\n\n- 「-a: 作者」\n    \n- 「-m: 备注」\n    \n- 「-c: 将Dockerfile指令应用于创建的映像」\n    \n- 「-p: 提交期间暂停容器（默认为true）」\n    \n\n## **四、Docker数据卷使用**\n\n1. ## **数据卷的基本使用**\n    \n\n```Go\n# 关联数据卷\ndocker run [可选参数] -v /主机路径/:/容器路径/ 镜像名称\n# 关联数据卷的实例命令\ndocker run -d -p 8080:8080 --name mytomcat -v /home/tomcat/webapps/:/usr/local/tomcat/webapps tomcat\n```\n\n2. ## **mysql安装实战**\n    \n\n```JavaScript\ndocker run -d -p 3366:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7\n```\n\n「命令解析：」\n\n- 「-d: 守护进程运行」\n    \n- 「-v: 添加数据卷（宿主机位置和容器位置映射）」\n    \n- 「-p: 堆对外映射端口」\n    \n- 「-e: 指定环境变量」\n    \n- 「--name: 容器名称」\n    \n\n## **五、Dockerfile**\n\n1. ## **构建镜像文件**\n    \n\n```Go\n# 创建一个Dockerfile\nvim Dockerfile\n\nFROM centos\n\nVOLUME [\"volume01\",\"volume02\"]CMD echo \"-----end---\"CMD /bin/bash\n\n:x\n\n# 构建docker镜像\n# -f dockerfile的路径   \n# -t 生成的镜像名称\n# . 以当前路径为上下文打包\ndocker build -f /home/docker-volom/Dockerfile -t huangfu/centos:1.0 .\n\n# 构建基本命令\ndocker build [OPTIONS] PATH | URL | -\n```\n\n2. ## **Dockerfile概念**\n    \n\n- 每个保留关键字（指令）都必须是大写字母\n    \n- 执行顺序从上到下\n    \n- `#` 表示注释\n    \n- 每一个指令都会创建提交一个新的镜像层并提交！\n    \n\n3. ## **Dockerfile语法浅析**\n    \n\n![](statistic/asynccode-570.png)\n\n- 「FROM: 基础镜像，一切都从这里开始构建」\n    \n- 「MAINTAINER: 镜像是谁写的，姓名+邮箱」\n    \n- 「RUN: 镜像构建需要运行的命令」\n    \n- 「ADD: 添加一个内容，比如需要添加一个tomcat，则需要传递一个压缩包，便于在容器内构建！」\n    \n- 「WORKDIR: 镜像的工作目录」\n    \n- 「VOLUME: 挂在的目录」\n    \n- 「EXPOSE: 暴露端口」\n    \n- 「CMD: 一个指令，指定这个容器启动的时候要运行的命令」\n    \n- 「ENTRYPOINT: 指定这个容器启动的时候要运行的命令！可以追加命令！」\n    \n- 「ONBUILD: 当构建一个被继承的Dockerfile 这个时候就会运行指令，触发命令！」\n    \n- 「COPY: 类似与ADD，将文件拷贝到镜像中」\n    \n- 「ENV：构建的时候设置环境变量」\n    \n\n```Go\n# 构建一个具有复杂命令行的centos\nvim Dockerfile\n\n# 镜像继承自centos\nFROM centos\n# 作者信息\nMAINTAINER huangfu\u003chuangfusuper@163.com\u003e\n# 设置环境变量\nENV MYPATH /usr/local\n# 设置工作目录\nWORKDIR $MYPATH\n# 执行命令安装指令\nRUN yum -y install vim\nRUN yum -y install net-tools\n# 暴露端口\nEXPOSE 80\n# 执行一些指令\nCMD echo \"-------end------\"CMD echo $MYPATHCMD /bin/bash\n\n:x\n\n# 构建镜像\ndocker build -f /home/docker-volom/Dockerfile -t huangfu/diycentos:1.0 .\n```\n\n## **六、自定义网络**\n\n1. ## **网络模式详解**\n    \n\n- 「bridge: 桥接网络（默认）」\n    \n- 「host：和宿主机共享」\n    \n- 「none：不配置网络」\n    \n- 「container：容器网络联通」\n    \n\n2. ## **查看所有的网络模式**\n    \n\n```Go\n# 查看所有的网络模式\ndocker network ls\n```\n\n3. ## **创建自定义的网络**\n    \n\n```Go\n# 创建一个网络\ndocker network create [OPTIONS] NETWORK\n\n# 创建一个mynet\n# create 创建\n# driver 使用的网络模式\n# subnet 子网掩码\n# gateway 网关\n# mynety 自定义的名称\ndocker netywork create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynety\n```\n\n4. ## **使用自定义网络**\n    \n\n```JavaScript\ndocker run -d --net mynety --name tom01  tomcat\ndocker run -d --net mynety --name tom02  tomcat\n\n# 进入到tom02\ndocker exec -it 7d75a637a90b865fe70259bd4e0b3f5c95133dc65693b05abaf078d31a362529 /bin/bash\n# 结果是互通的\nping tom01\n```\n\n![](statistic/asynccode-567.png)\n\n  \n\n5. ## **容器网络互通**\n    \n    ```Go\n    # 把自定义网络和容器打通    容器一个容器两个ip\n    # 把不在该网络的容器加入当前网络\n    docker network connect 自定义网络 容器\n    ```\n    \n\n## **七、打包SpringBoot jar项目**\n\n1. ## **Dockerfile编写**\n    \n\n```JavaScript\nFROM java:8COPY *.jar /app.jar\n\nCMD [\"--server.port=8080\"]EXPOSE 8080ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]\n```\n\n  \n\n2. ## **构建镜像**\n    \n\n```JavaScript\nmkdir idea\n\ncd idea\n\n# 将 Dockerfile与jar包发送到idea目录\n# 构建镜像\ndocker build -t huangfutest:1.0 .\n# 后面运行不说了\n```","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/docker/Docker-%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93":{"title":"Docker 镜像仓库","content":"# 什么是镜像仓库（Registry）\n\n![](statistic/asynccode-565.png)\n\n  \n\n右边的区域就是镜像仓库，术语叫 Registry，直译就是“注册中心”，意思是所有镜像的 Repository 都在这里登记保管，就像是一个巨大的档案馆。\n\n然后我们再来看左边的“docker pull”，虚线显示了它的工作流程，先到“Docker daemon”，再到 Registry，只有当 Registry 里存有镜像才能真正把它下载到本地。\n\n# 什么是 Docker Hub\n\n什么是 Docker Hub\n\n“Docker Hub”（https://hub.docker.com/）。\n\n# 如何在 Docker Hub 上挑选镜像\n\ndocker Hub 上有**官方镜像、认证镜像和非官方镜像**的区别。\n\n- 官方镜像（Official image）：是指 Docker 公司官方提供的高质量镜像（https://github.com/docker-library/official-images），都经过了严格的漏洞扫描和安全检测，支持 x86_64、arm64 等多种硬件架构，还具有清晰易读的文档，一般来说是我们构建镜像的首选，也是我们编写 Dockerfile 的最佳范例。\n    \n- 认证镜像（Verified publisher）：也就是认证发行商，比如 Bitnami、Rancher、Ubuntu 等。它们都是颇具规模的大公司，具有不逊于 Docker 公司的实力，所以就在 Docker Hub 上开了个认证账号\n    \n- 非官方镜像\n    \n    - “半官方”镜像。因为成为“Verified publisher”是要给 **Docker 公司交钱的**，而很多公司不想花这笔“冤枉钱”，所以只在 Docker Hub 上开了公司账号（比如 OpenResty ）\n        \n    - 第二类就是纯粹的“民间”镜像了，通常是个人上传到 Docker Hub 的，因为条件所限，测试不完全甚至没有测试，质量上难以得到保证，下载的时候需要小心谨慎。\n        \n\n  \n\n  \n\n# **Docker Hub 上镜像命名的规则是什么**\n\n- Docker Hub 也使用了同样的规则，就是“用户名 / 应用名”的形式，比如 bitnami/nginx、ubuntu/nginx、rancher/nginx 等等。\n    \n- 因为镜像还会有许多不同的版本，也就是“标签”（tag）。\n    \n    - 直接使用默认的“latest”虽然简单方便\n        \n    - 通常来说，镜像标签的格式是应用的版本号加上操作系统。\n        \n    \n    ![](statistic/asynccode-564.png)\n    \n    - 另外，有的标签还会加上 slim、fat，来进一步表示这个镜像的内容是经过精简的，还是包含了较多的辅助工具。\n        \n\n# 离线环境该怎么办\n\n- 在内网环境里仿造 Docker Hub，创建一个自己的私有 Registry 服务，比如 Docker Registry，还有 CNCF Harbor，\n    \n- Docker 提供了 save 和 load 这两个镜像归档命令，可以把镜像导出成压缩包，或者从压缩包导入 Docker，而压缩包是非常容易保管和传输的，可以联机拷贝，FTP 共享，甚至存在 U 盘上随身携带。\n    \n\n```Go\n\ndocker save ngx-app:latest -o ngx.tar\ndocker load -i ngx.tar\n```","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/docker/docker-compose":{"title":"docker compose","content":"[docker compose 文档](https://docs.docker.com/compose/)\n\n  \n\nCompose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。\n\nCompose 使用的三个步骤：\n\n- 使用 Dockerfile 定义应用程序的环境。\n    \n- 使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行。\n    \n- 最后，执行 docker-compose up 命令来启动并运行整个应用程序。\n    \n\n  \n\n# 使用\n\n## 准备\n\n```SQL\n$ mkdir composetest\n$ cd composetest\n```\n\n在测试目录中创建一个名为 app.py 的文件，并复制粘贴以下内容：\n\n```SQL\nimport time\n\nimport redis\nfrom flask import Flask\n\napp = Flask(__name__)\ncache = redis.Redis(host='redis', port=6379)\n\n\ndef get_hit_count():\n    retries = 5\n    while True:\n        try:\n            return cache.incr('hits')\n        except redis.exceptions.ConnectionError as exc:\n            if retries == 0:\n                raise exc\n            retries -= 1\n            time.sleep(0.5)\n\n\n@app.route('/')\ndef hello():\n    count = get_hit_count()\n    return 'Hello World! I have been seen {} times.\\n'.format(count)\n```\n\n在此示例中，redis 是应用程序网络上的 redis 容器的主机名，该主机使用的端口为 6379。\n\n在 composetest 目录中创建另一个名为 **requirements.txt** 的文件，内容如下：\n\n## Dockerfile\n\n```SQL\nFROM python:3.7-alpine\nWORKDIR /code\nENV FLASK_APP app.py\nENV FLASK_RUN_HOST 0.0.0.0\nRUN apk add --no-cache gcc musl-dev linux-headers\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"flask\", \"run\"]\n```\n\n- **FROM python:3.7-alpine**: 从 Python 3.7 映像开始构建镜像。\n    \n- **WORKDIR /code**: 将工作目录设置为 /code。\n    \n- 设置 flask 命令使用的环境变量。\n    \n\n```SQL\nENV FLASK_APP app.py\nENV FLASK_RUN_HOST 0.0.0.0\n```\n\n- **RUN apk add --no-cache gcc musl-dev linux-headers**: 安装 gcc，以便诸如 MarkupSafe 和 SQLAlchemy 之类的 Python 包可以编译加速。\n    \n- 复制 requirements.txt 并安装 Python 依赖项。\n    \n\n```SQL\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\n```\n\n- **COPY . .**: 将 . 项目中的当前目录复制到 . 镜像中的工作目录。\n    \n- **CMD [\"flask\", \"run\"]**: 容器提供默认的执行命令为：flask run\n    \n\n  \n\n## docker-compose.yml\n\n```SQL\n# yaml 配置\nversion: '3'\nservices:\n  web:\n    build: .\n    ports:\n     - \"5000:5000\"\n  redis:\n    image: \"redis:alpine\"\n```\n\n- **web**：该 web 服务使用从 Dockerfile 当前目录中构建的镜像。然后，它将容器和主机绑定到暴露的端口 5000。此示例服务使用 Flask Web 服务器的默认端口 5000 。\n    \n- **redis**：该 redis 服务使用 Docker Hub 的公共 Redis 映像。\n    \n\n  \n\n# 多个配置文件\n\n我们可以为同一个项目配置多个compose文件，使用多个 Compose 文件使您能够针对不同的环境或不同的工作流程自定义 Compose 应用程序。\n\n默认情况下，Compose 读取两个文件，`docker-compose.yml`和一个可选的`docker-compose.override.yml`文件。\n\n- `docker-compose.yml`包含您的基本配置。\n    \n- `docker-compose.override.yml` 文件，顾名思义，就是包含现有服务或全新服务的配置覆盖。\n    \n\n如果在两个文件中都定义了服务，Compose 会使用 override 进行合并配置。\n\n要使用多个覆盖文件或具有不同名称的覆盖文件，您可以使用该`-f`选项来指定文件列表。Compose 按照在命令行中指定的顺序合并文件。\n\n当您使用多个配置文件时，您必须确保文件中的所有路径都相对于基本 Compose 文件（ 指定的第一个 Compose 文件`-f`）\n\n```Bash\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n```\n\n  \n\n## docker-compose.yml的配置\n\n## version\n\n版本信息，定义关乎于docker的兼容性，Compose 文件格式有3个版本,分别为1, 2.x 和 3.x\n\n## build\n\n指定构建镜像的 dockerfile 的上下文路径，\n\n```Plain\nversion: \"3.9\"\nservices:\n  webapp:\n    build: ./dir #指定路径\n```\n\n或者详细配置对象。\n\n```SQL\nversion: \"3.9\"\nservices:\n  webapp:\n    build:\n      context: ./dir\n      dockerfile: Dockerfile-alternate\n      args:\n        buildno: 1\n```\n\n- context 上下文路径，可以是文件路径，也可以是到链接到 git 仓库的 url。当是相对路径时，它被解释为相对于 Compose 文件的位置。\n    \n- dockerfile 指定构建镜像的 Dockerfile 文件名\n    \n- args 构建参数，只能在构建过程中访问的环境变量\n    \n- cache_from 缓存解析镜像列表\n    \n- labels 设置构建镜像的元数据\n    \n- network 设置网络容器连接，`none` 表示在构建期间禁用网络\n    \n- shm_size 设置`/dev/shm`此构建容器的分区大小\n    \n- target 多阶段构建，可以指定构建哪一层\n    \n\n## network\n\n默认情况下，**Compose为您的应用程序设置单个网络。services 服务的每个容器都加入默认网络，并且可以被该网络上的其他容器访问。**\n\n您的应用程序网络的名称基于“项目名称”，也就是其所在目录的名称。您可以使用 --project-name 命令行选项 或 COMPOSE_PROJECT_NAME 环境变量覆盖项目名称。\n\n假设您的应用程序是在一个名为`myapp`目录下，`docker-compose.yml`\n\n```SQL\nversion: \"3.9\"\nservices:\n  web:\n    build: .\n    ports:\n      - \"8000:8000\"\n  db:\n    image: postgres\n    ports:\n      - \"8001:5432\"\n```\n\n运行`docker-compose up`，会发生以下情况\n\n1. 创建了一个名为 myapp_default 的网络。\n    \n2. 把`web`加入网络。\n    \n3. 把`db`加入网络。\n    \n\n上面例子还有一个注意点就是端口号，注意区分`HOST_PORT`和`CONTAINER_PORT`，以上面的db为例：\n\n- `8001` 是宿主机的端口\n    \n- `5432`（postgres的默认端口） 是容器的端口\n    \n\n当容器之间通讯时 ， 是通过 `CONTAINER_PORT` 来连接的。\n\n  \n\n我们可以通过设置一级配置network自定义网络，创建更复杂的网络选项，也可以用来连接已经存在的网络（不是通过compose创建的）\n\n  \n\n每个`service` 配置下也可以指定`networks`配置，来指定一级配置的网络\n\n```SQL\nversion: \"3\"\nservices:\n\n  proxy:\n    build: ./proxy\n    networks:\n      - frontend\n  app:\n    build: ./app\n    networks:\n      - frontend\n      - backend\n  db:\n    image: postgres\n    networks:\n      - backend\n\n# 一级配置networks 创建了自定义的网络\nnetworks:\n  frontend:\n    # Use a custom driver\n    driver: custom-driver-1\n  backend:\n    # Use a custom driver which takes special options\n    driver: custom-driver-2\n    driver_opts:\n      foo: \"1\"\n      bar: \"2\"\n```\n\n一级配置`networks` 创建了自定义的网络 。这里配置了两个`frontend`和`backend` ，且自定义了网络类型。\n\n  \n\n每一个services下，`proxy` , `app` , `db`都定义了`networks`配置。\n\n1. `proxy` 只加入到 `frontend`网络。\n    \n2. `db` 只加入到`backend`网络。\n    \n3. `app`同时加入到 `frontend`和`backend` 。\n    \n4. `db`和`proxy`不能通讯，因为不在一个网络中。\n    \n5. `app`和两个都能通讯，因为`app`在两个网络中都有配置。\n    \n6. `db`和`proxy`要通讯，只能通过`app`这个应用来连接。\n    \n\n  \n\n  \n\n同一网络上的其他容器可以使用服务名称或别名来连接到其他服务的容器\n\n```SQL\nservices:\n  some-service:\n    networks:\n      some-network:\n        aliases:\n          - alias1\n          - alias3\n      other-network:\n        aliases:\n          - alias2\n```\n\n加入网络时，还可以指定容器的静态 IP 地址\n\n```Plain\nversion: \"3.9\"\n\nservices:\n  app:\n    image: nginx:alpine\n    networks:\n      app_net:\n        ipv4_address: 172.16.238.10\n        ipv6_address: 2001:3984:3989::10\n\nnetworks:\n  app_net:\n    ipam:\n      driver: default\n      config:\n        - subnet: \"172.16.238.0/24\"\n        - subnet: \"2001:3984:3989::/64\"\n```\n\n### 一级networks还有如下这些配置：\n\n- driver 指定该网络应使用哪个驱动程序。默认使用`bridge`单个主机上的网络，overlay代表跨多个节点的网络群\n    \n\n```SQL\ndriver: overlay\n```\n\n- host or none 使用主机的网络堆栈，或者不使用网络。相当于`docker run --net=host`或`docker run --net=none`。仅在使用`docker stack`命令时使用。如果您使用该`docker-compose`命令，请改用 network_mode。\n    \n- driver_opts 将选项列表指定为键值对以传递给此网络的驱动程序\n    \n\n```SQL\ndriver_opts:\n  foo: \"bar\"\n  baz: 1\n```\n\n- attachable 仅在`driver`设置为 `overlay`时可用。如果设置为`true`，那么除了服务之外，独立容器也可以连接到此网络。\n    \n\n```SQL\nnetworks:\n  mynet1:\n    driver: overlay\n    attachable: true\n```\n\n- enable_ipv6 在此网络上启用 IPv6 网络。\n    \n- ipam 自定义 IPAM （IP地址管理）配置。\n    \n\n```SQL\nipam:\n  driver: default\n  config:\n    - subnet: 172.28.0.0/16\n```\n\n- internal 默认情况下，Docker 会将桥接网络连接到它提供外部连接。如果要创建外部隔离的覆盖网络，可以将此选项设置为true。\n    \n- labels 添加元数据\n    \n- external如果设置为`true`，则指定此网络是在 Compose 之外创建的。`docker-compose up`不会尝试创建它，如果它不存在，则会引发错误。在下面的例子中，`proxy`是通往外界的门户。\n    \n\n```SQL\nversion: \"3.9\"\n\nservices:\n  proxy:\n    build: ./proxy\n    networks:\n      - outside\n      - default\n  app:\n    build: ./app\n    networks:\n      - default\n\nnetworks:\n  outside:\n    external: true\n```\n\n- name为此网络设置自定义名称。\n    \n\n```SQL\nversion: \"3.9\"\nnetworks:\n  network1:\n    name: my-app-net\n```\n\n## cap_add, cap_drop\n\n添加或删除容器拥有的宿主机的内核功能。\n\n```Plain\ncap_add:\n  - ALL # 开启全部权限\n\ncap_drop:\n  - SYS_PTRACE # 关闭 ptrace权限\n```\n\n## cgroup_parent\n\n为容器指定父 cgroup 组，意味着将继承该组的资源限制。\n\n```Plain\ncgroup_parent: m-executor-abcd\n```\n\n## command\n\n覆盖容器启动后默认执行的命令\n\n```Plain\ncommand: bundle exec thin -p 3000\ncommand: [\"bundle\", \"exec\", \"thin\", \"-p\", \"3000\"]\n```\n\n  \n\n## configs\n\n为每个服务赋予相应的configs访问权限\n\n- 简短语法，指定配置名称即可。以下示例授予`redis`服务访问`my_config`和`my_other_config`configs 的权限。\n    \n\n```Plain\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis:latest\n    deploy:\n      replicas: 1\n    configs:\n      - my_config\n      - my_other_config\nconfigs:\n  my_config:\n    file: ./my_config.txt\n  my_other_config:\n    external: true\n```\n\n- 长语法\n    \n    - `source`：配置名称\n        \n    - `target`：要挂载文件的路径和名称\n        \n    - `uid`和`gid`：容器的数字 UID 或 GID\n        \n    - `mode`：挂载在服务的任务容器中的文件的权限，以八进制表示。例如，`0444` 代表可读。\n        \n    - 以下示例在容器内设置configs名称为`my_config ，路径为redis_config`，将模式设置为`0440`（组可读）并将用户和组设置为`103`。该`redis`服务无权访问`my_other_config`配置。\n        \n\n```Bash\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis:latest\n    deploy:\n      replicas: 1\n    configs:\n      - source: my_config\n        target: /redis_config\n        uid: '103'\n        gid: '103'\n        mode: 0440\nconfigs:\n  my_config:\n    file: ./my_config.txt\n  my_other_config:\n    external: true\n```\n\n一级`configs`详细配置：\n\n- `file`: 使用指定路径中的文件内容创建配置。\n    \n- `external`: 如果设置为 true，则指定此配置已经创建。Docker 不会尝试创建它，如果它不存在， 会报错`config not found`。\n    \n- `name`: Docker 中配置对象的名称。此字段可用于引用包含特殊字符的配置。\n    \n- `driver`和`driver_opts`：自定义驱动程序的名称，以及作为键/值对传递的特定于驱动程序的选项。\n    \n- `template_driver`：要使用的模板驱动程序的名称，它控制是否以及如何将配置负载作为模板。如果未设置驱动程序，则不使用模板。当前支持的唯一驱动程序是`golang`，它使用`golang`。\n    \n\n在下面例子中，`my_first_config`是通过confif_data文件内容创建的（就像 `\u003cstack_name\u003e_my_first_config)`部署堆栈时一样，并且`my_second_config`已经创建过。\n\n```Plain\nconfigs:\n  my_first_config:\n    file: ./config_data\n  my_second_config:\n    external: true \n```\n\n当 Docker 中的配置名称与服务中存在的名称不同时，可以使用name进行配置。\n\n```Plain\nconfigs:\n  my_first_config:\n    file: ./config_data\n  my_second_config:\n    external:\n      name: redis_config\n```\n\n## container_name\n\n指定自定义容器名称，而不是生成的默认名称。由于 Docker 容器名称必须是唯一的，因此如果您指定了自定义名称，则不能将服务扩展到 1 个以上的容器。\n\n## credential_spec\n\n为托管服务帐户配置凭据规范。此选项仅用于使用 Windows 容器的服务。在`credential_spec`上的配置列表格式为`file://\u003cfilename\u003e`或`registry://\u003cvalue-name\u003e`\n\n  \n\n## depends_on\n\n表示服务之间的依赖关系。服务依赖会导致以下行为：\n\n- `docker-compose up`按依赖顺序启动服务。在下面的例子中，`db`和`redis`在 `web`之前启动。\n    \n- `docker-compose up SERVICE`自动包含`SERVICE`的依赖项。在下面的示例中，`docker-compose up web`还创建并启动`db`和`redis`。\n    \n- `docker-compose stop`按依赖顺序停止服务。在以下示例中，`web`在`db`和`redis`之前停止。\n    \n\n```Bash\nversion: \"3.9\"\nservices:\n  web:\n    build: .\n    depends_on:\n      - db\n      - redis\n  redis:\n    image: redis\n  db:\n    image: postgres\n```\n\n## deploy\n\n指定与服务的部署和运行有关的配置。只在 swarm 模式下才会有用。\n\n- endpoint_mode 访问集群服务的方式。\n    \n    - vip ：Docker 集群服务一个对外的虚拟 ip。所有的请求都会通过这个虚拟 ip 到达集群服务内部的机器。\n        \n    - dnsrr ：DNS 轮询（DNSRR）。所有的请求会自动轮询获取到集群 ip 列表中的一个 ip 地址。\n        \n- labels 在服务上设置标签。可以用容器上的 labels（跟 deploy 同级的配置） 覆盖 deploy 下的 labels。\n    \n- mode 指定服务提供的模式\n    \n    - global：全局服务，服务将部署至集群的每个节点。\n        \n    - replicated：复制服务，复制指定服务到集群的机器上。\n        \n\n下图中黄色的方块是 replicated 模式的运行情况，灰色方块是 global 模式的运行情况。\n\n  \n\n![](statistic/asynccode-214.png)\n\n  \n\n- placement 指定约束和首选项的位置\n    \n    ```Bash\n    version: \"3.9\"\n    services:\n      db:\n        image: postgres\n        deploy:\n          placement:\n            constraints:\n              - \"node.role==manager\"\n              - \"engine.labels.operatingsystem==ubuntu 18.04\"\n            preferences:\n              - spread: node.labels.zone\n    ```\n    \n    - 您可以通过定义约束表达式来限制可以安排任务的节点集。约束表达式可以使用匹配(`==`) 或排除(`!=`) 规则。多个约束查找可以使用 AND 匹配。约束可以匹配节点或 Docker 引擎标签，如下所示：\n        \n\n|   |   |   |\n|---|---|---|\n|节点属性|匹配|例子|\n|node.id|节点 ID|node.id==2ivku8v2gvtg4|\n|node.hostname|节点主机名|node.hostname!=node-2|\n|node.role|节点角色 ( manager/ worker)|node.role==manager|\n|node.platform.os|节点操作系统|node.platform.os==windows|\n|node.platform.arch|节点架构|node.platform.arch==x86_64|\n|node.labels|用户定义的节点标签|node.labels.security==high|\n|engine.labels|Docker 引擎的标签|engine.labels.operatingsystem==ubuntu-14.04|\n\n- max_replicas_per_node 如果服务是`replicated`（默认值），则限制任何时间在节点上运行的副本数\n    \n- replicas 如果服务是`replicated`（默认值），指定在任何给定时间应运行的容器数量。\n    \n- resources 配置资源约束。在下面示例中，`redis`服务被限制为使用不超过 50M 的内存和`0.50`（单核的 50%）可用处理时间 (CPU)，并保留`20M`内存和`0.25`CPU 时间（始终可用）。\n    \n\n```Plain\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis:alpine\n    deploy:\n      resources:\n        limits:\n          cpus: '0.50'\n          memory: 50M\n        reservations:\n          cpus: '0.25'\n          memory: 20M\n```\n\n- restart_policy 配置是否以及如何在退出时重新启动容器。替换`[restart](https://link.zhihu.com/?target=https%3A//docs.docker.com/compose/compose-file/compose-file-v2/%23orig-resources)`\n    \n    - condition: none, on-failure 或 any (默认: any) 之一。\n        \n    - delay：重新启动尝试之间等待的时间（默认值：5s）。\n        \n    - max_attempts：在放弃之前尝试重新启动容器的次数（默认值：永不放弃）。如果在配置的窗口（window）内重新启动未成功，则此尝试不计入配置max_attempts值。例如，如果 max_attempts 设置为“2”，并且第一次尝试重启失败，则可能会尝试两次以上的重启。\n        \n    - window：在决定重启是否成功之前等待多长时间（默认值：立即重启）。\n        \n- rollback_config 在更新失败的情况下应如何回滚服务。\n    \n    - `parallelism`：一次回滚的容器数量。如果设置为 0，则所有容器同时回滚。\n        \n    - `delay`：每个容器组回滚之间等待的时间（默认为 0 秒）。\n        \n    - `failure_action`: 如果回滚失败怎么办。`continue`或者`pause`（默认`pause`）\n        \n    - `monitor`：每次任务更新后监控失败的持续时间`(ns|us|ms|s|m|h)`（默认 5s）注意：设置为 0 将使用默认 5s。\n        \n    - `max_failure_ratio`：回滚期间允许的故障率（默认为 0）。\n        \n    - `order`：回滚期间的操作顺序。`stop-first`（旧任务在开始新任务之前停止），或`start-first`（首先启动新任务，并且正在运行的任务短暂重叠）（默认`stop-first`）。\n        \n\nupdate_config 配置应如何更新服务。用于配置滚动更新。\n\n1. `parallelism`：一次更新的容器数量。\n    \n2. `delay`：更新一组容器之间的等待时间。\n    \n3. `failure_action`: 如果更新失败怎么办。`continue`，`rollback`或者`pause` （默认：`pause`）。\n    \n4. `monitor`：每次任务更新后监控失败的持续时间`(ns|us|ms|s|m|h)`（默认 5s）注意：设置为 0 将使用默认 5s。\n    \n5. `max_failure_ratio`：更新期间可容忍的故障率。\n    \n6. `order`：更新期间的操作顺序。`stop-first`（旧任务在开始新任务之前停止），或`start-first`（新任务首先启动，并且正在运行的任务短暂重叠）（默认`stop-first`）\n    \n\n  \n\n## devices\n\n设备映射列表。使用与`--device`docker 客户端创建选项格式相同。\n\n```Plain\ndevices:\n  - \"/dev/ttyUSB0:/dev/ttyUSB0\"\n```\n\n## dns\n\n自定义 DNS 服务器。可以是单个值或列表。\n\n```Plain\ndns: 8.8.8.8\ndns:\n  - 8.8.8.8\n  - 9.9.9.9\n```\n\n## dns_search\n\n自定义 DNS 搜索域。可以是单个值或列表。\n\n```Plain\ndns_search: example.com\ndns_search:\n  - dc1.example.com\n  - dc2.example.com\n```\n\n## entrypoint\n\n在 Dockerfile 中有一个指令叫做`ENTRYPOINT`指令，用于运行程序。在`docker-compose.yml`中可以定义覆盖 Dockerfile 中定义的 entrypoint：\n\n```Plain\nentrypoint: /code/entrypoint.sh\nentrypoint: [\"php\", \"-d\", \"memory_limit=-1\", \"vendor/bin/phpunit\"]\n```\n\n也可以是以下格式：\n\n```SQL\nentrypoint:\n    - php\n    - -d\n    - zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so\n    - -d\n    - memory_limit=-1\n    - vendor/bin/phpunit\n```\n\n## env_file\n\n从文件添加环境变量。可以是单个值或列表。\n\n如果您使用指定了 Compose 文件`docker-compose -f FILE`，则其中的路径 `env_file`相对于该文件所在的目录。\n\nenvironment 声明的环境变量会覆盖这些值——即使这些值是空的或未定义的。\n\n```Plain\nenv_file: .env\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/runtime_opts.env\n```\n\n## environment\n\n添加环境变量。您可以使用数组或字典。任何布尔值（true、false、yes、no）都需要用引号括起来，以确保它们不会被 YML 解析器转换为 True 或 False。\n\n一般 arg 标签的变量仅用在构建过程中。而`environment`和 Dockerfile 中的`ENV`指令一样会把变量一直保存在镜像、容器中，类似`docker run -e`的效果\n\n```Plain\nenvironment:\n  RACK_ENV: development\n  SHOW: 'true'\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SHOW=true\n  - SESSION_SECRET\n```\n\n## expose\n\n暴露端口，但不映射到宿主机，只被连接的服务访问。这个标签与 Dockerfile 中的`EXPOSE`指令一样，用于指定暴露的端口，但是只是作为一种参考，实际上`docker-compose.yml`的端口映射还得`ports`这样的标签。\n\n## external_links\n\n链接到 docker-compose.yml 外部的容器，甚至 并非 Compose 项目文件管理的容器。\n\n```Bash\nexternal_links:\n  - redis_1\n  - project_db_1:mysql\n  - project_db_1:postgresql\n```\n\n## extra_hosts\n\n添加主机名映射。类似 docker client --add-host。\n\n```Plain\nextra_hosts:\n  - \"somehost:162.242.195.82\"\n  - \"otherhost:50.31.209.229\"\n```\n\n会往`/etc/hosts`文件中添加一些记录，启动之后查看容器内部 hosts可以看到：\n\n```Plain\n162.242.195.82  somehost\n50.31.209.229   otherhost\n```\n\n## healthcheck\n\n配置运行的检查以确定此服务的容器是否“健康”。\n\n```Plain\nhealthcheck:\n  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n  interval: 1m30s\n  timeout: 10s\n  retries: 3\n  start_period: 40s\n```\n\n`interval`，`timeout 和 start_period`都是持续时间。`test`必须是字符串或列表。如果是列表，则第一项必须是`NONE`,`CMD`或`CMD-SHELL`。如果是字符串，则相当于指定`CMD-SHELL`后跟该字符串。\n\n```SQL\n# Hit the local web app\ntest: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\ntest: [\"CMD-SHELL\", \"curl -f http://localhost || exit 1\"]\ntest: curl -f https://localhost || exit 1\n```\n\n如果需要禁用镜像的所有检查项目，可以使用`disable:true`，相当于`test:[\"NONE\"]`\n\n```Plain\nhealthcheck:\n  disable: true \n```\n\n## image\n\n从指定的镜像中启动容器，可以是存储仓库、标签以及镜像 ID\n\n```SQL\nimage: redis\nimage: ubuntu:14.04\nimage: tutum/influxdb\nimage: example-registry.com:4000/postgresql\nimage: a4bc65fd # 镜像id\n```\n\n## init\n\n在容器内运行一个 init 来转发信号和取得进程。将此选项设置`true`为服务启用此功能。\n\n```Plain\nversion: \"3.9\"\nservices:\n  web:\n    image: alpine:latest\n    init: true\n```\n\n## isolation\n\n指定容器的隔离技术。在 Linux 上，唯一支持的值是`default`。在 Windows 上，可接受的值为`default`、`process`和`hyperv`。\n\n## labels\n\n使用 Docker 标签将元数据添加到容器，可以使用数组或字典。与 Dockerfile 中的`LABELS`类似：\n\n```Plain\nlabels:\n  - \"com.example.description=Accounting webapp\"\n  - \"com.example.department=Finance\"\n  - \"com.example.label-with-empty-value\"\n\nlabels:\n  com.example.description: \"Accounting webapp\"\n  com.example.department: \"Finance\"\n  com.example.label-with-empty-value: \"\"\n```\n\n## links\n\n链接到另一个服务中的容器。指定服务名称和链接别名 (`\"SERVICE:ALIAS\"`)，或仅指定服务名称。\n\n它们不需要启用服务进行通信 - 默认情况下，任何服务都可以以该服务的名称访问任何其他服务。在以下示例中，`web`可以访问`db，并且设置别名为database`：\n\n```Plain\nversion: \"3.9\"\nservices:\n\n  web:\n    build: .\n    links:\n      - \"db:database\"\n  db:\n    image: postgres\n```\n\n## logging\n\n日志记录配置。\n\ndriver：指定服务容器的日志记录驱动程序，默认值为json-file。有以下三个选项\n\n```SQL\ndriver: \"json-file\"\ndriver: \"syslog\"\ndriver: \"none\"\n```\n\n仅在 json-file 驱动程序下，可以使用以下参数，限制日志得数量和大小\n\n```Plain\nversion: \"3.9\"\nservices:\n  some-service:\n    image: some-service\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"200k\"\n        max-file: \"10\"\nnetwork_mode\n```\n\n当达到文件限制上限，会自动删除旧得文件。\n\nsyslog 驱动程序下，可以使用 syslog-address 指定日志接收地址\n\n```SQL\nlogging:\n  driver: syslog\n  options:\n    syslog-address: \"tcp://192.168.0.42:123\"\n```\n\n  \n\n## network_mode\n\n网络模式。使用与 docker 客户端`--network`相同，可以使用特殊形式`service:[service name]`。\n\n```Bash\nnetwork_mode: \"bridge\"\nnetwork_mode: \"host\"\nnetwork_mode: \"none\"\nnetwork_mode: \"service:[service name]\"\nnetwork_mode: \"container:[container name/id]\"\n```\n\n  \n\n## pid\n\n将 PID 模式设置为主机 PID 模式。这会在容器和主机操作系统之间共享 PID 地址空间。使用此标志启动的容器可以访问和操作裸机命名空间中的其他容器，反之亦然。\n\n```Plain\npid: \"host\"\n```\n\n## ports\n\n暴露端口。\n\n简短语法\n\n共有三种写法：\n\n- 指定两个端口 ( `HOST:CONTAINER`)\n    \n- 仅指定容器端口（为主机端口选择了一个临时主机端口）。\n    \n- 指定要绑定到两个端口的主机 IP 地址（默认为 0.0.0.0，表示所有接口）：( `IPADDR:HOSTPORT:CONTAINERPORT`)。如果 HOSTPORT 为空（例如`127.0.0.1::80`），则会选择一个临时端口来绑定到主机上。\n    \n\n```Plain\nports:\n  - \"3000\"\n  - \"3000-3005\"\n  - \"8000:8000\"\n  - \"9090-9091:8080-8081\"\n  - \"49100:22\"\n  - \"127.0.0.1:8001:8001\"\n  - \"127.0.0.1:5000-5010:5000-5010\"\n  - \"127.0.0.1::5000\"\n  - \"6060:6060/udp\"\n  - \"12400-12500:1240\"\n```\n\n长语法\n\n- `target`: 容器内的端口\n    \n- `published`: 公开的端口\n    \n- `protocol`：端口协议（`tcp`或`udp`）\n    \n- `mode`：`host`用于在每个节点上发布主机端口，或`ingress`用于负载平衡的群模式端口。\n    \n\n```Plain\nports:\n  - target: 80\n    published: 8080\n    protocol: tcp\n    mode: host\n```\n\n## profiles\n\n允许通过有选择地启用服务来针对各种用途和环境调整 Compose 应用程序模型。这是通过将每个服务分配给单个或多个配置文件来实现的。如果未分配，则始终启动该服务，但如果已分配，则仅在激活配置文件时才启动。\n\n这允许人们在单个`docker-compose.yml`文件中定义额外的服务，这些服务应该只在特定场景中启动，例如用于调试或开发任务。\n\n```Plain\nprofiles: [\"frontend\", \"debug\"]\nprofiles:\n  - frontend\n  - debug\n```\n\n## restart\n\n`no`是默认的重启策略，在任何情况下都不会重启容器。当`always`指定时，容器总是重新启动。`on-failure`如果退出代码指示失败错误，则该策略会重新启动容器。`unless-stopped`总是重新启动容器，除非容器停止（手动或其他方式）。\n\n```Plain\nrestart: \"no\"\nrestart: always\nrestart: on-failure\nrestart: unless-stopped\n```\n\n## secrets\n\n为每个服务机密授予相应的访问权限\n\n简短语法\n\n简短的语法仅指定机密名称。\n\n以下示例授予`redis`服务对`my_secret`和`my_other_secret`机密的访问权限。`./my_secret.txt`文件的内容被设置为 my_secret，`my_other_secret`被定义为外部机密，这意味着它已经在Docker中定义，无论是通过运行`docker secret create`命令还是通过另一个堆栈部署，都不会重新创建。如果外部机密不存在，堆栈部署将失败并显示`secret not found`错误。\n\n```Plain\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis:latest\n    deploy:\n      replicas: 1\n    secrets:\n      - my_secret\n      - my_other_secret\nsecrets:\n  my_secret:\n    file: ./my_secret.txt\n  my_other_secret:\n    external: true\n```\n\n长语法\n\n- `source`：定义机密标识符。\n    \n- `target`：要挂载在`/run/secrets/`服务的任务容器中的文件的名称，默认是 source。\n    \n- `uid`和`gid`：`/run/secrets/`在服务的任务容器中拥有文件的数字 UID 或 GID 。\n    \n- `mode`：要挂载在`/run/secrets/` 服务的任务容器中的文件的权限，以八进制表示法。例如，`0444` 代表可读。\n    \n\n下面的示例表示将my_secret 命名为`redis_secret`，模式为`0440`（组可读），和用户组为`103`。该`redis`服务无权访问该`my_other_secret`机密。\n\n```Plain\nversion: \"3.9\"\nservices:\n  redis:\n    image: redis:latest\n    deploy:\n      replicas: 1\n    secrets:\n      - source: my_secret\n        target: redis_secret\n        uid: '103'\n        gid: '103'\n        mode: 0440\nsecrets:\n  my_secret:\n    file: ./my_secret.txt\n  my_other_secret:\n    external: true\n```\n\n一级`secrets`详细配置：\n\n- `file`：使用指定路径中的文件内容创建机密。\n    \n- `external`：如果设置为 true，则指定此机密已创建。Docker 不会尝试创建它，如果它不存在， 会报错`secret not found`。\n    \n- `name`：Docker 中秘密对象的名称。此字段可用于引用包含特殊字符的机密。\n    \n- `template_driver`：要使用的模板驱动程序的名称，它控制是否以及如何将机密负载评估为模板。如果未设置驱动程序，则不使用模板。当前支持的唯一驱动程序是`golang`，它使用`golang`。\n    \n\n在本示例中，`my_first_secret`在`\u003cstack_name\u003e_my_first_secret` 部署堆栈时创建 ，并且`my_second_secret`已存在于 Docker 中。\n\n```Plain\nsecrets:\n  my_first_secret:\n    file: ./secret_data\n  my_second_secret:\n    external: true \n```\n\n## security_opt\n\n为每个容器覆盖默认的标签。简单说来就是管理全部服务的标签，比如设置全部服务的 user 标签值为`USER`\n\n```Plain\nsecurity_opt:\n  - label:user:USER\n  - label:role:ROLE \n```\n\n## stop_grace_period\n\n指定在尝试停止容器时等待多长时间。\n\n在docker stop命令执行的时候，会先向容器中的进程发送系统信号SIGTERM，然后等待容器中的应用程序终止执行。如果等待时间达到设定的超时时间，或者默认的10秒，会继续发送SIGKILL的系统信号强行kill掉进程。\n\n在容器中的应用程序，可以选择忽略和不处理SIGTERM信号，不过一旦达到超时时间，程序就会被系统强行kill掉，因为SIGKILL信号是直接发往系统内核的，应用程序没有机会去处理它。\n\n```Plain\nstop_grace_period: 1s\n```\n\n默认情况下，`stop`在发送 SIGKILL 之前等待容器退出 10 秒。\n\n## stop_signal\n\n设置一个替代信号来停止容器。默认情况下`stop`使用 SIGTERM。使用`stop_signal`设置替代信号来`stop`。\n\n```Plain\nstop_signal: SIGUSR1\n```\n\n## sysctls\n\n在容器中设置的内核参数，可以为数组或字典\n\n```Plain\nsysctls:\n  net.core.somaxconn: 1024\n  net.ipv4.tcp_syncookies: 0\n```\n\n## tmpfs\n\n在容器内挂载一个临时文件系统。可以是单个值或列表。\n\n```Plain\ntmpfs: /run\ntmpfs:\n  - /run\n  - /tmp\n```\n\n## ulimits\n\n设置当前进程以及其子进程的资源使用量，覆盖容器的默认限制，可以单一地将限制值设为一个整数，也可以将`soft/hard`限制指定为映射\n\n```Plain\nulimits:\n  nproc: 65535\n  nofile:\n    soft: 20000\n    hard: 40000\n```\n\n## userns_mode\n\n如果 Docker 守护程序配置了用户命名空间，则禁用此服务的用户命名空间。\n\n```Plain\nuserns_mode: \"host\"\n```\n\n## volumes\n\n挂载一个目录或者一个已存在的数据卷容器，\n\n- 可以直接使用`HOST:CONTAINER`这样的格式，\n    \n- 或者使用`HOST:CONTAINER:ro`这样的格式，后者对于容器来说，数据卷是只读的，这样可以有效保护宿主机的文件系统\n    \n\n您可以将主机路径挂载为单个服务定义的一部分，无需在一级`volumes`键中定义它。\n\n但是，如果您想在多个服务中重用一个卷，则需要在一级volumes 中定义一个命名卷。\n\n如下实例，web 服务使用命名卷 (`mydata`)，以及为单个服务定义的绑定安装（`db`service下的第一个路径`volumes`）。`db`服务还使用名为`dbdata`（`db`service下的第二个路径`volumes`）的命名卷，使用了旧字符串格式定义它以安装命名卷。命名卷必须列在顶级`volumes`键下。\n\n```Plain\nversion: \"3.9\"\nservices:\n  web:\n    image: nginx:alpine\n    volumes:\n      - type: volume\n        source: mydata\n        target: /data\n        volume:\n          nocopy: true\n      - type: bind\n        source: ./static\n        target: /opt/app/static\n\n  db:\n    image: postgres:latest\n    volumes:\n      - \"/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock\"\n      - \"dbdata:/var/lib/postgresql/data\"\n\nvolumes:\n  mydata:\n  dbdata:\n```\n\n#### 简短语法\n\n简短语法使用通用`[SOURCE:]TARGET[:MODE]`格式，其中 `SOURCE`可以是主机路径或卷名。`TARGET`是安装卷的容器路径。标准模式`ro`用于只读和`rw`读写（默认）。\n\n您可以在主机上挂载一个相对路径，该路径相对于正在使用的 Compose 配置文件的目录展开。相对路径应始终以`.`或开头`..`。\n\n```Plain\nvolumes:\n  # Just specify a path and let the Engine create a volume\n  - /var/lib/mysql\n\n  # Specify an absolute path mapping\n  - /opt/data:/var/lib/mysql\n\n  # Path on the host, relative to the Compose file\n  - ./cache:/tmp/cache\n\n  # User-relative path\n  - ~/configs:/etc/configs/:ro\n\n  # 命名卷\n  - datavolume:/var/lib/mysql\n```\n\n#### 长语法\n\n- `type`: 安装类型， `bind`,`tmpfs`或`npipe`\n    \n- `source`: 安装源、主机上用于绑定安装的路径或在顶级volumes 中定义的卷的名称 。不适用于 tmpfs 挂载。\n    \n- `target`：安装卷的容器中的路径\n    \n- `read_only`: 将卷设置为只读的标志\n    \n- `bind`: 配置额外的绑定选项\n    \n\n1. `propagation`：用于绑定的传播模式\n    \n\n`volume`: 配置额外的选项\n\n1. `nocopy`: 创建卷时禁用从容器复制数据的标志\n    \n\n`tmpfs`: 配置额外的 tmpfs 选项\n\n1. `size`：tmpfs 挂载的大小（以字节为单位）\n    \n\n```Plain\nversion: \"3.9\"\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - type: volume\n        source: mydata\n        target: /data\n        volume:\n          nocopy: true\n      - type: bind\n        source: ./static\n        target: /opt/app/static\n\nnetworks:\n  webnet:\n\nvolumes:\n  mydata:\n```\n\n  \n\n#### 一级 Volume 详细配置：\n\n- driver 指定该卷应使用哪个卷驱动程序\n    \n- driver_opts 将选项列表指定为键值对以传递给此卷的驱动程序\n    \n\n```Plain\nvolumes:\n  example:\n    driver_opts:\n      type: \"nfs\"\n      o: \"addr=10.40.0.199,nolock,soft,rw\"\n      device: \":/docker/example\"\n```\n\n- external 如果设置为true，则指定该卷是在 Compose 之外创建的\n    \n- labels 添加元数据\n    \n- name 为此卷设置自定义名称\n    \n\n```Plain\nversion: \"3.9\"\nvolumes:\n  data:\n    name: my-app-data\n```\n\n  \n\n## 变量置换\n\n你可以使用 $VARIABLE 或者 ${VARIABLE} 来置换变量\n\n- `${VARIABLE:-default}VARIABLE`在环境中未设置或为空时设置为default。\n    \n- `${VARIABLE-default}`仅当`VARIABLE`在环境中未设置时才设置为default。\n    \n- `${VARIABLE:?err}`退出并显示一条错误消息，其中包含环境中的`err`if `VARIABLE`未设置或为空。\n    \n- `${VARIABLE?err}`退出并显示一条错误消息，其中包含`err`if `VARIABLE`在环境中未设置。\n    \n- 如果想使用一个不被compose处理的变量，可用使用 \n    \n\n  \n\n  \n\n一个配置文件\n\n```SQL\nversion: \"3\"\n\nservices:\n  zookeeper:\n    image: docker.io/bitnami/zookeeper:3.8\n    ports:\n      - \"22181:2181\"\n    volumes:\n      - \"zookeeper_data:/bitnami\"\n    environment:\n      - ALLOW_ANONYMOUS_LOGIN=yes\n  kafka:\n    container_name: kafka1\n    image: docker.io/bitnami/kafka:3.4\n    ports:\n      - \"9192:9092\"\n    volumes:\n      - \"kafka_data:/bitnami\"\n    environment:\n      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181\n      - ALLOW_PLAINTEXT_LISTENER=yes\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://宿主机ip:9192\n      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true\n    depends_on:\n      - zookeeper\n\n  kafka-ui:\n    image: provectuslabs/kafka-ui:latest\n    container_name: kafka-ui\n    restart: always\n    ports:\n        - 10010:8080\n    volumes:\n        - /etc/localtime:/etc/localtime\n    environment:\n        - KAFKA_CLUSTERS_0_NAME=local\n        - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka1:9092\n\nvolumes:\n  zookeeper_data:\n    driver: local\n  kafka_data:\n    driver: local\n\n```","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/docker/dockerfile":{"title":"dockerfile","content":"\n\n\n\n\n\nDockerfile 是用来描述文件的构成的文本文档，其中包含了用户可以在使用行调用以组合 Image 的所有命令，用户还可以使用 Docker build 实现连续执行多个命令行的自动构建。\n\n  \n\n通过编写Dockerfile生磁镜像，可以为开发、测试团队提供基本一致的环境，从而提升开发、测试团队的效率，不用再为环境不统一而发愁，同时运维也能更加方便地管理我们的镜像。\n\n  \n# 镜像的内部机制是什么\n\n镜像就是一个打包文件，里面包含了应用程序还有它运行所依赖的环境，例如文件系统、环境变量、配置参数等等\n\n  \n\n环境变量、配置参数这些东西还是比较简单的，随便用一个 manifest 清单就可以管理，真正**麻烦的是文件系统。**\n\n  \n\n容器镜像的一个重大创新点：**分层，术语叫“Layer”**\n\n  \n\n容器镜像内部并不是一个平坦的结构，而是由许多的镜像层组成的，每层都是只读不可修改的一组文件，相同的层可以在镜像之间共享，然后多个层像搭积木一样堆叠起来，再使用一种叫**“Union FS 联合文件系统**”\n\n![](statistic/asynccode-573.png)\n\n可以用命令 docker inspect 来查看镜像的分层信息，比如 nginx:alpine\n\n```Go\n\ndocker inspect nginx:alpine\n```\n\n![](statistic/asynccode-574.png)\n\n# Dockerfile 是什么\n\ndockerfile 非常普通，它就是一个纯文本，里面记录了一系列的**构建指令，比如选择基础镜像、拷贝文件、运行脚本等等**，**每个指令都会生成一个 Layer**，而 Docker 顺序执行这个文件里的所有步骤，最后就会创建出一个新的镜像出来。\n\n  \n\n```Go\n\n# Dockerfile.busybox\nFROM busybox                  # 选择基础镜像\nCMD echo \"hello world\"        # 启动容器时默认运行的命令\n```\n\n- 第一条指令是 FROM，所有的 Dockerfile 都要从它开始，表示选择构建使用的基础镜像，相当于“打地基”，这里我们使用的是 busybox。\n    \n- 第二条指令是 CMD，它指定 docker run 启动容器时默认运行的命令，这里我们使用了 echo 命令，输出“hello world”字符串。\n    \n\n可以用docker build ,通过Dockerfile 来创建镜像\n\n```Go\n\ndocker build -f Dockerfile.busybox .\n\nSending build context to Docker daemon   7.68kB\nStep 1/2 : FROM busybox\n ---\u003e d38589532d97\nStep 2/2 : CMD echo \"hello world\"\n ---\u003e Running in c5a762edd1c8\nRemoving intermediate container c5a762edd1c8\n ---\u003e b61882f42db7\nSuccessfully built b61882f42db7\n```\n\n# 怎样编写正确、高效的 Dockerfile\n\n- 先因为构建镜像的**第一条指令必须是 FROM**，所以基础镜像的选择非常关键。如果关注的是镜像的安全和大小，那么一般会选择 **Alpine**\n    \n- 本机上开发测试时会产生一些源码、配置等文件，需要打包进镜像里，这时可以使用 **COPY** 命令，它的用法和 Linux 的 cp 差不多，不过拷贝的源文件必须是“**构建上下文”路径里的**\n    \n\n```Go\n\nCOPY ./a.txt  /tmp/a.txt    # 把构建上下文里的a.txt拷贝到镜像的/tmp目录\nCOPY /etc/hosts  /tmp       # 错误！不能使用构建上下文之外的文件\n```\n\n- **RUN** ，它可以执行任意的 Shell 命令，比如更新系统、安装应用、下载文件、创建目录、编译程序等等，实现任意的镜像构建步骤\n    \n    - RUN 通常会是 Dockerfile 里最复杂的指令，会包含很多的 Shell 命令，但 Dockerfile 里一条指令只能是一行，所以有的 RUN 指令会在每行的末尾使用续行符 \\，命令之间也会用 \u0026\u0026 来连接\n        \n    \n    ```Go\n    \n    RUN apt-get update \\\n        \u0026\u0026 apt-get install -y \\\n            build-essential \\\n            curl \\\n            make \\\n            unzip \\\n        \u0026\u0026 cd /tmp \\\n        \u0026\u0026 curl -fSL xxx.tar.gz -o xxx.tar.gz\\\n        \u0026\u0026 tar xzf xxx.tar.gz \\\n        \u0026\u0026 cd xxx \\\n        \u0026\u0026 ./config \\\n        \u0026\u0026 make \\\n        \u0026\u0026 make clean\n    ```\n    \n    - 把这些 Shell 命令集中到一个脚本文件里，用 COPY 命令拷贝进去再用 RUN 来执行：\n        \n    \n    ```Go\n    \n    COPY setup.sh  /tmp/                # 拷贝脚本到/tmp目录\n    \n    RUN cd /tmp \u0026\u0026 chmod +x setup.sh \\  # 添加执行权限\n        \u0026\u0026 ./setup.sh \u0026\u0026 rm setup.sh    # 运行脚本然后再删除\n    ```\n    \n- 变量，可以用来实现参数化运行,**ARG 和 ENV**。\n    \n    - ARG 创建的变量只在镜像构建过程中可见，容器运行时不可见\n        \n    - ENV 创建的变量不仅能够在构建镜像的过程中使用，在容器运行时也能够以环境变量的形式被应用程序使用。\n        \n- **EXPOSE**，它用来声明容器对外服务的端口号，对现在基于 Node.js、Tomcat、Nginx、Go 等开发的微服务系统来说非常有用：\n    \n\n  \n\n# docker build 是怎么工作的\n\n“docker”是一个简单的客户端，真正的镜像构建工作是由服务器端的“Docker daemon”来完成的，所以“docker”客户端就只能把“构建上下文”目录打包上传（显示信息 Sending build context to Docker daemon ），所以尽量不要在“构建上下文”中存放多余的文件。\n\n  \n\n# Dockerfile语法\n\nDockerfile的语法非常简单，常用的只有11个：\n\n  \n\n|   |   |\n|---|---|\n|FROM|基于那个镜像来实现|\n|MAINTAINER|镜像的创建者|\n|ENV|声明环境变量|\n|RUN|执行的命令|\n|ADD|添加宿主机文件到容器中，有需要解压的文件会自动解压|\n|COPY|添加宿主机文件到容器|\n|WORKDIR|工作目录，你可以理解为后续所有的命令都将以此为基准路径|\n|EXPOSE|容器类应用可使用的端口|\n|CMD|容器启动后所执行的程序，如果执行docker run 后面跟启动命令会被覆盖掉|\n|ENTRYPOINT|和CMD功能相同，但是docker run 不会覆盖如果需要覆盖可以增加参数-entrypoint来覆盖|\n|VOLUME|Volume 将宿主机的目录挂载到容器里面|\n\n  \n\n例子\n\n  \n\n```SQL\n# syntax=docker/dockerfile:1\n\nFROM python:3.8-slim-buster\n\nRUN apt-get update \u0026\u0026 apt-get install -y procps vim apache2-utils \u0026\u0026 rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip3 install -r requirements.txt\n\nCOPY . .\n\nCMD [ \"python3\", \"-m\" , \"flask\", \"run\", \"--host=0.0.0.0\"]\n```\n\n- 第一行以 syntax 开头的是解析器注释，它与 Docker 构建镜像的工具 buildkit 相关，在一般情况，我都建议你使用 docker/dockerfile:1，它代表始终指向最新的语法版本。\n    \n- FROM 命令，表示使用官方仓库的 python:3.8-slim-buster 镜像作为基础镜像。在我们熟悉的编程方法中，你可以理解为从该镜像继承。这个镜像已经安装了 Python3 和 Pip3 等所有的 Python 相关的工具和包，我们可以直接使用。\n    \n- RUN 的含义是在镜像内运行指定的命令，这里我们为镜像安装了一些必要的工具。\n    \n- WORKDIR 的含义是镜像的工作目录，你可以理解为后续所有的命令都将以此为基准路径。这样，我们就可以在后续的命令中使用相对路径而不是完整路径了。\n    \n- COPY 的含义是将本地的文件或目录复制到镜像内指定的位置。第一个参数代表本地文件或目录，第二个参数代表要复制到镜像内的位置。例如，第七行 COPY 表示，将本地当前目录下的 requirements.txt 文件复制到镜像工作目录 /app 中，文件命名同样为 requirements.txt。\n    \n- 第十行 RUN 的含义是在镜像里运行 pip3 安装 Python 依赖。请注意，这些依赖将会被安装在镜像里而不是本地。\n    \n- 接下来，第十二行又出现了一个 COPY 命令，它的含义是将当前目录所有的源代码复制到镜像的工作目录 /app 下，复制目录的语法和我们之前提到的复制文件是类似的。\n    \n- 最后一行 CMD 的含义是镜像的启动命令。在一个 Dockerfile 中，只能有一个 CMD 命令，如果有多个，那么只有最后一个 CMD 命令会起作用。例如，我们希望在镜像被运行时启动 Python Flask Web 服务器，并监听在特定主机上。CMD 的第一个参数 python3 是我们希望运行的可执行命令，后面的参数表示运行 python3 命令所需要的参数。\n    \n\n## 编写DockerFile\n\n- Dockerfile文件不宜过长，层级越多最终制作出来的镜像也就越大。\n    \n- 构建出来的镜像不要包含不需要的内容，如日志、安装临时文件等。\n    \n- 尽量使用运行时的基础镜像，不需要将构建时的过程也放到运行时的Dockerfile里。\n    \n\n```Plain\nFROM ubuntu:16.04\nRUN apt-get update\nRUN apt-get install -y apt-utils libjpeg-dev \\     \npython-pip\nRUN pip install --upgrade pip\nRUN easy_install -U setuptools\nRUN apt-get clean\n```\n\n  \n\n```SQL\nFROM ubuntu:16.04\nRUN apt-get update \u0026\u0026 apt-get install -y apt-utils \\\n  libjpeg-dev python-pip \\\n           \u0026\u0026 pip install --upgrade pip \\\n      \u0026\u0026 easy_install -U setuptools \\\n    \u0026\u0026 apt-get clean\n```\n\n  \n\n- 第一个Dockerfile的好处是：当正在执行的过程某一层出错，对其进行修正后再次Build，前面已经执行完成的层不会再次执行。这样能大大减少下次Build的时间，而它的问题就是会因层级变多了而使镜像占用的空间也变大。\n    \n- 第二个Dockerfile把所有的组件全部在一层解决，这样做能一定程度上减少镜像的占用空间，但在制作基础镜像的时候若其中某个组编译出错，修正后再次Build就相当于重头再来了，前面编译好的组件在一个层里，得全部都重新编译一遍，比较消耗时间\n    \n\n  \n\n## 使用多阶段构建\n\n  \n\nDocker在升级到Docker 17.05之后就能支持多阶构建了，为了使镜像更加小巧，我们采用多阶构建的方式来打包镜像\n\n  \n\n## 单文件构建\n\n在多阶构建出来之前使用单个文件进行构建，单文件就是将所有的构建过程（包括项目的依赖、编译、测试、打包过程）全部包含在一个Dockerfile中之下：\n\n```SQL\nFROM golang:1.11.4-alpine3.8 AS build-env\nENV GO111MODULE=off\nENV GO15VENDOREXPERIMENT=1\nENV BUILDPATH=github.com/lattecake/hello\nRUN mkdir -p /go/src/${BUILDPATH}\nCOPY ./ /go/src/${BUILDPATH}\nRUN cd /go/src/${BUILDPATH} \u0026\u0026 CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go install –v\n\nCMD [/go/bin/hello]\n```\n\n这样做会带来一些问题\n\n- Dockerfile文件会特别长，当需要的东西越来越多的时候可维护性指数级将会下降；\n    \n- 镜像层次过多，镜像的体积会逐步增大，部署也会变得越来越慢；\n    \n- 代码存在泄漏风险\n    \n\n## 多文件构建\n\n多文件构建，其实就是使用多个Dockerfile，然后通过脚本将它们进行组合。假设有三个文件分别是：Dockerfile.run、Dockerfile.build、build.sh。\n\n- Dockerfile.run就是运行时程序所必须需要的一些组件的Dockerfile，它包含了最精简的库；\n    \n- Dockerfile.build只是用来构建，构建完就没用了；\n    \n- build.sh的功能就是将Dockerfile.run和Dockerfile.build进行组成，把Dockerfile.build构建好的东西拿出来，然后再执行Dockerfile.run，算是一个调度的角色。\n    \n\n### Dockerfile.build\n\n```SQL\nFROM golang:1.11.4-alpine3.8 AS build-env\nENV GO111MODULE=off\nENV GO15VENDOREXPERIMENT=1\nENV BUILDPATH=github.com/lattecake/hello\nRUN mkdir -p /go/src/${BUILDPATH}\nCOPY ./ /go/src/${BUILDPATH}\nRUN cd /go/src/${BUILDPATH} \u0026\u0026 CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go install –v\n```\n\n### Dockerfile.run\n\n```SQL\nFROM alpine:latest\nRUN apk –no-cache add ca-certificates\nWORKDIR /root\nADD hello .\nCMD [\"./hello\"]\n```\n\n### Dockerfile.sh\n\n```SQL\n#!/bin/sh\ndocker build -t –rm hello:build . -f Dockerfile.build\ndocker create –name extract hello:build\ndocker cp extract:/go/bin/hello ./hello\ndocker rm -f extract\ndocker build –no-cache -t –rm hello:run . -f Dockerfile.run\nrm -rf ./hello\n```\n\n多文件构建大大减小了镜像的占用空间，但它有三个文件需要管理，维护成本也更高一些。\n\n## 多阶段构建\n\n完成多阶段构建我们只需要在Dockerfile中多次使用FORM声明，每次FROM指令可以使用不同的基础镜像，并且每次FROM指令都会开始新的构建，我们可以选择将一个阶段的构建结果复制到另一个阶段，在最终的镜像中只会留下最后一次构建的结果\n\n在Dockerfile里可以使用as来为某一阶段取一个别名”build-env”：\n\n```SQL\nFROM golang:1.11.2-alpine3.8 AS build-env\n```\n\n然后从上一阶段的镜像中复制文件，也可以复制任意镜像中的文件：\n\n```SQL\nCOPY –from=build-env /go/bin/hello /usr/bin/hello\n```\n\n看个例子\n\n```SQL\nFROM golang:1.11.4-alpine3.8 AS build-env\n \nENV GO111MODULE=off\nENV GO15VENDOREXPERIMENT=1\nENV GITPATH=github.com/lattecake/hello\nRUN mkdir -p /go/src/${GITPATH}\nCOPY ./ /go/src/${GITPATH}\nRUN cd /go/src/${GITPATH} \u0026\u0026 CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go install -v\n \nFROM alpine:latest\nENV apk –no-cache add ca-certificates\nCOPY --from=build-env /go/bin/hello /root/hello\nWORKDIR /root\nCMD [\"/root/hello\"]\n```\n\n# 为不同语言构建镜像\n\n  \n\n## Java 应用容器化\n\n常见的 Java 应用启动方式有两种，这也就意味着镜像构建方式也有两种。\n\n- 一种是将应用打包成 Jar 包，在镜像内直接启动应用 Jar 包来构建镜像。\n    \n- 另一种是在容器里通过 Spring Boot 插件直接启动应用\n    \n\n  \n\n### 启动Jar包构建方式\n\n有应用\n\n```SQL\n\n$ cd gitops/docker/13/spring-boot\n$ ls -al\ntotal 80\ndrwxr-xr-x  12 weiwang  staff    384 10  5 11:17 .\ndrwxr-xr-x   4 weiwang  staff    128 10  5 11:17 ..\n-rw-r--r--   1 weiwang  staff      6 10  5 10:30 .dockerignore\n-rw-r--r--   1 weiwang  staff    374 10  5 11:05 Dockerfile\ndrwxr-xr-x   4 weiwang  staff    128 10  5 11:17 src\n......\n```\n\nsrc 目录下的 src/main/java/com/example/demo/DemoApplication.java\n\n```SQL\nrc 目录下的 src/main/java/com/example/demo/DemoApplication.java\n```\n\n它代表了 Spring Boot + Maven 的典型组合，只要符合这两种技术选型，你都可以直接参考这里的例子来容器化你的业务应用。\n\n  \n\n是构建镜像的核心内容 Dockerfile 文件\n\n```SQL\n# syntax=docker/dockerfile:1\n\nFROM eclipse-temurin:17-jdk-jammy as builder\nWORKDIR /opt/app\nCOPY .mvn/ .mvn\nCOPY mvnw pom.xml ./\nRUN ./mvnw dependency:go-offline\nCOPY ./src ./src\nRUN ./mvnw clean install\n \n \nFROM eclipse-temurin:17-jre-jammy\nWORKDIR /opt/app\nEXPOSE 8080\nCOPY --from=builder /opt/app/target/*.jar /opt/app/*.jar\nCMD [\"java\", \"-jar\", \"/opt/app/*.jar\" ]\n```\n\n- 第一阶段的构建，也就是从第 3 行到第 9 行。第 3 行 FROM 表示把 eclipse-temurin:17-jdk-jammy 作为 build 阶段的基础镜像，然后使用 WORKDIR 关键字指定了工作目录为 /opt/app，后续的文件操作都会在这个工作目录下展开。\n    \n- 接下来，第 5 和第 6 行通过 COPY 关键字将 .mvn 目录和 mvnw、pom.xml 文件复制到了工作目录下，**第 7 行通过 RUN 关键字运行 ./mvnw dependency:go-offline 来安装依赖**。然后，第 8 行将 src 目录复制到了镜像中，第 9 行使用 RUN 关键字执行 ./mvnw clean install 进行编译。\n    \n- 第 12 行到 16 行是第二个构建阶段。第 12 行表示使用 eclipse-temurin:17-jre-jammy 作为基础镜像，第 13 行同样指定了工作目录为 /opt/app，第 14 行的 EXPOSE 关键字之前我们有提到过，它是一个备注功能，并不是要暴露真实容器端口的意思\n    \n- 第 15 行的 COPY 语句比较复杂，它指的是从 builder 阶段也就是将第一个阶段位于 /opt/app/target/ 目录下所有的 .jar 文件都拷贝到当前构建阶段镜像的 /opt/app/ 目录下。第 16 行使用 CMD 关键字定义了启动命令，也就是通过 java -jar 的方式启动应用。\n    \n- 最后，.dockerignore 的功能和我们熟悉的 .gitignore 文件功能类似，它指的是在构建过程中需要忽略的文件或目录，合理的文件忽略策略将有助于提高构建镜像的速度\n    \n\n  \n\n### Spring Boot 插件的构建方式\n\n```SQL\n# syntax=docker/dockerfile:1\n\nFROM eclipse-temurin:17-jdk-jammy\n\nWORKDIR /app\n\nCOPY .mvn/ .mvn\nCOPY mvnw pom.xml ./\nRUN ./mvnw dependency:resolve\n\nCOPY src ./src\nCMD [\"./mvnw\", \"spring-boot:run\"]\n```\n\n相比较 Jar 的启动方式，Spring Boot 插件的启动方式显得更加简单。在构建过程中，我们实际上还用了一个小技巧：**第 7 和第 8 行代表单独复制了依赖清单文件 pom.xml 而不是复制整个根目录，**\n\n在这个 Dockerfile 文件中有两条关键的命令，**一个 mvnw dependency:resolve 用于安装依赖，另一个 mvnw spring-boot:run 命令用来启动应用。**\n\n接下来，我们使用 docker build 命令构建镜像，这里要注意增加 -f 参数指定新的 Dockerfile 文件\n\n```SQL\n\n$ docker build -t spring-boot . -f Dockerfile-Boot\n```\n\n  \n\n## Go 应用容器化\n\n以 Echo 框架为例，我提前编写好了一个简单的示例应用\n\n```SQL\n\n$ cd gitops/docker/13/golang\n$ ls -al\n-rw-r--r--  1 weiwang  staff   292 10  5 14:16 Dockerfile\n-rw-r--r--  1 weiwang  staff   599 10  5 14:12 go.mod\n-rw-r--r--  1 weiwang  staff  2825 10  5 14:12 go.sum\n-rw-r--r--  1 weiwang  staff   235 10  5 14:13 main.go\n```\n\nmain.go\n\n```SQL\n\npackage main\n\nimport (\n    \"net/http\"\n    \"github.com/labstack/echo/v4\"\n)\n\nfunc main() {\n    e := echo.New()\n    e.GET(\"/hello\", func(c echo.Context) error {\n        return c.String(http.StatusOK, \"Hello World Golang\")\n    })\n    e.Logger.Fatal(e.Start(\":8080\"))\n}\n```\n\nDockerfile\n\n```SQL\n\n# syntax=docker/dockerfile:1\nFROM golang:1.17 as builder\nWORKDIR /opt/app\nCOPY . .\nRUN go build -o example\n\nFROM ubuntu:latest\nWORKDIR /opt/app\nCOPY --from=builder /opt/app/example /opt/app/example\nEXPOSE 8080\nCMD [\"/opt/app/example\"]\n```\n\n- 第一个构建阶段是以 golang:1.17 为基础镜像，然后我们执行 go build 命令编译并输出可执行文件，将其命名为 example。\n    \n- 第二个构建阶段是以 ubuntu:latest 为基础镜像，第 9 行通过 COPY 关键字将第一个阶段构建的 example 可执行文件复制到镜像的 /opt/app/ 目录下，最后，使用 CMD 来运行 example 启动应用。\n    \n\n  \n\n## Node.js 应用容器化\n\n以 Express.js 框架为例，我已经提前编写好了一个简单示例，在将示例应用克隆到本地后，你可以进入 docker/13/node 目录并查看\n\n```SQL\n\n$ cd gitops/docker/13/node\n$ ls -al\n-rw-r--r--   1 weiwang  staff     12 10  5 16:45 .dockerignore\n-rw-r--r--   1 weiwang  staff    589 10  5 16:39 Dockerfile\n-rw-r--r--   1 weiwang  staff    230 10  5 16:44 app.js\ndrwxr-xr-x  60 weiwang  staff   1920 10  5 16:26 node_modules\n-rw-r--r--   1 weiwang  staff  39326 10  5 16:26 package-lock.json\n-rw-r--r--   1 weiwang  staff    251 10  5 16:26 package.json\n```\n\napp.js 是示例应用的主体文件，包含一个 /hello 接口。当我们通过 Get 请求访问时，会返回“Hello World Node.js”字符串\n\n```SQL\n\nconst express = require('express')\nconst app = express()\nconst port = 3000\n\napp.get('/hello', (req, res) =\u003e {\n  res.send('Hello World Node.js')\n})\n\napp.listen(port, () =\u003e {\n  console.log(`Example app listening on port ${port}`)\n})\n```\n\nDockerfile\n\n```SQL\n# syntax=docker/dockerfile:1\nFROM node:latest AS build\nRUN sed -i \"s@http://\\(deb\\|security\\).debian.org@https://mirrors.aliyun.com@g\" /etc/apt/sources.list\nRUN apt-get update \u0026\u0026 apt-get install -y dumb-init\nWORKDIR /usr/src/app\nCOPY package*.json ./\nRUN npm ci --only=production\n \n\nFROM node:16.17.0-bullseye-slim\nENV NODE_ENV production\nCOPY --from=build /usr/bin/dumb-init /usr/bin/dumb-init\nUSER node\nWORKDIR /usr/src/app\nCOPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules\nCOPY --chown=node:node . /usr/src/app\nCMD [\"dumb-init\", \"node\", \"app.js\"]\n```\n\n  \n\n## Vue 容器化\n\n#### Http-server 构建方式\n\n```SQL\n\n$ cd gitops/docker/13/vue/example\n$ ls -al\n-rw-r--r--   1 weiwang  staff     12 10  5 17:26 .dockerignore\n-rw-r--r--   1 weiwang  staff    172 10  5 17:27 Dockerfile\n-rw-r--r--   1 weiwang  staff      0 10  5 17:34 Dockerfile-Nginx\n-rw-r--r--   1 weiwang  staff    631 10  5 17:23 README.md\n-rw-r--r--   1 weiwang  staff    337 10  5 17:23 index.html\n......\n```\n\n  \n\n```SQL\n\n# syntax=docker/dockerfile:1\n\nFROM node:lts-alpine\nRUN npm install -g http-server\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\nEXPOSE 8080\nCMD [ \"http-server\", \"dist\" ]\n```\n\n#### Nginx 构建方式\n\n- 在上面的例子中，我们使用 http-server 来对外提供服务，这在开发和测试场景，或者是在小型的使用场景中是完全可以的。\n    \n- 不过，在正式的生产环境中，我推荐你把 Nginx 作为反向代理服务器来对外提供服务，它也是性能最好、使用最广泛和稳定性最高的一种方案。\n    \n\n```SQL\n# syntax=docker/dockerfile:1\n\nFROM node:lts-alpine as build-stage\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\nFROM nginx:stable-alpine as production-stage\nCOPY --from=build-stage /app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n```\n\n- 第一阶段的构建过程和我们在上面提到的 http-server 的构建方式非常类似，它是以 node:lts-alpine 为基础镜像，同时复制 package.json 和 package-lock.json 并安装依赖，然后再复制项目源码并且执行 npm run build 来构建项目，生成 dist 目录\n    \n- 第二个阶段的构建过程则是引入了一个新的 nginx:stable-alpine 镜像作为运行镜像，还将第一阶段构建的 dist 目录复制到了第二阶段的 /usr/share/nginx/html 目录中。这个目录是 Nginx 默认的网页目录，默认情况下，Nginx 将使用该目录的内容作为静态资源。最后第 13 行以前台的方式启动 Nginx。\n    \n\n  \n\n  \n\n  \n\n# 构建多平台镜像--buildx\n\n  \n\n上面的案例，**我们都是通过在本地执行 docker build 命令来构建镜像，然后在本地通过 docker run 命令来执行的。实际上，在构建镜像时，Docker 会默认构建本机对应平台的镜像**，例如常见的 AMD64 平台，这在大多数情况是适用的。\n\n但是，当我们使用不同平台的设备尝试启动这个镜像时，可能会遇到下面的问题。\n\n```SQL\n\nWARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64) and no specific platform was requested\n```\n\n产生这个问题的原因是，构建和运行设备的 CPU 平台存在差异。在实际项目中，最典型的例子是构建镜像的计算机是 AMD64 架构，但运行镜像的机器是 ARM64。\n\n那么，怎么才能真正实现跨平台的“**一次构建，到处运行**”目标呢？Docker 为我们提供了构建多平台镜像的方法：**buildx。**\n\n  \n\n## 初始化\n\n要使用 Buildx，首先需要创建构建器，你可以使用 docker buildx create 命令来创建它，并将其命名为 mybuilder\n\n```SQL\n\n$ docker buildx create --name mybuilder\nbuilder\n```\n\n然后，将 mybuilder 设置为默认的构建器。\n\n```SQL\n\n$ docker buildx use builder\n```\n\n接下来，初始化构建器，这一步主要是启动 buildkit 容器。\n\n```SQL\n\n$ $ docker buildx inspect --bootstrap\n[+] Building 19.1s (1/1) FINISHED                                                                                                                                                                        \n =\u003e [internal] booting buildkit                                                                                                                                                                    19.1s\n =\u003e =\u003e pulling image moby/buildkit:buildx-stable-1                                                                                                                                                 18.3s\n =\u003e =\u003e creating container buildx_buildkit_mybuilder0                                                                                                                                                0.8s\nName:   builder\nDriver: docker-container\n\nNodes:\nName:      mybuilder0\nEndpoint:  unix:///var/run/docker.sock\nStatus:    running\nBuildkit:  v0.10.4\nPlatforms: linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/mips64le, linux/mips64, linux/arm/v7, linux/arm/v6\n```\n\n## 构建多平台镜像\n\n```SQL\n\n$ cd gitops/docker/13/multi-arch\n$ ls -al\n-rw-r--r--  1 weiwang  staff   439 10  5 23:49 Dockerfile\n-rw-r--r--  1 weiwang  staff  1075 10  5 18:34 go.mod\n-rw-r--r--  1 weiwang  staff  6962 10  5 18:34 go.sum\n-rw-r--r--  1 weiwang  staff   397 10  5 18:39 main.go\n```\n\nmain.go 是示例应用的主体文件，我们启动一个 HTTP 服务器，访问根路径可以返回 Runtime 包的一些内置变量。\n\n```SQL\n\npackage main\nimport (\n    \"net/http\"\n    \"runtime\"\n    \"github.com/gin-gonic/gin\"\n)\nvar (\n    r = gin.Default()\n)\nfunc main() {\n    r.GET(\"/\", indexHandler)\n    r.Run(\":8080\")\n}\nfunc indexHandler(c *gin.Context) {\n    var osinfo = map[string]string{\n        \"arch\":    runtime.GOARCH,\n        \"os\":      runtime.GOOS,\n        \"version\": runtime.Version(),\n    }\n    c.JSON(http.StatusOK, osinfo)\n}\n```\n\nDockerfile\n\n```SQL\n# syntax=docker/dockerfile:1\nFROM --platform=$BUILDPLATFORM golang:1.18 as build\nARG TARGETOS TARGETARCH\nWORKDIR /opt/app\nCOPY go.* ./\nRUN go mod download\nCOPY . .\nRUN --mount=type=cache,target=/root/.cache/go-build \\\nGOOS=$TARGETOS GOARCH=$TARGETARCH go build -o /opt/app/example .\n\nFROM ubuntu:latest\nWORKDIR /opt/app\nCOPY --from=build /opt/app/example ./example\nCMD [\"/opt/app/example\"]\n```\n\n### 我们先看第一个构建阶段。\n\n- 第 2 行 FROM 基础镜像增加了一个 --platform=$BUILDPLATFORM 参数，它代表“强制使用不同平台的基础镜像”，例如 Linux/amd64。在没有该参数配置的情况下，Docker 默认会使用构建平台（本机）对应架构的基础镜像。\n    \n- 第 3 行 ARG 声明了使用两个内置变量 TARGETOS 和 TARGETARCH，TARGETOS 代表系统，例如 Linux，TARGETARCH 则代表平台，例如 Amd64。这两个参数将会在 Golang 交叉编译时生成对应平台的二进制文件。\n    \n- 第 4 行 WORKDIR 声明了工作目录。\n    \n- 第 5 行的意思是通过 COPY 将 go.mod 和 go.sum 拷贝到镜像中，并在\n    \n- 第 6 行使用 RUN 来运行 go mod download 下载依赖。这样，在这两个文件不变的前提下，Docker 将使用构建缓存来加快构建速度。\n    \n- 在下载完依赖之后，我们通过第 7 行把所有源码文件复制到镜像内。\n    \n- 第 8 行有两个含义，首先， --mount=type=cache,target=/root/.cache/go-build 的目的是告诉 Docker 使用 Golang 构建缓存，加快镜像构建的速度。接下来，GOOS=TARGETOSGOARCH=TARGETARCH go build -o /opt/app/example . 代表的含义是 Golang 交叉编译。注意，TARGETOS和TARGETARCH 是我们提到的内置变量，在具体构建镜像的时候，Docker 会帮我们填充进去。\n    \n\n### 第二个构建阶段比较简单，\n\n主要是使用 ubuntu:latest 基础镜像，将第一个构建阶段生成的二进制文件复制到镜像内，然后指定镜像的启动命令。\n\n  \n\n接下来，我们就可以开始构建多平台镜像了。\n\n在开始构建之前，先执行 docker login 登录到 DockerHub。\n\n```SQL\n在开始构建之前，先执行 docker login 登录到 DockerHub。\n```\n\n接下来，使用 docker buildx build 一次性构建多平台镜像\n\n```SQL\n\n$ docker buildx build --platform linux/amd64,linux/arm64 -t lyzhang1999/multi-arch:latest --push  .\n```\n\n- 我们使用 --platform 参数指定了两个平台：Linux/amd64 和 Linux/arm64，同时 -t 参数指定了镜像的 Tag，而 --push 参数则代表构建完成后直接将镜像推送到 DockerHub。\n    \n- 还记得我们在 Dockerfile 第 2 行增加的 --platform=$BUILDPLATFORM 参数吗？当执行这条命令时，Docker 会分别使用 Amd64 和 Arm64 两个平台的 golang:1.18 镜像，并且在对应的镜像内执行编译过程。\n    \n\n  \n\n  \n\n# 压缩镜像体积\n\n  \n\n## 从构建Golang 镜像开始\n\n对于大多数 Docker 初学者来说，首要目标是能够成功构建镜像，所以，大部分人在最开始编写的 Dockerfile 的时候都以“能用”作为首要目标，内容和 Golang 应用中的 Dockerfile-1 文件类似。\n\n```SQL\n# syntax=docker/dockerfile:1\nFROM golang:1.17\nWORKDIR /opt/app\nCOPY . .\nRUN go build -o example\nCMD [\"/opt/app/example\"]\n```\n\n```SQL\n\n$ docker build -t golang:1 -f Dockerfile-1 .\n```\n\n个 Dockerfile 构建的镜像大小非常惊人，Golang 示例程序使用 go build 命令编译后，二进制可执行文件大约 6M 左右，但容器化之后，镜像达到 900M\n\n## 替换基础镜像\n\n我们构建的 Golang 镜像的大小很大程度是由引入的基础镜像的大小决定的，在这种情况下，替换基础镜像是一个快速并且非常有效的办法。例如，将 Golang:1.17 基础镜像替换为 golang:1.17-alpine 版本\n\n一般来说，**Alpine 版本的镜像相比较普通镜像来说删除了一些非必需的系统应用，所以镜像体积更小**\n\n```SQL\n\n# syntax=docker/dockerfile:1\nFROM golang:1.17-alpine\nWORKDIR /opt/app\nCOPY . .\nRUN go build -o example\nCMD [\"/opt/app/example\"]\n```\n\n## 重新思考Dockerfile\n\n```SQL\n# syntax=docker/dockerfile:1\nFROM golang:1.17-alpine\nWORKDIR /opt/app\nCOPY . .\nRUN go build -o example\nCMD [\"/opt/app/example\"]\n```\n\n从这段 Dockerfile 可以看出，我们在容器内运行了 go build -o example，这条命令将会编译生成二进制的可执行文件，由于编译的过程中需要 Golang 编译工具的支持，所以我们必须要使用 Golang 镜像作为基础镜像，这是导致镜像体积过大的直接原因。\n\n### 可以使用多阶段构建\n\n阶段构建的本质其实就是将镜像构建过程拆分成编译过程和运行过程。第一个阶段对应编译的过程，负责生成可执行文件；第二个阶段对应运行过程，也就是拷贝第一阶段的二进制可执行文件，并为程序提供运行环境，最终镜像也就是第二阶段生成的镜像如下图所示\n\n![](statistic/asynccode-157.png)\n\n  \n\n  \n\n```SQL\n# syntax=docker/dockerfile:1\n\n# Step 1: build golang binary\nFROM golang:1.17 as builder\nWORKDIR /opt/app\nCOPY . .\nRUN go build -o example\n\n# Step 2: copy binary from step1\nFROM ubuntu:latest\nWORKDIR /opt/app\nCOPY --from=builder /opt/app/example ./example\nCMD [\"/opt/app/example\"]\n```\n\n  \n\n## 进一步压缩\n\n当我们使用多阶段构建时，最终生成的镜像大小其实取决于第二阶段引用的镜像大小，它在上面的例子中对应的是 ubuntu:latest 镜像大小。\n\n要进一步缩小体积，我们可以继续使用其他更小的镜像作为第二阶段的运行镜像，这就要说到 **Alpine** 了。\n\nAlpine 镜像是专门为容器化定制的 Linux 发行版，它的最大特点是体积非常小。将第二阶段构建的镜像替换为 Alpine 镜像\n\n```SQL\n\n# syntax=docker/dockerfile:1\n\n# Step 1: build golang binary\nFROM golang:1.17 as builder\nWORKDIR /opt/app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o example\n\n# Step 2: copy binary from step1\nFROM alpine\nWORKDIR /opt/app\nCOPY --from=builder /opt/app/example ./example\nCMD [\"/opt/app/example\"]\n```\n\n由于 Alpine 镜像并没有 glibc，所以我们在编译可执行文件时指定了 CGO_ENABLED=0，这意味着我们禁用了 CGO，这样程序才能在 Alpine 镜像中运行。\n\n不过，由于 Alpine 镜像和常规 Linux 发行版存在一些差异，作为初学者，**我并不推荐你在生产环境下把 Alpine 镜像作为业务的运行镜像**\n\n  \n\n## 极限压缩--scratch\n\n把第二个阶段的镜像替换为一个“空镜像”，这个空镜像称为 scratch 镜像\n\n```SQL\n\n # syntax=docker/dockerfile:1\n\n# Step 1: build golang binary\nFROM golang:1.17 as builder\nWORKDIR /opt/app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o example\n\n# Step 2: copy binary from step1\nFROM scratch\nWORKDIR /opt/app\nCOPY --from=builder /opt/app/example ./example\nCMD [\"/opt/app/example\"]\n```\n\n注意，由于 scratch 镜像不包含任何内容，所以我们在编译 Golang 可执行文件的时候禁用了 CGO，这样才能让编译出来的程序在 scratch 镜像中运行。\n\nscratch 镜像是一个空白镜像，甚至连 shell 都没有，所以我们也无法进入容器查看文件或进行调试。在生产环境中，如果对安全有极高的要求，你可以考虑把 scratch 作为程序的运行镜像。\n\n  \n\n## 复用构建缓存\n\n  \n\n上面的 Dockerfile 还可以做进一步的优化，我还想再插播一个知识点。\n\n- 比如，在第一阶段的构建过程中，我们先是用 COPY . . 的方式拷贝了源码，又进行了编译，这会产生一个缺点，那就是如果只是源码变了，但依赖并没有变，Docker 将无法复用依赖的镜像层缓存。在实际构建过程中，你会发现 Docker 每次都会重新下载 Golang 依赖。\n    \n- 这就引出了另外一个构建镜像的小技巧：**尽量使用 Docker 构建缓存。**\n    \n\n要使用 Golang 依赖的缓存，最简单的办法是：先复制依赖文件，再下载依赖，最后再复制源码进行编译。基于这种思路，我们可以将第一阶段的构建修改如下。\n\n```SQL\n\n# Step 1: build golang binary\nFROM golang:1.17 as builder\nWORKDIR /opt/app\nCOPY go.* ./\nRUN go mod download\nCOPY . .\nRUN go build -o example\n```\n\n  \n\n# 基础镜像的选择\n\n镜像并不是从头开始构建的，我们会基于一个基础镜像来构建，这个基础镜像包含编译工具和运行环境，它负责构建和运行我们的业务代码。\n\n```SQL\n\nFROM golang:1.17 as builder\n# 第一阶段\n\nFROM ubuntu:latest\n# 第二节阶段\n```\n\n为什么我们在第一个阶段使用 golang:1.17 而不是 ubuntu:latest 呢？这是因为 golang:1.17 包含 Golang 的构建工具，这是 ubuntu:latest 所没有的。为了不需要手动在 Dockerfile 中安装 Golang，我们引用了语言的专用镜像。\n\n  \n\n我将**镜像类型归纳为了通用镜像和专用镜像**。在这个例子中，ubuntu:latest 是一种通用镜像，golang:1.17 则是一种专用镜像\n\n## 通用镜像\n\n  \n\n**你可以把通用镜像理解成 Linux 发行版，他们是一个全新安装的 Linux 系统，除了系统工具以外，不包含任何特定语言的编译和运行环境**\n\n  \n\n- 使用通用镜像构建符合业务特定要求的镜像，例如特定的工具链和依赖，特定的构建环境和安全工具等。\n    \n- 单纯作为业务的运行环境，常见于多阶段构建\n    \n\n除了 ubuntu:latest 以外，你还可以选择其他的通用镜像\n\n![](statistic/asynccode-159.png)\n\n### Ubuntu/Debian\n\nUbuntu 和 Debian 是综合能力非常强的 Linux 发行版，非常适合作为通用镜像使用，它们主要的优点如下。\n\n- 支持的软件包众多\n    \n- 。镜像体积较小。\n    \n- 用户数量大，社区活跃，容易及时发现和修复安全问题。\n    \n- 相比较 Alpine 具有更通用的 C 语言标准库 glibc。\n    \n- 文档和教程丰富。\n    \n\n  \n\n### Alpine\n\n我们再来看另一种通用镜像 Alpine。在很长的时间里，Alpine 发行版并没有受到太多的关注。直到 Docker 时代，大家为了追求更小的镜像体积才开始大量使用 Alpine 镜像\n\n- 快速的包安装体验。\n    \n- 极小的镜像体积。\n    \n- 只包含少量的系统级程序，安全性更高。\n    \n- 更轻量的初始化系统 OpenRC。\n    \n\n## 专用镜像\n\n**专用镜像提供了特定语言的编译和运行环境**，绝大多数语言都有 Docker 官方维护的专用镜像。在实际工作中，专用镜像一般有下面两种使用场景\n\n- 作为解释型语言的运行镜像使用，例如 python:latest、php:8.1-fpm-buster 等。\n    \n- 作为编译型语言多阶段构建中编译阶段的基础镜像使用，例如 golang:latest。\n    \n\n  \n\n### Golang\n\n对于编译型语言，我推荐你使用多阶段构建的方法将构建环境和运行镜像区分开，包括 C 语言。当构建 Golang 业务镜像时，我们就需要选择合适的第一阶段镜像（构建镜像）和第二阶段镜像（运行镜像）。\n\n我介绍了如何通过多阶段构建来打包 Docker 镜像，我使用了 golang:1.17 作为构建镜像，ubuntu:latest 作为运行镜像。\n\n```SQL\n\nFROM golang:1.17 as builder\n# 第一阶段\n......\nRUN go build -o example\n\nFROM ubuntu:latest\n# 第二阶段\n......\nCOPY --from=builder /opt/app/example /opt/app/example\n```\n\n实际上，在选择第一阶段和第二阶段的镜像时，还有其他的组合。例如，你还可以使用 Alpine 镜像。\n\n```SQL\n\nFROM golang:1.17\n# 第一阶段\nRUN go build -o example\n\nFROM alpine:latest\n# 第二阶段\n```\n\n这里需要注意，我们运行镜像时，会得到错误信息。\n\n```SQL\n\nexec /opt/app/example: no such file or directory\n```\n\n这是因为 golang:1.17 镜像的 C 语言标准库是 glibc，而 alpine:latest 使用的是 musl。\n\n在第一阶段 Golang 的构建过程中，虽然我们编译生成了二进制可执行文件，但这个二进制文件并不是“纯静态”的。在默认条件下，CGO 将被开启，这意味着当程序内包含底层是由 C 语言库实现的功能时，二进制可执行文件仍然需要依赖外部的动态链接库。所以，在第二阶段的 Alpine 镜像中，自然找不到第一阶段的外部动态链接库，这就会导致抛出“no such file or directory”的异常。\n\n  \n\n我们有两种方法解决这个问题。\n\n- 禁用 CGO，也就是在多阶段构建的编译过程中指定 CGO_ENABLED=0，达到“纯静态”编译的效果，然后，你就可以使用任意的通用镜像来运行了。\n    \n- 在第一阶段和第二阶段同时使用 Alpine 版本的镜像，例如第一阶段使用 golang:alpine，第二阶段使用 alpine:latest 镜像。\n    \n\n  \n\n### Java\n\n我同样推荐你使用多阶段构建来将编译和运行镜像区分开。和 Golang 不同的是，J**ava 程序需要 JVM 的支持才能运行。**所以，**在编译阶段，我们需要 JDK 工具提供完整的编译环境；而在运行阶段，我们只需要 JRE 即可**。\n\n  \n\n![](statistic/asynccode-158.png)\n\n## 其他解释型语言\n\n常见的解释型语言有 Python、Node、PHP 和 Ruby，和编译型语言不同，它不需要编译阶段，所以多阶段构建自然也就失去了价值。\n\n- 在一些情况下，**如果你确信你的业务程序不依赖于外部的 C 代码库，那么你可以考虑使用专用镜像的 Alpine 版本作为基础镜像**，\n    \n- **但是，在大多数情况下，我们是很难确定的**。所以，对于解释型语言，我并不推荐你使用专用镜像的 Alpine 版本作为基础镜像\n    \n\n实际上，除了 Alpine 版本以外，大多数专用镜像都会提供 Slim 版本，Slim 版本大都基于 Ubuntu、Debian 或 CentOS 等标准 Linux 发行版构建，并且删除了一些不必要的系统应用，体积也相对较小，非常适合作为首选镜像\n\n![](statistic/asynccode-160.png)","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/lua/Lua%E9%AB%98%E7%BA%A7":{"title":"Lua高级","content":"# 元表\n\n元表 _(metatable)_ 的**表现行为类似于 C++ 语言中的操作符重载**，例如我们可以重载 \"__add\" 元方法 _(metamethod)_，来计算两个 Lua 数组的并集；或者重载 \"__index\" 方法，来定义我们自己的 Hash 函数。Lua 提供了两个十分重要的用来处理元表的方法\n\n- setmetatable(table, metatable)：此方法用于为一个表设置元表。\n    \n- getmetatable(table)：此方法用于获取表的元表对象\n    \n\n设置元表\n\n```Lua\nlocal mytable = {}\nlocal mymetatable = {}\nsetmetatable(mytable, mymetatable)\n```\n\n  \n\n## **修改表的操作符行为**\n\n  \n\n通过重载 \"__add\" 元方法来计算集合的并集实例\n\n```Lua\nlocal set1 = {10, 20, 30}   -- 集合\nlocal set2 = {20, 40, 50}   -- 集合\n\n-- 将用于重载__add的函数，注意第一个参数是self\nlocal union = function (self, another)\n    local set = {}\n    local result = {}\n\n    -- 利用数组来确保集合的互异性\n    for i, j in pairs(self) do set[j] = true end\n    for i, j in pairs(another) do set[j] = true end\n\n    -- 加入结果集合\n    for i, j in pairs(set) do table.insert(result, i) end\n    return result\nend\nsetmetatable(set1, {__add = union}) -- 重载 set1 表的 __add 元方法\n\nlocal set3 = set1 + set2\nfor _, j in pairs(set3) do\n    io.write(j..\" \")               --\u003eoutput：30 50 20 40 10\nend\n```\n\n除了加法可以被重载之外，Lua 提供的所有操作符都可以被重载：\n| 元方法        | 含义                                                                         |\n|------------|----------------------------------------------------------------------------|\n| \"__add    | #NAME?                                                                     |\n| \"__sub   | - 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__mul    | * 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__div   | / 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__mod   | % 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__pow    | ^ （幂）操作 其行为类似于 \"add\" 操作                                                    |\n| \"__unm\"   | 一元 - 操作                                                                    |\n| \"__concat\" | .. （字符串连接）操作                                                               |\n| \"__len\"    | # 操作                                                                       |\n| \"__eq\"     | == 操作 函数 getcomphandler 定义了 Lua 怎样选择一个处理器来作比较操作 仅在两个对象类型相同且有对应操作相同的元方法时才起效 |\n| \"__lt\"     | \u003c 操作                                                                       |\n| \"__le\"     | \u003c= 操作                                                                      |\n\n\n除了操作符之外，如下元方法也可以被重载，下面会依次解释使用方法：\n\n|   |   |\n|---|---|\n|元方法|含义|\n|\"__index\"|取下标操作用于访问 table[key]|\n|\"__newindex\"|赋值给指定下标 table[key] = value|\n|\"__tostring\"|转换成字符串|\n|\"__call\"|当 Lua 调用一个值时调用|\n|\"__mode\"|用于弱表(week table)|\n|\"__metatable\"|用于保护metatable不被访问|\n\n## **__index 元方法**\n\n```Lua\nmytable = setmetatable({key1 = \"value1\"},   --原始表\n{__index = function(self, key)            --重载函数\n    if key == \"key2\" then\n        return \"metatablevalue\"\n    end\nend\n})\n\nprint(mytable.key1,mytable.key2)  --\u003e output：value1 metatablevalue\n```\n\n关于 __index 元方法，有很多比较高阶的技巧，例如：__index 的元方法不需要非是一个函数，他也可以是一个表。\n\n```Lua\nt = setmetatable({[1] = \"hello\"}, {__index = {[2] = \"world\"}})\nprint(t[1], t[2])   --\u003ehello wor\n```\n\n## **__tostring 元方法**\n\n  \n\n与 Java 中的 toString() 函数类似，可以实现自定义的字符串转换。\n\n```Lua\narr = {1, 2, 3, 4}\narr = setmetatable(arr, {__tostring = function (self)\n    local result = '{'\n    local sep = ''\n    for _, i in pairs(self) do\n        result = result ..sep .. i\n        sep = ', '\n    end\n    result = result .. '}'\n    return result\nend})\nprint(arr)  --\u003e {1, 2, 3, 4}\n```\n\n## **__call 元方法**\n\n__call 元方法的功能类似于 C++ 中的仿函数，使得普通的表也可以被调用。\n\n  \n\n```Lua\nfunctor = {}\nfunction func1(self, arg)\n    print (\"called from\", arg)\nend\nsetmetatable(functor, {__call = func1})\n\nfunctor(\"functor\")  --\u003e called from functor\nprint(functor)      --\u003e output：0x00076fc8 （后面这串数字可能不一样）\n```\n\n## **__metatable 元方法**\n\n假如我们想保护我们的对象使其使用者既看不到也不能修改 metatables。我**们可以对 metatable 设置了 __metatable 的值，getmetatable 将返回这个域的值，而调用 setmetatable 将会出错**：\n\n```Lua\nbject = setmetatable({}, {__metatable = \"You cannot access here\"})\n\nprint(getmetatable(Object)) --\u003e You cannot access heresetmetatable(Object, {})    --\u003e 引发编译器报错\n```\n\n  \n\n# 面向对象\n\n## 类\n\n在 Lua 中，我们可以使用表和函数实现面向对象。**将函数和相关的数据放置于同一个表中就形成了一个对象。**\n\n```Plaintext\nlocal _M = {}\n\nlocal mt = { __index = _M }\n\nfunction _M.deposit (self, v)\n    self.balance = self.balance + v\nend\n\nfunction _M.withdraw (self, v)\n    if self.balance \u003e v then\n        self.balance = self.balance - v\n    else\n        error(\"insufficient funds\")\n    end\nend\n\nfunction _M.new (self, balance)\n    balance = balance or 0\n    return setmetatable({balance = balance}, mt)\nend\n\nreturn _M\n```\n\n引用\n\n```Lua\nlocal account = require(\"account\")\n\nlocal a = account:new()\na:deposit(100)\n\nlocal b = account:new()\nb:deposit(50)\n\nprint(a.balance)  --\u003e output: 100\nprint(b.balance)  --\u003e output: 50\n```\n\n上面这段代码 \"setmetatable({balance = balance}, mt)\"，其中 mt 代表 `{ __index = _M }` ，这句话值得注意。根据我们在元表这一章学到的知识，我们明白，setmetatable 将 `_M` 作为新建表的原型，所以在自己的表内找不到 'deposit'、'withdraw' 这些方法和变量的时候，便会到 __index 所指定的 _M 类型中去寻找。\n\n  \n\n## 继承\n\n继承可以用元表实现，它提供了在父类中查找存在的方法和变量的机制。在 Lua 中是不推荐使用继承方式完成构造的，这样做引入的问题可能比解决的问题要多，下面一个是字符串操作类库，给大家演示一下。\n\n```Lua\n---------- s_base.lualocal _M = {}\n\nlocal mt = { __index = _M }\n\nfunction _M.upper (s)return string.upper(s)\nendreturn _M\n\n---------- s_more.lualocal s_base = require(\"s_base\")\n\nlocal _M = {}\n_M = setmetatable(_M, { __index = s_base })\n\n\nfunction _M.lower (s)return string.lower(s)\nendreturn _M\n\n---------- test.lualocal s_more = require(\"s_more\")\n\nprint(s_more.upper(\"Hello\"))   -- output: HELLOprint(s_more.lower(\"Hello\"))   -- output: hello\n```\n\n  \n\n## 成员私有性\n\n在动态语言中引入成员私有性并没有太大的必要，反而会显著增加运行时的开销，毕竟这种检查无法像许多静态语言那样在编译期完成。下面的技巧把对象作为各方法的 upvalue，本身是很巧妙的，但会让子类继承变得困难，同时构造函数动态创建了函数，会导致构造函数无法被 JIT 编译。\n\n在 Lua 中，成员的私有性，使用类似于函数闭包的形式来实现。在我们之前的银行账户的例子中，我们使用一个工厂方法来创建新的账户实例，通过工厂方法对外提供的闭包来暴露对外接口。而不想暴露在外的例如 balance 成员变量，则被很好的隐藏起来。\n\n```Lua\nfunction newAccount (initialBalance)\n    local self = {balance = initialBalance}\n    local withdraw = function (v)\n        self.balance = self.balance - v\n    end\n    local deposit = function (v)\n        self.balance = self.balance + v\n    end\n    local getBalance = function () \n        return self.balance \n    end\n    \n    return {\n        withdraw = withdraw,\n        deposit = deposit,\n        getBalance = getBalance\n    }\nend\n\na = newAccount(100)\na.deposit(100)\nprint(a.getBalance()) --\u003e 200print(a.balance)      --\u003e nil\n```\n\n  \n\n# 局部变量\n\nLua 的设计有一点很奇怪，**在一个 block 中的变量，如果之前没有定义过，那么认为它是一个全局变量**，**而不是这个 block 的局部变量**。这一点和别的语言不同。**容易造成不小心覆盖了全局同名变量的错误**。\n\n## **定义**\n\nLua 中的局部变量要用 local 关键字来显式定义，不使用 local 显式定义的变量就是全局变量\n\n```Lua\ng_var = 1         -- global var\nlocal l_var = 2   -- local var\n```\n\n## **作用域**\n\n**局部变量的生命周期是有限的，它的作用域仅限于声明它的块（block）**。一个块是一个控制结构的执行体、或者是一个函数的执行体再或者是一个程序块（chunk）。\n\n```Lua\nx = 10\nlocal i = 1         -- 程序块中的局部变量 i\n\nwhile i \u003c=x do\n    local x = i * 2   -- while 循环体中的局部变量 x\n    print(x)          -- output： 2, 4, 6, 8, ...\n    i = i + 1\nend\n\nif i \u003e 20 then\n    local x           -- then 中的局部变量 x\n    x = 20\n    print(x + 2)      -- 如果i \u003e 20 将会打印 22，此处的 x 是局部变量\nelse\n    print(x)          -- 打印 10，这里 x 是全局变量\nend\n\nprint(x)            -- 打印 10\n```\n\n  \n\n## 使用局部变量的好处\n\n  \n\n1. 局部变量可以避免因为命名问题污染了全局环境\n    \n2. local 变量的访问比全局变量更快\n    \n3. 由于局部变量出了作用域之后生命周期结束，这样可以被垃圾回收器及时释放\n    \n\n  \n\n  \n\n## 检测模块的函数使用局部变量\n\nfoo.lua\n\n```Lua\nlocal _M = { _VERSION = '0.01' }\n\nfunction _M.add(a, b)     --两个number型变量相加\n    return a + b\nend\n\nfunction _M.update_A()    --更新变量值\n    A = 365               -- A 是全局变量\nend\n\nreturn _M\n```\n\nuse_foo.lua\n\n```Lua\nA = 360     --定义全局变量\n\nlocal foo = require(\"foo\")\n\nlocal b = foo.add(A, A)\nprint(\"b = \", b)\n\nfoo.update_A()\nprint(\"A = \", A)\n```\n\n因为A 是全局变量，改变了A的值\n\nLua 上下文中应当严格避免使用自己定义的全局变量。**可以使用一个 lj-releng 工具来扫描 Lua 代码，定位使用 Lua 全局变量的地方**。lj-releng 的相关链接：[https://github.com/openresty/openresty-devel-utils/blob/master/lj-releng](https://github.com/openresty/openresty-devel-utils/blob/master/lj-releng)\n\nWindows 用户把 lj-releng 文件所在的目录的绝对路径添加进 PATH 环境变量。然后进入你自己的 Lua 文件所在的工作目录，得到如下结果：\n\n```Lua\n#  lj-releng\nfoo.lua: 0.01 (0.01)\nChecking use of Lua global variables in file foo.lua...\nop no.  line  instruction args  ; code\n2  [8] SETGLOBAL 0 -1  ; A\nChecking line length exceeding 80...\nWARNING: No \"_VERSION\" or \"version\" field found in `use_foo.lua`.\nChecking use of Lua global variables in file use_foo.lua...\nop no.  line  instruction args  ; code\n2  [1] SETGLOBAL 0 -1  ; A\n7  [4] GETGLOBAL 2 -1  ; A\n8  [4] GETGLOBAL 3 -1  ; A\n18 [8] GETGLOBAL 4 -1  ; A\n```\n\n当然，更推荐采用 **luacheck 来检查项目中全局变量，之后的“代码静态分析”一节，我们还会讲到如何使用 luacheck**。\n\n  \n\n# 判断数组的大小\n\n- table.getn(t) 等价于 #t 但**计算的是数组元素，不包括 hash 键值**。而且数组是以第一个 nil 元素来判断数组结束。\n    \n- `#` 只计算 array 的元素个数，它实际上调用了对象的 metatable 的 `__len` 函数。对于有 `__len` 方法的函数返回函数返回值，不然就返回数组成员数目\n    \n- _Lua_ 内部实际采用哈希表和数组分别保存键值对、普通值，所以不推荐混合使用这两种赋值方式。\n    \n- Lua 数组中允许 nil 值的存在，但是数组默认结束标志却是 nil。这类比于 C 语言中的字符串，字符串中允许 '\\0' 存在，但当读到 '\\0' 时，就认为字符串已经结束了。\n    \n- 初始化是例外，在 Lua 相关源码中，初始化数组时首先判断数组的长度，若长度大于 0 ，并且最后一个值不为 nil，返回包括 nil 的长度；若最后一个值为 nil，则返回截至第一个非 nil 值的长度。\n    \n- **如果你要删除一个数组中的元素，请使用 remove 函数，而不是用 nil 赋值**\n    \n\n```Lua\n-- test.lua\nlocal tblTest1 = { 1, a = 2, 3 }\nprint(\"Test1 \" .. #(tblTest1))\n\nlocal tblTest2 = { 1, nil }\nprint(\"Test2 \" .. #(tblTest2))\n\nlocal tblTest3 = { 1, nil, 2 }\nprint(\"Test3 \" .. #(tblTest3))\n\nlocal tblTest4 = { 1, nil, 2, nil }\nprint(\"Test4 \" .. #(tblTest4))\n\nlocal tblTest5 = { 1, nil, 2, nil, 3, nil }\nprint(\"Test5 \" .. #(tblTest5))\n\nlocal tblTest6 = { 1, nil, 2, nil, 3, nil, 4, nil }\nprint(\"Test6 \" .. #(tblTest6))\n```\n\n我们分别使用 Lua 和 LuaJIT 来执行一下：\n\n```Lua\n➜ luajit test.lua\nTest1 2\nTest2 1\nTest3 1\nTest4 1\nTest5 1\nTest6 1\n\n➜ lua test.lua\nTest1 2\nTest2 1\nTest3 3\nTest4 1\nTest5 3\nTest6 1\n```\n\n这一段的输出结果，就是这么 **匪夷所思**。不要在 Lua 的 table 中使用 nil 值，**如果一个元素要删除，直接 remove，不要用 nil 去代替**。\n\n  \n\n# 非空判断\n\n  \n\n有时候不小心引用了一个没有赋值的变量，这时它的值默认为 nil。如果对一个 nil 进行索引的话，会导致异常。\n\n```Plaintext\nlocal person = {name = \"Bob\", sex = \"M\"}\n\n-- do something\nperson = nil\n-- do something\nprint(person.name)\n```\n\n会报错\n\n```Lua\nstdin:1:attempt to index global 'person' (a nil value)\nstack traceback:\n   stdin:1: in main chunk\n   [C]: ?\n```\n\n在实际的工程代码中，我们很难这么轻易地发现我们引用了 nil 变量。因此，在很多情况下我们在访问一些 table 型变量时，需要先判断该变量是否为 nil，例如将上面的代码改成\n\n```Lua\nlocal person = {name = \"Bob\", sex = \"M\"}\n\n-- do something\nperson = nil\n-- do something\nif person ~= nil and person.name ~= nil then\n    print(person.name)\nelse\n-- do somethingend\n```\n\n对于简单类型的变量，我们可以用 _if (var == nil) then_ 这样的简单句子来判断。**但是对于 table 型的 Lua 对象，就不能这么简单判断它是否为空了。一个 table 型变量的值可能是** **`{}`**，这时它不等于 nil。我们来看下面这段代码：\n\n```Lua\nlocal next = next\nlocal a = {}\nlocal b = {name = \"Bob\", sex = \"Male\"}\nlocal c = {\"Male\", \"Female\"}\nlocal d = nil\n\nprint(#a)\nprint(#b)\nprint(#c)\n--print(#d)    -- error\n\nif a == nil then\n    print(\"a == nil\")\nend\n\nif b == nil then\n    print(\"b == nil\")\nend\n\nif c == nil then\n    print(\"c == nil\")\nend\n\nif d== nil then\n    print(\"d == nil\")\nend\n\nif next(a) == nil then\n    print(\"next(a) == nil\")\nend\n\nif next(b) == nil then\n    print(\"next(b) == nil\")\nend\n\nif next(c) == nil then\n    print(\"next(c) == nil\")\nend\n```\n\n输出\n\n```Lua\n0\n0\n2\nd == nil\nnext(a) == nil\n```\n\n因此，我们要判断一个 table 是否为 `{}`，不能采用 `#table == 0` 的方式来判断。可以用下面这样的方法来判断：\n\n```Plaintext\nfunction isTableEmpty(t)\n    return t == nil or next(t) == nil\nend\n```\n\n注意：**`next`** **指令是不能被 LuaJIT 的 JIT 编译优化，并且 LuaJIT 貌似没有明确计划支持这个指令优化，在不是必须的情况下，尽量少用。**\n\n  \n\n# 正则表达式\n\n同时存在两套正则表达式规范：_Lua_ 语言的规范和 `ngx.re.*` 的规范，即使您对 _Lua_ 语言中的规范非常熟悉，我们仍不建议使用 _Lua_ 中的正则表达式。\n\n- 一是因为 _Lua_ 中正则表达式的性能并不如 `ngx.re.*` 中的正则表达式优秀；\n    \n- 二是 _Lua_ 中的正则表达式并不符合 _POSIX_ 规范，而 `ngx.re.*` 中实现的是标准的 _POSIX_ 规范，后者明显更具备通用性。\n    \n\n`ngx.re.*` 中的 `o` 选项，指明该参数，被编译的 Pattern 将会在工作进程中缓存，并且被当前工作进程的每次请求所共享。Pattern 缓存的上限值通过 `lua_regex_cache_max_entries` 来修改，它的默认值为1024。\n\n`ngx.re.*` 中的 `j` 选项，指明该参数，如果使用的 PCRE 库支持 JIT，OpenResty 会在编译 Pattern 时启用 JIT。启用 JIT 后正则匹配会有明显的性能提升。较新的平台，自带的 PCRE 库均支持 JIT。如果系统自带的 PCRE 库不支持 JIT，出于性能考虑，最好自己编译一份 libpcre.so，然后在编译 OpenResty 时链接过去。要想验证当前 PCRE 库是否支持 JIT，可以这么做\n\n1. 编译 OpenResty 时在 `./configure` 中指定 `--with-debug` 选项\n    \n2. 在 `error_log` 指令中指定日志级别为 `debug`\n    \n3. 运行正则匹配代码，查看日志中是否有 `pcre JIT compiling result: 1`\n    \n\n即使运行在不支持 JIT 的 OpenResty 上，加上 `j` 选项也不会带来坏的影响。在 OpenResty 官方的 Lua 库中，正则匹配至少都会带上 `jo` 这两个选项。\n\n```Lua\nlocation /test {\n    content_by_lua_block {\n        local regex = [[\\d+]]\n\n        -- 参数 \"j\" 启用 JIT 编译，参数 \"o\" 是开启缓存必须的\n        local m = ngx.re.match(\"hello, 1234\", regex, \"jo\")\n        if m then\n            ngx.say(m[0])\n        else\n            ngx.say(\"not matched!\")\n        end\n    }\n}\n```\n\n#### **Lua 正则简单汇总**\n\n_Lua_ 中正则表达式语法上最大的区别，_Lua_ 使用 _'%'_ 来进行转义，而其他语言的正则表达式使用 _'\\'_ 符号来进行转义。其次，_Lua_ 中并不使用 _'?'_ 来表示非贪婪匹配，而是定义了不同的字符来表示是否是贪婪匹配。定义如下：\n\n|符号|匹配次数|匹配模式|\n|---|---|---|\n|+|匹配前一字符 1 次或多次|非贪婪|\n|`*`|匹配前一字符 0 次或多次|贪婪|\n|-|匹配前一字符 0 次或多次|非贪婪|\n|?|匹配前一字符 0 次或1次|仅用于此，不用于标识是否贪婪|\n\n|符号|匹配模式|\n|---|---|\n|.|任意字符|\n|%a|字母|\n|%c|控制字符|\n|%d|数字|\n|%l|小写字母|\n|%p|标点字符|\n|%s|空白符|\n|%u|大写字母|\n|%w|字母和数字|\n|%x|十六进制数字|\n|%z|代表 0 的字符|\n\n  \n\n# 虚变量\n\n当一个方法返回多个值时，有些返回值有时候用不到，要是声明很多变量来一一接收，显然不太合适（不是不能）。**Lua 提供了一个虚变量(dummy variable)的概念， 按照****[惯例](https://www.lua.org/pil/1.3.html)****以一个下划线（“_”）来命名，用它来表示丢弃不需要的数值，仅仅起到占位的作用。**\n\n  \n\n## 返回值\n\n```Lua\n-- string.find (s,p) 从string 变量s的开头向后匹配 string\n-- p，若匹配不成功，返回nil，若匹配成功，返回第一次匹配成功\n-- 的起止下标。\n\nlocal start, finish = string.find(\"hello\", \"he\") --start 值为起始下标，finish\n--值为结束下标\nprint ( start, finish )                          --输出 1   2\n\nlocal start = string.find(\"hello\", \"he\")      -- start值为起始下标\nprint ( start )                               -- 输出 1\n\n\nlocal _,finish = string.find(\"hello\", \"he\")   --采用虚变量（即下划线），接收起\n--始下标值，然后丢弃，finish接收\n--结束下标值\nprint ( finish )                              --输出 2\nprint ( _ )    \n```\n\n  \n\n## 迭代\n\n```Lua\n-- test.lua 文件\nlocal t = {1, 3, 5}\n\nprint(\"all  data:\")\nfor i,v in ipairs(t) do\n    print(i,v)\nend\n\nprint(\"\")\nprint(\"part data:\")\nfor _,v in ipairs(t) do\n    print(v)\nend\n```\n\n输出\n\n```Lua\n# luajit test.lua\nall  data:\n1   1\n2   3\n3   5\n\npart data:\n1\n3\n5\n```\n\n# **抵制使用 module() 定义模块**\n\n旧式的模块定义方式是通过 `module(\"filename\"[,package.seeall])*` 来显式声明一个包，现在官方不推荐再使用这种方式\n\n这种方式将会返回一个由 `filename` 模块函数组成的 `table`，并且还会定义一个包含该 `table` 的全局变量。\n\n  \n\n1. `package.seeall` 这种方式破坏了模块的高内聚，原本引入 \"filename\" 模块只想调用它的 _foobar()_ 函数，但是它却可以读写全局属性，例如 `\"filename.os\"`。\n    \n2. `module` 函数压栈操作引发的副作用，污染了全局环境变量。例如 `module(\"filename\")` 会创建一个 `filename` 的 `table`，并将这个 `table` 注入全局环境变量中，这样使得没有引用它的文件也能调用 `filename` 模块的方法。\n    \n\n  \n\n推荐的模块定义\n\n```Lua\n-- square.lua 长方形模块\nlocal _M = {}           -- 局部的变量\n_M._VERSION = '1.0'     -- 模块版本\n\nlocal mt = { __index = _M }\n\nfunction _M.new(self, width, height)\n    return setmetatable({ width=width, height=height }, mt)\nend\n\nfunction _M.get_square(self)\n    return self.width * self.height\nend\n\nfunction _M.get_circumference(self)\n    return (self.width + self.height) * 2\nend\n\nreturn _M\n```\n\n使用\n\n```Lua\nlocal square = require \"square\"\nlocal s1 = square:new(1, 2)\nprint(s1:get_square())          --output: 2\nprint(s1:get_circumference())   --output: 6\n```\n\n另一个跟 Lua 的 module 模块相关需要注意的点是，当 lua_code_cache on 开启时，require 加载的模块是会被缓存下来的，这样我们的模块就会以最高效的方式运行，直到被显式地调用如下语句（这里有点像模块卸载）：\n\n```Plaintext\npackage.loaded[\"square\"] = nil\n```\n\n  \n\n## 调用函数前先定义函数\n\nLua 里面的函数必须放在调用的代码之前，下面的代码是一个常见的错误：\n\n```Lua\n-- test.lua 文件local i = 100\ni = add_one(i)\n\nfunction add_one(i)\n    return i + 1\nend\n```\n\n因此在函数定义之前使用函数相当于在变量赋值之前使用变量，Lua 世界对于没有赋值的变量，默认都是 nil，所以这里也就产生了一个 nil 的错误。\n\n  \n\n# 点号操作符和冒号操作符的区别\n\n```Plaintext\nlocal str = \"abcde\"\n\nprint(\"case 1:\", str:sub(1, 2))\nprint(\"case 2:\", str.sub(str, 1, 2))\n```\n\n输出\n\n```Lua\ncase 1: ab\ncase 2: ab\n```\n\n- **冒号操作会带入一个** **`self`** **参数，用来代表** **`自己`****。**\n    \n- 而点号操作，只是 `内容` 的展开。\n    \n\n在函数定义时，使用冒号将默认接收一个 `self` 参数，而使用点号则需要显式传入 `self` 参数\n\n示例代码：\n\n```Plaintext\nobj = { x = 20 }\n\nfunction obj:fun1()\n    print(self.x)\nend\n```\n\n等价于\n\n```Plaintext\nobj = { x = 20 }\n\nfunction obj.fun1(self)\n    print(self.x)\nend\n```\n\n# module的缺点\n\n由于 `lua_code_cache off` 情况下，缓存的代码会伴随请求完结而释放。module 的最大好处缓存这时候是无法发挥的，所以本章的内容都是基于 `lua_code_cache on` 的情况下。\n\n先看看下面代码：\n\n```Plaintext\nlocal ngx_socket_tcp = ngx.socket.tcp           -- ①\n\nlocal _M = { _VERSION = '0.06' }                -- ②\nlocal mt = { __index = _M }                     -- ③\n\nfunction _M.new(self)\n    local sock, err = ngx_socket_tcp()          -- ④\n    if not sock then\n        return nil, err\n    end\n    return setmetatable({ sock = sock }, mt)    -- ⑤\nend\n\nfunction _M.set_timeout(self, timeout)\n    local sock = self.sock\n    if not sock then\n        return nil, \"not initialized\"\n    end\n\n    return sock:settimeout(timeout)\nend\n\n-- ... 其他功能代码，这里简略\n\nreturn _M\n```\n\n1. 对于比较底层的模块，内部使用到的非本地函数，都需要 local 本地化，这样做的好处：\n    \n    1. 避免命名冲突：防止外部是 `require(...)` 的方法调用造成全局变量污染\n        \n    2. 访问局部变量的速度比全局变量更快、更快、更快（重要事情说三遍）\n        \n\n  \n\n2. 每个基础模块最好有自己 `_VERSION` 标识，方便后期利用 `_VERSION` 完成热代码部署等高级特性，也便于使用者对版本有整体意识。\n    \n3. 其实 `_M` 和 `mt` 对于不同的请求实例（require 方法得到的对象）是相同的，因为 module 会被缓存到全局环境中。所以在这个位置千万不要放单请求内个性信息，例如 ngx.ctx 等变量。\n    \n4. **这里需要实现的是给每个实例绑定不同的 tcp 对象**，后**面 setmetatable 确保了每个实例拥有自己的 socket 对象，所以必须放在 new 函数中**。如果放在 ③ 的下面，那么这时候所有的不同实例内部将绑定了同一个 socket 对象。\n    \n\n```Plaintext\nlocal mt = { __index = _M }                     -- ③\nlocal sock = ngx_socket_tcp()                   -- ④ 错误的\n\nfunction _M.new(self)\n    return setmetatable({ sock = sock }, mt)    -- ⑤\nend\n```\n\n5. Lua 的 module 有两种类型：\n    \n    1. 支持面向对象痕迹可以保留私有属性；静态方法提供者，没有任何私有属性。\n        \n    2. 真正起到区别作用的就是 setmetatable 函数，是否有自己的个性元表，最终导致两种不同的形态。\n        \n\n# FFI\n\nhttps://moonbingbing.gitbooks.io/openresty-best-practices/content/lua/FFI.html\n\nFFI 库，是 LuaJIT 中最重要的一个扩展库。它允许从纯 Lua 代码调用外部 C 函数，使用 C 数据结构。\n\n  \n\nFFI 库最大限度的省去了使用 C 手工编写繁重的 `Lua/C` 绑定的需要。不需要学习一门独立/额外的绑定语言——它解析普通 C 声明。这样可以从 C 头文件或参考手册中，直接剪切，粘贴。它的任务就是绑定很大的库，但不需要捣鼓脆弱的绑定生成器。\n\nFFI 紧紧的整合进了 LuaJIT（几乎不可能作为一个独立的模块）。`JIT` 编译器在 C 数据结构上所产生的代码，等同于一个 C 编译器应该生产的代码。在 `JIT` 编译过的代码中，调用 C 函数，可以被内连处理，不同于基于 `Lua/C API` 函数调用。\n\n  \n\n## **ffi 库 词汇**\n\n|   |   |\n|---|---|\n|noun|Explanation|\n|cdecl|A definition of an abstract C type(actually, is a lua string)|\n|ctype|C type object|\n|cdata|C data object|\n|ct|C type format, is a template object, may be cdecl, cdata, ctype|\n|cb|callback object|\n|VLA|An array of variable length|\n|VLS|A structure of variable length|\n\n## **ffi.* API**\n\n**功能：** _Lua ffi 库的 API，与 LuaJIT 不可分割。_\n\n毫无疑问，在 `lua` 文件中使用 `ffi` 库的时候，必须要有下面的一行。\n\n```Plaintext\nlocal ffi = require \"ffi\"\n```\n\n# JIT\n\n看一下 LuaJIT 官方的解释：LuaJIT is a Just-In-Time Compilerfor the Lua programming language。\n\n**LuaJIT 的运行时环境包括一个用手写汇编实现的 Lua 解释器和一个可以直接生成机器代码的 JIT 编译器**\n\n- 一开始的时候，Lua 字节码总是被 LuaJIT 的解释器解释执行。LuaJIT 的解释器会在执行字节码时同时记录一些运行时的统计信息，比如每个 Lua 函数调用入口的实际运行次数，还有每个 Lua 循环的实际执行次数。\n    \n- 当这些次数超过某个预设的阈值时，便认为对应的 Lua 函数入口或者对应的 Lua 循环足够的“热”，这时便会触发 JIT 编译器开始工作。\n    \n- JIT 编译器会从热函数的入口或者热循环的某个位置开始尝试编译对应的 Lua 代码路径。编译的过程是把 LuaJIT 字节码先转换成 LuaJIT 自己定义的中间码（IR），然后再生成针对目标体系结构的机器码（比如 x86_64 指令组成的机器码）\n    \n- 如果当前 Lua 代码路径上的所有的操作都可以被 JIT 编译器顺利编译，则这条编译过的代码路径便被称为一个“trace”，在物理上对应一个 `trace` 类型的 GC 对象（即参与 Lua GC 的对象）。\n    \n\n  \n\nJIT 编译器不支持的原语被称为 **NYI（Not Yet Implemented）原语**。比较完整的 NYI 列表在这篇文档里面：\n\n```Plaintext\nhttp://wiki.luajit.org/NYI\n```\n\n所谓“让更多的 Lua 代码被 JIT 编译”，其实就是帮助更多的 Lua 代码路径能为 JIT 编译器所接受。这一般通过两种途径来实现：\n\n1. 调整对应的 Lua 代码，**避免使用 NYI 原语**。\n    \n2. 增强 JIT 编译器，让越来越多的 NYI 原语能够被编译。\n    \n\n## **可以被 JIT 编译的元操作**\n\n下面给大家列一下截止到目前已经可以被 JIT 编译的元操作。 其他还有 IO、Bit、FFI、Coroutine、OS、Package、Debug、JIT 等分类，使用频率相对较低，这里就不罗列了，可以参考官网：[http://wiki.luajit.org/NYI](http://wiki.luajit.org/NYI)。\n\n### **基础库的支持情况**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|assert|yes||\n|collectgarbage|no||\n|dofile|never||\n|error|never||\n|getfenv|2.1 partial|只有 getfenv(0) 能编译|\n|getmetatable|yes||\n|ipairs|yes||\n|load|never||\n|loadfile|never||\n|loadstring|never||\n|next|no||\n|pairs|no||\n|pcall|yes||\n|print|no||\n|rawequal|yes||\n|rawget|yes||\n|rawlen (5.2)|yes||\n|rawset|yes||\n|select|partial|第一个参数是静态变量的时候可以编译|\n|setfenv|no||\n|setmetatable|yes||\n|tonumber|partial|不能编译非10进制，非预期的异常输入|\n|tostring|partial|只能编译：字符串、数字、布尔、nil 以及支持 __tostring元方法的类型|\n|type|yes||\n|unpack|no||\n|xpcall|yes||\n\n### **字符串库**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|string.byte|yes||\n|string.char|2.1||\n|string.dump|never||\n|string.find|2.1 partial|只有字符串样式查找（没有样式）|\n|string.format|2.1 partial|不支持 %p 或 非字符串参数的 %s|\n|string.gmatch|no||\n|string.gsub|no||\n|string.len|yes||\n|string.lower|2.1||\n|string.match|no||\n|string.rep|2.1||\n|string.reverse|2.1||\n|string.sub|yes||\n|string.upper|2.1||\n\n### **表**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|table.concat|2.1||\n|table.foreach|no|2.1: 内部编译，但还没有外放|\n|table.foreachi|2.1||\n|table.getn|yes||\n|table.insert|partial|只有 push 操作|\n|table.maxn|no||\n|table.pack (5.2)|no||\n|table.remove|2.1|部分，只有 pop 操作|\n|table.sort|no||\n|table.unpack (5.2)|no||\n\n### **math 库**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|math.abs|yes||\n|math.acos|yes||\n|math.asin|yes||\n|math.atan|yes||\n|math.atan2|yes||\n|math.ceil|yes||\n|math.cos|yes||\n|math.cosh|yes||\n|math.deg|yes||\n|math.exp|yes||\n|math.floor|yes||\n|math.fmod|no||\n|math.frexp|no||\n|math.ldexp|yes||\n|math.log|yes||\n|math.log10|yes||\n|math.max|yes||\n|math.min|yes||\n|math.modf|yes||\n|math.pow|yes||\n|math.rad|yes||\n|math.random|yes||\n|math.randomseed|no||\n|math.sin|yes||\n|math.sinh|yes||\n|math.sqrt|yes||\n|math.tan|yes||\n|math.tanh|yes||","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/lua/lua-overview":{"title":"lua overview","content":"# Lua 简介\n\nLua 是一个小巧的脚本语言。是巴西里约热内卢天主教大学（Pontifical Catholic University of Rio de Janeiro）里的一个研究小组并于 1993 年开发。**其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能**。Lua 由标准 C 编写而成，几乎在所有操作系统和平台上都可以编译、运行。Lua 并没有提供强大的库，这是由它的定位决定的。所以 Lua 不适合作为开发独立应用程序的语言。**Lua 有一个同时进行的 JIT 项目，提供在特定平台上的即时编译功能**。\n\n- **Lua 脚本可以很容易的被 C/C++ 代码调用，也可以反过来调用 C/C++ 的函数，这使得 Lua 在应用程序中可以被广泛应用**。\n    \n- **不仅仅作为扩展脚本，也可以作为普通的配置文件，代替 XML、ini 等文件格式，并且更容易理解和维护**。\n    \n- 标准 Lua 5.1 解释器由标准 C 编写而成，代码简洁优美，几乎在所有操作系统和平台上都可以编译和运行；\n    \n- 一个完整的标准 Lua 5.1 解释器不足 200 KB。而本书推荐使用的 LuaJIT 2 的代码大小也只有不足 500 KB\n    \n- 同时也支持大部分常见的体系结构。在目前所有脚本语言引擎中，LuaJIT 2 实现的速度应该算是最快的之一。这一切都决定了 Lua 是作为嵌入式脚本的最佳选择。\n    \n\nLua 语言的各个版本是不相兼容的。因此本书只介绍 Lua 5.1 语言，这是为标准 Lua 5.1 解释器和 LuaJIT 2 所共同支持的。LuaJIT 支持的对 Lua 5.1 向后兼容的 Lua 5.2 和 Lua 5.3 的特性，我们也会在方便的时候予以介绍。\n\n  \n\n# Lua 环境搭建\n\n[http://openresty.org](http://openresty.org/)\n\n  \n\n## Helloworld\n\n```Go\n# cat hello.lua\nprint(\"hello world\")\n# luajit hello.lua\nhello world\n```\n\n  \n\n# 基本数据类型\n\n  \n\n```Go\nprint(type(\"helloworld\"))\nprint(type('helloworld'))\nprint(type('true'))\nprint(type(1))\nprint(type(2.1))\nprint(type(nil))\nfunction hello()\n    print(\"hello\")\nend\nprint(type(hello))\n```\n\n输出\n\n```Go\nstring\nstring\nstring\nnumber\nnumber\nnil\nfunction\n```\n\n## Nil\n\nNil 是一种类型，Lua 将 nil 用于表示“无效值”。\n\n- 一个变量在第一次赋值前的默认值是 nil，\n    \n- 将 nil 赋予给一个全局变量就等同于删除它。\n    \n\n```Go\nlocal num\nprint(num)        --\u003eoutput:nil\n\nnum = 100\nprint(num)        --\u003eoutput:100\n```\n\n## Boolean (布尔)\n\n布尔类型，可选值 true/false；\n\n- Lua 中 nil 和 false 为“假”\n    \n- 其它所有值均为“真”。比如 0 和空字符串就是“真”；\n    \n\n```Go\nlocal a = true\nlocal b = 0\nlocal c = nil\nif a then\n    print(\"a\")        --\u003eoutput:a\nelse\n    print(\"not a\")    --这个没有执行\nend\n\nif b then\n    print(\"b\")        --\u003eoutput:b\nelse\n    print(\"not b\")    --这个没有执行\nend\n\nif c then\n    print(\"c\")        --这个没有执行\nelse\n    print(\"not c\")    --\u003eoutput:not c\nend\n```\n\n## **number（数字）**\n\nNumber 类型用于表示实数，和 C/C++ 里面的 double 类型很类似。可以使用数学函数 math. Floor（向下取整）和 math. Ceil（向上取整）进行取整操作。\n\n一般地，Lua 的 number 类型就是用双精度浮点数来实现的。值得一提的是，LuaJIT 支持所谓的“dual-number”（双数）模式，\n\n- 即 **LuaJIT 会根据上下文用整型来存储整数，而用双精度浮点数来存放浮点数。**\n    \n\n```Go\nlocal order = 3.99\nlocal score = 98.01\nprint(math.floor(order))   --\u003eoutput:3\nprint(math.ceil(score))    --\u003eoutput:99\nprint(9223372036854775807LL - 1)  --\u003eoutput:9223372036854775806LL\n```\n\n## String（字符串）\n\nLua 中有三种方式表示字符串:\n\n1. 使用一对匹配的单引号。例：'hello'。\n    \n2. 使用一对匹配的双引号。例：\"abclua\"。\n    \n3. 字符串还可以用一种长括号（即 [[ ]]）括起来的方式定义\n    \n    1. 我们把两个正的方括号（即[[）间插入 n 个等号定义为第 n 级正长括号。\n        \n    2. 0 级正的长括号写作 [[ ，一级正的长括号写作 [=[\n        \n    3. 反的长括号也作类似定义；举个例子，4 级反的长括号写作 ]====]\n        \n    4. **一个长字符串可以由任何一级的正的长括号开始，而由第一个碰到的同级反的长括号结束**。整个词法分析过程将**不受分行限制，不处理任何转义符，并且忽略掉任何不同级别的长括号**\n        \n\n  \n\n```Plaintext\nlocal str1 = 'hello world'\nlocal str2 = \"hello lua\"\nlocal str3 = [[\"add\\name\",'hello']]\nlocal str4 = [=[string have a [[]].]=]\nlocal str5 = [=[asdfasd]=]\n\nprint(str1)    --\u003eoutput:hello world\nprint(str2)    --\u003eoutput:hello lua\nprint(str3)    --\u003eoutput:\"add\\name\",'hello'\nprint(str4)    --\u003eoutput:string have a [[]].\nprint(str5)    --\u003eoutput:asdfasd\n```\n\n在 Lua 实现中，Lua 字符串一般都会经历一个“内化”（intern）的过程，**即两个完全一样的 Lua 字符串在 Lua 虚拟机中只会存储一份**。每一个 Lua 字符串在创建时都会**插入到 Lua 虚拟机内部的一个全局的哈希表**中\n\n1. 创建相同的 Lua 字符串并不会引入新的动态内存分配操作，所以相对便宜（但仍有全局哈希表查询的开销），\n    \n2. 内容相同的 Lua 字符串不会占用多份存储空间，\n    \n3. 已经创建好的 Lua 字符串之间进行相等性比较时是 `O(1)` 时间度的开销，而不是通常见到的 `O(n)`.\n    \n\n## Table (表)\n\nTable 类型实现了一种抽象的“关联数组”。“关联数组”是一种具有特殊索引方式的数组，\n\n- 索引通常是**字符串（string）或者 number 类型，但也可以是除** **`nil`** **以外的任意类型的值**\n    \n\n```Go\n\nlocal corp = {\n    web = \"www.google.com\",   --索引为字符串，key = \"web\",\n    --            value = \"www.google.com\"\n    telephone = \"12345678\",   --索引为字符串\n    staff = {\"Jack\", \"Scott\", \"Gary\"}, --索引为字符串，值也是一个表\n    100876,              --相当于 [1] = 100876，此时索引为数字\n    --      key = 1, value = 100876\n    100191,              --相当于 [2] = 100191，此时索引为数字\n    [10] = 360,          --直接把数字索引给出\n    [\"city\"] = \"Beijing\" --索引为字符串\n}\n\nprint(corp.web)               --\u003eoutput:www.google.com\nprint(corp[\"web\"])               --\u003eoutput:www.google.com\nprint(corp[\"telephone\"])      --\u003eoutput:12345678\nprint(corp[2])                --\u003eoutput:100191\nprint(corp[\"city\"])           --\u003eoutput:\"Beijing\"\nprint(corp.staff[1])          --\u003eoutput:Jack\nprint(corp[\"staff\"][1])          --\u003eoutput:Jack\nprint(corp[10])               --\u003eoutput:360\n```\n\n在内部实现上，table 通常实现为一个哈希表、一个数组、或者两者的混合。具体的实现为何种形式，动态依赖于具体的 table 的键分布特点。\n\n## Function (函数)\n\n在 Lua 中，**函数** 也是一种数据类型，函数可以存储在变量中，可以通过参数传递给其他函数，还可以作为其他函数的返回值\n\n```Go\nlocal function foo()\n    print(\"in the function\")\n    --dosomething()\n    local x = 10\n    local y = 20\n    return x + y\nend\n\nlocal a = foo    --把函数赋给变量\n\nprint(a())\n\n--output:\n--in the function\n--30\n\nfunction foo()\nend\n--等价于\n\nfoo = function ()\nend\n\nlocal function foo()\nend\n-- 等价于\n\nlocal foo = function ()\nend\n```\n\n  \n\n# 表达式\n\n## 算术运算符\n\n|            |      |\n| ---------- | ---- |\n| 算术运算符 | 说明 |\n| +          | 加法 |\n| -          | 减法 |\n| *          | 乘法 |\n| /          | 除法 |\n| ^          | 指数 |\n| %          | 取模 |\n\n```Go\nprint(1 + 2)       --\u003e打印 3\nprint(5 / 10)      --\u003e打印 0.5。 这是Lua不同于c语言的\nprint(5.0 / 10)    --\u003e打印 0.5。 浮点数相除的结果是浮点数\n-- print(10 / 0)   --\u003e注意除数不能为0，计算的结果会出错\nprint(2 ^ 10)      --\u003e打印 1024。 求2的10次方\n\nlocal num = 1357\nprint(num % 2)       --\u003e打印 1\nprint((num % 2) == 1) --\u003e打印 true。 判断num是否为奇数\n```\n\n## 关系运算符\n\n  \n\n|            |          |\n| ---------- | -------- |\n| 关系运算符 | 说明     |\n| \u003c          | 小于     |\n| \u003e          | 大于     |\n| \u003c=         | 小于等于 |\n| \u003e=         | 大于等于 |\n| ==         | 等于     |\n| ~=         | 不等于   |\n\n  \n\n```Go\nprint(1 \u003c 2)    --\u003e打印 true\nprint(1 == 2)   --\u003e打印 false\nprint(1 ~= 2)   --\u003e打印 true\nlocal a, b = true, false\nprint(a == b)  --\u003e打印 false\n```\n\n- 在使用“==”做等于判断时，要注意对于 table, userdate 和函数， Lua 是作引用比较的。也就是说，只有当两个变量引用同一个对象时，才认为它们相等\n    \n\n```Go\nlocal a = { x = 1, y = 0}\nlocal b = { x = 1, y = 0}\nif a == b then\n    print(\"a==b\")\nelse\n    print(\"a~=b\")\nend\n---output:\na~=b\n```\n\n- Lua 字符串总是会被“内化”，即相同内容的字符串只会被保存一份，因此 Lua 字符串之间的相等性比较可以简化为其内部存储地址的比较。\n    \n- 这意味着 Lua 字符串的相等性比较总是为 O (1)\n    \n\n## 逻辑运算符\n\n|            |        |\n| ---------- | ------ |\n| 逻辑运算符 | 说明   |\n| and        | 逻辑与 |\n| or         | 逻辑或 |\n| not        | 逻辑非 |\n\n在 c 语言中，and 和 or 只得到两个值 1 和 0，其中 1 表示真，0 表示假。而 Lua 中 and 的执行过程是这样的：\n\n- `a and b` 如果 a 为 nil，则返回 a，否则返回 b;\n    \n- `a or b` 如果 a 为 nil，则返回 b，否则返回 a。\n    \n- **所有逻辑操作符将 false 和 nil 视作假，其他任何值视作真，对于 and 和 or，“短路求值”，对于 not，永远只返回 true 或者 false。**\n    \n\n```Go\nlocal c = nil\nlocal d = 0\nlocal e = 100\nprint(c and d)  --\u003e打印 nil\nprint(c and e)  --\u003e打印 nil\nprint(d and e)  --\u003e打印 100\nprint(c or d)   --\u003e打印 0\nprint(c or e)   --\u003e打印 100\nprint(not c)    --\u003e打印 true\nprint(not d)    --\u003e打印 false\n```\n\n## 字符串连接\n\nLua 中连接两个字符串，可以使用操作符“..”（两个点）\n\n- 如果其任意一个操作数是数字的话，Lua 会将这个数字转换成字符串。\n    \n- 注意，连接操作符只会创建一个新字符串，而不会改变原操作数\n    \n- 也可以使用 string 库函数 `string.format` 连接字符串\n    \n\n```Go\nprint(\"Hello \" .. \"World\")    --\u003e打印 Hello Worldprint(0 .. 1)                 --\u003e打印 01\n\nstr1 = string.format(\"%s-%s\",\"hello\",\"world\")\nprint(str1)              --\u003e打印 hello-world\n\nstr2 = string.format(\"%d-%s-%.2f\",123,\"world\",1.21)\nprint(str2)              --\u003e打印 123-world-1.21\n```\n\n于 Lua 字符串本质上是只读的，**因此字符串连接运算符几乎总会创建一个新的（更大的）字符串**。这意味着如果有很多这样的连接操作（比如在循环中使用 .. 来拼接最终结果），则性能损耗会非常大。在这种情况下，推荐使用 table 和 `table.concat()` 来进行很多字符串的拼接\n\n```Go\nlocal pieces = {}\nfor i, elem in ipairs(my_list) do\n    pieces[i] = my_process(elem)\nend\nlocal res = table.concat(pieces)\n```\n\n上面的例子还可以使用 LuaJIT 独有的 `table.new` 来恰当地初始化 `pieces` 表的空间，以避免该表的动态生长。\n\n## 优先级\n\n| f               |     |\n| --------------- | --- |\n| ^               |     |\n| not # -         |     |\n| * / %           |     |\n| + -             |     |\n| ..              |     |\n| \u003c \u003e \u003c= \u003e= == ~= |     |\n| and             |     |\n| or              |     |\n  \n\n```Go\nlocal a, b = 1, 2\nlocal x, y = 3, 4\nlocal i = 10\nlocal res = 0\nres = a + i \u003c b/2 + 1  --\u003e等价于res =  (a + i) \u003c ((b/2) + 1)\nres = 5 + x^2*8        --\u003e等价于res =  5 + ((x^2) * 8)\nres = a \u003c y and y \u003c=x  --\u003e等价于res =  (a \u003c y) and (y \u003c= x)\n```\n\n  \n\n# 控制结构\n\n## If-else\n\n### **单个 if 分支型**\n\n```Go\nx = 10\nif x \u003e 0 then\n    print(\"x is a positive number\")\nend\n```\n\n### **两个分支 if-else 型**\n\n```Go\nx = 10\nif x \u003e 0 then\n    print(\"x is a positive number\")\nelse\n    print(\"x is a non-positive number\")\nend\n```\n\n### 多个分支的 if-elseif-else\n\n```Go\n\nscore = 90\nif score == 100 then\n    print(\"Very good!Your score is 100\")\nelseif score \u003e= 60 then\n    print(\"Congratulations, you have passed it,your score greater or equal to 60\")\n    --此处可以添加多个elseif\nelse\n    print(\"Sorry, you do not pass the exam! \")\nend\n```\n\n与 C 语言的不同之处是 else 与 if 是连在一起的，若将 else 与 if 写成 \"else if\" 则相当于在 else 里嵌套另一个 if 语句，如下代码：\n\n```Go\nscore = 0\nif score == 100 then\n    print(\"Very good!Your score is 100\")\nelseif score \u003e= 60 then\n    print(\"Congratulations, you have passed it,your score greater or equal to 60\")\nelse\n    if score \u003e 0 then\n        print(\"Your score is better than 0\")\n    else\n        print(\"My God, your score turned out to be 0\")\n    end --与上一示例代码不同的是，此处要添加一个end\nend\n```\n\n## While\n\n```Go\nwhile 表达式 do\n    --body\nend\n```\n\n  \n\n## Repeat\n\nLua 中的 repeat 控制结构类似于其他语言（如：C++ 语言）中的 do-while，但是控制方式是刚好相反的。简单点说，**执行 repeat 循环体后，直到 until 的条件为真时才结束**\n\n```Lua\n-- 以下代码会死循环\nx = 10\nrepeat\n    print(x)\nuntil false\n```\n\n  \n\n## For\n\n### **for 数字型**\n\n```Lua\nfor var = begin, finish, step do\n    --body\nend\n```\n\n1. Var 从 begin 变化到 finish，每次变化都以 step 作为步长递增 var\n    \n2. Begin、finish、step 三个表达式只会在循环开始时执行一次\n    \n3. 第三个表达式 step 是可选的，默认为 1\n    \n4. 控制变量 var 的作用域仅在 for 循环内，需要在外面控制，则需将值赋给一个新的变量\n    \n5. 循环过程中不要改变控制变量的值，那样会带来不可预知的影响\n    \n\n```Lua\nfor i = 1, 5 do\n    print(i)\nend\n-- output:\n1\n2\n3\n4\n5\n\nfor i = 1, 10, 2 do\n    print(i)\nend\n-- output:\n1\n3\n5\n7\n9\n```\n\n## For 泛型\n\n泛型 for 循环通过一个迭代器（iterator）函数来遍历所有值：\n\n```Lua\n-- 打印数组a的所有值local a = {\"a\", \"b\", \"c\", \"d\"}\nfor i, v in ipairs(a) do\n    print(\"index:\", i, \" value:\", v)\nend\n-- output:\nindex:  1  value: a\nindex:  2  value: b\nindex:  3  value: c\nindex:  4  value: d\n```\n\nLua 的基础库提供了 **ipairs，这是一个用于遍历数组的迭代器函数**。在每次循环中，i 会被赋予一个索引值，同时 v 被赋予一个对应于该索引的数组元素值。\n\n```Lua\n-- 打印table t中所有的\nkeyfor k in pairs(t) do\n    print(k)\nend\n```\n\n通过不同的迭代器，几乎可以遍历所有的东西，而且写出的代码极具可读性。标准库提供了几种迭代器，包括用于迭代文件中每行的（io. Lines）、迭代 table 元素的（pairs）、迭代数组元素的（ipairs）、迭代字符串中单词的（string. Gmatch）\n\n泛型 for 循环与数字型 for 循环有两个相同点：\n\n1. 循环变量是循环体的局部变量；\n    \n2. 决不应该对循环变量作任何赋值。\n    \n\n在 LuaJIT 2.1 中，**`ipairs()`** **内建函数是可以被 JIT 编译的，而** **`pairs()`** **则只能被解释执行。因此在性能敏感的场景，应当合理安排数据结构，避免对哈希表进行遍历**\n\n  \n\n## Break\n\n语句 `break` 用来终止 `while`、`repeat` 和 `for` 三种循环的执行，并跳出当前循环体，继续执行当前循环之后的语句\n\n```Lua\n-- 计算最小的x,使从1到x的所有数相加和大于100\nsum = 0\ni = 1while true do\n    sum = sum + i\n    if sum \u003e 100 then\n        break\n    end\n    i = i + 1\nend\nprint(\"The result is \" .. i)  \n--\u003eoutput:The result is 14\n```\n\n## Return\n\n  \n\n`return` 主要用于从函数中返回结果，或者用于简单的结束一个函数的执行。\n\n```Lua\nlocal function add(x, y)\n    return x + y\n    --print(\"add: I will return the result \" .. (x + y))\n    --因为前面有个return，若不注释该语句，则会报错\nend\n\nlocal function is_positive(x)\n    if x \u003e 0 then\n        return x .. \" is positive\"\n    else\n        return x .. \" is non-positive\"\n    end\n\n    --由于return只出现在前面显式的语句块，所以此语句不注释也不会报错\n    --，但是不会被执行，此处不会产生输出\n    print(\"function end!\")\nend\n\nlocal sum = add(10, 20)\nprint(\"The sum is \" .. sum)  --\u003eoutput:The sum is 30\nlocal answer = is_positive(-10)\nprint(answer)                --\u003eoutput:-10 is non-positive\n```\n\n  \n\n## Goto\n\n有了 `goto`，我们可以实现 `continue` 的功能：\n\n```Lua\nfor i=1, 3 do\n    if i \u003c= 2 then\n        print(i, \"yes continue\")\n        goto continue\n    end\n    print(i, \" no continue\")\n\n    ::continue::\n    print([[i'm end]])\nend\n```\n\n输出结果\n\n```Lua\n$ luajit test.lua\n1   yes continue\ni'm end\n2   yes continue\ni'm end\n3    no continue\ni'm end\n```\n\n# 函数\n\n## 定义\n\n```Lua\nfunction function_name (arc)  -- arc 表示参数列表，函数的参数列表可以为空\n    -- body\nend\n```\n\n上面的语法定义了一个全局函数，名为 `function_name`. 全局函数本质上就是函数类型的值赋给了一个全局变量，即上面的语法等价于\n\n```Lua\nfunction_name = function (arc)\n     -- body\nend\n```\n\n由于全局变量一般会污染全局名字空间，同时也有性能损耗（即查询全局环境表的开销），因此我们应当尽量使用“局部函数”，其记法是类似的，只是开头加上 `local` 修饰符：\n\n```Lua\nlocal function function_name (arc)\n    -- body\nend\n```\n\n定义函数\n\n1. 利用名字来解释函数、变量的目的，使人通过名字就能看出来函数、变量的作用。\n    \n2. 每个函数的长度要尽量控制在一个屏幕内，一眼可以看明白。\n    \n3. 让代码自己说话，不需要注释最好。\n    \n\n  \n\n由于函数定义等价于变量赋值，我们也可以把函数名替换为某个 Lua 表的某个字段，例如\n\n```Lua\nlocal foo = {}\nfunction foo.pr()\n    print(\"ssss\")\nend\n\nfoo.pr()\n```\n\n  \n\n## 参数\n\n### 按值传递\n\n**Lua 函数的参数大部分是按值传递的**。**当函数参数是 table 类型时，传递进来的是实际参数的引用**\n\n值传递就是调用函数时，实参把它的值通过赋值运算传递给形参，然后形参的改变和实参就没有关系了。在这个过程中，实参是通过它在参数表中的位置与形参匹配起来的。\n\n```Lua\nlocal function swap(a, b) --定义函数swap,函数内部进行交换两个变量的值\n    local temp = a\n    a = b\n    b = temp\n    print(a, b)\nend\n\nlocal x = \"hello\"\nlocal y = 20\nprint(x, y)\nswap(x, y)    --调用swap函数\nprint(x, y)   --调用swap函数后，x和y的值并没有交换\n\n--\u003eoutput\nhello 20\n20  hello\nhello 20\n```\n\n在调用函数的时候，**若形参个数和实参个数不同时，Lua 会自动调整实参个数**。调整规则：\n\n- 若实参个数大于形参个数，从左向右，多余的实参被忽略；\n    \n- 若实参个数小于形参个数，从左向右，**没有被实参初始化的形参会被初始化为 nil**\n    \n\n```Lua\nlocal function fun1(a, b)       --两个形参，多余的实参被忽略掉\n    print(a, b)\nend\n\nlocal function fun2(a, b, c, d) --四个形参，没有被实参初始化的形参，用nil初始化\n    print(a, b, c, d)\nend\n\nlocal x = 1\nlocal y = 2\nlocal z = 3\n\nfun1(x, y, z)         -- z被函数fun1忽略掉了，参数变成 x, y\nfun2(x, y, z)         -- 后面自动加上一个nil，参数变成 x, y, z, nil\n\n--\u003eoutput\n1   2\n1   2   3   nil\n```\n\n### 变长参数\n\n其实 Lua 还支持变长参数。若形参为 `...`，表示该函数可以接收不同长度的参数。访问参数的时候也要使用 `...`\n\n```Lua\n\nlocal function func( ... )                -- 形参为 ... ,表示函数采用变长参数\n\n    local temp = {...}                     -- 访问的时候也要使用 ...\n    local ans = table.concat(temp, \" \")    -- 使用 table.concat 库函数对数\n    -- 组内容使用 \" \" 拼接成字符串。\n    print(ans)\nend\n\nfunc(1, 2)        -- 传递了两个参数\nfunc(1, 2, 3, 4)  -- 传递了四个参数\n\n--\u003eoutput\n1 2\n\n1 2 3 4\n```\n\n### **具名参数**\n\nLua 还支持通过名称来指定实参，这时候要把所有的实参组织到一个 table 中，并将这个 table 作为唯一的实参传给函数。\n\n```Lua\nlocal function change(arg) -- change 函数，改变长方形的长和宽，使其各增长一倍\n  arg.width = arg.width * 2\n  arg.height = arg.height * 2return arg\nendlocal rectangle = { width = 20, height = 15 }\nprint(\"before change:\", \"width  =\", rectangle.width,\n                        \"height =\", rectangle.height)\nrectangle = change(rectangle)\nprint(\"after  change:\", \"width  =\", rectangle.width,\n                        \"height =\", rectangle.height)\n\n--\u003eoutput\nbefore change: width = 20  height =  15\nafter  change: width = 40  height =  30\n```\n\n  \n\n### 按引用传递\n\n**当函数参数是 table 类型时，传递进来的是实际参数的引用**，此时在函数内部对该 table 所做的修改，会直接对调用者所传递的实际参数生效，而无需自己返回结果和让调用者进行赋值\n\n```Plaintext\nfunction change(arg) --change函数，改变长方形的长和宽，使其各增长一倍\n  arg.width = arg.width * 2  --表arg不是表rectangle的拷贝，他们是同一个表\n  arg.height = arg.height * 2end                  -- 没有return语句了local rectangle = { width = 20, height = 15 }\nprint(\"before change:\", \"width = \", rectangle.width,\n                        \" height = \", rectangle.height)\nchange(rectangle)\nprint(\"after change:\", \"width = \", rectangle.width,\n                       \" height =\", rectangle.height)\n\n--\u003e output\nbefore change: width = 20  height = 15\nafter  change: width = 40  height = 30\n```\n\n## 函数返回值\n\nLua 具有一项与众不同的特性，允许函数返回多个值。\n\n```Lua\nlocal function swap(a, b)   \n    -- 定义函数 swap，实现两个变量交换值\n    return b, a              \n    -- 按相反顺序返回变量的值\nend\n\nlocal x = 1\nlocal y = 20\nx, y = swap(x, y)           -- 调用 swap 函数\nprint(x, y)                 --\u003e output   20     1\n```\n\n  \n\n当函数返回值的个数和接收返回值的变量的个数不一致时，Lua 也会自动调整参数个数调整规则：\n\n- 若返回值个数大于接收变量的个数，多余的返回值会被忽略掉；\n    \n- 若返回值个数小于参数个数，从左向右，没有被返回值初始化的变量会被初始化为 nil。\n    \n\n```Lua\nfunction init()             \n    --init 函数 返回两个值 1 和 \"lua\"\n    return 1, \"lua\"\nend\n\nx = init()\nprint(x)\n\nx, y, z = init()\nprint(x, y, z)\n\n--output\n1\n1 lua nil\n```\n\n  \n\n当一个函数有一个以上返回值，且函数调用不是一个列表表达式的最后一个元素，那么函数调用只会产生一个返回值, 也就是第一个返回值。\n\n```Lua\nlocal function init()       -- init 函数 返回两个值 1 和 \"lua\"\n    return 1, \"lua\"\nend\n\nlocal x, y, z = init(), 2   -- init 函数的位置不在最后，此时只返回 1\nprint(x, y, z)              --\u003eoutput  1  2  nil\n\nlocal a, b, c = 2, init()   -- init 函数的位置在最后，此时返回 1 和 \"lua\"\nprint(a, b, c)              --\u003eoutput  2  1  lua\n```\n\n函数调用的实参列表也是一个列表表达式。考虑下面的例子：\n\n```Lua\nlocal function init()\n    return 1, \"lua\"\nend\n\nprint(init(), 2)   --\u003eoutput  1  2\nprint(2, init())   --\u003eoutput  2  1  lua\n```\n\n如果你确保只取函数返回值的第一个值，可以使用括号运算符\n\n```Lua\nlocal function init()\n    return 1, \"lua\"\nend\nprint((init()), 2)   --\u003eoutput  1  2\nprint(2, (init()))   --\u003eoutput  2  1\n```\n\n**值得一提的是，如果实参列表中某个函数会返回多个值，同时调用者又没有显式地使用括号运算符来筛选和过滤，则这样的表达式是不能被 LuaJIT 2 所 JIT 编译的，而只能被解释执行。**\n\n  \n\n  \n\n# 全动态函数调用\n\n调用回调函数，并把一个数组参数作为回调函数的参数。\n\n```Lua\nlocal args = {...} or {}\nmethod_name(unpack(args, 1, table.maxn(args)))\n```\n\n```Lua\nlocal function run(x, y)\n    print('run', x, y)\nend\n\nlocal function attack(targetId)\n    print('targetId', targetId)\nend\n\nlocal function do_action(method, ...)\n    local args = {...} or {}\n    method(unpack(args, 1, table.maxn(args)))\nend\n\ndo_action(run, 1, 2)         -- output: run 1 2\ndo_action(attack, 1111)      -- output: targetId    1111\n```\n\n  \n\n# 模块\n\n从 Lua 5.1 语言添加了对模块和包的支持。一**个 Lua 模块的数据结构是用一个 Lua 值（通常是一个 Lua 表或者 Lua 函数）**。**一个 Lua 模块代码就是一个会返回这个 Lua 值的代码块**\n\n- 可以使用内建函数 `require()` 来加载和缓存模块。\n    \n- 简单的说，一个代码模块就是一个程序库，可以通过 `require` 来加载。**模块加载后的结果通过是一个 Lua table**\n    \n- **这个表就像是一个命名空间**，其内容就是模块中导出的所有东西，**比如函数和变量**。`require` 函数会返回 Lua 模块加载后的结果，即用于表示该 Lua 模块的 Lua 值。\n    \n\n  \n\n  \n\nLua 提供了一个名为 `require` 的函数用来加载模块。**要加载一个模块，只需要简单地调用** **`require`** **\"file\" 就可以了，file 指模块所在的文件名**。这个调用会返回一个由模块函数组成的 table，并且还会定义一个包含该 table 的全局变量。\n\n在 Lua 中创建一个模块最简单的方法是：**创建一个 table，并将所有需要导出的函数放入其中，最后返回这个 table 就可以了。相当于将导出的函数作为 table 的一个字段，在 Lua 中函数是第一类值，提供了天然的优势。**\n\n- 创建 my. Lua\n    \n\n```Lua\nlocal _M = {}\n\nlocal function get_name()\n    return \"Lucy\"\n    end\nfunction _M.greeting()\n    print(\"hello \" .. get_name())\nend\n\nreturn _M\n```\n\n- 把下面代码保存在文件 main. Lua 中，然后执行 main. Lua，调用上述模块。\n    \n\n```Lua\nlocal my_module = require(\"my\")\nmy_module.greeting()     --\u003eoutput: hello Lucy\n```\n\n  \n\n\u003e - 对于需要导出给外部使用的公共模块，处于安全考虑，**是要避免全局变量的出现**。我们可以使用 lj-releng 或 luacheck 工具完成全局变量的检测。至于如何做，到后面再讲。\n\u003e     \n\u003e - 另一个要注意的是，由于在 LuaJIT 中，**require 函数内不能进行上下文切换**，**所以不能够在模块的顶级上下文中调用 cosocket 一类的 API**。否则会报 `attempt to yield across C-call boundary` 错误。\n\u003e     \n\n  \n\n# String\n\nLua 字符串总是由字节构成的。Lua 核心并不尝试理解具体的字符集编码（比如 GBK 和 UTF-8 这样的多字节字符编码）\n\nLua 字符串内部用来标识各个组成字节的下标是从 1 开始的，这不同于像 C 和 Perl 这样的编程语言。这样数字符串位置的时候再也不用调整，对于非专业的开发者来说可能也是一个好事情，**string.Sub (str, 3, 7) 直接表示从第三个字符开始到第七个字符（含）为止的子串。**\n\n## **string.Byte (s [, i [, j ]])**\n\n返回字符 s[i]、s[i + 1]、s[i + 2]、······、s[j] 所对应的 ASCII 码\n\n```Lua\nprint(string.byte(\"abc\", 1, 3))\nprint(string.byte(\"abc\", 3)) -- 缺少第三个参数，第三个参数默认与第二个相同，此时为 3\nprint(string.byte(\"abc\"))    -- 缺少第二个和第三个参数，此时这两个参数都默认为 1\n\n--\u003eoutput\n97    98    99\n99\n97\n```\n\n## **string. Char (...)**\n\n接收 0 个或更多的整数（整数范围：0~255），返回这些整数所对应的 ASCII 码字符组成的字符串。当参数为空时，默认是一个 0。\n\n```Lua\nprint(string.char(96, 97, 98))\nprint(string.char())        -- 参数为空，默认是一个0，-- 你可以用string.byte(string.char())测试一下print(string.char(65, 66))\n\n--\u003e output\n`ab\n\nAB\n```\n\n## **string.Upper (s)**\n\n接收一个字符串 s，返回一个把所有小写字母变成大写字母的字符串。\n\n```Lua\nprint(string.upper(\"Hello Lua\"))  --\u003eoutput  HELLO LUA\n```\n\n## **string.Lower (s)**\n\n接收一个字符串 s，返回一个把所有大写字母变成小写字母的字符串。\n\n```Lua\nprint(string.lower(\"Hello Lua\"))  --\u003eoutput   hello lua\n```\n\n## **string.Len (s)**\n\n接收一个字符串，返回它的长度。\n\n```Lua\nprint(string.len(\"hello lua\")) --\u003eoutput  9\n```\n\n使用此函数是不推荐的。应当总是使用 `#` 运算符来获取 Lua 字符串的长度\n\n## **string.Find (s, p [, init [, plain]])**\n\n在 s 字符串中第一次匹配 p 字符串。若匹配成功，则返回 p 字符串在 s 字符串中出现的开始位置和结束位置；若匹配失败，则返回 nil,\n\n第三个参数第三个参数 init 默认为 1，并且可以为负整数，\n\n当 init 为负数时，表示从 s 字符串的 string.Len (s) + init + 1 索引处开始向后匹配字符串 p 。\n\n第四个参数默认为 false，当其为 true 时，只会把 p 看成一个字符串对待。\n\n```Lua\nlocal find = string.find\nprint(find(\"abc cba\", \"ab\"))\nprint(find(\"abc cba\", \"ab\", 2))     -- 从索引为2的位置开始匹配字符串：ab\nprint(find(\"abc cba\", \"ba\", -1))    -- 从索引为7的位置开始匹配字符串：ba\nprint(find(\"abc cba\", \"ba\", -3))    -- 从索引为5的位置开始匹配字符串：ba\nprint(find(\"abc cba\", \"(%a+)\", 1))  -- 从索引为1处匹配最长连续且只含字母的字符串\nprint(find(\"abc cba\", \"(%a+)\", 1, true)) --从索引为1的位置开始匹配字符串：(%a+)\n\n--\u003eoutput\n1   2\nnil\nnil\n6   7\n1   3   abc\nnil\n```\n\n## **string.Format (formatstring, ...)**\n\n按照格式化参数 formatstring，返回后面 `...` 内容的格式化版本\n\n```Plaintext\nprint(string.format(\"%.4f\", 3.1415926))     -- 保留4位小数\nprint(string.format(\"%d %x %o\", 31, 31, 31))-- 十进制数31转换成不同进制\nd = 29; m = 7; y = 2015                     -- 一行包含几个语句，用；分开\nprint(string.format(\"%s %02d/%02d/%d\", \"today is:\", d, m, y))\n\n--\u003eoutput\n3.1416\n31 1f 37\ntoday is: 29/07/2015\n```\n\n## **string.Match (s, p [, init])**\n\n在字符串 s 中匹配（模式）字符串 p，若匹配成功，则返回目标字符串中与模式匹配的子串；否则返回 nil。第三个参数 init 默认为 1，并且可以为负整数，当 init 为负数时，表示从 s 字符串的 string.Len (s) + init + 1 索引处开始向后匹配字符串 p。\n\n```Lua\nprint(string.match(\"hello lua\", \"lua\"))\nprint(string.match(\"lua lua\", \"lua\", 2))  --匹配后面那个luaprint(string.match(\"lua lua\", \"hello\"))\nprint(string.match(\"today is 27/7/2015\", \"%d+/%d+/%d+\"))\n\n--\u003eoutput\nlua\nlua\nnil27/7/2015\n```\n\n## **string.Gmatch (s, p)**\n\n返回一个迭代器函数，通过这个迭代器函数可以遍历到在字符串 s 中出现模式串 p 的所有地方。\n\n```Lua\ns = \"hello world from Lua\"\nfor w in string.gmatch(s, \"%a+\") do  --匹配最长连续且只含字母的字符串\n    print(w)\nend\n\n--\u003eoutput\nhello\nworld\nfrom\nLua\n\n\nt = {}\ns = \"from=world, to=Lua\"\nfor k, v in string.gmatch(s, \"(%a+)=(%a+)\") do  --匹配两个最长连续且只含字母的\n    t[k] = v                                    --字符串，它们之间用等号连接\nend\nfor k, v in pairs(t) do\n    print (k,v)\nend\n\n--\u003eoutput\nto      Lua\nfrom    worl\n```\n\n## **string.Rep (s, n)**\n\n返回字符串 s 的 n 次拷贝。\n\n```Lua\nprint(string.rep(\"abc\", 3)) \n\n--拷贝3次\"abc\"--\u003eoutput  abcabcabc\n```\n\n## **string.Sub (s, i [, j])**\n\n返回字符串 s 中，索引 i 到索引 j 之间的子字符串。当 j 缺省时，默认为 -1，也就是字符串 s 的最后位置。I 可以为负数。当索引 i 在字符串 s 的位置在索引 j 的后面时，将返回一个空字符串。\n\n```Lua\nprint(string.sub(\"Hello Lua\", 4, 7))\nprint(string.sub(\"Hello Lua\", 2))\nprint(string.sub(\"Hello Lua\", 2, 1))    --看到返回什么了吗print(string.sub(\"Hello Lua\", -3, -1))\n\n--\u003eoutput\nlo L\nello Lua\n\nLua\n```\n\n## **string.Gsub (s, p, r [, n])**\n\n将目标字符串 s 中所有的子串 p 替换成字符串 r。可选参数 n，表示限制替换次数。返回值有两个，第一个是被替换后的字符串，第二个是替换了多少次。\n\n```Plaintext\nprint(string.gsub(\"Lua Lua Lua\", \"Lua\", \"hello\"))\nprint(string.gsub(\"Lua Lua Lua\", \"Lua\", \"hello\", 2)) --指明第四个参数--\u003eoutput\nhello hello hello   3\nhello hello Lua     2\n```\n\n## **string. Reverse (s)**\n\n接收一个字符串 s，返回这个字符串的反转\n\n```Lua\nprint(string.reverse(\"Hello Lua\"))  --\u003e output: auL olleH\n```\n\n  \n\n# Table\n\n## **下标从 1 开始**\n\n数组下标从 1 开始计数。\n\n而 Lua 最初设计是一种类似 XML 的数据描述语言，所以索引（index）反应的是数据在里面的位置，而不是偏移量。\n\n  \n\n在初始化一个数组的时候，**若不显式地用键值对方式赋值，则会默认用数字作为下标**，从 1 开始。由于在 _Lua_ 内部实际采用哈希表和数组分别保存键值对、普通值，所以不推荐混合使用这两种赋值方式\n\n```Lua\nlocal color={first=\"red\", \"blue\", third=\"green\", \"yellow\"}\nprint(color[\"first\"])                 --\u003e output: red\nprint(color[1])                       --\u003e output: blue\nprint(color[\"third\"])                 --\u003e output: green\nprint(color[2])                       --\u003e output: yellow\nprint(color[3])                       --\u003e output: nil\n```\n\n- **当我们把 table 当作栈或者队列使用的时候，容易犯错，追加到 table 的末尾用的是** **`s[#s+1] = something`****, 而不是** **`s[#s] = something`**\n    \n- 而且如果这个 something 是一个 nil 的话**，会导致这一次压栈（或者入队列）没有存入任何东西**， #s 的值没有变\n    \n- 如果 `s = { 1, 2, 3, 4, 5, 6 }`，你令 `s[4] = nil`， #s 会令你“匪夷所思”地变成 3。\n    \n\n## **table. Getn 获取长度**\n\n取长度操作符写作一元操作 #。字符串的长度是它的字节数（就是以一个字符一个字节计算的字符串长度）\n\n- 对于常规的数组，里面从 1 到 n 放着一些非空的值的时候，它的长度就精确的为 n，即最后一个值的下标\n    \n- 如果数组有一个“空洞”（**就是说，nil 值被夹在非空值之间**），**那么 #t 可能是指向任何一个是 nil 值的前一个位置的下标**\n    \n- 这也就说明对于有“空洞”的情况，table 的长度存在一定的 **不可确定性**\n    \n\n```Lua\nlocal tblTest1 = { 1, a = 2, 3 }\nprint(\"Test1 \" .. table.getn(tblTest1))\n\nlocal tblTest2 = { 1, nil }\nprint(\"Test2 \" .. table.getn(tblTest2))\n\nlocal tblTest3 = { 1, nil, 2 }\nprint(\"Test3 \" .. table.getn(tblTest3))\n\nlocal tblTest4 = { 1, nil, 2, nil }\nprint(\"Test4 \" .. table.getn(tblTest4))\n\nlocal tblTest5 = { 1, nil, 2, nil, 3, nil }\nprint(\"Test5 \" .. table.getn(tblTest5))\n\nlocal tblTest6 = { 1, nil, 2, nil, 3, nil, 4, nil }\nprint(\"Test6 \" .. table.getn(tblTest6))\n```\n\n我们使用 Lua 5.1 和 LuaJIT 2.1 分别执行这个用例，结果如下：\n\n```Lua\n# lua test.lua\nTest1 2\nTest2 1\nTest3 3\nTest4 1\nTest5 3\nTest6 1\n# luajit test.lua\nTest1 2\nTest2 1\nTest3 1\nTest4 1\nTest5 1\nTest6 1\n```\n\n不要在 Lua 的 table 中使用 nil 值，**如果一个元素要删除，直接 remove，不要用 nil 去代替**。\n\n## **table. Concat (table [, sep [, i [, j ] ] ])**\n\n对于元素是 string 或者 number 类型的表 table，返回 `table[i]..sep..table[i+1] ··· sep..table[j]` 连接成的字符串。填充字符串 sep 默认为空白字符串。起始索引位置 i 默认为 1，结束索引位置 j 默认是 table 的长度。\n\n```Lua\nlocal a = {1, 3, 5, \"hello\" }\nprint(table.concat(a))              -- output: 135hello\nprint(table.concat(a, \"|\"))         -- output: 1|3|5|hello\nprint(table.concat(a, \" \", 4, 2))   -- output:\nprint(table.concat(a, \" \", 2, 4))   -- output: 3 5 hello\n```\n\n## **table. Insert (table, [pos ,] value)**\n\n在（数组型）表 table 的 pos 索引位置插入 value，其它元素向后移动到空的地方。Pos 的默认值是表的长度加一，即默认是插在表的最后\n\n```Lua\nlocal a = {1, 8}             --a[1] = 1,a[2] = 8\ntable.insert(a, 1, 3)   --在表索引为1处插入3\nprint(a[1], a[2], a[3])\ntable.insert(a, 10)    --在表的最后插入10\nprint(a[1], a[2], a[3], a[4])\n\n--\u003eoutput\n3    1    8\n3    1    8    10\n```\n\n## **table. Maxn (table)**\n\n返回（数组型）表 table 的最大索引编号；如果此表没有正的索引编号，返回 0。\n\n```Lua\nlocal a = {}\na[-1] = 10\nprint(table.maxn(a))\na[5] = 10\nprint(table.maxn(a))\n\n--\u003eoutput05\n```\n\n## **table. Remove (table [, pos])**\n\n在表 table 中删除索引为 pos（pos 只能是 number 型）的元素，并返回这个被删除的元素，它后面所有元素的索引值都会减一。Pos 的默认值是表的长度，即默认是删除表的最后一个元素。\n\n```Lua\nlocal a = { 1, 2, 3, 4}\nprint(table.remove(a, 1)) --删除速索引为1的元素print(a[1], a[2], a[3], a[4])\n\nprint(table.remove(a))   --删除最后一个元素print(a[1], a[2], a[3], a[4])\n\n--\u003eoutput12    3    4    nil42    3    nil    nil\n```\n\n## **table. Sort (table [, comp])**\n\n按照给定的比较函数 comp 给表 table 排序，也就是从 table[1] 到 table[n]，这里 n 表示 table 的长度。比较函数有两个参数，如果希望第一个参数排在第二个的前面，就应该返回 true，否则返回 false。如果比较函数 comp 没有给出，默认从小到大排序。\n\n```Lua\n\nlocal function compare(x, y) --从大到小排序\n    return x \u003e y         --如果第一个参数大于第二个就返回true，否则返回false\nend\n\nlocal a = { 1, 7, 3, 4, 25}\ntable.sort(a)           --默认从小到大排序\nprint(a[1], a[2], a[3], a[4], a[5])\ntable.sort(a, compare) --使用比较函数进行排序\nprint(a[1], a[2], a[3], a[4], a[5])\n\n--\u003eoutput\n1    3    4    7    25\n25    7    4    3    1\n```\n\n## 其他\n\nLuaJIT 2.1 新增加的 `table.new` 和 `table.clear` 函数是非常有用的。前者主要用来预分配 Lua table 空间，后者主要用来高效的释放 table 空间，并且它们都是可以被 JIT 编译的\n\n  \n\n# 日期时间\n\n函数 time、date 和 difftime 提供了所有的日期和时间功能。\n\n在 OpenResty 的世界里，不推荐使用这里的标准时间函数，因为这些函数通常会引发不止一个昂贵的系统调用，同时无法为 LuaJIT JIT 编译，对性能造成较大影响。推荐使用 ngx_lua 模块提供的带缓存的时间接口，如 `ngx.today`, `ngx.time`, `ngx.utctime`, `ngx.localtime`, `ngx.now`, `ngx.http_time`，以及 `ngx.cookie_time` 等。\n\n  \n\n## **os. Time ([table])**\n\n如果不使用参数 table 调用 time 函数，\n\n- 它会返回当前的时间和日期（它表示从某一时刻到现在的秒数）。\n    \n- 如果用 table 参数，它会返回一个数字，表示该 table 中所描述的日期和时间（它表示从某一时刻到 table 中描述日期和时间的秒数）。Table 的字段如下：\n    \n\n|          |                            |\n| -------- | -------------------------- |\n| 字段名称 | 取值范围                   |\n| year     | 四位数字                   |\n| month    | 1--12                      |\n| day      | 1--31                      |\n| hour     | 0--23                      |\n| min      | 0--59                      |\n| sec      | 0--61                      |\n| isdst    | boolean（true 表示夏令时） |\n\n对于 time 函数，如果参数为 table，那么 table 中必须含有 year、month、day 字段。其他字缺省时段默认为中午（12:00:00）。\n\n\u003e 示例代码：（地点为北京）\n\n```Plaintext\nprint(os.time())    --\u003eoutput  1438243393\na = { year = 1970, month = 1, day = 1, hour = 8, min = 1 }\nprint(os.time(a))   --\u003eoutput  60\n```\n\n## **os. Difftime (t 2, t 1)**\n\n返回 t 1 到 t 2 的时间差，单位为秒。\n\n\u003e 示例代码:\n\n```Plaintext\nlocal day1 = { year = 2015, month = 7, day = 30 }\nlocal t1 = os.time(day1)\n\nlocal day2 = { year = 2015, month = 7, day = 31 }\nlocal t2 = os.time(day2)\nprint(os.difftime(t2, t1))   --\u003eoutput  86400\n```\n\n## **os. Date ([format [, time]])**\n\n把一个表示日期和时间的数值，转换成更高级的表现形式。\n\n- 其第一个参数 format 是一个格式化字符串，描述了要返回的时间形式。\n    \n- 第二个参数 time 就是日期和时间的数字表示，缺省时默认为当前的时间。\n    \n- 使用格式字符 \"*t\"，创建一个时间表。\n    \n\n\u003e 示例代码：\n\n```Plaintext\nlocal tab1 = os.date(\"*t\")  --返回一个描述当前日期和时间的表\nlocal ans1 = \"{\"\nfor k, v in pairs(tab1) do  --把tab1转换成一个字符串\n    ans1 = string.format(\"%s %s = %s,\", ans1, k, tostring(v))\nend\n\nans1 = ans1 .. \"}\"\nprint(\"tab1 = \", ans1)\n\n\nlocal tab2 = os.date(\"*t\", 360)  --返回一个描述日期和时间数为360秒的表\nlocal ans2 = \"{\"\nfor k, v in pairs(tab2) do      --把tab2转换成一个字符串\n    ans2 = string.format(\"%s %s = %s,\", ans2, k, tostring(v))\nend\n\nans2 = ans2 .. \"}\"\nprint(\"tab2 = \", ans2)\n\n--\u003eoutput\ntab1 = { hour = 17, min = 28, wday = 5, day = 30, month = 7, year = 2015, sec = 10, yday = 211, isdst = false,}\ntab2 = { hour = 8, min = 6, wday = 5, day = 1, month = 1, year = 1970, sec = 0, yday = 1, isdst = false,}\n```\n\n该表中除了使用到了 time 函数参数 table 的字段外，这还提供了星期（wday，星期天为 1）和一年中的第几天（yday，一月一日为 1）。除了使用 \"*t\" 格式字符串外，如果使用带标记（见下表）的特殊字符串，os. Date 函数会将相应的标记位以时间信息进行填充，得到一个包含时间的字符串。表如下：\n\n|          |                                           |\n| -------- | ----------------------------------------- |\n| 格式字符 | 含义                                      |\n| %a       | 一星期中天数的简写（例如：Wed）           |\n| %A       | 一星期中天数的全称（例如：Wednesday）     |\n| %b       | 月份的简写（例如：Sep）                   |\n| %B       | 月份的全称（例如：September）             |\n| %c       | 日期和时间（例如：07/30/15 16:57:24）     |\n| %d       | 一个月中的第几天[01 ~ 31]                 |\n| %H       | 24 小时制中的小时数[00 ~ 23]              |\n| %I       | 12 小时制中的小时数[01 ~ 12]              |\n| %j       | 一年中的第几天[001 ~ 366]                 |\n| %M       | 分钟数[00 ~ 59]                           |\n| %m       | 月份数[01 ~ 12]                           |\n| %p       | “上午（am）”或“下午（pm）”                |\n| %S       | 秒数[00 ~ 59]                             |\n| %w       | 一星期中的第几天[1 ~ 7 = 星期天 ~ 星期六] |\n| %x       | 日期（例如：07/30/15）                    |\n| %X       | 时间（例如：16:57:24）                    |\n| %y       | 两位数的年份[00 ~ 99]                     |\n| %Y       | 完整的年份（例如：2015）                  |\n| %%       | 字符'%'                                   |\n\n\u003e 示例代码：\n\n```Plaintext\nprint(os.date(\"today is %A, in %B\"))\nprint(os.date(\"now is %x %X\"))\n\n--\u003eoutput\ntoday is Thursday, in July\nnow is 07/30/15 17:39:22\n```\n\n  \n\n# 数学库\n\nUa 数学库由一组标准的数学函数构成。数学库的引入丰富了 Lua 编程语言的功能，同时也方便了程序的编写。常用数学函数见下表：\n\n|               asd           |                                                                                        sdfa                                                                                                      | \n| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 函数名                   | 函数功能                                                                                                                                                                                     |\n| math.Rad (x)             | 角度 x 转换成弧度                                                                                                                                                                            |\n| math.Deg (x)             | 弧度 x 转换成角度                                                                                                                                                                            |\n| math.Max (x, ...)        | 返回参数中值最大的那个数，参数必须是 number 型                                                                                                                                               |\n| math.Min (x, ...)        | 返回参数中值最小的那个数，参数必须是 number 型                                                                                                                                               |\n| math. Random ([m [, n]]) | 不传入参数时，返回一个在区间[0,1)内均匀分布的伪随机实数；只使用一个整数参数 m 时，返回一个在区间[1, m]内均匀分布的伪随机整数；使用两个整数参数时，返回一个在区间[m, n]内均匀分布的伪随机整数 |\n| math. Randomseed (x)     | 为伪随机数生成器设置一个种子 x，相同的种子将会生成相同的数字序列                                                                                                                             |\n| math.Abs (x)             | 返回 x 的绝对值                                                                                                                                                                              |\n| math.Fmod (x, y)         | 返回 x 对 y 取余数                                                                                                                                                                           |\n| math.Pow (x, y)          | 返回 x 的 y 次方                                                                                                                                                                             |\n| math.Sqrt (x)            | 返回 x 的算术平方根                                                                                                                                                                          |\n| math.Exp (x)             | 返回自然数 e 的 x 次方                                                                                                                                                                       |\n| math.Log (x)             | 返回 x 的自然对数                                                                                                                                                                            |\n| math. Log 10 (x)         | 返回以 10 为底，x 的对数                                                                                                                                                                     |\n| math.Floor (x)           | 返回最大且不大于 x 的整数                                                                                                                                                                    |\n| math.Ceil (x)            | 返回最小且不小于 x 的整数                                                                                                                                                                    |\n| math. Pi                 | 圆周率                                                                                                                                                                                       |\n| math.Sin (x)             | 求弧度 x 的正弦值                                                                                                                                                                            |\n| math.Cos (x)             | 求弧度 x 的余弦值                                                                                                                                                                            |\n| math.Tan (x)             | 求弧度 x 的正切值                                                                                                                                                                            |\n| math.Asin (x)            | 求 x 的反正弦值                                                                                                                                                                              |\n| math.Acos (x)            | 求 x 的反余弦值                                                                                                                                                                              |\n| math.Atan (x)            | 求 x 的反正切值                                                                                                                                                                              |\n\n```Lua\nprint(math.pi)           --\u003eoutput  3.1415926535898\nprint(math.rad(180))     --\u003eoutput  3.1415926535898\nprint(math.deg(math.pi)) --\u003eoutput  180\n\nprint(math.sin(1))       --\u003eoutput  0.8414709848079\nprint(math.cos(math.pi)) --\u003eoutput  -1\nprint(math.tan(math.pi / 4))  --\u003eoutput  1\n\nprint(math.atan(1))      --\u003eoutput  0.78539816339745\nprint(math.asin(0))      --\u003eoutput  0\n\nprint(math.max(-1, 2, 0, 3.6, 9.1))     --\u003eoutput  9.1\nprint(math.min(-1, 2, 0, 3.6, 9.1))     --\u003eoutput  -1\n\nprint(math.fmod(10.1, 3))   --\u003eoutput  1.1\nprint(math.sqrt(360))      --\u003eoutput  18.97366596101\n\nprint(math.exp(1))         --\u003eoutput  2.718281828459\nprint(math.log(10))        --\u003eoutput  2.302585092994\nprint(math.log10(10))      --\u003eoutput  1\n\nprint(math.floor(3.1415))  --\u003eoutput  3\nprint(math.ceil(7.998))    --\u003eoutput  8\n```\n\n使用 `math.random()` 函数获得伪随机数时，如果不使用 `math.randomseed()` 设置伪随机数生成种子或者设置相同的伪随机数生成种子，那么得得到的伪随机数序列是一样的。\n\n```Lua\nmath.randomseed (100) --把种子设置为100\nprint(math.random())         --\u003eoutput  0.0012512588885159\nprint(math.random(100))      --\u003eoutput  57\nprint(math.random(100, 360)) --\u003eoutput  150\n```\n\n稍等片刻，再次运行上面的代码。\n\n```Lua\nmath.randomseed (100) --把种子设置为100\nprint(math.random())         --\u003eoutput  0.0012512588885159\nprint(math.random(100))      --\u003eoutput  57\nprint(math.random(100, 360)) --\u003eoutput  150\n```\n\n两次运行的结果一样。为了避免每次程序启动时得到的都是相同的伪随机数序列，通常是使用当前时间作为种子。\n\n\u003e 修改上例中的代码：\n\n```Lua\nmath.randomseed (os.time())   --把100换成os.time()\nprint(math.random())          --\u003eoutput 0.88369396038697\nprint(math.random(100))       --\u003eoutput 66\nprint(math.random(100, 360))  --\u003eoutput 228\n```\n\n稍等片刻，再次运行上面的代码。\n\n```Plaintext\nmath.randomseed (os.time())   --把100换成os.time()\nprint(math.random())          --\u003eoutput 0.88946195867794\nprint(math.random(100))       --\u003eoutput 68\nprint(math.random(100, 360))  --\u003eoutput 129\n```\n\n  \n\n# 文件\n\nLua I/O 库提供两种不同的方式处理文件：隐式文件描述，显式文件描述。\n\n这些文件 I/O 操作，**在 OpenResty 的上下文中对事件循环是会产生阻塞效应**。OpenResty 比较擅长的是高并发网络处理，在这个环境中，任何文件的操作，都将阻塞其他并行执行的请求。**实际中的应用，在 OpenResty 项目中应尽可能让网络处理部分、文件 I/0 操作部分相互独立，不要揉和在一起**。\n\n## **隐式文件描述**\n\n设置一个默认的输入或输出文件，然后在这个文件上进行所有的输入或输出操作。所有的操作函数由 io 表提供。\n\n\u003e 打开已经存在的 `test1.txt` 文件，并读取里面的内容\n\n```Plaintext\nfile = io.input(\"test1.txt\")    -- 使用 io.input() 函数打开文件repeat\n    line = io.read()            -- 逐行读取内容，文件结束时返回nil\n    if nil == line then\n        break\n    end\n    print(line)\nuntil (false)\n\nio.close(file)                  -- 关闭文件--\u003e output\nmy test file\nhello\nlua\n```\n\n\u003e 在 `test1.txt` 文件的最后添加一行 \"hello world\"\n\n```Plaintext\nfile = io.open(\"test1.txt\", \"a+\")   -- 使用 io.open() 函数，以添加模式打开文件\nio.output(file)                     -- 使用 io.output() 函数，设置默认输出文件\nio.write(\"\\nhello world\")           -- 使用 io.write() 函数，把内容写到文件\nio.close(file)\n```\n\n在相应目录下打开 `test1.txt` 文件，查看文件内容发生的变化。\n\n## **显式文件描述**\n\n使用 file: XXX () 函数方式进行操作, 其中 file 为 io.Open () 返回的文件句柄。\n\n\u003e 打开已经存在的 test 2. Txt 文件，并读取里面的内容\n\n```Plaintext\nfile = io.open(\"test2.txt\", \"r\")    -- 使用 io.open() 函数，以只读模式打开文件\n\nfor line in file:lines() do         -- 使用 file:lines() 函数逐行读取文件\n    print(line)\nend\n\nfile:close()\n\n--\u003eoutput\nmy test2\nhello lua\n```\n\n\u003e 在 test 2. Txt 文件的最后添加一行 \"hello world\"\n\n```Plaintext\nfile = io.open(\"test2.txt\", \"a\")  -- 使用 io.open() 函数，以添加模式打开文件\nfile:write(\"\\nhello world\")       -- 使用 file:write() 函数，在文件末尾追加内容\nfile:close()\n```\n\n在相应目录下打开 `test2.txt` 文件，查看文件内容发生的变化。\n\n## **文件操作函数**\n\n#### **io. Open (filename [, mode])**\n\n按指定的模式 mode，打开一个文件名为 `filename` 的文件，成功则返回文件句柄，失败则返回 nil 加错误信息。模式：\n\n|      |                                                |                     | \n| ---- | ---------------------------------------------- | ------------------- |\n| 模式 | 含义                                           | 文件不存在时        |\n| \"r\"  | 读模式 (默认)                                  | 返回 nil 加错误信息 |\n| \"w\"  | 写模式                                         | 创建文件            |\n| \"a\"  | 添加模式                                       | 创建文件            |\n| \"r+\" | 更新模式，保存之前的数据                       | 返回 nil 加错误信息 |\n| \"w+\" | 更新模式，清除之前的数据                       | 创建文件            |\n| \"a+\" | 添加更新模式，保存之前的数据, 在文件尾进行添加 | 创建文件            |\n\n模式字符串后面可以有一个 'b'，用于在某些系统中打开二进制文件。\n\n注意 \"w\" 和 \"wb\" 的区别\n\n- \"w\" 表示文本文件。某些文件系统 (如 Linux 的文件系统)认为 0 x 0 A 为文本文件的换行符，Windows 的文件系统认为 0 x 0 D 0 A 为文本文件的换行符。为了兼容其他文件系统（如从 Linux 拷贝来的文件），Windows 的文件系统在写文件时，会在文件中 0 x 0 A 的前面加上 0 x 0 D。使用 \"w\"，其属性要看所在的平台。\n    \n- \"wb\" 表示二进制文件。文件系统会按纯粹的二进制格式进行写操作，因此也就不存在格式转换的问题。（Linux 文件系统下 \"w\" 和 \"wb\" 没有区别）\n    \n\n#### **file: close ()**\n\n关闭文件。注意：当文件句柄被垃圾收集后，文件将自动关闭。句柄将变为一个不可预知的值。\n\n#### **io. Close ([file])**\n\n关闭文件，和 file: close () 的作用相同。没有参数 file 时，关闭默认输出文件。\n\n#### **file: flush ()**\n\n把写入缓冲区的所有数据写入到文件 file 中。\n\n#### **io. Flush ()**\n\n相当于 file: flush ()，把写入缓冲区的所有数据写入到默认输出文件。\n\n#### **io. Input ([file])**\n\n当使用一个文件名调用时，打开这个文件（以文本模式），并设置文件句柄为默认输入文件；当使用一个文件句柄调用时，设置此文件句柄为默认输入文件；当不使用参数调用时，返回默认输入文件句柄。\n\n#### **file: lines ()**\n\n返回一个迭代函数, 每次调用将获得文件中的一行内容, 当到文件尾时，将返回 nil，但不关闭文件。\n\n#### **io. Lines ([filename])**\n\n打开指定的文件 filename 为读模式并返回一个迭代函数, 每次调用将获得文件中的一行内容, 当到文件尾时，将返回 nil，并自动关闭文件。若不带参数时 io.Lines () 等价于 io.Input (): lines () 读取默认输入设备的内容，结束时不关闭文件。\n\n#### **io. Output ([file])**\n\n类似于 io. Input，但操作在默认输出文件上。\n\n#### **file: read (...)**\n\n按指定的格式读取一个文件。按每个格式将返回一个字符串或数字, 如果不能正确读取将返回 nil，若没有指定格式将指默认按行方式进行读取。格式：\n\n|        |                                                                                                        |\n| ------ | ------------------------------------------------------------------------------------------------------ |\n| 格式   | 含义                                                                                                   |\n| \"*n\"   | 读取一个数字                                                                                           |\n| \"*a\"   | 从当前位置读取整个文件。若当前位置为文件尾，则返回空字符串                                             |\n| \"*l\"   | 读取下一行的内容。若为文件尾，则返回 nil。(默认)                                                       |\n| number | 读取指定字节数的字符。若为文件尾，则返回 nil。如果 number 为 0, 则返回空字符串，若为文件尾, 则返回 nil |\n\n#### **io. Read (...)**\n\n相当于 io.Input ():read\n\n#### **io. Type (obj)**\n\n检测 obj 是否一个可用的文件句柄。如果 obj 是一个打开的文件句柄，则返回 \"file\" 如果 obj 是一个已关闭的文件句柄，则返回 \"closed file\" 如果 obj 不是一个文件句柄，则返回 nil。\n\n#### **file: write (...)**\n\n把每一个参数的值写入文件。参数必须为字符串或数字，若要输出其它值，则需通过 tostring 或 string. Format 进行转换。\n\n#### **io. Write (...)**\n\n相当于 io.Output (): write。\n\n#### **file: seek ([whence] [, offset])**\n\n设置和获取当前文件位置，成功则返回最终的文件位置 (按字节，相对于文件开头), 失败则返回 nil 加错误信息。缺省时，whence 默认为 \"cur\"，offset 默认为 0 。参数 whence：\n\n|        |                     |\n| ------ | ------------------- |\n| whence | 含义                |\n| \"set\"  | 文件开始            |\n| \"cur\"  | 文件当前位置 (默认) |\n| \"end\"  | 文件结束            |\n\n#### **file: setvbuf (mode [, size])**\n\n设置输出文件的缓冲模式。模式：\n\n|        |                                                              |\n| ------ | ------------------------------------------------------------ |\n| 模式   | 含义                                                         |\n| \"no\"   | 没有缓冲，即直接输出                                         |\n| \"full\" | 全缓冲，即当缓冲满后才进行输出操作 (也可调用 flush 马上输出) |\n| \"line\" | 以行为单位，进行输出                                         |\n\n最后两种模式，size 可以指定缓冲的大小（按字节），忽略 size 将自动调整为最佳的大小。","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/maven/maven-overview":{"title":"maven overview","content":"# 什么是 Maven\n\nMaven 是一种用来管理 Java 项目的工具，但不是那种用来管理资源规划和调度的工具。 相反，它处理的是管理一个具体的项目所涉及的各种任务，如编译、测试、打包、文档以及分发。\n\nMaven 包括以下的几个部分。\n\n- 一组用于处理依赖管理、目录结构以及构建工作流的约定。\n    \n    - 基于这些约定实现的标准化 ，可以极大地简化开发过程。例如，一个常用的目录结构使得开发者可以更加容易地跟上不熟悉的项目的节奏。\n        \n- 一个用于项目配置的XML Schema：项目对象模型（Project Object Model），简称POM①。\n    \n    - 每一个Maven项目都拥有一个POM文件 ，默认命名为pom.xml，包含了Maven用于管理 该项目的所有的配置信息。\n        \n- 一个委托外部组件来执行项目任务的插件架构。\n    \n    - 这简化了更新以及扩展 Maven 能力的过程。\n        \n\n  \n\n# Maven 的基本概念\n\n## 标准的目录结构\n\n  \n\n![](statistic/asynccode-213.png)\n\n  \n\n## POM大纲\n\n![](statistic/asynccode-211.png)\n\n  \n\n## 构建\n\n任何可以被 Maven 的坐标系统（参见接下来的关于 GAV 坐标的讨论）唯一标识的对象都是一个 Maven 构件。\n\n- 大多数情况下，构件是构建 Maven 项目所生成的文件，如 JAR。\n    \n- 但是，只包 含其他 POM（该文件本身并不产生构件）使用的定义的 POM 文件也是 Maven 构件。\n    \n- Maven 构件的类型由其 POM 文件的`\u003cpackaging\u003e`元素指定。最常用的值是 pom、jar、 ear、war 以及 maven-plugin\n\n### POM 文件的用例\n\n可以通过以下的方式来使用 POM 文件。\n\n- 默认的——用于构建一个构件。\n    \n- 父 POM——提供一个由子项目继承的单个配置信息源——声明这个 POM 文件作为它们 的`\u003cparent\u003e`元素的值。\n    \n- 聚合器——用于构建一组声明为`\u003cmodules\u003e`的项目，这些子项目位于其当前聚合器项目的文件夹中，每个都包含有它自己的 POM 文件。\n    \n\n作为**父 POM 或者聚合器的 POM 文件的`\u003cpackaging\u003e`元素的值将是 pom**。注意，一个 POM\n\n文件可能同时提供两项功能。\n\n  \n\n## GAV 坐标\n\nPOM 定义了 5 种称为坐标的元素，用于标识 Maven 构件。首字母缩写 GAV 指的是必须始终指定的 3 个坐标`\u003cgroupId\u003e`、`\u003cartifactId\u003e`以及`\u003cversion\u003e`的首字\n\n1. `\u003cgroupId\u003e`是项目或者项目组的全局的唯一标识符。这通常是 Java 源代码中使用的全限定的 Java 包名。例如，io.netty、com.google。\n    \n2. `\u003cartifactId\u003e`用于标识和某个`\u003cgroupId\u003e`相关的不同的构件。例如，netty-all、 netty-handler。\n    \n3. `\u003ctype\u003e`是指和项目相关的主要构件的类型（对应于构件的 POM 文件中的`\u003cpackaging\u003e` 值）。它的默认值是 jar。例如，pom、war、ear。\n    \n4. `\u003cversion\u003e`标识了构件的版本。例如，1.1、2.0-SNAPSHOT①POM 文件必须声明它所管理的构件的坐标。一个具有如下坐标的项目： 、4.1.9.Final。\n    \n5. `\u003cclassifier\u003e`用于区分属于相同的 POM 但是却被以不同的方式构建的构件。例如，javadoc、sources、jdk16、jdk17\n    \n\n  \n\n一个GAV\n\n```Go\n\u003cgroupId\u003eio.netty\u003c/groupId\u003e \n\u003cartifactId\u003enetty-all\u003c/artifactId\u003e \n\u003cversion\u003e4.1.9.Final\u003c/version\u003e \n\u003cpackaging\u003ejar\u003c/packaging\u003e\n```\n\n在这种情况下，它将产生这个构件：\n\n```Go\nnetty-all\n-4.1.9.Final.jar\n```\n\n## 依赖\n\n项目的依赖是指编译和执行它所需要的外部构件\n\n在大多数情况下，你的项目的依赖项也会有它自己的依赖。我们称这些依赖为你的项目的传递依赖。一个复杂的项目可能会有一个深层级 的依赖树；Maven提供了各种用于帮助理解和管理它的工具。\n\n```Go\n\u003cdependencies\u003e \n    \u003cdependency\u003e \n        \u003cgroupId/\u003e \n        \u003cartifactId/\u003e \n        \u003cversion/\u003e \n        \u003ctype/\u003e \n        \u003cscope/\u003e \n        \u003csystemPath/\u003e \n    \u003c/dependency\u003e \n    ... \n\u003c/dependencies\u003e\n```\n\ntype以及scope元素对于那些值不分别是默认值jar和compile的依赖来说也是必需的\n\n`\u003cscope\u003e`元素可以具有以下值\n\n- compile—编译和执行需要的（默认值）。\n    \n- runtime—只有执行需要。\n    \n- optional——不被引用了这个项目所产生的构件的其他项目，视为传递依赖。\n    \n- provided——不会被包含在由这个 POM 产生的 WAR 文件的 WEB_INF/lib 目录中。\n    \n- test——只有编译和测试的执行需要。\n    \n- import——这将在后面的“依赖管理”一节进行讨论。\n    \n\n`\u003csystemPath\u003e`元素用来指定文件系统中的绝对位置。\n\n  \n\n## 依赖管理\n\n引用了`\u003cdependencyManagement\u003e`元素的项目可以使用它所声明的依赖，而不需要指定它们的`\u003cversion\u003e`坐标。如果`\u003cdependencyManagement\u003e`中的`\u003cversion\u003e`在稍后有所改变，则它将被所有引用它的 POM 拾起。\n\n在下面的示例中，所使用的 Netty 版本是在 POM 的`\u003cproperties\u003e`部分中定义，在 `\u003cdependencyManagement\u003e`中引用的\n\n```Go\n\u003cproperties\u003e \n    \u003cnetty.version\u003e4.1.9\u003c/netty.version\u003e \n    ... \n    ... \n\u003c/properties\u003e \n\u003cdependencyManagement\u003e \n    \u003cdependencies\u003e \n        \u003cdependency\u003e \n            \u003cgroupId\u003eio.netty\u003c/groupId\u003e \n            \u003cartifactId\u003enetty-all\u003c/artifactId\u003e \n            \u003cversion\u003e${netty.version}\u003c/version\u003e \n        \u003c/dependency\u003e \n    \u003c/dependencies\u003e \n    ... \n\u003c/dependencyManagement\u003e\n```\n\n对于这种使用场景，依赖的`\u003cscope\u003e`元素有一个特殊的 import 值：它将把外部 POM（没有被声明为`\u003cparent\u003e`）的`\u003cdependencyManagement\u003e`元素的内容导入到当前 POM 的`\u003cdependencyManagement\u003e`元素中\n\n  \n\n## 构建的生命周期\n\nMaven 构建的生命周期是一个明确定义的用于构建和分发构件的过程。有 3 个内置的构建生命周期：clean、default 和 site。\n\n一个构建的生命周期由一系列的阶段所组成。下面是默认的构建生命周期的各个阶段的一个部分清单。\n\n-  validate——检查项目是否正确，所有必需的信息是否已经就绪。\n    \n-  process-sources——处理源代码，如过滤任何值。\n    \n-  compile——编译项目的源代码。\n    \n-  process-test-resources——复制并处理资源到测试目标目录中。\n    \n-  test-compile——将测试源代码编译到测试目标目录中。\n    \n-  test——使用合适的单元测试框架测试编译的源代码。\n    \n-  package——将编译的代码打包为它的可分发格式，如 JAR。\n    \n-  integration-test——处理并将软件包部署到一个可以运行集成测试的环境中。\n    \n-  verify——运行任何的检查以验证软件包是否有效，并且符合质量标准。\n    \n-  install——将软件包安装到本地存储库中，在那里其他本地构建项目可以将它引用为依赖。\n    \n-  deploy——将最终的构件上传到远程存储库，以与其他开发人员和项目共享。\n    \n\n## 插件\n\n插件可能拥有多个内部步骤，或者目标，其也可以被单独调用。例如，在一个 JAR 项目中，默认的构建生命周期由 maven-jar-plugin 处理，其将构建的各个阶段映射到了它自己的以及其他插件的目标中，\n\n![](statistic/asynccode-210.png)\n\n注意插件的声明，它被打包为 JAR 包，使用了和`\u003cdependency\u003e`的 GAV 坐标相同的 GAV 坐标。\n\n```Go\n\u003cplugin\u003e \n    \u003cgroupId\u003eorg.codehaus.mojo\u003c/groupId\u003e \n    \u003cartifactId\u003eexec-maven-plugin\u003c/artifactId\u003e \n    \u003cversion\u003e1.2.1\u003c/version\u003e \n\u003c/plugin\u003e\n \n```\n\n  \n\n## 插件管理\n\n如同`\u003cdependencyManagement\u003e`，\u003cpluginManagement\u003e声明了其他 POM 可以使用的\n\n信息\n\n```Go\n\u003cbuild\u003e \n    \u003cpluginManagement\u003e \n        \u003cplugins\u003e \n            \u003cplugin\u003e \n                \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \n                \u003cversion\u003e3.2\u003c/version\u003e \n                \u003cconfiguration\u003e \n                \u003csource\u003e1.7\u003c/source\u003e \n                \u003ctarget\u003e1.7\u003c/target\u003e \n            \u003c/configuration\u003e \n            \u003c/plugin\u003e \n            \u003cplugin\u003e \n                \u003cgroupId\u003eorg.codehaus.mojo\u003c/groupId\u003e \n                \u003cartifactId\u003eexec-maven-plugin\u003c/artifactId\u003e \n                \u003cversion\u003e1.2.1\u003c/version\u003e \n            \u003c/plugin\u003e \n        \u003c/plugins\u003e \n    \u003c/pluginManagement\u003e \n\u003c/build\u003e\n```\n\n  \n\n## 配置文件\n\n配置文件（在\u003cprofiles\u003e中定义）是一组自定义的 POM 元素，可以通过自动或者手动启用（激活）来改变 POM 的行为。\n\n例如，你可以定义一个配置文件，它将根据 JDK 版本、操作系 统或者目标部署环境（如开发、测试或者生产环境）来设置构建参数。 可以通过命令行的-P 标志来显式地引用配置文件。下面的例子将激活一个将 POM 自定义为 使用 JDK1.6 的配置文件。\n\n```Go\nmvn -P jdk16 clean install \n```\n\n  \n\n## 存储库\n\nMaven的构件存储库 ①\n\n- 远程存储库是一个 Maven 从其下载 POM 文件中所引用的依赖的服务。如果你有上传权 限，那么这些依赖中可能也会包含由你自己的项目所产生的构件。大量开放源代码的 Maven 项目（包含 Netty）都将它们的构件发布到可以公开访问的 Maven 存储库。\n    \n- 本地存储库是一个本地的目录，其包含从远程存储库下载的构件，以及你在本地机器上构建并安装的构件。它通常放在你的主目录下，\n    \n\nMaven 存储库的物理目录结构使用 GAV 坐标，如同 Java 编译器使用包名一样。例如，在 Maven 下载了下面的依赖之后：\n\n```Go\nMaven 存储库的物理目录结构使用 GAV 坐标，如同 Java 编译器使用包名一样。例如，在 \nMaven 下载了下面的依赖之后：\n```\n\nMaven 存储库的物理目录结构使用 GAV 坐标，如同 Java 编译器使用包名一样。例如，在 Maven 下载了下面的依赖之后：\n\n![](statistic/asynccode-212.png)\n\n  \n\n## 快照和发布\n\n远程存储库通常会为正在开发的构件，以及那些稳定发布或者生产发布的构件，定义不同的 区域。这**些区域被分别称为快照存储库和发布存储库。**\n\n- 一个`\u003cversion\u003e`值由-SNAPSHOT 结尾的构件将被认为是还没有发布的**。这种构件可以重复 地使用相同的`\u003cversion\u003e`值被上传到存储库**。每次它都会被分配一个唯一的时间戳。**当项目检 索构件时，下载的是最新实例。**\n    \n- 一个`\u003cversion\u003e`值不具有-SNAPSHOT 后缀的构件将会被认为是一个发布版本。通常，存储库策略只允某一特定的发布版本上传一次\n    \n\n  \n\nSNAPSHOT 的拉取\n\n- 当构建一个具有 SNAPSHOT 依赖的项目时，Maven 将检查本地存储库中是否有对应的副本。 如果没有，它将尝试从指定的远程存储库中检索，在这种情况下，它将接收到具有最新时间戳的构件。\n    \n- 如果本地的确有这个构件，并且当前构建也是这一天中的第一个，那么 Maven 将默认尝试更新该本地副本。这个行为可以通过使用 Maven 配置文件（settings.xml）中的配置或者命令行 标志来进行配置\n    \n\n  \n\n# POM 实例\n\n```Go\n\u003c?xml version=\"1.0\" encoding=\"ISO-8859-15\"?\u003e \n\u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" \n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 \n    http://maven.apache.org/maven-v4_0_0.xsd\"\u003e\n    \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \n    \u003cgroupId\u003ecom.example\u003c/groupId\u003e \n    \u003cartifactId\u003emyproject\u003c/artifactId\u003e \n    \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \n    \u003cpackaging\u003ejar\u003c/packaging\u003e \n    \u003cname\u003eMy Jar Project\u003c/name\u003e \n    \n    \u003cdependencies\u003e \n        \u003cdependency\u003e \n        \u003cgroupId\u003eio.netty\u003c/groupId\u003e \n        \u003cartifactId\u003enetty-all\u003c/artifactId\u003e \n        \u003cversion\u003e4.1.9.Final\u003c/version\u003e \n        \u003c/dependency\u003e \n    \u003c/dependencies\u003e \n    \u003cbuild\u003e \n        \u003cplugins\u003e \n            \u003cplugin\u003e \n                \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \n                \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \n                \u003cversion\u003e3.2\u003c/version\u003e \n                \u003cconfiguration\u003e \n                    \u003csource\u003e1.7\u003c/source\u003e \n                    \u003ctarget\u003e1.7\u003c/target\u003e \n                \u003c/configuration\u003e \n            \u003c/plugin\u003e \n        \u003c/plugins\u003e \n    \u003c/build\u003e \n\u003c/project\u003e\n```\n\n  \n\n## 继承和组合\n\n```Go\n\u003cproject\u003e \n    \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \n    \u003cparent\u003e \n        \u003cgroupId\u003enia\u003c/groupId\u003e \n        \u003cartifactId\u003enia-samples-parent\u003c/artifactId\u003e \n        \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \n    \u003c/parent\u003e \n    \u003cartifactId\u003echapter2\u003c/artifactId\u003e \n    \u003cpackaging\u003epom\u003c/packaging\u003e \n    \u003cname\u003e2. Echo Client and Server\u003c/name\u003e \n    \u003cmodules\u003e \n        \u003cmodule\u003eClient\u003c/module\u003e \n        \u003cmodule\u003eServer\u003c/module\u003e \n    \u003c/modules\u003e \n    \u003cproperties\u003e \n        \u003cecho-server.hostname\u003elocalhost\u003c/echo-server.hostname\u003e \n        \u003cecho-server.port\u003e9999\u003c/echo-server.port\u003e \n        \u003c/properties\u003e \n    \u003cdependencies\u003e \n    \u003cdependency\u003e \n        \u003cgroupId\u003eio.netty\u003c/groupId\u003e \n        \u003cartifactId\u003enetty-all\u003c/artifactId\u003e \n        \u003c/dependency\u003e \n    \u003c/dependencies\u003e \n    \u003cbuild\u003e \n        \u003cplugins\u003e \n            \u003cplugin\u003e \n                \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \n            \u003c/plugin\u003e \n            \u003cplugin\u003e \n                \u003cartifactId\u003emaven-failsafe-plugin\u003c/artifactId\u003e \n            \u003c/plugin\u003e \n            \u003cplugin\u003e \n                \u003cartifactId\u003emaven-surefire-plugin\u003c/artifactId\u003e \n            \u003c/plugin\u003e \n            \u003cplugin\u003e \n                \u003cgroupId\u003eorg.codehaus.mojo\u003c/groupId\u003e \n                \u003cartifactId\u003eexec-maven-plugin\u003c/artifactId\u003e \n            \u003c/plugin\u003e \n        \u003c/plugins\u003e \n    \u003c/build\u003e \n\u003c/project\u003e\n```\n\n### POM 继承\n\n- POM 文件可能包含子项目要继承（并可能重写）的信息。\n    \n- 通过parent 实现\n    \n\nPOM 聚合\n\n- 聚合器 POM 会构建一个或者多个子项目，这些子项目驻留在该 POM 所在目录的子目录中。 子项目，或者`\u003cmodules\u003e`标签，是由它们的目录名标识的：\n    \n- 通过`\u003cmodules\u003e` 实现","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/shell/shell-overview":{"title":"shell overview","content":"# Shell 综述\n\n所谓的Shell，就是运行在终端中的文本互动程序。Shell分析你的文本输入，然后把文本转换成相应的计算机动作。\n\n  \n\n在后面的内容中，我将用$来表示Linux系统Shell的命令提示符。比如说输入date命令：\n\n```Go\n$date\n```\n\ndate用于日期时间的相关功能。敲击回车键Enter后，Shell会显示出系统当前的时间。\n\n  \n\nshell是Unix体系下的文本交互界面。你只需要用键盘来输入文本，就可以和操作系统交互。但这还是不够具体。说到底，Shell其实是一个运行着的程序。这个程序接收到你按下回车键之间的输入，就会对输入的文本进行分析。比如下面这个命令：\n\n```Go\n$free -h\n```\n\n包括空格在内总共7个字符。Shell程序会通过空格，区分出命令的不同部分\n\n- 第一个部分是命令名。\n    \n- 剩下的部分是选项和参数。\n    \n\n在这个例子中，Shell会进一步分析第二个部分，发现这一部分的开头是\"-\"字符，从而知道它是一个选项。\n\n  \n\n有了命令名，Shell下一步就要执行该命令名对应的动作。这听起来就像是在戏剧舞台上，演员按照脚本演戏。Shell命令可以分为如下三类：\n\n- Shell内建函数（built-in function）\n    \n    - Shell的内建函数是Shell自带的功能，\n        \n- 可执行文件（executable file）\n    \n    - 可执行文件是保存在Shell之外的脚本，提供了额外的功能\n        \n    - Shell必须在系统中找到对应命令名的可执行文件，才能正确执行。\n        \n        - 我们可以用绝对路径来告诉Shell可执行文件所在的位置。\n            \n        - 如果用户只是给出了命令名，而没有给出准确的位置，那么Shell必须自行搜索一些特殊的位置，也就是所谓的默认路径。\n            \n            - Shell会执行第一个名字和命令名相同的可执行文件。这就相当于，Shell帮我们自动补齐了可执行文件的位置信息。\n                \n            - 我们可以通过which命令，来确定命令名对应的是哪个可执行文件：\n                \n\n```Go\n$which date\n```\n\n- 别名（alias）\n    \n    - 别名是给某个命令一个简称，以后在Shell中就可以通过这个简称来调用对应的命令。在Shell中，我们可以用alias来定义别名：\n        \n\n```Go\n$alias freak=\"free -h\"\n```\n\n  \n\n  \n\n我们可以通过type命令来了解命令的类型\n\n```Go\n$type date\n$type pwd\n```\n\n  \n\n# Shell的选择\n\n  \n\nShell是文本解释器程序的统称，所以包括了不止一种Shell。常见的Shell有sh、bash、ksh、rsh、csh等\n\n\u003e sh的全名是Bourne Shell。名字中的玻恩就是这个Shell的作者。\n\u003e \n\u003e bash的全名是Bourne Again Shell\n\n  \n\n可以使用下面的命令查看sh 的版本\n\n```Go\n$echo $SHELL\n```\n\n- echo用于在终端打印出文本。\n    \n- 而$是一个新的Shell特殊符号。它提示Shell，后面跟随的不是一般的文本，而是用于存储数据的变量。\n    \n\n可以使用下面的命令更换shell\n\n```Go\n# chsh -s /bin/csh //改变当前设置为 /bin/csh\nChanging shell for root.\nShell not changed.\n```\n\n  \n\n# 命令的选项和参数\n\n一行命令里还可以包含着选项和参数。总的来说，\n\n- 选项用于控制命令的行为，\n    \n- 而参数说明了命令的作用对象。\n    \n\n比如说：\n\n```Go\n$uname -m\n```\n\n选项-m影响了命令uname的行为，导致uname输出了树莓派的CPU型号。如果不是该选项的影响，uname输出的将是\"Linux\"。\n\n- 由一个\"-\"引领一个英文字母，这成为短选项。多个短选项的字母可以合在一起，跟在同一个\"-\"后面。比如，下面的两个命令就等价：\n    \n\n```Go\n$uname -m -r\n$uname -mr\n```\n\n- 此外还有一种长选项，是用\"--\"引领一整个英文单词\n    \n\n```Go\n$date --version\n```\n\n有的时候，选项也会携带变量\n\n```Bash\n$sudo date --set=\"1999-01-01 08:00:00\"\n```\n\n  \n\n值得注意的是，Shell对空格敏感。**当一整个参数信息中包含了空格时，我们需要用引号把参数包裹起来**，以便Shell能识别出这是一个整体。\n\n  \n\n  \n\n# 变量\n\n我们可以通过变量名来引用变量中保持的数据。借助变量，程序员可以复用出现过的数据。**Bash中也有变量，但Bash的变量只能存储文本。**\n\n## 定义变量\n\n定义变量时，变量名不加美元符号\n\n```Go\nyour_name=\"a.com\"\n```\n\n变量的命名规则\n\n- 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。\n    \n- 中间不能有空格，可以使用下划线 **_**。\n    \n- 不能使用标点符号。\n    \n- 不能使用bash里的关键字（可用help命令查看保留关键字）\n    \n\n## 变量赋值\n\n- 用“=”来表示赋值\n    \n\n```Go\nvar=World\n```\n\n就是把文本World存入名为var的变量，即赋值。根据Bash的语法，**赋值符号“=”的前后不留空格**。赋值号右边的文本内容会存入赋值号左边的变量。\n\n```Go\nyour_name=\"tom\"\necho $your_name\nyour_name=\"alibaba\"\necho $your_name\n但注意，第二次赋值的时候不能写$your_name=\"alibaba\"，使用变量的时候才加美元符（$）。\n```\n\n- 如果文本中包含空格，那么你可以用单引号或双引号来包裹文本\n    \n\n```Go\nvar='abc bcd'\n\n或者\n\nvar=\"abc bcd\"\n```\n\n- 可以把一个命令输出的文本直接赋予给一个变量，借助``符号，date命令的输出存入了变量now\n    \n\n```Go\nnow=`date`\n```\n\n- 我们还可以把一个变量中的数据赋值给另一个变量：\n    \n\n```Go\nanother=$var\n```\n\n  \n\n## 引用变量\n\n在Bash中，**所谓的引用变量就是把变量翻译成变量中存储的文本，使用变量时，需要加美元符**\n\n- 我们可以用$var的方式来引用变量。\n    \n\n```Go\nvar=World\n$echo $var\n```\n\n- 在Bash中，你还可以在一段文本中嵌入变量。Bash也会把变量替换成变量中保存的文本\n    \n\n```Go\n$echo Hello$var\n```\n\n- 为了避免变量名和尾随的普通文本混淆，我们也可以换用${}的方式来标识变量\n    \n\n```Go\n$echo $varIsGood\n\n由于Bash中并没有varIsGood这个变量，所以Bash将打印空白行。但如果将命令改为：\n$echo ${var}IsGood\n\nBash通过${}识别出变量var，并把它替换成数据。最终echo命令打印出WorldIsGood。\n```\n\n- 在Bash中，为了把一段包含空格的文本当做单一参数，就需要用到单引号或双引号。\n    \n    - **你可以在双引号中使用变量**。\n        \n    \n    ```Go\n    $echo \"Hello $var\"\n    ```\n    \n    - Bash会忽视单引号中的变量引用，所以单引号中的变量名只会被当做普通文本\n        \n    \n    ```Go\n    $echo 'Hello $var'\n    \n    将打印Hello $var。\n    ```\n    \n\n  \n\n## 删除变量\n\n使用 unset 命令可以删除变量。语法：\n\n```Go\nunset variable_name\n\n\nmyUrl=\"https://www.runoob.com\"\nunset myUrl\necho $myUrl\n```\n\n  \n\n## 变量类型\n\n- **1) 局部变量** 局部变量在脚本或命令中定义，仅在当前shell实例中有效，其他shell启动的程序不能访问局部变量。\n    \n- **2) 环境变量** 所有的程序，包括shell启动的程序，都能访问环境变量，有些程序需要环境变量来保证其正常运行。必要的时候shell脚本也可以定义环境变量。\n    \n- **3) shell变量** shell变量是由shell程序设置的特殊变量。shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了shell的正常运行\n    \n\n  \n\n# 字符串\n\n字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），\n\n- 字符串可以用单引号，\n    \n- 也可以用双引号，\n    \n- 也可以不用引号。\n    \n\n## 单引号\n\n```Go\nstr='this is a string'\n```\n\n单引号字符串的限制：\n\n- 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；\n    \n- 单引号字串中不能出现单独一个的单引号（对单引号使用转义符后也不行），但可成对出现，作为字符串拼接使用。\n    \n\n## 双引号\n\n```Go\nyour_name=\"runoob\"\nstr=\"Hello, I know you are \\\"$your_name\\\"! \\n\"\necho -e $str\n```\n\n双引号的优点：\n\n1. 双引号里可以有变量\n    \n2. 双引号里可以出现转义字符\n    \n\n  \n\n- **拼接字符串**\n    \n\n```Go\nyour_name=\"runoob\"\n# 使用双引号拼接\ngreeting=\"hello, \"$your_name\" !\"\ngreeting_1=\"hello, ${your_name} !\"\necho $greeting  $greeting_1\n\n# 使用单引号拼接\ngreeting_2='hello, '$your_name' !'\ngreeting_3='hello, ${your_name} !'\necho $greeting_2  $greeting_3\n```\n\n输出\n\n```Go\nhello, runoob ! hello, runoob !\nhello, runoob ! hello, ${your_name} !\n```\n\n- 获取字符串长度\n    \n\n```Go\nstring=\"abcd\"\necho ${#string}   # 输出 4\n```\n\n  \n\n- 提取子字符串\n    \n\n```Go\nstring=\"runoob is a great site\"\necho ${string:1:4} # 输出 unoo\n```\n\n- 查找字符串\n    \n\n查找字符 **i** 或 **o** 的位置(哪个字母先出现就计算哪个)：\n\n```Go\nstring=\"runoob is a great site\"\necho `expr index \"$string\" io`  # 输出 4\n```\n\n# 数组\n\n**bash支持一维数组（不支持多维数组），并且没有限定数组的大小。**\n\n**数组元素的下标由 0 开始编号**。获取数组中的元素要利用下标，**下标可以是整数或算术表达式**，其值应大于或等于 0\n\n## 定义数组\n\n```Go\n数组名=(值1 值2 ... 值n)\n```\n\n  \n\n```Go\narray_name=(value0 value1 value2 value3)\n```\n\n或者\n\n```Go\narray_name=(\nvalue0\nvalue1\nvalue2\nvalue3\n)\n```\n\n或者\n\n```Go\narray_name[0]=value0\narray_name[1]=value1\narray_name[n]=valuen\n```\n\n  \n\n## 读取数组\n\n```Go\n${数组名[下标]}\n```\n\n```Go\nvaluen=${array_name[n]}\n```\n\n使用 **@** 符号可以获取数组中的所有元素\n\n```Go\necho ${array_name[@]}\n```\n\n## 获取数组长度\n\n```Go\n# 取得数组元素的个数\nlength=${#array_name[@]}\n# 或者\nlength=${#array_name[*]}\n# 取得数组单个元素的长度\nlengthn=${#array_name[n]}\n```\n\n## 关联数组\n\nBash 支持关联数组，可以使用任意的字符串、或者整数作为下标来访问数组元素。**-A** 选项就是用于声明一个关联数组。\n\n```Go\ndeclare -A array_name\n```\n\n**关联数组的键是唯一的。**\n\n  \n\n```Go\ndeclare -A site=([\"google\"]=\"www.google.com\" [\"runoob\"]=\"www.runoob.com\" [\"taobao\"]=\"www.taobao.com\")\n```\n\n或者\n\n```Go\ndeclare -A site\nsite[\"google\"]=\"www.google.com\"\nsite[\"runoob\"]=\"www.runoob.com\"\nsite[\"taobao\"]=\"www.taobao.com\"\n\necho \"数组的键为: ${!site[*]}\"\necho \"数组的键为: ${!site[@]}\"\n\n\n输出\n\n数组的键为: google runoob taobao\n数组的键为: google runoob taobao\n```\n\n  \n\n# 注释\n\n## 单行注释\n\n```Go\n#--------------------------------------------\n# 这是一个注释\n# author：菜鸟教程\n# site：www.runoob.com\n# slogan：学的不仅是技术，更是梦想！\n#--------------------------------------------\n##### 用户配置区 开始 #####\n#\n#\n# 这里可以添加脚本描述信息\n#\n#\n##### 用户配置区 结束  #####\n```\n\n多行注释\n\n```Go\n:\u003c\u003cEOF\n注释内容...\n注释内容...\n注释内容...\nEOF\n```\n\n或者\n\n```Go\n:\u003c\u003c'\n注释内容...\n注释内容...\n注释内容...\n'\n\n:\u003c\u003c!\n注释内容...\n注释内容...\n注释内容...\n!\n```\n\n  \n\n# Echo\n\nShell 的 echo 指令与 PHP 的 echo 指令类似，都是用于字符串的输出。命令格式：\n\n```Go\necho string\n```\n\n您可以使用echo实现更复杂的输出格式控制。\n\n### **显示普通字符串:**\n\n```Go\necho \"It is a test\"\n```\n\n这里的双引号完全可以省略，以下命令与上面实例效果一致：\n\n```Go\necho It is a test\n```\n\n  \n\n### **显示转义字符**\n\n```Go\necho \"\\\"It is a test\\\"\"\n```\n\n结果将是:\n\n```Go\n\"It is a test\"\n```\n\n同样，双引号也可以省略\n\n### **显示变量**\n\nread 命令从标准输入中读取一行,并把输入行的每个字段的值指定给 shell 变量\n\n```Go\n#!/bin/sh\nread name \necho \"$name It is a test\"\n```\n\n以上代码保存为 test.sh，name 接收标准输入的变量，结果将是:\n\n```Go\n[root@www ~]# sh test.sh\nOK                     #标准输入\nOK It is a test        #输出\n```\n\n  \n\n### **显示换行**\n\n```Go\n\necho -e \"OK! \\n\" # -e 开启转义\necho \"It is a test\"\n```\n\n输出结果：\n\n```Go\nOK!\n\nIt is a test\n```\n\n  \n\n### **显示不换行**\n\n```Go\n#!/bin/sh\necho -e \"OK! \\c\" # -e 开启转义 \\c 不换行\necho \"It is a test\"\n```\n\n输出结果：\n\n```Go\nOK! It is a test\n```\n\n### **显示结果定向至文件**\n\n```Go\necho \"It is a test\" \u003e myfile\n```\n\n  \n\n### **原样输出字符串，不进行转义或取变量(用单引号)**\n\n```Go\necho '$name\\\"'\n```\n\n  \n\n输出结果：\n\n```Go\n$name\\\"\n```\n\n### **显示命令执行结果**\n\n```Go\necho `date`\n```\n\n**注意：** 这里使用的是反引号 **`**, 而不是单引号 **'**。\n\n结果将显示当前日期\n\n```Go\nWed Apr 5 00:52:17 CST 2023\n```\n\n  \n\n  \n\n# 数学运算\n\n- 在Bash中，数字和运算符都被当做普通文本。**所以你无法像C语言一样便捷地进行数学运算**。比如执行下面的命令\n    \n\n```Go\n$result=1+2\n$echo $result\n\nBash并不会进行任何运算。它只会打印文本“1+2”。\n```\n\n- 在Bash中，**你还可以通过$(())语法来进行数值运算**。在双括号中你可以放入整数的加减乘除表达式。Bash会对其中的内容进行数值运算。比如\n    \n\n```Go\n$echo $((2 + (5*2)))\n```\n\n- 在$(())中，你也可以使用变量\n    \n\n```Go\n$var=1\n$echo $(($var + (5*2)))\n\n将打印运算结果11。\n```\n\n- 可以把数学运算的结果存入变量\n    \n\n```Go\n$result=$(( 1 + 2 ))\n```\n\n  \n\n# 运算符-expr和**$((表达式))**\n\n**原生bash不支持简单的数学运算，**但是可以通过其他命令来实现，例如 awk 和 expr，expr 最常用。\n\n- expr 是一款表达式计算工具，使用它能完成表达式的求值操作。\n    \n- 在 MAC 中 shell 的 expr 语法是：**$((表达式))**\n    \n- 注意点\n    \n    - 表**达式和运算符之间要有空**格，例如 2+2 是不对的，必须写成 2 + 2，这与我们熟悉的大多数编程语言不一样。\n        \n    - **完整的表达式要被 `` 包含**，注意这个字符不是常用的单引号，在 Esc 键下边。\n        \n    \n    ```Go\n    #!/bin/bash\n    \n    val=`expr 2 + 2`\n    echo \"两数之和为 : $val\"\n    ```\n    \n\n## 算数运算符\n\n|   |   |   |\n|---|---|---|\n|运算符|说明|举例|\n|+|加法|`expr $a + $b` 结果为 30。|\n|-|减法|`expr $a - $b` 结果为 -10。|\n|*|乘法|`expr $a \\* $b` 结果为 200。|\n|/|除法|`expr $b / $a` 结果为 2。|\n|%|取余|`expr $b % $a` 结果为 0。|\n|=|赋值|a=$b 把变量 b 的值赋给 a。|\n|==|相等。用于比较两个数字，相同则返回 true。|[ $a == $b ] 返回 false。|\n|!=|不相等。用于比较两个数字，不相同则返回 true。|[ $a != $b ] 返回 true。|\n\n```Go\n#!/bin/bash\n# author:菜鸟教程\n# url:www.runoob.com\n\na=10\nb=20\n\nval=`expr $a + $b`\necho \"a + b : $val\"\n\nval=`expr $a - $b`\necho \"a - b : $val\"\n\nval=`expr $a \\* $b`\necho \"a * b : $val\"\n\nval=`expr $b / $a`\necho \"b / a : $val\"\n\nval=`expr $b % $a`\necho \"b % a : $val\"\n\nif [ $a == $b ]\nthen\n   echo \"a 等于 b\"\nfi\nif [ $a != $b ]\nthen\n   echo \"a 不等于 b\"\nfi\n```\n\n注意\n\n- 乘号(*)前边必须加反斜杠(\\)才能实现乘法运算；\n    \n- 在 MAC 中 shell 的 expr 语法是：**$((表达式))**，此处表达式中的 \"*\" 不需要转义符号 \"\\\" 。\n    \n\n## 关系运算符\n\n|   |   |   |\n|---|---|---|\n|运算符|说明|举例|\n|-eq|检测两个数是否相等，相等返回 true。|[ $a -eq $b ] 返回 false。|\n|-ne|检测两个数是否不相等，不相等返回 true。|[ $a -ne $b ] 返回 true。|\n|-gt|检测左边的数是否大于右边的，如果是，则返回 true。|[ $a -gt $b ] 返回 false。|\n|-lt|检测左边的数是否小于右边的，如果是，则返回 true。|[ $a -lt $b ] 返回 true。|\n|-ge|检测左边的数是否大于等于右边的，如果是，则返回 true。|[ $a -ge $b ] 返回 false。|\n|-le|检测左边的数是否小于等于右边的，如果是，则返回 true。|[ $a -le $b ] 返回 true。|\n\n```Go\n#!/bin/bash\n# author:菜鸟教程\n# url:www.runoob.com\n\na=10\nb=20\n\nif [ $a -eq $b ]\nthen\n   echo \"$a -eq $b : a 等于 b\"\nelse\n   echo \"$a -eq $b: a 不等于 b\"\nfi\nif [ $a -ne $b ]\nthen\n   echo \"$a -ne $b: a 不等于 b\"\nelse\n   echo \"$a -ne $b : a 等于 b\"\nfi\nif [ $a -gt $b ]\nthen\n   echo \"$a -gt $b: a 大于 b\"\nelse\n   echo \"$a -gt $b: a 不大于 b\"\nfi\nif [ $a -lt $b ]\nthen\n   echo \"$a -lt $b: a 小于 b\"\nelse\n   echo \"$a -lt $b: a 不小于 b\"\nfi\nif [ $a -ge $b ]\nthen\n   echo \"$a -ge $b: a 大于或等于 b\"\nelse\n   echo \"$a -ge $b: a 小于 b\"\nfi\nif [ $a -le $b ]\nthen\n   echo \"$a -le $b: a 小于或等于 b\"\nelse\n   echo \"$a -le $b: a 大于 b\"\nfi\n```\n\n## 布尔运算符\n\n|   |   |   |\n|---|---|---|\n|运算符|说明|举例|\n|!|非运算，表达式为 true 则返回 false，否则返回 true。|[ ! false ] 返回 true。|\n|-o|或运算，有一个表达式为 true 则返回 true。|[ $a -lt 20 -o $b -gt 100 ] 返回 true。|\n|-a|与运算，两个表达式都为 true 才返回 true。|[ $a -lt 20 -a $b -gt 100 ] 返回 false。|\n\n  \n\n```Go\n#!/bin/bash\n# author:菜鸟教程\n# url:www.runoob.com\n\na=10\nb=20\n\nif [ $a != $b ]\nthen\n   echo \"$a != $b : a 不等于 b\"\nelse\n   echo \"$a == $b: a 等于 b\"\nfi\nif [ $a -lt 100 -a $b -gt 15 ]\nthen\n   echo \"$a 小于 100 且 $b 大于 15 : 返回 true\"\nelse\n   echo \"$a 小于 100 且 $b 大于 15 : 返回 false\"\nfi\nif [ $a -lt 100 -o $b -gt 100 ]\nthen\n   echo \"$a 小于 100 或 $b 大于 100 : 返回 true\"\nelse\n   echo \"$a 小于 100 或 $b 大于 100 : 返回 false\"\nfi\nif [ $a -lt 5 -o $b -gt 100 ]\nthen\n   echo \"$a 小于 5 或 $b 大于 100 : 返回 true\"\nelse\n   echo \"$a 小于 5 或 $b 大于 100 : 返回 false\"\nfi\n```\n\n## 逻辑运算符\n\n|   |   |   |\n|---|---|---|\n|运算符|说明|举例|\n|\u0026\u0026|逻辑的 AND|[[ $a -lt 100 \u0026\u0026 $b -gt 100 ]] 返回 false|\n|\\||逻辑的 OR|[[ $a -lt 100 \\| $b -gt 100 ]] 返回 true|\n\n```Go\n#!/bin/bash\n# author:菜鸟教程\n# url:www.runoob.com\n\na=10\nb=20\n\nif [[ $a -lt 100 \u0026\u0026 $b -gt 100 ]]\nthen\n   echo \"返回 true\"\nelse\n   echo \"返回 false\"\nfi\n\nif [[ $a -lt 100 || $b -gt 100 ]]\nthen\n   echo \"返回 true\"\nelse\n   echo \"返回 false\"\nfi\n```\n\n## 字符串运算符\n\n|   |   |   |\n|---|---|---|\n|运算符|说明|举例|\n|=|检测两个字符串是否相等，相等返回 true。|[ $a = $b ] 返回 false。|\n|!=|检测两个字符串是否不相等，不相等返回 true。|[ $a != $b ] 返回 true。|\n|-z|检测字符串长度是否为0，为0返回 true。|[ -z $a ] 返回 false。|\n|-n|检测字符串长度是否不为 0，不为 0 返回 true。|[ -n \"$a\" ] 返回 true。|\n|$|检测字符串是否不为空，不为空返回 true。|[ $a ] 返回 true。|\n\n```Go\n#!/bin/bash\n# author:菜鸟教程\n# url:www.runoob.com\n\na=\"abc\"\nb=\"efg\"\n\nif [ $a = $b ]\nthen\n   echo \"$a = $b : a 等于 b\"\nelse\n   echo \"$a = $b: a 不等于 b\"\nfi\nif [ $a != $b ]\nthen\n   echo \"$a != $b : a 不等于 b\"\nelse\n   echo \"$a != $b: a 等于 b\"\nfi\nif [ -z $a ]\nthen\n   echo \"-z $a : 字符串长度为 0\"\nelse\n   echo \"-z $a : 字符串长度不为 0\"\nfi\nif [ -n \"$a\" ]\nthen\n   echo \"-n $a : 字符串长度不为 0\"\nelse\n   echo \"-n $a : 字符串长度为 0\"\nfi\nif [ $a ]\nthen\n   echo \"$a : 字符串不为空\"\nelse\n   echo \"$a : 字符串为空\"\nfi\n```\n\n  \n\n## 文件测试运算符\n\n|   |   |   |\n|---|---|---|\n|b file|检测文件是否是块设备文件，如果是，则返回 true。|[ -b $file ] 返回 false。|\n|-c file|检测文件是否是字符设备文件，如果是，则返回 true。|[ -c $file ] 返回 false。|\n|-d file|检测文件是否是目录，如果是，则返回 true。|[ -d $file ] 返回 false。|\n|-f file|检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。|[ -f $file ] 返回 true。|\n|-g file|检测文件是否设置了 SGID 位，如果是，则返回 true。|[ -g $file ] 返回 false。|\n|-k file|检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。|[ -k $file ] 返回 false。|\n|-p file|检测文件是否是有名管道，如果是，则返回 true。|[ -p $file ] 返回 false。|\n|-u file|检测文件是否设置了 SUID 位，如果是，则返回 true。|[ -u $file ] 返回 false。|\n|-r file|检测文件是否可读，如果是，则返回 true。|[ -r $file ] 返回 true。|\n|-w file|检测文件是否可写，如果是，则返回 true。|[ -w $file ] 返回 true。|\n|-x file|检测文件是否可执行，如果是，则返回 true。|[ -x $file ] 返回 true。|\n|-s file|检测文件是否为空（文件大小是否大于0），不为空返回 true。|[ -s $file ] 返回 true。|\n|-e file|检测文件（包括目录）是否存在，如果是，则返回 true。|[ -e $file ] 返回 true。|\n\n  \n\n```Go\n#!/bin/bash\n# author:菜鸟教程\n# url:www.runoob.com\n\nfile=\"/var/www/runoob/test.sh\"\nif [ -r $file ]\nthen\n   echo \"文件可读\"\nelse\n   echo \"文件不可读\"\nfi\nif [ -w $file ]\nthen\n   echo \"文件可写\"\nelse\n   echo \"文件不可写\"\nfi\nif [ -x $file ]\nthen\n   echo \"文件可执行\"\nelse\n   echo \"文件不可执行\"\nfi\nif [ -f $file ]\nthen\n   echo \"文件为普通文件\"\nelse\n   echo \"文件为特殊文件\"\nfi\nif [ -d $file ]\nthen\n   echo \"文件是个目录\"\nelse\n   echo \"文件不是个目录\"\nfi\nif [ -s $file ]\nthen\n   echo \"文件不为空\"\nelse\n   echo \"文件为空\"\nfi\nif [ -e $file ]\nthen\n   echo \"文件存在\"\nelse\n   echo \"文件不存在\"\nfi\n```\n\n  \n\n# 返回代码\n\nLinux中，每个可执行程序会有一个整数的返回代码。按照Linux惯例，当程序正常运行完毕并返回时，将返回整数0。\n\n比如foo.c\n\n```Go\nint main(void) {\n    int a;\n    int b;\n    int c;\n    a = 6;\n    b = 2;\n    c = 6/2;\n    return 0;\n}\n```\n\n这段程序可以正常运行。因此，它将在最后一句执行return语句，程序的返回代码是0\n\n- 在Shell中，我们运行了程序后，**可以通过$?变量来获知返回码**\n    \n\n```Go\n$gcc foo.c\n$./a.out\n$echo $?\n```\n\n- 如果一个程序运行异常，那么这个程序将返回非0的返回代码。比如删除一个不存在的文件：\n    \n\n```Go\n$rm none_exist.file\n$echo $?\n```\n\n- 在Linux中，可以在一个行命令中执行多个程序\n    \n    ```Go\n    $touch demo.file; ls;\n    ```\n    \n    - **我们可以让后一个程序的运行参考前一个程序的返回代码。比如说，只有前一个程序返回成功代码0，才让后一个程序运行：可以改写为**\n        \n        ```Go\n        $rm demo.file \u0026\u0026 echo \"rm succeed\"\n        ```\n        \n    - 还有一种情况，**是等到前一个程序失败了，才运行后一个程序**\n        \n    \n    ```Go\n    $rm demo.file || echo \"rm fail\"\n    ```\n    \n\n  \n\n  \n\n# Bash 脚本\n\n你还可以把多行的Bash命令写入一个文件，成为所谓的Bash脚本，当Bash脚本执行时，Shell将逐行执行脚本中的命令。\n\n  \n\n## 一个脚本例子\n\n```Go\n#!/bin/bash\necho Hello\necho World\n```\n\n- 脚本的第一行说明了该脚本使用的Shell，即/bin/bash路径的Bash程序\n    \n- 脚本正文是两行echo命令，两行命令将按照由上至下的顺序依次执行\n    \n\n  \n\n运行脚本的方式和运行可执行程序的方式类似，都是：\n\n```Go\n$./hello_world.bash\n```\n\n\u003e 如果用户不具有执行Bash脚本文件的权限，那么他将无法执行Bash脚本。此时，用户必须更换文件权限，或者以其他身份登录，才能执行脚本。\n\nBash脚本是一种复用代码的方式。我们看一个简单的Bash脚本**hw_info.bash**，它将计算机的信息存入到名为log的文件中：\n\n```Go\n#!/bin/bash\necho \"Information of Vamei's computer:\" \u003e log\nlscpu \u003e\u003e log\nuname –a \u003e\u003e log\nfree –h \u003e\u003e log\n```\n\n  \n\n## 脚本参数\n\n和可执行程序类似，Bash脚本运行时，也可以携带参数。这些**参数可以在Bash脚本中以变量的形式使用**。\n\n在Bash中，**你可以用$0、$1、$2……的方式，来获得Bash脚本运行时的参数**\n\n比如\n\n```Go\n#!/bin/bash\necho $0 \necho $1 \necho $2\n```\n\n我们用下面的方式运行Bash脚本：\n\n```Go\n$./test_arg.bash hello world\n```\n\n$0是命令的第一部分，也就是./test_arg.bash。$1代表了参数hello，而$2代表了参数world。因此，上面程序将打印：\n\n```Go\n./test_arg.bash\nhello\nworld\n```\n\n  \n\n如果变更参数，同一段脚本将有不同的行为。这大大提高了Bash脚本的灵活性。上面的hw_info.bash脚本中，我们把输出文件名写死成log\n\n```Go\n#!/bin/bash\necho \"Information of Vamei's computer:\" \u003e $1\nlscpu \u003e\u003e $1 \nuname –a \u003e\u003e $1 \nfree –h \u003e\u003e $1\n```\n\n可以这样执行代码\n\n```Go\n$./hw_info.bash output.file\n```\n\n- 脚本内获取参数的格式为：**$n**。**n** 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数，以此类推……\n    \n- 另外，还有几个特殊字符用来处理参数：\n    \n\n|   |   |\n|---|---|\n|参数处理|说明|\n|$#|传递到脚本的参数个数|\n|$*|以一个单字符串显示所有向脚本传递的参数。  \u003cbr\u003e如\"$*\"用「\"」括起来的情况、以\"$1 $2 … $n\"的形式输出所有参数。|\n|$$|脚本运行的当前进程ID号|\n|$!|后台运行的最后一个进程的ID号|\n|$@|与$*相同，但是使用时加引号，并在引号中返回每个参数。  \u003cbr\u003e如\"$@\"用「\"」括起来的情况、以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。|\n|$-|显示Shell使用的当前选项，与[set命令](https://www.runoob.com/linux/linux-comm-set.html)功能相同。|\n|$?|显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。|\n\n$* 与 $@ 区别：\n\n- 相同点：都是引用所有参数。\n    \n- 不同点：只有在双引号中体现出来。假设在脚本运行时写了三个参数 1、2、3，，则 \" * \" 等价于 \"1 2 3\"（传递了一个参数），而 \"@\" 等价于 \"1\" \"2\" \"3\"（传递了三个参数）。\n    \n\n## 脚本的返回代码\n\n和可执行程序类似，脚本也可以有返回代码。还是按照惯例，脚本正常退出时返回代码0在脚本的末尾，**我们可以用exit命令来设置脚本的返回代码**\n\n```Go\n#!/bin/bash\necho Hello\necho World\nexit 0\n```\n\n其实在脚本的末尾加一句exit 0并不必要。一个脚本如果正常运行完最后一句，会自动的返回代码0。\n\n在脚本运行后，我们可以通过$?变量查询脚本的返回代码\n\n```Go\n$./hello_world.bash\n$echo $?\n```\n\n如果在脚本中部出现exit命令，脚本会直接在这一行停止，并返回该exit命令给出的返回代码\n\n```Go\n#!/bin/bash\necho hello\nexit 1 echo world\n\n不会打印 world\n```\n\n## 运行Shell 脚本\n\n**1、作为可执行程序**\n\n```Go\nchmod +x ./test.sh  #使脚本具有执行权限\n./test.sh  #执行脚本\n```\n\n**2、作为解释器参数**\n\n```Go\n/bin/sh test.sh\n/bin/php test.php\n```\n\n# 函数\n\n在Bash中，脚本和函数有很多相似的地方。\n\n- 脚本实现了一整个脚本文件的程序复用，\n    \n- 而函数复用了脚本内部的部分程序。\n    \n\n在定义函数时，我们需要花括号来标识函数包括的部分：\n\n```Go\n#!/bin/bash\nfunction my_info (){\n  lscpu \u003e\u003e log\n  uname –a \u003e\u003e log\n  free –h \u003e\u003e log\n}\n\nmy_info\n```\n\n需要强调的是，**函数定义只是食谱，并没有转化成具体的动作**。脚本的最后一行是在调用函数。只有通过函数调用，函数内包含的命令才能真正执行\n\n  \n\n## 函数参数\n\n像脚本一样，函数调用时还可以携带参数。在函数内部，我**们同样可以用$1、$2这种形式的变量来使用参数**：\n\n当n\u003e=10时，需要使用${n}来获取参数。\n\n```Go\n#!/bin/bash\nfunction my_info (){\n  lscpu \u003e\u003e $1 \n  uname –a \u003e\u003e $1 \n  free –h \u003e\u003e $1\n }\n\nmy_info output.file\nmy_info another_output.file\n```\n\n还有几个特殊字符用来处理参数：\n\n|   |   |\n|---|---|\n|$#|传递到脚本或函数的参数个数|\n|$*|以一个单字符串显示所有向脚本传递的参数|\n|$$|脚本运行的当前进程ID号|\n|$!|后台运行的最后一个进程的ID号|\n|$@|与$*相同，但是使用时加引号，并在引号中返回每个参数。|\n|$-|显示Shell使用的当前选项，与set命令功能相同。|\n|$?|显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。|\n\n  \n\n# 跨脚本调用\n\n在Bash中**使用source命令，可以实现函数的跨脚本调用**。**命令source的作用是在同一个进程中执行另一个文件中的Bash脚本**\n\n比如说，有两个脚本，my_info.bash和app.bash。脚本my_info.sh中的内容是：\n\n```Go\n#!/bin/bash\nfunction my_info (){\n  lscpu \u003e\u003e $1 \n  uname –a \u003e\u003e $1 \n  free –h \u003e\u003e $1\n}\n```\n\n脚本app.bash中的内容是：\n\n```Go\n#!/bin/bash\nsource my_info.bash\nmy_info output.file\n```\n\n  \n\n# 逻辑判断\n\nBash除了可以进行数值运算，还可以进行逻辑判断。逻辑判断是决定某个说法的真假。逻辑判断就是对一个说法判断真假。**在Bash中，我们可以用test命令来进行逻辑判断**：\n\n```Go\n$test 3 -gt 2; echo $?\n```\n\n命令test后面跟有一个判断表达式，其中的-gt表示大于，即greater than。\n\n  \n\n## 数值判断\n\n数值大小和相等关系的判断，是最常见的逻辑判断。除了上面的大于和小于判断，我们还可以进行以下的数值判断：\n\n- 等于： $test 3 -eq 3; echo $?\n    \n- 不等于： $test 3 -ne 1; echo $?\n    \n- 大于等于： $test 5 -ge 2; echo $?\n    \n- 小于等于： $test 3 -le 1; echo $?\n    \n- 大于：$test 3 -gt 2; echo $?\n    \n- 小于：$test 3 -lt 2; echo $\n    \n\n  \n\n### 文本判断\n\nBash中最常见的数据形式是文本，因此也提供了很多关于文本的判断：\n\n- 文本相同: $test abc = abx; echo $?\n    \n- 文本不同： $test abc != abx; echo $?\n    \n- 按照词典顺序，一个文本在另一个文本之前： $test apple \u003e tea; echo $?\n    \n- 按照词典顺序，一个文本在另一个文本之后： $test apple \u003c tea; echo $?\n    \n\n  \n\n## 文件状态判断\n\n  \n\n- 检查一个文件是否存在： $test –e a.out; echo $?\n    \n- 检查一个文件是否存在，而且是普通文件： $test –f file.txt; echo $?\n    \n- 检查一个文件是否存在，而且是目录文件： $test –d myfiles; echo $?\n    \n- 检查一个文件是否存在，而且是软连接： $test –L a.out; echo $?\n    \n- 检查一个文件是否可读： $test –r file.txt; echo $?\n    \n- 检查一个文件是否可写： $test –w file.txt; echo $?\n    \n- 检查一个文件是否可执行： $test –x file.txt; echo $?\n    \n\n  \n\n## 与或非\n\n- 非，与expression的真假相反：\n    \n\n```Plain\n! expression\n```\n\n- 与，必须expression1和expression2都为真时，结果才为真：\n    \n\n```Plain\nexpression1 –a expression2\n```\n\n- 或，只要expression1和expression2的一个为真时，结果就为真：\n    \n\n```Plain\nexpression1 –o expression2\n```\n\n  \n\n  \n\n# 选择结构\n\n选择结构是一种语法结构，可以让程序根据条件决定执行哪一部分的指令，我们可以根据条件来决定是否执行某一部分程序，比如下面的demo_if.bash脚本\n\n## if-fi\n\n```Go\nvar=`whoami`\n\nif [ $var = \"root\" ]then   \n  echo \"You are root\"   \n  echo \"You are my God.\"\nfi\n```\n\n  \n\n## if - else-fi\n\n```Go\n#!/bin/bash\nfilename=$1 \n\nif [ -e $filename ]then   \n  echo \"$filename exists\" \nelse   \n  echo \"$filename NOT exists\" \nfi \n\necho \"The End\"\n```\n\n## case\n\n在Bash下，我们还可以用case语法来实现多程序块的选择执行。比如下面的脚本demo_case.bash：\n\n```Go\n#!/bin/bash\nvar=`whoami`\necho \"You are $var\" \ncase $var in\nroot)\n    echo \"You are God.\"\n;;\nvamei)\n    echo \"You are a happy user.\"\n;;\n*)\n    echo \"You are the Others.\"\n;;\nesac\n```\n\n关键字case后面不再是逻辑表达式，**而是一个作为条件的文本**。后面的代码块分为三个部分，**都以文本标签)的形式开始，以;;结束**。在case结构运行时，会逐个检查文本标签。当条件文本和文本标签可以对应上时，Bash就会执行隶属于该文本标签的代码块。\n\n  \n\n如果是用户vamei执行该Bash脚本，那么条件文本和vamei标签对应上，脚本就会打印：\n\n```Go\nYou are a happy user.\n```\n\n## 文本标签\n\n注意）\n\n文本标签除了是一串具体的文本，还可以包含文本通配符。结构case中常用的通配符包括：\n\n|   |   |   |   |\n|---|---|---|---|\n|通配符|含义|文本标签例子|符合条件的文本|\n|*|任意文本|*)|Xyz, 12a3, …|\n|?|任意一个字符|a?c)|abc, axc, …|\n|[]|范围内一个字符|[1-5][b-d])|2b, 3d, …|\n\n  \n\n# 循环结构\n\n## while\n\n- 在while语法中，Bash会循环执行隶属于while的代码块，直到逻辑表达式不成立。比如下面的demo_while.bash\n    \n\n```Go\n#!/bin/bash\nnow=`date +'%Y%m%d%H%M'`\ndeadline=`date --date='1 hour' +'%Y%m%d%H%M'`\nwhile [ $now -lt $deadline ]do\n   date\n   echo \"not yet\"\n   sleep 10\n   now=`date +'%Y%m%d%H%M'`\ndone \n```\n\n- 如果while的条件始终是真，那么循环会一直进行下去。下面的程序就是以无限循环的形式\n    \n\n```Go\n#!/bin/bash\nwhile true\ndo\n  date\n  sleep 1 \ndone\n```\n\n语法while的终止条件是一个逻辑判断。**如果在循环过程中改变逻辑判断的内容，那么我们很难在程序执行之前预判循环进行的次数**。\n\n## for\n\n- 与while语法对应的是for循环。这种语法会在程序进行前确定好循环进行的次数，比如demo_for.bash：\n    \n\n```Bash\n#!/bin/bash\nfor var in `ls log*`\ndo \n  rm $var\ndone\n```\n\n在这个例子中，命**令ls log*将返回所有以log开头的文件名。这些文件名之间由空格分隔。循环进行时，Bash会依次取出一个文件名，赋值给变量var，并执行do和done之间隶属于for结构的程序块。**\n\n由于ls命令返回的内容在是确定的，因此for循环进行的次数也会在一开始确定下来。\n\n- 在for语法中，**我们也可以使用自己构建一个由空格分隔的文本。由空格区分出来的每个子文本会在循环中赋值给变量**\n    \n\n```Go\n#!/bin/bash\nfor user in vamei anna yutian\ndo\n    echo $user\ndone\n```\n\n- for循环还可以和seq命令配合使用。\n    \n\n命令seq用于生成一个等差的整数序列，命令后面可以跟3个参数，第\n\n- 一个参数表示整数序列的开始数字，\n    \n- 第二个参数表示每次增加多少，\n    \n- 最后一个参数表示序列的终点\n    \n\n```Bash\n$seq 1 2 10\n```\n\n输出效果为\n\n```Go\n1 3 5 7 9\n```\n\n结合for循环和seq命令，我们可以解一些有趣的数学问题。比如高斯求和，是要计算从1到100的所有整数的和\n\n```Go\n#!/bin/bash\n\ntotal=0 \nfor number in `seq 1 1 100`\ndo \n    total=$(( $total + $number ))\ndone \n\necho $total\n```\n\n## **无限循环**\n\n无限循环语法格式：\n\n```Go\nwhile :\ndo\n    command\ndone\n```\n\n  \n\n或者\n\n```Go\nwhile true\ndo\n    command\ndone\n```\n\n  \n\n或者\n\n```Go\nfor (( ; ; ))\n```\n\n## until 循环\n\n  \n\nuntil 循环执行一系列命令直至条件为 true 时停止。\n\nuntil 循环与 while 循环在处理方式上刚好相反。\n\n一般 while 循环优于 until 循环，但在某些时候—也只是极少数情况下，until 循环更加有用。\n\nuntil 语法格式:\n\n```Go\nuntil condition\ndo\n    command\ndone\n```\n\n```Go\n#!/bin/bash\n\na=0\n\nuntil [ ! $a -lt 10 ]\ndo\n   echo $a\n   a=`expr $a + 1`\ndone\n```\n\n  \n\n## break\n\n```Go\n#!/bin/bash\n\ntotal=0\nnumber=1 \n\nwhile :\ndo \n  if [ $number -gt 100 ]\n  then\n      break\n  fi\n  total=$(( $total + $number ))\n  number=$(($number + 1))\ndone \n\necho $total\n```\n\n这里break语句的作用是在满足条件时跳出循环。\n\n  \n\n## continue\n\n```Go\n#!/bin/bash\n\ntotal=0 \n\nfor number in `seq 1 1 100`do \n  if (( $number % 3 == 0 )) \n  then\n      continue\n  fi\n  total=$(( $total + $number ))\ndone \n\necho $total\n```\n\n  \n\n  \n\n# 输入输出重定向\n\n大多数 UNIX 系统命令从你的终端接受输入并将所产生的输出发送回到您的终端。一个命令通常从一个叫标准输入的地方读取输入，默认情况下，这恰好是你的终端\n\n重定向命令列表如下：\n\n|   |   |\n|---|---|\n|命令|说明|\n|command \u003e file|将输出重定向到 file。|\n|command \u003c file|将输入重定向到 file。|\n|command \u003e\u003e file|将输出以追加的方式重定向到 file。|\n|n \u003e file|将文件描述符为 n 的文件重定向到 file。|\n|n \u003e\u003e file|将文件描述符为 n 的文件以追加的方式重定向到 file。|\n|n \u003e\u0026 m|将输出文件 m 和 n 合并。|\n|n \u003c\u0026 m|将输入文件 m 和 n 合并。|\n|\u003c\u003c tag|将开始标记 tag 和结束标记 tag 之间的内容作为输入。|\n\n一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件：\n\n- 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。\n    \n- 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。\n    \n- 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。\n    \n\n默认情况下，command \u003e file 将 stdout 重定向到 file，command \u003c file 将stdin 重定向到 file。\n\n  \n\n  \n\n# Bash 和C语言\n\nBash语言和C语言都是Linux下的常用语言。它们都能通过特定的语法来编写程序，而程序运行后都能实现某些功能。尽管在语法细节上存在差异，但两种语言都有以下语法：\n\n- 变量：在内存中储存数据\n    \n- 循环结构：重复执行代码块\n    \n- 选择结构：根据条件执行代码块\n    \n- 函数：复用代码块\n    \n\n  \n\nBash和C的相似性，也来自于它们共同遵守的编程范式——面向过程编程。支持面向过程编程的语言，一**般都会提供类似于函数的代码封装方式**\n\n  \n\n我们还应该注意到Bash和C程序的区别\n\n- Bash的变量只能是文本类型，C的变量却可以有整数、浮点数、字符等类型。\n    \n- Bash的很多功能，如加减乘除运算，都是调用其他程序实现的。而C直接就可以进行加减乘除运算。\n    \n\n  \n\n**另一方面，Bash是一个Shell。它本质上是一个命令解释器程序，而不是编程语言。**用户可以通过命令行的方式，来调用该程序的某些功能。所谓的Bash编程，只是命令解释器程序提供的一种互动方法。Bash脚本只能和Bash进程互动。它不能像C语言一样，直接调用CPU的功能。因此，Bash能实现的功能会受限，运行速度上也比不上可执行文件。\n\n但另一反面，Bash脚本也有它的好处。 C语言能接触到很底层的东西，但使用起来也很复杂。有时候，即使你已经知道如何用C实现一个功能，写代码依然是一个很繁琐的过程。Bash正相反。**由于Bash可以便捷地调用已有的程序，因此很多工作可以用数行的脚本解决**。此外，Bash脚本不需要编辑，就可以由Bash进程理解并执行。因此，开发Bash脚本比写C程序要快很多。Linux的系统运维工作，如定期备份、文件系统管理等，就经常使用到Bash脚本。总之，Bash编程知识是晋级为资深Linux用户的必要条件。","lastmodified":"2023-07-31T03:20:22.32908836Z","tags":[]},"/zookeeper/%E9%9D%A2%E8%AF%95%E9%A2%98":{"title":"面试题","content":"![[zookeeper面试专题.pdf]]![[Zookeeper分布式过程.pdf]]","lastmodified":"2023-07-31T03:20:23.401086603Z","tags":[]}}