{"/":{"title":"_index","content":"\n\nHello !😀😘😆😂😁\n\n\n\n\n\n\n# Obsidian\n\n* [[Obsidian/dataview]]\n\n- [[Obsidian/excalidraw]]\n\n- [[Obsidian/Front Matter]]\n\n- [[Obsidian/obsidian overview]]\n\n- [[Obsidian/Obsidian-plugin]]\n\n- [[Obsidian/publish]]\n\n- [[Obsidian/template]]\n\n\n# Lua\n\n- [[lua/lua基础]]\n\n- [[lua/Lua高级]]\n\n","lastmodified":"2023-08-01T09:41:32.852106344Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/10_learn/draw":{"title":"draw","content":"# 11.2 画图经验分享\n\n小林写这么多篇图解文章，你们猜我收到的最多的读者问题是什么？没错，就是问我是使用什么**画图**工具，看来对这一点大家都相当好奇，那干脆不如写一篇介绍下我是怎么画图的。\n\n如果我的文章缺少了自己画的图片，相当于失去了灵魂，技术文章本身就很枯燥，如果文章中没有几张图片，读者被劝退的概率飙飙升，剩下没被劝退的估计看着看着就睡着了。所以，精美的图片可以说是必不可少的一部分，不仅在阅读时能带来视觉的冲击，而且图片相比文字能涵盖更多的信息，不然怎会有一图胜千言的说法呢？\n\n这时，可能有的读者会说自己不写文章呀，是不是没有必要了解画图了？我觉得这是不对，画图在我们工作中其实也是有帮助的，比如如果你想跟领导汇报一个业务流程的问题，把业务流程画出来，肯定用图的方式比用文字的方式交流起来会更有效率，更轻松些；如果你参与了一个比较复杂的项目开发，你也可以把代码的流程图给画出来，不仅能帮助自己加深理解，也能帮助后面参与的同事能更快的接手这个项目；甚至如果你要晋升级别了，演讲 PTT 里的配图也是必不可少的。\n\n不过很多人都是纠结用什么画图工具，其实小林觉得再烂的画图工具，只要你思路清晰，确定自己要表达出什么信息，也是能把图画好的，所以不必纠结哪款画图工具，挑一款自己画起来舒服的就行了。\n\n\u003e “小林，你说的我都懂，我就是喜欢你的画图风格嘛，你就说说你用啥画的？”\n\n咳咳，没问题，直接坦白讲，我用的是一个在线的画图网址，地址是：\n- *https://draw.io*\n\n用它的原因是使用方便和简单，当然最重要的是它完全免费，没有什么限制，甚至还能直接把图片保存到 GoogleDrive 、 OneDrive 和 Github，我就是保存到 Github，然后用 Github 作为我的图床。\n\n\n既然要认识它，那就先来看看它长什么样子，它主要分为三个区域，从左往右的顺序是「图形选择区域、绘图区域、属性设置区域」。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/正面图.png)\n\n其中，最左边的「图形选择区域」可以选择的图案有很多种，常见的流程图、时序图、表格图都有，甚至还可以在最左下角的「更多图形」找到其他种类的图形，比如网络设备图标等。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/图形选择.png)\n\n再来，最右边「属性设置区域」可以设置文字的大小，图片颜色、线条形状等，而我最常用颜色板块是下面这三种，都是比较浅色的，这样看起来舒服些。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/浅色风格.png)\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/浅色风格2.png)\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/深浅色风格.png)\n\n我最近常用的一个图形是圆角方块图，它的位置如下图，但是它默认的颜色过于深色，如果要在方框图中描述文字，则可能看不清楚，这时我会在最右侧的「属性设置区域」把方块颜色设置成浅色系列的。另外，还有一点需要注意的是，默认的字体大小比较小，我一般会调成 `16px` 大小。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/常用的方块.png)\n\n如果你不喜欢上图的带有「划痕」的圆角方块图形，可以选择下图中这个最简洁的圆角方框图形。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/圆角方块图形.png)\n\n这个简洁的圆角方框图形，再搭配颜色，能组合成很多结构图，比如我用过它组成过 CPU Cache 的结构图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/存储结构/CPU-Cache.png)\n\n那直角方框图形，我主要是用来组成「表格」，原因自带的表格不好看，也不方便调。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/表格.png)\n\n如果觉得直直的线条太死板，你可以把图片属性中的「*Comic*」勾上，于是就会变成歪歪扭扭的效果啦，有点像手绘风格，挺多人喜欢这种风格。\n\n比如，我用过这种风格画过 TCP 三次握手流程的图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/14.jpg)\n\n方块图形再加上菱形，就可以组合成简单程序流程图了，比如我画过存储器缓存流程图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/存储结构/缓存体系.png)\n\n所以，不要小看这些基本图形，只要构思清晰，再基本的图形，也是能构成层次分明并且好看的图。\n\n基本的图形介绍完后，相信你画一些简单程序流程图等图形是没问题的了，接下来就是各种**图形 + 线条**的组合的了。\n\n通过一些基本的图形组合，你还可以画出时序图，时序图可以用来描述多个对象之间的交互流程，比如我画过多个线程获取互斥锁的时序图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/锁/互斥锁工作流程.png)\n\n再来，为了更好表达零拷贝技术的过程，那么用图的方式会更清晰。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)\n\n前面也提到，图形不只是简单图形，还有其他自带的设备类图形，比如我用网络设备图画过单播、广播、多播通信的区别图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/13.jpg)\n\n你要说，我画过最复杂的图，那就是写 TCP 流量控制的时候，把整个交互过程 + 文字描述 + 滑动窗口状况都画出来了，现在回想起来还是觉得累人。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/22.jpg)\n\n还有好多好多，我就比一一列举，这半年下来，小林至少画了 `500+` 张图了，每一张图其实还是挺费时间的，相信画过图的朋友后，都能体会到这种感觉了。但没办法，谁叫小林是图解工具人呢，画图可以更好的诠释文章内容，但最重要的是，把你们吸引过来了，这是件让我非常高兴的事情，也是让我感觉画图这个事情值得认真做。\n\n另外，细心的读者也发现了，小林贴代码的时候，使用的是图片的形式，原因是代码通常都是比较长，在手机看文章用图片的呈现的方式会更舒服清晰。\n\n在这里也推荐下这个代码截图网址：\n- *https://carbon.now.sh/*\n\n网站页面如下图，代码显示的效果是不是很美观？\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/carbon.png)\n\n文字的分享有局限性，关键还是要你自己动手摸索摸索，形成自己一套画图的方法论，练习的时候可以先从模仿画起，后面再结合工作或文章的需求画出自己心中的那个图。","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/10_learn/learn_os":{"title":"learn_os","content":"# 11.1 操作系统怎么学？\n\n操作系统真的可以说是 `Super Man`，它为了我们做了非常厉害的事情，以至于我们根本察觉不到，只有通过学习它，我们才能深刻体会到它的精妙之处，甚至会被计算机科学家设计思想所震撼，有些思想实际上也是可以应用于我们工作开发中。\n\n操作系统比较重要的四大模块，分别是[内存管理](https://mp.weixin.qq.com/s/HJB_ATQFNqG82YBCRr97CA)、[进程管理](https://mp.weixin.qq.com/s/YXl6WZVzRKCfxzerJWyfrg)、[文件系统管理](https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw)、[输入输出设备管理](https://mp.weixin.qq.com/s/04BkLtnPBmmx6CtdQPXiRA)。这是我学习操作系统的顺序，也是我推荐给大家的学习顺序，因为内存管理不仅是最重要、最难的模块，也是和其他模块关联性最大的模块，先把它搞定，后续的模块学起来我认为会相对轻松一些。  \n\n学习的过程中，你可能会遇到很多「虚拟」的概念，比如虚拟内存、虚拟文件系统，实际上它们的本质上都是一样的，都是**向下屏蔽差异，向上提供统一的东西**，以方便我们程序员使用。\n\n还有，你也遇到各种各样的[调度算法](https://mp.weixin.qq.com/s/JWj6_BF9Xc84kQcyx6Nf_g)，在这里你可以看到数据结构与算法的魅力，重要的是我们要理解为什么要提出那么多调度算法，你当然可以说是为了更快更有效率，但是因什么问题而因此引入新算法的这个过程，更是我们重点学习的地方。\n\n你也会开始明白进程与线程最大的区别在于上下文切换过程中，**线程不用切换虚拟内存**，因为同一个进程内的线程都是共享虚拟内存空间的，线程就单这一点不用切换，就相比进程上下文切换的性能开销减少了很多。由于虚拟内存与物理内存的映射关系需要查询页表，页表的查询是很慢的过程，因此会把常用的地址映射关系缓存在 TLB 里的，这样便可以提高页表的查询速度，如果发生了进程切换，那 TLB 缓存的地址映射关系就会失效，缓存失效就意味着命中率降低，于是虚拟地址转为物理地址这一过程就会很慢。\n\n\n你也开始不会傻傻的认为 read 或 write 之后数据就直接写到硬盘了，更不会觉得多次操作 read 或 write 方法性能会很低，因为你发现操作系统会有个「**磁盘高速缓冲区**」，它已经帮我们做了缓存的工作，它会预读数据、缓存最近访问的数据，以及使用 I/O 调度算法来合并和排队磁盘调度 I/O，这些都是为了减少操作系统对磁盘的访问频率。\n\n……\n\n还有太多太多了，我在这里就不赘述了，剩下的就交给你们在学习操作系统的途中去探索和发现了。\n\n\n还有一点需要注意，学操作系统的时候，不要误以为它是在说 Linux 操作系统，这也是我初学的时候犯的一个错误，操作系统是集合大多数操作系统实现的思想，跟实际具体实现的 Linux 操作系统多少都会有点差别，如果要想 Linux 操作系统的具体实现方式，可以选择看 Linux 内核相关的资料，但是在这之前你先掌握了操作系统的基本知识，这样学起来才能事半功倍。\n\n\n\n\n## 入门系列\n\n对于没学过操作系统的小白，我建议学的时候，不要直接闷头看书。相信我，你不用几分钟就会打退堂鼓，然后就把厚厚的书拿去垫显示器了，从此再无后续，毕竟直接看书太特喵的枯燥了，当然不如用来垫显示器玩游戏来着香。\n\nB 站关于操作系统课程资源很多，我在里面也看了不同老师讲的课程，觉得比较好的入门级课程是《**操作系统 - 清华大学**》，该课程由清华大学老师向勇和陈渝授课，虽然我们上不了清华大学，但是至少我们可以在网上选择听清华大学的课嘛。课程授课的顺序，就如我前面推荐的学习顺序：「内存管理 -\u003e 进程管理 -\u003e 文件系统管理 -\u003e 输入输出设备管理」。\n\n![《操作系统 - 清华大学》](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6.png)\n\n\u003e B 站清华大学操作系统视频地址：https://www.bilibili.com/video/BV1js411b7vg?from=search\u0026seid=2361361014547524697\n\n该清华大学的视频教学搭配的书应该是《**现代操作系统**》，你可以视频和书籍两者结合一起学，比如看完视频的内存管理，然后就看书上对应的章节，这样相比直接啃书相对会比较好。\n\n\n![《现代操作系统》](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E7%8E%B0%E4%BB%A3%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.png)\n\n清华大学的操作系统视频课讲的比较精炼，涉及到的内容没有那么细，《**操作系统 - 哈工大**》李治军老师授课的视频课程相对就会比较细节，老师会用 Linux 内核代码的角度带你进一步理解操作系统，也会用生活小例子帮助你理解。\n\n![《操作系统 - 哈工大》](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E5%93%88%E5%B7%A5%E5%A4%A7.png)\n\n\u003e B 站哈工大操作系统视频地址：https://www.bilibili.com/video/BV1d4411v7u7?from=search\u0026seid=2361361014547524697\n\n\n## 深入学习系列\n\n《现代操作系统》这本书我感觉缺少比较多细节，说的还是比较笼统，而且书也好无聊。\n\n推荐一个说的更细的操作系统书 —— 《**操作系统导论**》，这本书不仅告诉你 What，还会告诉你 How，书的内容都是循序渐进，层层递进的，阅读起来还是觉得挺有意思的，这本书的内存管理和并发这两个部分说的很棒，这本书的中文版本我也没找到资源，不过微信读书可以免费看这本书。\n\n![《操作系统导论》](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AF%BC%E8%AE%BA.png)\n\n当然，少不了这本被称为神书的《**深入理解计算机系统**》，豆瓣评分高达 `9.8` 分，这本书严格来说不算操作系统书，它是以程序员视角理解计算机系统，不只是涉及到操作系统，还涉及到了计算机组成、C 语言、汇编语言等知识，是一本综合性比较强的书。\n\n![《深入理解计算机系统》](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F.jpg)\n\n它告诉我们计算机是如何设计和工作的，操作系统有哪些重点，它们的作用又是什么，这本书的目标其实便是要讲清楚原理，但并不会把某个话题挖掘地过于深入，过于细节。看看这本书后，我们就可以对计算机系统各组件的工作方式有了理性的认识。在一定程度上，其实它是在锻炼一种思维方式 —— 计算思维。\n\n\n----\n\n## 最后\n\n文中推荐的书，小林都已经把电子书整理好给大家了，只需要在小林的公众号后台回复「**我要学习**」，即可获取百度网盘下载链接。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/cpu_mesi":{"title":"cpu_mesi","content":"# 2.4 CPU 缓存一致性\n\n直接上，不多 BB 了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/缓存一致性提纲.png)\n\n---\n\n##  CPU Cache 的数据写入\n\n随着时间的推移，CPU 和内存的访问性能相差越来越大，于是就在 CPU 内部嵌入了 CPU Cache（高速缓存），CPU Cache 离 CPU 核心相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存角色。\n\nCPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核心越近，访问速度也快，但是存储容量相对就会越小。其中，在多核心的 CPU 里，每个核心都有各自的 L1/L2 Cache，而 L3 Cache 是所有核心共享使用的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/CPU-Cache.png)\n\n我们先简单了解下 CPU Cache 的结构，CPU Cache 是由很多个 Cache Line 组成的，CPU Line 是 CPU 从内存读取数据的基本单位，而 CPU Line 是由各种标志（Tag）+ 数据块（Data Block）组成，你可以在下图清晰的看到：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/Cache的数据结构.png)\n\n\n\n我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，而不是每一次都要从内存中获取数据。所以，身为程序员，我们要尽可能写出缓存命中率高的代码，这样就有效提高程序的性能，具体的做法，你可以参考我上一篇文章[「如何写出让 CPU 跑得更快的代码？」](https://mp.weixin.qq.com/s/-uhAhBD2zGl_h19E4fNJzQ)\n\n\n事实上，数据不光是只有读操作，还有写操作，那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。\n\n\n问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？为了应对这个问题，下面介绍两种针对写入数据的方法：\n\n- 写直达（*Write Through*）\n- 写回（*Write Back*）\n\n### 写直达\n\n保持内存与 Cache 一致性最简单的方式是，**把数据同时写入内存和 Cache 中**，这种方法称为**写直达（*Write Through*）**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/写直达.png)\n\n\n在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：\n\n- 如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；\n- 如果数据没有在 Cache 里面，就直接把数据更新到内存里面。\n\n写直达法很直观，也很简单，但是问题明显，无论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费大量的时间，无疑性能会受到很大的影响。\n\n### 写回\n\n既然写直达由于每次写操作都会把数据写回到内存，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了**写回（*Write Back*）的方法**。\n\n\n在写回机制中，**当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提高系统的性能。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/写回1.png)\n\n\n那具体如何做到的呢？下面来详细说一下：\n\n- 如果当发生写操作时，数据已经在 CPU Cache 里的话，则把数据更新到 CPU Cache 里，同时标记 CPU Cache 里的这个 Cache Block 为脏（Dirty）的，这个脏的标记代表这个时候，我们 CPU Cache 里面的这个 Cache Block 的数据和内存是不一致的，这种情况是不用把数据写到内存里的；\n- 如果当发生写操作时，数据所对应的 Cache Block 里存放的是「别的内存地址的数据」的话，就要检查这个 Cache Block 里的数据有没有被标记为脏的：\n  - 如果是脏的话，我们就要把这个 Cache Block 里的数据写回到内存，然后再把当前要写入的数据，先从内存读入到 Cache Block 里（注意，这一步不是没用的，具体为什么要这一步，可以看这个「[回答](https://stackoverflow.com/questions/26672661/for-write-back-cache-policy-why-data-should-first-be-read-from-memory-before-w)」），然后再把当前要写入的数据写入到  Cache Block，最后也把它标记为脏的；\n  - 如果不是脏的话，把当前要写入的数据先从内存读入到 Cache Block 里，接着将数据写入到这个 Cache Block 里，然后再把这个 Cache Block 标记为脏的就好了。\n\n\n可以发现写回这个方法，在把数据写入到 Cache 的时候，只有在缓存不命中，同时数据对应的 Cache 中的 Cache Block 为脏标记的情况下，才会将数据写到内存中，而在缓存命中的情况下，则在写入后 Cache 后，只需把该数据对应的 Cache Block 标记为脏即可，而不用写到内存里。\n\n这样的好处是，如果我们大量的操作都能够命中缓存，那么大部分时间里 CPU 都不需要读写内存，自然性能相比写直达会高很多。\n\n为什么缓存没命中时，还要定位 Cache Block？这是因为此时是要判断数据即将写入到 cache block 里的位置，是否被「其他数据」占用了此位置，如果这个「其他数据」是脏数据，那么就要帮忙把它写回到内存。\n\nCPU 缓存与内存使用「写回」机制的流程图如下，左半部分就是读操作的流程，右半部分就是写操作的流程，也就是我们上面讲的内容。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/redis/%E5%85%AB%E8%82%A1%E6%96%87/writeback.png)\n\n## 缓存一致性问题\n\n现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的**缓存一致性（*Cache Coherence*）** 的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。\n\n那缓存一致性的问题具体是怎么发生的呢？我们以一个含有两个核心的 CPU  作为例子看一看。\n\n假设 A 号核心和 B 号核心同时运行两个线程，都操作共同的变量 i（初始值为 0 ）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/缓存一致性问题例子.png)\n\n这时如果 A 号核心执行了 `i++` 语句的时候，为了考虑性能，使用了我们前面所说的写回策略，先把值为 `1` 的执行结果写入到 L1/L2 Cache 中，然后把 L1/L2 Cache 中对应的 Block 标记为脏的，这个时候数据其实没有被同步到内存中的，因为写回策略，只有在 A 号核心中的这个 Cache Block 要被替换的时候，数据才会写入到内存里。\n\n如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。**这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/缓存一致性问题例子2.png)\n\n\n那么，要解决这一问题，就需要一种机制，来同步两个不同核心里面的缓存数据。要实现的这个机制的话，要保证做到下面这 2 点：\n\n- 第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为**写传播（*Write Propagation*）**；\n- 第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（*Transaction Serialization*）**。\n\n第一点写传播很容易就理解，当某个核心在 Cache 更新了数据，就需要同步到其他核心的 Cache 里。而对于第二点事务的串行化，我们举个例子来理解它。\n\n假设我们有一个含有 4 个核心的 CPU，这 4 个核心都操作共同的变量 i（初始值为 0 ）。A 号核心先把 i 值变为 100，而此时同一时间，B 号核心先把 i 值变为 200，这里两个修改，都会「传播」到 C 和 D 号核心。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/事件顺序问题.png)\n\n\n那么问题就来了，C 号核心先收到了 A 号核心更新数据的事件，再收到 B 号核心更新数据的事件，因此 C 号核心看到的变量 i 是先变成 100，后变成 200。\n\n而如果 D 号核心收到的事件是反过来的，则 D 号核心看到的是变量 i 先变成 200，再变成 100，虽然是做到了写传播，但是各个 Cache 里面的数据还是不一致的。\n\n\n所以，我们要保证 C 号核心和 D 号核心都能看到**相同顺序的数据变化**，比如变量 i 都是先变成 100，再变成 200，这样的过程就是事务的串行化。\n\n要实现事务串行化，要做到 2 点：\n\n- CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；\n- 要引入「锁」的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。\n\n那接下来我们看看，写传播和事务串行化具体是用什么技术实现的。\n\n---\n\n## 总线嗅探\n\n写传播的原则就是当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常见实现的方式是**总线嗅探（*Bus Snooping*）**。\n\n我还是以前面的 i 变量例子来说明总线嗅探的工作机制，当 A 号 CPU 核心修改了 L1 Cache 中 i 变量的值，通过总线把这个事件广播通知给其他所有的核心，然后每个 CPU 核心都会监听总线上的广播事件，并检查是否有相同的数据在自己的 L1 Cache 里面，如果 B 号 CPU 核心的 L1 Cache 中有该数据，那么也需要把该数据更新到自己的 L1 Cache。\n\n可以发现，总线嗅探方法很简单， CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。\n\n另外，总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串行化。\n\n于是，有一个协议基于总线嗅探机制实现了事务串行化，也用状态机机制降低了总线带宽压力，这个协议就是 MESI 协议，这个协议就做到了 CPU 缓存一致性。\n\n---\n\n## MESI 协议\n\nMESI 协议其实是 4 个状态单词的开头字母缩写，分别是：\n\n- *Modified*，已修改\n- *Exclusive*，独占\n- *Shared*，共享\n- *Invalidated*，已失效\n\n这四个状态来标记 Cache Line 四个不同的状态。\n\n「已修改」状态就是我们前面提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。而「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。\n\n「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。\n\n「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。\n\n另外，在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。\n\n那么，「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。\n\n我们举个具体的例子来看看这四个状态的转换：\n\n1. 当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是一致的；\n2. 然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是一致的；\n3. 当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后 A 号 CPU 核心才更新 Cache 里面的数据，同时标记 Cache Line 为「已修改」状态，此时 Cache 中的数据就与内存不一致了。\n4. 如果 A 号 CPU 核心「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态，因此不需要给其他 CPU 核心发送消息，直接更新数据即可。\n5. 如果 A 号 CPU 核心的 Cache 里的 i 变量对应的  Cache Line 要被「替换」，发现  Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。\n\n\n所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送广播给其他 CPU 核心，这在一定程度上减少了总线带宽压力。\n\n事实上，整个 MESI 的状态可以用一个有限状态机来表示它的状态流转。还有一点，对于不同状态触发的事件操作，可能是来自本地 CPU 核心发出的广播事件，也可以是来自其他 CPU 核心通过总线发出的广播事件。下图即是 MESI 协议的状态图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU缓存一致性/MESI协议.png)\n\n MESI 协议的四种状态之间的流转过程，我汇总成了下面的表格，你可以更详细的看到每个状态转换的原因：\n\n ![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%20MESI%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2%E8%A1%A8%E6%A0%BC.png)\n\n\n---\n\n## 总结\n\nCPU 在读写数据的时候，都是在 CPU Cache 读写数据的，原因是 Cache 离 CPU 很近，读写性能相比内存高出很多。对于 Cache 里没有缓存 CPU 所需要读取的数据的这种情况，CPU 则会从内存读取数据，并将数据缓存到 Cache 里面，最后 CPU 再从 Cache 读取数据。\n\n而对于数据的写入，CPU 都会先写入到 Cache 里面，然后再在找个合适的时机写入到内存，那就有「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性：\n\n- 写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；\n- 写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；\n\n当今 CPU 都是多核的，每个核心都有各自独立的 L1/L2 Cache，只有 L3 Cache 是多个核心之间共享的。所以，我们要确保多核缓存是一致性的，否则会出现错误的结果。\n\n要想实现缓存一致性，关键是要满足 2 点：\n\n- 第一点是写传播，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心；\n- 第二点是事物的串行化，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；\n\n基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。\n\nMESI 协议，是已修改、独占、共享、已失效这四个状态的英文缩写的组合。整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心。\n\n\n---\n\n## 关注作者\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/float":{"title":"float","content":"# 2.7  为什么 0.1 + 0.2 不等于 0.3 ？\n\n我们来思考几个问题：\n\n- 为什么负数要用补码表示？\n- 十进制小数怎么转成二进制？\n- 计算机是怎么存小数的？\n- 0.1 + 0.2 == 0.3 吗？\n- ...\n\n别看这些问题都看似简单，但是其实还是有点东西的这些问题。\n\n---\n\n## 为什么负数要用补码表示？\n\n十进制转换二进制的方法相信大家都熟能生巧了，如果你说你还不知道，我觉得你还是太谦虚，可能你只是忘记了，即使你真的忘记了，不怕，贴心的小林在和你一起回忆一下。\n\n 十进制数转二进制采用的是**除 2 取余法**，比如数字 8 转二进制的过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/十进制转二进制.png)\n\n\n接着，我们看看「整数类型」的数字在计算机的存储方式，这其实很简单，也很直观，就是将十进制的数字转换成二进制即可。\n\n我们以 `int` 类型的数字作为例子，int 类型是 `32` 位的，其中**最高位是作为「符号标志位」**，正数的符号位是 `0`，负数的符号位是 `1`，**剩余的 31 位则表示二进制数据**。\n\n那么，对于 int 类型的数字 1 的二进制数表示如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/int1.png)\n\n而负数就比较特殊了点，负数在计算机中是以「补码」表示的，**所谓的补码就是把正数的二进制全部取反再加 1**，比如 -1 的二进制是把数字 1 的二进制取反后再加 1，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/反码.png)\n\n\n不知道你有没有想过，为什么计算机要用补码的方式来表示负数？在回答这个问题前，我们假设不用补码的方式来表示负数，而只是把最高位的符号标志位变为 1 表示负数，如下图过程：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/非反码.png)\n\n\n如果采用这种方式来表示负数的二进制的话，试想一下 `-2 + 1` 的运算过程，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/非反码运算.png)\n\n\n按道理，`-2 + 1 = -1`，但是上面的运算过程中得到结果却是 `-3`，所可以发现，这种负数的表示方式是不能用常规的加法来计算了，就需要特殊处理，要先判断数字是否为负数，如果是负数就要把加法操作变成减法操作才可以得到正确对结果。\n\n\n到这里，我们就可以回答前面提到的「负数为什么要用补码方式来表示」的问题了。\n\n如果负数不是使用补码的方式表示，则在做基本对加减法运算的时候，**还需要多一步操作来判断是否为负数，如果为负数，还得把加法反转成减法，或者把减法反转成加法**，这就非常不好了，毕竟加减法运算在计算机里是很常使用的，所以为了性能考虑，应该要尽量简化这个运算过程。\n\n**而用了补码的表示方式，对于负数的加减法操作，实际上是和正数加减法操作一样的**。你可以看到下图，用补码表示的负数在运算 `-2 + 1` 过程的时候，其结果是正确的：\n\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/补码运算过程.png)\n\n---\n\n## 十进制小数与二进制的转换\n\n好了，整数十进制转二进制我们知道了，接下来看看小数是怎么转二进制的，小数部分的转换不同于整数部分，它采用的是**乘 2 取整法**，将十进制中的小数部分乘以 2 作为二进制的一位，然后继续取小数部分乘以 2 作为下一位，直到不存在小数为止。\n\n\n话不多说，我们就以 `8.625` 转二进制作为例子，直接上图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/十进制小数转二进制.png)\n\n\n最后把「整数部分 + 小数部分」结合在一起后，其结果就是 `1000.101`。\n\n但是，并不是所有小数都可以用二进制表示，前面提到的 0.625 小数是一个特例，刚好通过乘 2 取整法的方式完整的转换成二进制。\n\n如果我们用相同的方式，来把 `0.1` 转换成二进制，过程如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/无限小数.png)\n\n可以发现，`0.1` 的二进制表示是无限循环的。\n\n**由于计算机的资源是有限的，所以是没办法用二进制精确的表示 0.1，只能用「近似值」来表示，就是在有限的精度情况下，最大化接近 0.1 的二进制数，于是就会造成精度缺失的情况**。\n\n对于二进制小数转十进制时，需要注意一点，小数点后面的指数幂是**负数**。\n\n比如，二进制 `0.1` 转成十进制就是 `2^(-1)`，也就是十进制 `0.5`，二进制 `0.01` 转成十进制就是 `2^-2`，也就是十进制 `0.25`，以此类推。\n\n举个例子，二进制 `1010.101` 转十进制的过程，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/小数转二进制2.png)\n\n\n---\n\n## 计算机是怎么存小数的？\n\n`1000.101` 这种二进制小数是「定点数」形式，代表着小数点是定死的，不能移动，如果你移动了它的小数点，这个数就变了， 就不再是它原来的值了。\n\n然而，计算机并不是这样存储的小数的，计算机存储小数的采用的是**浮点数**，名字里的「浮点」表示小数点是可以浮动的。\n\n比如 `1000.101` 这个二进制数，可以表示成 `1.000101 x 2^3`，类似于数学上的科学记数法。\n\n既然提到了科学计数法，我再帮大家复习一下。\n\n比如有个很大的十进制数 1230000，我们可以也可以表示成 `1.23 x 10^6`，这种方式就称为科学记数法。\n\n该方法在小数点左边只有一个数字，而且把这种整数部分没有前导 0 的数字称为**规格化**，比如 `1.0 x 10^(-9)` 是规格化的科学记数法，而 `0.1 x 10^(-9)` 和 `10.0 x 10^(-9)` 就不是了。\n\n因此，如果二进制要用到科学记数法，同时要规范化，那么不仅要保证基数为 2，还要保证小数点左侧只有 1 位，而且必须为 1。\n\n所以通常将 `1000.101` 这种二进制数，规格化表示成 `1.000101 x 2^3`，其中，最为关键的是 000101 和 3 这两个东西，它就可以包含了这个二进制小数的所有信息：\n\n- `000101` 称为**尾数**，即小数点后面的数字；\n- `3` 称为**指数**，指定了小数点在数据中的位置；\n\n\n\n现在绝大多数计算机使用的浮点数，一般采用的是 IEEE 制定的国际标准，这种标准形式如下图：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/IEEE标准.png)\n\n\n这三个重要部分的意义如下：\n\n- *符号位*：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；\n- *指数位*：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，**指数位的长度越长则数值的表达范围就越大**；\n- *尾数位*：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且**尾数的长度决定了这个数的精度**，因此如果要表示精度更高的小数，则就要提高尾数位的长度；\n\n\n\n用 `32` 位来表示的浮点数，则称为**单精度浮点数**，也就是我们编程语言中的 `float` 变量，而用 `64` 位来表示的浮点数，称为**双精度浮点数**，也就是 `double`  变量，它们的结构如下：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/float.png)\n\n\n可以看到：\n\n- double 的尾数部分是 52 位，float 的尾数部分是 23 位，由于同时都带有一个固定隐含位（这个后面会说），所以 double 有 53 个二进制有效位，float 有 24 个二进制有效位，所以所以它们的精度在十进制中分别是 `log10(2^53)` 约等于 `15.95` 和 `log10(2^24)` 约等于 `7.22`  位，因此 double 的有效数字是 `15~16` 位，float 的有效数字是 `7~8` 位，这些有效位是包含整数部分和小数部分；\n- double 的指数部分是 11 位，而 float 的指数位是 8 位，意味着 double 相比 float 能表示更大的数值范围；\n\n\n那二进制小数，是如何转换成二进制浮点数的呢？\n\n我们就以 `10.625` 作为例子，看看这个数字在 float 里是如何存储的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/float存储.png)\n\n首先，我们计算出 10.625 的二进制小数为 1010.101。\n\n然后**把小数点，移动到第一个有效数字后面**，即将 1010.101 右移 `3` 位成 `1.010101`，右移 3 位就代表 +3，左移 3 位就是 -3。\n\n**float 中的「指数位」就跟这里移动的位数有关系，把移动的位数再加上「偏移量」，float 的话偏移量是 127，相加后就是指数位的值了**，即指数位这 8 位存的是 `10000010`（十进制 130），因此你可以认为「指数位」相当于指明了小数点在数据中的位置。\n\n`1.010101` 这个数的**小数点右侧的数字就是 float 里的「尾数位」**，由于尾数位是 23 位，则后面要补充 0，所以最终尾数位存储的数字是 `01010100000000000000000`。\n\n\n在算指数的时候，你可能会有疑问为什么要加上偏移量呢？\n\n前面也提到，指数可能是正数，也可能是负数，即指数是有符号的整数，而有符号整数的计算是比无符号整数麻烦的，所以为了减少不必要的麻烦，在实际存储指数的时候，需要把指数转换成**无符号整数**。\n\nfloat 的指数部分是 8 位，IEEE 标准规定单精度浮点的指数取值范围是 `-126 ~ +127`，于是为了把指数转换成无符号整数，就要加个**偏移量**，比如 float 的指数偏移量是 `127`，这样指数就不会出现负数了。\n\n比如，指数如果是 8，则实际存储的指数是 8 + 127（偏移量）= 135，即把 135 转换为二进制之后再存储，而当我们需要计算实际的十进制数的时候，再把指数减去「偏移量」即可。\n\n\n细心的朋友肯定发现，移动后的小数点左侧的有效位（即 1）消失了，它并没有存储到 float 里。\n\n这是因为 IEEE 标准规定，二进制浮点数的小数点左侧只能有 1 位，并且还只能是 1，**既然这一位永远都是 1，那就可以不用存起来了**。\n\n于是就让 23 位尾数只存储小数部分，然后在计算时会**自动把这个 1 加上，这样就可以节约 1 位的空间，尾数就能多存一位小数，相应的精度就更高了一点**。\n\n那么，对于我们在从 float 的二进制浮点数转换成十进制时，要考虑到这个隐含的 1，转换公式如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/float公式.png)\n\n举个例子，我们把下图这个 float 的数据转换成十进制，过程如下：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/float转二进制例子.png)\n\n---\n\n## 0.1 + 0.2 == 0.3 ?\n\n前面提到过，并不是所有小数都可以用「完整」的二进制来表示的，比如十进制 0.1 在转换成二进制小数的时候，是一串无限循环的二进制数，计算机是无法表达无限循环的二进制数的，毕竟计算机的资源是有限。\n\n因此，计算机只能用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。\n\n现在基本都是用  IEEE 754 规范的「单精度浮点类型」或「双精度浮点类型」来存储小数的，根据精度的不同，近似值也会不同。\n\n那计算机是存储 0.1 是一个怎么样的二进制浮点数呢？\n\n偷个懒，我就不自己手动算了，可以使用 binaryconvert 这个工具，将十进制 0.1 小数转换成 float 浮点数：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/0.1工具.png)\n\n\n可以看到，8 位指数部分是 `01111011`，23 位的尾数部分是 `10011001100110011001101`，可以看到尾数部分是 `0011` 是一直循环的，只不过尾数是有长度限制的，所以只会显示一部分，所以是一个近似值，精度十分有限。\n\n\n接下来，我们看看 0.2 的 float 浮点数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/0.2工具.png)\n\n\n可以看到，8 位指数部分是 `01111100`，稍微和 0.1 的指数不同，23 位的尾数部分是 `10011001100110011001101` 和 0.1 的尾数部分是相同的，也是一个近似值。\n\n\n\n 0.1 的二进制浮点数转换成十进制的结果是 `0.100000001490116119384765625`：\n\n  ![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/0.1浮点数转二进制小数.png)\n\n 0.2 的二进制浮点数转换成十进制的结果是 `0.20000000298023223876953125`：\n\n ![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/0.2浮点数转换.png)\n\n 这两个结果相加就是 `0.300000004470348358154296875`：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/0.1%2B0.2.png)\n\n\n所以，你会看到**在计算机中 0.1 + 0.2 并不等于完整的 0.3**。\n\n这主要是**因为有的小数无法可以用「完整」的二进制来表示，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数**。\n\n\n我们在 JavaScript 里执行 0.1 + 0.2，你会得到下面这个结果：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/js0.1%2B0.2.png)\n\n结果和我们前面推到的类似，因为 JavaScript 对于数字都是使用 IEEE 754 标准下的双精度浮点类型来存储的。\n\n而我们二进制只能精准表达 2 除尽的数字 1/2, 1/4, 1/8，但是对于 0.1(1/10) 和 0.2(1/5)，在二进制中都无法精准表示时，需要根据精度舍入。\n\n我们人类熟悉的十进制运算系统，可以精准表达 2 和 5 除尽的数字，例如 1/2, 1/4, 1/5(0.2), 1/8, 1/10(0.1)。\n\n当然，十进制也有无法除尽的地方，例如 1/3, 1/7，也需要根据精度舍入。\n\n---\n\n## 总结\n\n最后，再来回答开头的问题。\n\n\u003e 为什么负数要用补码表示？\n\n负数之所以用补码的方式来表示，主要是为了统一和正数的加减法操作一样，毕竟数字的加减法是很常用的一个操作，就不要搞特殊化，尽量以统一的方式来运算。\n\n\u003e 十进制小数怎么转成二进制？\n\n十进制整数转二进制使用的是「除 2 取余法」，十进制小数使用的是「乘 2 取整法」。\n\n\u003e 计算机是怎么存小数的？\n\n计算机是以浮点数的形式存储小数的，大多数计算机都是 IEEE 754 标准定义的浮点数格式，包含三个部分：\n\n- 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；\n- 指数位：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大；\n- 尾数位：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度；\n\n用 32 位来表示的浮点数，则称为单精度浮点数，也就是我们编程语言中的 float 变量，而用 64 位来表示的浮点数，称为双精度浮点数，也就是 double 变量。\n\n\u003e 0.1 + 0.2 == 0.3 吗？\n\n不是的，0.1 和 0.2 这两个数字用二进制表达会是一个一直循环的二进制数，比如 0.1 的二进制表示为 0.0 0011 0011 0011… （0011 无限循环)，对于计算机而言，0.1 无法精确表达，这是浮点数计算造成精度损失的根源。\n\n因此，IEEE 754 标准定义的浮点数只能根据精度舍入，然后用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。\n\n 0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个小数无法用「完整」的二进制来表示，只能根据精度舍入，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数。","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/how_cpu_deal_task":{"title":"how_cpu_deal_task","content":"# 2.5 CPU 是如何执行任务的？\n\n你清楚下面这几个问题吗？\n\n- 有了内存，为什么还需要 CPU Cache？\n- CPU 是怎么读写数据的？\n- 如何让 CPU 能读取数据更快一些？\n- CPU 伪共享是如何发生的？又该如何避免？\n- CPU 是如何调度任务的？如果你的任务对响应要求很高，你希望它总是能被先调度，这该怎么办？\n- ...\n\n这篇，我们就来回答这些问题。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/提纲.png)\n\n---\n\n## CPU 如何读写数据的？\n\n先来认识 CPU 的架构，只有理解了 CPU 的 架构，才能更好地理解 CPU 是如何读写数据的，对于现代 CPU 的架构图如下：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/CPU架构.png)\n\n可以看到，一个 CPU 里通常会有多个 CPU 核心，比如上图中的 1 号和 2 号 CPU 核心，并且每个 CPU 核心都有自己的 L1 Cache 和 L2 Cache，而 L1 Cache 通常分为 dCache（数据缓存） 和 iCache（指令缓存），L3 Cache 则是多个核心共享的，这就是 CPU 典型的缓存层次。\n\n上面提到的都是 CPU 内部的 Cache，放眼外部的话，还会有内存和硬盘，这些存储设备共同构成了金字塔存储层次。如下图所示：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储器的层次关系图.png)\n\n\n从上图也可以看到，从上往下，存储设备的容量会越大，而访问速度会越慢。至于每个存储设备的访问延时，你可以看下图的表格：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储器成本的对比.png)\n\n\n你可以看到， CPU 访问 L1 Cache 速度比访问内存快 100 倍，这就是为什么 CPU 里会有 L1~L3 Cache 的原因，目的就是把 Cache 作为 CPU 与内存之间的缓存层，以减少对内存的访问频率。\n\n\nCPU 从内存中读取数据到 Cache 的时候，并不是一个字节一个字节读取，而是一块一块的方式来读取数据的，这一块一块的数据被称为 CPU Cache  Line（缓存块），所以 **CPU Cache  Line 是 CPU 从内存读取数据到 Cache 的单位**。\n\n\n至于 CPU Cache  Line 大小，在 Linux 系统可以用下面的方式查看到，你可以看我服务器的 L1 Cache Line 大小是 64 字节，也就意味着 **L1 Cache 一次载入数据的大小是 64 字节**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/查看CPULine大小.png)\n\n\n 那么对数组的加载， CPU 就会加载数组里面连续的多个数据到 Cache 里，因此我们应该按照物理内存地址分布的顺序去访问元素，这样访问数组元素的时候，Cache 命中率就会很高，于是就能减少从内存读取数据的频率， 从而可提高程序的性能。\n\n 但是，在我们不使用数组，而是使用单独的变量的时候，则会有 Cache 伪共享的问题，Cache 伪共享问题上是一个性能杀手，我们应该要规避它。\n\n接下来，就来看看 Cache 伪共享是什么？又如何避免这个问题？\n\n现在假设有一个双核心的 CPU，这两个 CPU 核心并行运行着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是类型为 `long` 的变量 A 和 B，这个两个数据的地址在物理内存上是**连续**的，如果 Cahce Line 的大小是 64 字节，并且变量 A 在 Cahce Line 的开头位置，那么这两个数据是位于**同一个 Cache Line 中**，又因为 CPU Cache  Line 是 CPU 从内存读取数据到 Cache 的单位，所以这两个数据会被同时读入到了两个 CPU 核心中各自 Cache 中。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/同一个缓存行.png)\n\n我们来思考一个问题，如果这两个不同核心的线程分别修改不同的数据，比如 1 号 CPU 核心的线程只修改了 变量 A，或 2 号 CPU 核心的线程的线程只修改了变量 B，会发生什么呢？\n\n### 分析伪共享的问题\n\n现在我们结合保证多核缓存一致的 MESI 协议，来说明这一整个的过程，如果你还不知道 MESI 协议，你可以看我这篇文章「[10 张图打开 CPU 缓存一致性的大门](https://mp.weixin.qq.com/s/PDUqwAIaUxNkbjvRfovaCg)」。\n\n\n①. 最开始变量 A 和 B 都还不在 Cache 里面，假设 1 号核心绑定了线程 A，2 号核心绑定了线程 B，线程 A 只会读写变量 A，线程 B 只会读写变量 B。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/分析伪共享1.png)\n\n\n②. 1 号核心读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和 变量 B 的数据归属于同一个 Cache Line，所以 A 和 B 的数据都会被加载到 Cache，并将此 Cache Line 标记为「独占」状态。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/分析伪共享2.png)\n\n③.  接着，2 号核心开始从内存里读取变量 B，同样的也是读取 Cache Line 大小的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和 变量 B，此时 1 号和 2 号核心的 Cache Line 状态变为「共享」状态。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/分析伪共享3.png)\n\n④. 1 号核心需要修改变量 A，发现此 Cache Line 的状态是「共享」状态，所以先需要通过总线发送消息给 2 号核心，通知 2 号核心把 Cache 中对应的 Cache Line 标记为「已失效」状态，然后 1 号核心对应的 Cache Line 状态变成「已修改」状态，并且修改变量 A。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/分析伪共享4.png)\n\n⑤. 之后，2 号核心需要修改变量 B，此时 2 号核心的 Cache 中对应的 Cache Line 是已失效状态，另外由于 1 号核心的 Cache 也有此相同的数据，且状态为「已修改」状态，所以要先把 1 号核心的 Cache 对应的 Cache Line 写回到内存，然后 2 号核心再从内存读取 Cache Line 大小的数据到 Cache 中，最后把变量 B 修改到 2 号核心的 Cache 中，并将状态标记为「已修改」状态。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/分析伪共享5.png)\n\n所以，可以发现如果 1 号和 2 号 CPU 核心这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两个步骤，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属于一个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从而出现 ④ 和 ⑤ 这两个步骤。\n\n因此，这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为**伪共享（*False Sharing*）**。\n\n\n### 避免伪共享的方法\n\n因此，对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，否则就会出现为伪共享的问题。\n\n接下来，看看在实际项目中是用什么方式来避免伪共享的问题的。\n\n\n在 Linux 内核中存在 `__cacheline_aligned_in_smp` 宏定义，是用于解决伪共享的问题。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/__cacheline_aligned.png)\n\n\n\n从上面的宏定义，我们可以看到：\n\n- 如果在多核（MP）系统里，该宏定义是 `__cacheline_aligned`，也就是 Cache Line 的大小；\n- 而如果在单核系统里，该宏定义是空的；\n\n因此，针对在同一个 Cache Line 中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在 Cache Line 里是对齐的。\n\n举个例子，有下面这个结构体：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/struct_test.png)\n\n\n结构体里的两个成员变量 a 和 b 在物理内存地址上是连续的，于是它们可能会位于同一个 Cache Line 中，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/struct_ab.png)\n\n\n所以，为了防止前面提到的 Cache 伪共享问题，我们可以使用上面介绍的宏定义，将 b 的地址设置为 Cache Line 对齐地址，如下：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/struct_test1.png)\n\n这样 a 和 b 变量就不会在同一个 Cache Line 中了，如下图：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/struct_ab1.png)\n\n所以，避免  Cache 伪共享实际上是用空间换时间的思想，浪费一部分 Cache 空间，从而换来性能的提升。\n\n我们再来看一个应用层面的规避方案，有一个 Java 并发框架 Disruptor 使用「字节填充 + 继承」的方式，来避免伪共享的问题。\n\nDisruptor 中有一个 RingBuffer 类会经常被多个线程使用，代码如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/Disruptor.png)\n\n你可能会觉得 RingBufferPad 类里 7 个 long 类型的名字很奇怪，但事实上，它们虽然看起来毫无作用，但却对性能的提升起到了至关重要的作用。\n\n我们都知道，CPU Cache 从内存读取数据的单位是 CPU Cache  Line，一般 64 位 CPU 的 CPU Cache  Line 的大小是 64 个字节，一个 long 类型的数据是 8 个字节，所以 CPU 一下会加载 8 个 long 类型的数据。\n\n\n根据 JVM 对象继承关系中父类成员和子类成员，内存地址是连续排列布局的，因此 RingBufferPad 中的 7 个 long 类型数据作为 Cache Line **前置填充**，而 RingBuffer 中的 7 个 long 类型数据则作为 Cache Line **后置填充**，这 14 个 long 变量没有任何实际用途，更不会对它们进行读写操作。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/填充字节.png)\n\n另外，RingBufferFelds 里面定义的这些变量都是 `final` 修饰的，意味着第一次加载之后不会再修改， 又**由于「前后」各填充了 7 个不会被读写的 long 类型变量，所以无论怎么加载 Cache Line，这整个 Cache Line 里都没有会发生更新操作的数据，于是只要数据被频繁地读取访问，就自然没有数据被换出 Cache 的可能，也因此不会产生伪共享的问题**。\n\n---\n\n## CPU 如何选择线程的？\n\n\n了解完 CPU 读取数据的过程后，我们再来看看 CPU 是根据什么来选择当前要执行的线程。\n\n\n在 Linux 内核中，进程和线程都是用 `task_struct` 结构体表示的，区别在于线程的 task_struct 结构体里部分资源是共享了进程已创建的资源，比如内存地址空间、代码段、文件描述符等，所以 Linux 中的线程也被称为轻量级进程，因为线程的 task_struct 相比进程的 task_struct 承载的 资源比较少，因此以「轻」得名。 \n\n一般来说，没有创建线程的进程，是只有单个执行流，它被称为是主线程。如果想让进程处理更多的事情，可以创建多个线程分别去处理，但不管怎么样，它们对应到内核里都是 `task_struct`。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/任务.png)\n\n\n所以，Linux 内核里的调度器，调度的对象就是 `task_struct`，接下来我们就把这个数据结构统称为**任务**。\n\n\n在 Linux 系统中，根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越小，优先级越高：\n\n- 实时任务，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在 `0~99` 范围内的就算实时任务；\n- 普通任务，响应时间没有很高的要求，优先级在 `100~139` 范围内都是普通任务级别；\n\n### 调度类\n\n由于任务有优先级之分，Linux 系统为了保障高优先级的任务能够尽可能早的被执行，于是分为了这几种调度类，如下图：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/调度类.png)\n\n\nDeadline 和 Realtime 这两个调度类，都是应用于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作用如下： \n\n- *SCHED_DEADLINE*：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度；\n- *SCHED_FIFO*：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以「插队」；\n- *SCHED_RR*：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；\n\n\n而 Fair 调度类是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：\n\n- *SCHED_NORMAL*：普通任务使用的调度策略；\n- *SCHED_BATCH*：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。\n\n### 完全公平调度\n\n\n我们平日里遇到的基本都是普通任务，对于普通任务来说，公平性最重要，在 Linux 里面，实现了一个基于 CFS 的调度算法，也就是**完全公平调度（*Completely Fair Scheduling*）**。\n\n这个算法的理念是想让分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 vruntime，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，vruntime 是不会变化的。\n\n那么，**在 CFS 算法调度的时候，会优先选择 vruntime 少的任务**，以保证每个任务的公平性。\n\n这就好比，让你把一桶的奶茶平均分到 10 杯奶茶杯里，你看着哪杯奶茶少，就多倒一些；哪个多了，就先不倒，这样经过多轮操作，虽然不能保证每杯奶茶完全一样多，但至少是公平的。\n\n当然，上面提到的例子没有考虑到优先级的问题，虽然是普通任务，但是普通任务之间还是有优先级区分的，所以在计算虚拟运行时间 vruntime 还要考虑普通任务的**权重值**，注意权重值并不是优先级的值，内核中会有一个 nice 级别与权重值的转换表，nice 级别越低的权重值就越大，至于 nice 值是什么，我们后面会提到。\n于是就有了以下这个公式：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/vruntime.png)\n\n\n你可以不用管 NICE_0_LOAD 是什么，你就认为它是一个常量，那么在「同样的实际运行时间」里，高权重任务的 vruntime 比低权重任务的 vruntime **少**，你可能会奇怪为什么是少的？你还记得 CFS 调度吗，它是会优先选择 vruntime 少的任务进行调度，所以高权重的任务就会被优先调度了，于是高权重的获得的实际运行时间自然就多了。\n\n### CPU 运行队列\n\n一个系统通常都会运行着很多任务，多任务的数量基本都是远超 CPU 核心数量，因此这时候就需要**排队**。\n\n事实上，每个 CPU 都有自己的**运行队列（*Run Queue, rq*）**，用于描述在此 CPU 上所运行的所有进程，其队列包含三个运行队列，Deadline 运行队列 dl_rq、实时任务运行队列 rt_rq 和 CFS 运行队列 cfs_rq，其中 cfs_rq 是用红黑树来描述的，按 vruntime 大小来排序的，最左侧的叶子节点，就是下次会被调度的任务。\n\nPS：下图中的 csf_rq 应该是 `cfs_rq`，由于找不到原图了，我偷个懒，我就不重新画了，嘻嘻。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/CPU队列.png)\n\n\n这几种调度类是有优先级的，优先级如下：Deadline \u003e Realtime \u003e Fair，这意味着 Linux 选择下一个任务执行的时候，会按照此优先级顺序进行选择，也就是说先从 `dl_rq` 里选择任务，然后从 `rt_rq` 里选择任务，最后从 `cfs_rq` 里选择任务。因此，**实时任务总是会比普通任务优先被执行**。\n\n### 调整优先级\n\n如果我们启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务，普通任务的调度类是 Fair，由 CFS 调度器来进行管理。CFS 调度器的目的是实现任务运行的公平性，也就是保障每个任务的运行的时间是差不多的。\n\n如果你想让某个普通任务有更多的执行时间，可以调整任务的 `nice` 值，从而让优先级高一些的任务执行更多时间。nice 的值能设置的范围是 `-20～19`， 值越低，表明优先级越高，因此 -20 是最高优先级，19 则是最低优先级，默认优先级是 0。\n\n是不是觉得 nice 值的范围很诡异？事实上，nice 值并不是表示优先级，而是表示优先级的修正数值，它与优先级（priority）的关系是这样的：priority(new) = priority(old) + nice。内核中，priority 的范围是 0~139，值越低，优先级越高，其中前面的 0~99 范围是提供给实时任务使用的，而 nice 值是映射到 100~139，这个范围是提供给普通任务用的，因此 nice 值调整的是普通任务的优先级。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/优先级.png)\n\n在前面我们提到了，权重值与 nice 值是有关系的，nice 值越低，权重值就越大，计算出来的 vruntime 就会越少，由于 CFS 算法调度的时候，就会优先选择 vruntime 少的任务进行执行，所以 nice 值越低，任务的优先级就越高。\n\n\n我们可以在启动任务的时候，可以指定 nice 的值，比如将 mysqld 以 -3 优先级：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/nice.png)\n\n\n如果想修改已经运行中的任务的优先级，则可以使用 `renice` 来调整 nice 值：\n\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/renice.png)\n\n\n\nnice 调整的是普通任务的优先级，所以不管怎么缩小 nice 值，任务永远都是普通任务，如果某些任务要求实时性比较高，那么你可以考虑改变任务的优先级以及调度策略，使得它变成实时任务，比如：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU伪共享/chrt.png)\n\n\n\n---\n\n## 总结\n\n\n理解 CPU 是如何读写数据的前提，是要理解 CPU 的架构，CPU 内部的多个 Cache + 外部的内存和磁盘都就构成了金字塔的存储器结构，在这个金字塔中，越往下，存储器的容量就越大，但访问速度就会小。\n\nCPU 读写数据的时候，并不是按一个一个字节为单位来进行读写，而是以 CPU Cache  Line 大小为单位，CPU Cache  Line 大小一般是 64 个字节，也就意味着 CPU 读写数据的时候，每一次都是以 64 字节大小为一块进行操作。\n\n因此，如果我们操作的数据是数组，那么访问数组元素的时候，按内存分布的地址顺序进行访问，这样能充分利用到 Cache，程序的性能得到提升。但如果操作的数据不是数组，而是普通的变量，并在多核 CPU 的情况下，我们还需要避免 Cache Line 伪共享的问题。\n\n所谓的 Cache Line 伪共享问题就是，多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象。那么对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，避免的方式一般有 Cache Line 大小字节对齐，以及字节填充等方法。\n\n系统中需要运行的多线程数一般都会大于 CPU 核心，这样就会导致线程排队等待 CPU，这可能会产生一定的延时，如果我们的任务对延时容忍度很低，则可以通过一些人为手段干预 Linux 的默认调度策略和优先级。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/how_cpu_run":{"title":"how_cpu_run","content":"# 2.1 CPU 是如何执行程序的？\n\n代码写了那么多，你知道 `a = 1 + 2` 这条代码是怎么被 CPU 执行的吗？\n\n软件用了那么多，你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运行在 64 位的电脑上吗？64 位的操作系统可以运行在 32 位的电脑上吗？如果不行，原因是什么？\n\nCPU 看了那么多，我们都知道 CPU 通常分为 32 位和 64 位，你知道 64 位相比 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能一定比 32 位 CPU 高很多吗？\n\n不知道也不用慌张，接下来就循序渐进的、一层一层的攻破这些问题。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/程序执行提纲.png)\n\n---\n\n## 图灵机的工作方式\n\n要想知道程序执行的原理，我们可以先从「图灵机」说起，图灵的基本思想是用机器来模拟人们用纸笔进行数学运算的过程，而且还定义了计算机由哪些部分组成，程序又是如何执行的。\n\n图灵机长什么样子呢？你从下图可以看到图灵机的实际样子：\n\n\n![图来源自：http://www.kristergustafsson.me/turing-machine/](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/Turing%2Bmachine%2B1.jpeg)\n\n图灵机的基本组成如下：\n\n- 有一条「纸带」，纸带由一个个连续的格子组成，每个格子可以写入字符，纸带就好比内存，而纸带上的格子的字符就好比内存中的数据或程序；\n- 有一个「读写头」，读写头可以读取纸带上任意格子的字符，也可以把字符写入到纸带的格子；\n- 读写头上有一些部件，比如存储单元、控制单元以及运算单元：\n  1、存储单元用于存放数据；\n  2、控制单元用于识别字符是数据还是指令，以及控制程序的流程等；\n  3、运算单元用于执行运算指令；\n\n知道了图灵机的组成后，我们以简单数学运算的 `1 + 2` 作为例子，来看看它是怎么执行这行代码的。\n\n- 首先，用读写头把 「1、2、+」这 3 个字符分别写入到纸带上的 3 个格子，然后读写头先停在 1 字符对应的格子上；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/图灵机-第一步.png)\n\n\n- 接着，读写头读入 1 到存储设备中，这个存储设备称为图灵机的状态；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/图灵机-第二步.png)\n\n- 然后读写头向右移动一个格，用同样的方式把 2 读入到图灵机的状态，于是现在图灵机的状态中存储着两个连续的数字， 1 和 2；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/图灵机-第三步.png)\n\n- 读写头再往右移动一个格，就会碰到 + 号，读写头读到 + 号后，将 + 号传输给「控制单元」，控制单元发现是一个 + 号而不是数字，所以没有存入到状态中，因为 `+` 号是运算符指令，作用是加和目前的状态，于是通知「运算单元」工作。运算单元收到要加和状态中的值的通知后，就会把状态中的 1 和 2 读入并计算，再将计算的结果 3 存放到状态中；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/图灵机-第四步.png)\n\n\n- 最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头向右移动，把结果 3 写入到纸带的格子中；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/图灵机-第五步.png)\n\n\n通过上面的图灵机计算 `1 + 2`  的过程，可以发现图灵机主要功能就是读取纸带格子中的内容，然后交给控制单元识别字符是数字还是运算符指令，如果是数字则存入到图灵机状态中，如果是运算符，则通知运算符单元读取状态中的数值进行计算，计算结果最终返回给读写头，读写头把结果写入到纸带的格子中。\n\n事实上，图灵机这个看起来很简单的工作方式，和我们今天的计算机是基本一样的。接下来，我们一同再看看当今计算机的组成以及工作方式。\n\n\n---\n\n## 冯诺依曼模型\n\n在 1945 年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，而且还提出用电子元件构造计算机，并约定了用二进制进行计算和存储。\n\n最重要的是定义计算机基本结构为 5 个部分，分别是**运算器、控制器、存储器、输入设备、输出设备**，这 5 个部分也被称为**冯诺依曼模型**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/操作系统/Von_Neumann_architecture.svg)\n\n运算器、控制器是在中央处理器里的，存储器就我们常见的内存，输入输出设备则是计算机外接的设备，比如键盘就是输入设备，显示器就是输出设备。\n\n存储单元和输入输出设备要与中央处理器打交道的话，离不开总线。所以，它们之间的关系如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/冯诺依曼模型.png)\n\n接下来，分别介绍内存、中央处理器、总线、输入输出设备。\n\n### 内存\n\n我们的程序和数据都是存储在内存，存储的区域是线性的。\n\n在计算机数据存储中，存储数据的基本单位是**字节（*byte*）**，1 字节等于 8 位（8 bit）。每一个字节都对应一个内存地址。\n\n内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。\n\n\n### 中央处理器\n\n中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于一次能计算多少字节数据：\n\n- 32 位 CPU 一次可以计算 4 个字节；\n- 64 位 CPU 一次可以计算 8 个字节；\n\n这里的 32 位和 64 位，通常称为 CPU 的位宽。\n\n之所以 CPU 要这样设计，是为了能计算更大的数值，如果是 8 位的 CPU，那么一次只能计算 1 个字节 `0~255` 范围内的数值，这样就无法一次完成计算 `10000 * 500` ，于是为了能一次计算大数的运算，CPU 需要支持多个 byte 一起计算，所以 CPU 位宽越大，可以计算的数值就越大，比如说 32 位 CPU 能计算的最大整数是 `4294967295`。\n\nCPU 内部还有一些组件，常见的有**寄存器、控制单元和逻辑运算单元**等。其中，控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。\n\nCPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。\n\n常见的寄存器种类：\n\n- *通用寄存器*，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。\n- *程序计数器*，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。\n- *指令寄存器*，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。\n\n\n### 总线\n\n总线是用于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种：\n\n- *地址总线*，用于指定 CPU 将要操作的内存地址；\n- *数据总线*，用于读写内存的数据；\n- *控制总线*，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；\n\n当 CPU 要读写内存数据的时候，一般需要通过下面这三个总线：\n\n- 首先要通过「地址总线」来指定内存的地址；\n- 然后通过「控制总线」控制是读或写命令；\n- 最后通过「数据总线」来传输数据；\n\n### 输入、输出设备\n\n输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键时是需要和 CPU 进行交互的，这时就需要用到控制总线了。\n\n\n---\n\n## 线路位宽与 CPU 位宽\n\n\n数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示 0，高压电压则表示 1。\n\n如果构造了高低高这样的信号，其实就是 101 二进制数据，十进制则表示 5，如果只有一条线路，就意味着每次只能传递 1 bit 的数据，即 0 或 1，那么传输 101 这个数据，就需要 3 次才能传输完成，这样的效率非常低。\n\n\n这样一位一位传输的方式，称为串行，下一个 bit 必须等待上一个 bit 传输完成才能进行传输。当然，想一次多传一些数据，增加线路即可，这时数据就可以并行传输。\n\n为了避免低效率的串行传输的方式，线路的位宽最好一次就能访问到所有的内存地址。 \n\nCPU 要想操作的内存地址就需要地址总线：\n\n- 如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种地址，所以 CPU 能操作的内存地址最大数量为 2（2^1）个（注意，不要理解成同时能操作 2 个内存地址）；\n- 如果地址总线有 2 条，那么能表示 00、01、10、11 这四种地址，所以 CPU 能操作的内存地址最大数量为 4（2^2）个。\n\n那么，想要 CPU 操作 4G 大的内存，那么就需要 32 条地址总线，因为 `2 ^ 32 = 4G`。\n\n知道了线路位宽的意义后，我们再来看看 CPU 位宽。\n\nCPU 的位宽最好不要小于线路位宽，比如 32 位 CPU 控制 40 位宽的地址总线和数据总线的话，工作起来就会非常复杂且麻烦，所以 32 位的 CPU 最好和 32 位宽的线路搭配，因为 32 位 CPU 一次最多只能操作 32 位宽的地址总线和数据总线。\n\n如果用 32 位 CPU 去加和两个 64 位大小的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个高位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个高位的 32 位数字，最后再加上进位，就能算出结果了，可以发现 32 位 CPU 并不能一次性计算出加和两个 64 位数字的结果。\n\n对于 64 位 CPU 就可以一次性算出加和两个 64 位数字的结果，因为 64 位 CPU 可以一次读入 64 位的数字，并且 64 位 CPU 内部的逻辑运算单元也支持 64 位数字的计算。\n\n但是并不代表 64 位 CPU 性能比 32 位 CPU 高很多，很少应用需要算超过 32 位的数字，所以**如果计算的数额不超过 32 位数字的情况下，32 位和 64 位 CPU 之间没什么区别的，只有当计算超过 32 位数字的情况下，64 位的优势才能体现出来**。\n\n另外，32 位 CPU 最大只能操作 4GB 内存，就算你装了 8 GB 内存条，也没用。而 64 位 CPU 寻址范围则很大，理论最大的寻址空间为 `2^64`。\n\n\n---\n\n## 程序执行的基本过程\n\n在前面，我们知道了程序在图灵机的执行过程，接下来我们来看看程序在冯诺依曼模型上是怎么执行的。\n\n\n程序实际上是一条一条指令，所以程序的运行过程就是把每一条指令一步一步的执行起来，负责执行指令的就是 CPU 了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/CPU执行程序.png)\n\n那 CPU 执行程序的过程如下：\n\n- 第一步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存入到「指令寄存器」。\n- 第二步，「程序计数器」的值自增，表示指向下一条指令。这个自增的大小，由 CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4；\n- 第三步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；\n\n简单总结一下就是，一个程序执行的时候，CPU 会根据程序计数器里的内存地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。\n\nCPU 从程序计数器读取指令、到执行、再到下一条指令，这个过程会不断循环，直到程序执行结束，这个不断循环的过程被称为 **CPU 的指令周期**。\n\n---\n\n## a = 1 + 2 执行具体过程\n\n知道了基本的程序执行过程后，接下来用 `a = 1 + 2` 的作为例子，进一步分析该程序在冯诺伊曼模型的执行过程。\n\nCPU 是不认识 `a = 1 + 2` 这个字符串，这些字符串只是方便我们程序员认识，要想这段程序能跑起来，还需要把整个程序翻译成**汇编语言**的程序，这个过程称为编译成汇编代码。\n\n针对汇编代码，我们还需要用汇编器翻译成机器码，这些机器码由 0 和 1 组成的机器语言，这一条条机器码，就是一条条的**计算机指令**，这个才是 CPU 能够真正认识的东西。\n\n下面来看看  `a = 1 + 2` 在 32 位 CPU 的执行过程。\n\n程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：\n\n- 数据 1 被存放到 0x200 位置；\n- 数据 2 被存放到 0x204 位置；\n\n注意，数据和指令是分开区域存放的，存放指令区域的地方称为「正文段」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/数据段与正文段.png)\n\n编译器会把 `a = 1 + 2` 翻译成 4 条指令，存放到正文段中。如图，这 4 条指令被存放到了 0x100 ~ 0x10c 的区域中： \n\n- 0x100 的内容是 `load` 指令将 0x200 地址中的数据 1 装入到寄存器 `R0`；\n- 0x104 的内容是 `load` 指令将 0x204 地址中的数据 2 装入到寄存器 `R1`；\n- 0x108 的内容是 `add` 指令将寄存器 `R0` 和 `R1` 的数据相加，并把结果存放到寄存器 `R2`；\n- 0x10c 的内容是 `store` 指令将寄存器 `R2` 中的数据存回数据段中的 0x208 地址中，这个地址也就是变量 `a` 内存中的地址；\n\n编译完成后，具体执行程序的时候，程序计数器会被设置为 0x100 地址，然后依次执行这 4 条指令。\n\n上面的例子中，由于是在 32 位 CPU 执行的，因此一条指令是占 32 位大小，所以你会发现每条指令间隔 4 个字节。\n\n而数据的大小是根据你在程序中指定的变量类型，比如 `int` 类型的数据则占 4 个字节，`char` 类型的数据则占 1 个字节。\n\n### 指令\n\n上面的例子中，图中指令的内容我写的是简易的汇编代码，目的是为了方便理解指令的具体内容，事实上指令的内容是一串二进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。\n\n不同的 CPU 有不同的指令集，也就是对应着不同的汇编语言和不同的机器码，接下来选用最简单的 MIPS 指集，来看看机器码是如何生成的，这样也能明白二进制的机器码的具体含义。\n\nMIPS 的指令是一个 32 位的整数，高 6 位代表着操作码，表示这条指令是一条什么样的指令，剩下的 26 位不同指令类型所表示的内容也就不相同，主要有三种类型R、I 和 J。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/MIPS指令集.png)\n\n一起具体看看这三种类型的含义：\n\n- *R 指令*，用在算术和逻辑操作，里面有读取和写入数据的寄存器地址。如果是逻辑位移操作，后面还有位移操作的「位移量」，而最后的「功能码」则是再前面的操作码不够的时候，扩展操作码来表示对应的具体指令的；\n- *I 指令*，用在数据传输、条件分支等。这个类型的指令，就没有了位移量和功能码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或一个常数；\n- *J 指令*，用在跳转，高 6 位之外的 26 位都是一个跳转后的地址；\n\n接下来，我们把前面例子的这条指令：「`add` 指令将寄存器 `R0` 和 `R1` 的数据相加，并把结果放入到 `R2`」，翻译成机器码。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/add的MIPS指令.png)\n\n加和运算 add 指令是属于 R 指令类型：\n\n- add 对应的 MIPS 指令里操作码是 `000000`，以及最末尾的功能码是 `100000`，这些数值都是固定的，查一下 MIPS 指令集的手册就能知道的；\n- rs 代表第一个寄存器 R0 的编号，即 `00000`；\n- rt 代表第二个寄存器 R1 的编号，即 `00001`；\n- rd 代表目标的临时寄存器 R2 的编号，即 `00010`；\n- 因为不是位移操作，所以位移量是 `00000`\n\n把上面这些数字拼在一起就是一条 32 位的 MIPS 加法指令了，那么用 16 进制表示的机器码则是 `0x00011020`。\n\n编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执行程序的时候，就会解析指令，这个过程叫作指令的解码。\n\n现代大多数 CPU 都使用来流水线的方式来执行指令，所谓的流水线就是把一个任务拆分成多个小任务，于是一条指令通常分为 4 个阶段，称为 4 级流水线，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/CPU指令周期.png)\n\n四个阶段的具体含义：\n\n1. CPU 通过程序计数器读取对应内存地址的指令，这个部分称为 **Fetch（取得指令）**；\n2. CPU 对指令进行解码，这个部分称为 **Decode（指令译码）**；\n3. CPU 执行指令，这个部分称为 **Execution（执行指令）**；\n4. CPU 将计算结果存回寄存器或者将寄存器的值存入内存，这个部分称为 **Store（数据回写）**；\n\n\n上面这 4 个阶段，我们称为**指令周期（*Instrution Cycle*）**，CPU 的工作就是一个周期接着一个周期，周而复始。\n\n事实上，不同的阶段其实是由计算机中的不同组件完成的：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/指令周期工作组件.png)\n\n- 取指令的阶段，我们的指令是存放在**存储器**里的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由**控制器**操作的；\n- 指令的译码过程，也是由**控制器**进行的；\n- 指令执行的过程，无论是进行算术操作、逻辑操作，还是进行数据传输、条件分支操作，都是由**算术逻辑单元**操作的，也就是由**运算器**处理的。但是如果是一个简单的无条件地址跳转，则是直接在**控制器**里面完成的，不需要用到运算器。\n\n\n### 指令的类型\n\n\n指令从功能角度划分，可以分为 5 大类：\n\n- *数据传输类型的指令*，比如 `store/load` 是寄存器与内存间数据传输的指令，`mov` 是将一个内存地址的数据移动到另一个内存地址的指令；\n- *运算类型的指令*，比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；\n- *跳转类型的指令*，通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的 `if-else`、`switch-case`、函数调用等。\n- *信号类型的指令*，比如发生中断的指令 `trap`；\n- *闲置类型的指令*，比如指令 `nop`，执行后 CPU 会空转一个周期；\n\n### 指令的执行速度\n\nCPU 的硬件参数都会有 `GHz` 这个参数，比如一个 1 GHz 的 CPU，指的是时钟频率是 1 G，代表着 1 秒会产生 1G 次数的脉冲信号，每一次脉冲信号高低电平的转换就是一个周期，称为时钟周期。\n\n对于 CPU 来说，在一个时钟周期内，CPU 仅能完成一个最基本的动作，时钟频率越高，时钟周期就越短，工作速度也就越快。\n\n一个时钟周期一定能执行完一条指令吗？答案是不一定的，大多数指令不能在一个时钟周期完成，通常需要若干个时钟周期。不同的指令需要的时钟周期是不同的，加法和乘法都对应着一条 CPU 指令，但是乘法需要的时钟周期就要比加法多。\n\n\u003e 如何让程序跑的更快？\n\n程序执行的时候，耗费的 CPU 时间少就说明程序是快的，对于程序的 CPU 执行时间，我们可以拆解成 **CPU 时钟周期数（*CPU Cycles*）和时钟周期时间（*Clock Cycle Time*）的乘积**。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/程序的CPU执行时间公式1.png)\n\n时钟周期时间就是我们前面提及的 CPU 主频，主频越高说明 CPU 的工作速度就越快，比如我手头上的电脑的 CPU 是 2.4 GHz 四核 Intel Core i5，这里的 2.4 GHz 就是电脑的主频，时钟周期时间就是 1/2.4G。\n\n要想 CPU 跑的更快，自然缩短时钟周期时间，也就是提升 CPU 主频，但是今非彼日，摩尔定律早已失效，当今的 CPU 主频已经很难再做到翻倍的效果了。\n\n另外，换一个更好的 CPU，这个也是我们软件工程师控制不了的事情，我们应该把目光放到另外一个乘法因子 —— CPU 时钟周期数，如果能减少程序所需的 CPU 时钟周期数量，一样也是能提升程序的性能的。\n\n\n对于 CPU 时钟周期数我们可以进一步拆解成：「**指令数 x 每条指令的平均时钟周期数（*Cycles Per Instruction*，简称 `CPI`）**」，于是程序的 CPU 执行时间的公式可变成如下：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/程序执行/程序的CPU执行时间公式2.png)\n\n\n因此，要想程序跑的更快，优化这三者即可：\n\n- *指令数*，表示执行程序所需要多少条指令，以及哪些指令。这个层面是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示方式。\n- *每条指令的平均时钟周期数 CPI*，表示一条指令需要多少个时钟周期数，现代大多数 CPU 通过流水线技术（Pipeline），让一条指令需要的 CPU 时钟周期数尽可能的少；\n- *时钟周期时间*，表示计算机主频，取决于计算机硬件。有的 CPU 支持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU 工作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压力就会越大，CPU 会很容易奔溃。\n\n很多厂商为了跑分而跑分，基本都是在这三个方面入手的哦，特别是超频这一块。\n\n\n---\n\n## 总结\n\n最后我们再来回答开头的问题。\n\n\u003e  64 位相比 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能一定比 32 位 CPU 高很多吗？\n\n64 位相比 32 位 CPU 的优势主要体现在两个方面：\n\n- 64 位 CPU 可以一次计算超过 32 位的数字，而 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进行计算，效率就没那么高，但是大部分应用程序很少会计算那么大的数字，所以**只有运算大数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不大**。\n- 64 位 CPU 可以**寻址更大的内存空间**，32 位 CPU 最大的寻址地址是 4G，即使你加了 8G 大小的内存，也还是只能寻址到 4G，而 64 位 CPU 最大寻址地址是 `2^64`，远超于 32 位 CPU 最大寻址地址的 `2^32`。\n\n\n\u003e 你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运行在 64 位的电脑上吗？64 位的操作系统可以运行在 32 位的电脑上吗？如果不行，原因是什么？\n\n\n64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的：\n\n- 如果 32 位指令在 64 位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是**如果 64 位指令在 32 位机器上执行，就比较困难了，因为 32 位的寄存器存不下 64 位的指令**；\n- 操作系统其实也是一种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，比如 64 位操作系统，指令也就是 64 位，因此不能装在 32 位机器上。\n\n总之，硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽。\n\n----\n\n## 关注作者\n\n**哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/how_to_make_cpu_run_faster":{"title":"how_to_make_cpu_run_faster","content":"# 2.3 如何写出让 CPU 跑得更快的代码？\n\n代码都是由 CPU 跑起来的，我们代码写的好与坏就决定了 CPU 的执行效率，特别是在编写计算密集型的程序，更要注重 CPU 的执行效率，否则将会大大影响系统性能。\n\nCPU 内部嵌入了 CPU Cache（高速缓存），它的存储容量很小，但是离 CPU 核心很近，所以缓存的读写速度是极快的，那么如果 CPU 运算时，直接从 CPU Cache 读取数据，而不是从内存的话，运算速度就会很快。\n\n但是，大多数人不知道 CPU Cache 的运行机制，以至于不知道如何才能够写出能够配合 CPU Cache 工作机制的代码，一旦你掌握了它，你写代码的时候，就有新的优化思路了。\n\n那么，接下来我们就来看看，CPU Cache 到底是什么样的，是如何工作的呢，又该如何写出让 CPU 执行更快的代码呢？\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/CPUCache%E6%8F%90%E7%BA%B2.png)\n\n---\n\n## CPU Cache 有多快？\n\n你可能会好奇为什么有了内存，还需要 CPU Cache？根据摩尔定律，CPU 的访问速度每 18 个月就会翻倍，相当于每年增长 60% 左右，内存的速度当然也会不断增长，但是增长的速度远小于 CPU，平均每年只增长 7% 左右。于是，CPU 与内存的访问性能的差距不断拉大。\n\n到现在，一次内存访问所需时间是 `200~300` 多个时钟周期，这意味着 CPU 和内存的访问速度已经相差 `200~300` 多倍了。\n\n为了弥补 CPU 与内存两者之间的性能差异，就在 CPU 内部引入了  CPU Cache，也称高速缓存。\n\n CPU Cache 通常分为大小不等的三级缓存，分别是 **L1 Cache、L2 Cache 和 L3 Cache**。\n\n 由于 CPU Cache 所使用的材料是 SRAM，价格比内存使用的 DRAM 高出很多，在当今每生产 1 MB 大小的 CPU Cache 需要 7 美金的成本，而内存只需要 0.015 美金的成本，成本方面相差了 466 倍，所以 CPU Cache 不像内存那样动辄以 GB 计算，它的大小是以 KB 或 MB 来计算的。\n\n\n在 Linux 系统中，我们可以使用下图的方式来查看各级 CPU Cache 的大小，比如我这手上这台服务器，离 CPU 核心最近的 L1 Cache 是 32KB，其次是 L2 Cache 是 256KB，最大的 L3 Cache 则是 3MB。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/查看CPU高速缓存大小.png)\n\n\n其中，**L1 Cache 通常会分为「数据缓存」和「指令缓存」**，这意味着数据和指令在 L1 Cache 这一层是分开缓存的，上图中的 `index0` 也就是数据缓存，而 `index1` 则是指令缓存，它两的大小通常是一样的。\n\n另外，你也会注意到，L3 Cache 比 L1 Cache 和 L2 Cache 大很多，这是因为 **L1 Cache 和 L2 Cache 都是每个 CPU 核心独有的，而 L3 Cache 是多个 CPU 核心共享的。**\n\n程序执行时，会先将内存中的数据加载到共享的 L3 Cache 中，再加载到每个核心独有的 L2 Cache，最后进入到最快的 L1 Cache，之后才会被 CPU 读取。它们之间的层级关系，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/CPU-Cache.png)\n\n越靠近 CPU 核心的缓存其访问速度越快，CPU 访问 L1 Cache 只需要 `2~4` 个时钟周期，访问 L2 Cache 大约 `10~20` 个时钟周期，访问 L3 Cache 大约 `20~60` 个时钟周期，而访问内存速度大概在 `200~300` 个 时钟周期之间。如下表格：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/访问速度表格.png)\n\n\n**所以，CPU 从 L1 Cache 读取数据的速度，相比从内存读取的速度，会快 `100` 多倍。**\n\n---\n\n## CPU Cache 的数据结构和读取过程是什么样的？\n\n我们先简单了解下 CPU Cache 的结构，CPU Cache 是由很多个 Cache Line 组成的，Cache Line 是 CPU 从内存读取数据的基本单位，而 Cache Line 是由各种标志（Tag）+ 数据块（Data Block）组成，你可以在下图清晰的看到：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/Cache%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png)\n\nCPU Cache 的数据是从内存中读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 **Cache Line（缓存块）**。\n\n你可以在你的 Linux 系统，用下面这种方式来查看 CPU 的 Cache Line，你可以看我服务器的 L1 Cache Line 大小是 64 字节，也就意味着 **L1 Cache 一次载入数据的大小是 64 字节**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/查看CPULine大小.png)\n\n\n比如，有一个 `int array[100]` 的数组，当载入 `array[0]` 时，由于这个数组元素的大小在内存只占 4 字节，不足 64 字节，CPU 就会**顺序加载**数组元素到 `array[15]`，意味着 `array[0]~array[15]` 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，而不用再从内存中读取，大大提高了 CPU 读取数据的性能。\n\n事实上，CPU 读取数据的时候，无论数据是否存放到 Cache 中，CPU 都是先访问 Cache，只有当 Cache 中找不到数据时，才会去访问内存，并把内存中的数据读入到 Cache 中，CPU 再从 CPU Cache 读取数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/缓存逻辑.png)\n\n这样的访问机制，跟我们使用「内存作为硬盘的缓存」的逻辑是一样的，如果内存有缓存的数据，则直接返回，否则要访问龟速一般的硬盘。\n\n那 CPU 怎么知道要访问的内存数据，是否在 Cache 里？如果在的话，如何找到 Cache 对应的数据呢？我们从最简单、基础的**直接映射 Cache（*Direct Mapped Cache*）** 说起，来看看整个 CPU Cache 的数据结构和访问逻辑。\n\n\n前面，我们提到 CPU 访问内存数据时，是一小块一小块数据读取的，具体这一小块数据的大小，取决于 `coherency_line_size` 的值，一般 64 字节。在内存中，这一块的数据我们称为**内存块（*Block*）**，读取的时候我们要拿到数据所在内存块的地址。\n\n对于直接映射 Cache 采用的策略，就是把内存块的地址始终「映射」在一个 CPU Cache Line（缓存块） 的地址，至于映射关系实现方式，则是使用「取模运算」，取模运算的结果就是内存块地址对应的 CPU Cache Line（缓存块） 的地址。\n\n举个例子，内存共被划分为 32 个内存块，CPU Cache 共有 8 个 CPU Cache Line，假设 CPU 想要访问第 15 号内存块，如果 15 号内存块中的数据已经缓存在 CPU Cache Line 中的话，则是一定映射在 7 号 CPU Cache Line 中，因为 `15 % 8` 的值是 7。\n\n机智的你肯定发现了，使用取模方式映射的话，就会出现多个内存块对应同一个 CPU Cache Line，比如上面的例子，除了 15 号内存块是映射在 7 号 CPU Cache Line 中，还有 7 号、23 号、31 号内存块都是映射到 7 号 CPU Cache Line 中。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E6%B1%82%E6%A8%A1%E6%98%A0%E5%B0%84%E7%AD%96%E7%95%A5.png)\n\n\n因此，为了区别不同的内存块，在对应的 CPU Cache Line 中我们还会存储一个**组标记（Tag）**。这个组标记会记录当前 CPU Cache Line 中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。\n\n除了组标记信息外，CPU Cache Line 还有两个信息：\n\n- 一个是，从内存加载过来的实际存放**数据（*Data*）**。\n- 另一个是，**有效位（*Valid bit*）**，它是用来标记对应的 CPU Cache Line 中的数据是否是有效的，如果有效位是 0，无论 CPU Cache Line 中是否有数据，CPU 都会直接访问内存，重新加载数据。\n\n\nCPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Cache Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个**字（*Word*）**。那怎么在对应的 CPU Cache Line 中数据块中找到所需的字呢？答案是，需要一个**偏移量（Offset）**。\n\n因此，一个内存的访问地址，包括**组标记、CPU Cache Line 索引、偏移量**这三种信息，于是 CPU 就能通过这些信息，在 CPU Cache 中找到缓存的数据。而对于 CPU Cache 里的数据结构，则是由**索引 + 有效位 + 组标记 + 数据块**组成。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/直接Cache映射.png)\n\n\n\n\n如果内存中的数据已经在 CPU Cahe 中了，那 CPU 访问一个内存地址的时候，会经历这 4 个步骤：\n\n1. 根据内存地址中索引信息，计算在 CPU Cahe 中的索引，也就是找出对应的 CPU Cache Line 的地址；\n2. 找到对应 CPU Cache Line 后，判断 CPU Cache Line 中的有效位，确认 CPU Cache Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；\n3. 对比内存地址中组标记和 CPU Cache Line 中的组标记，确认 CPU Cache Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；\n4. 根据内存地址中偏移量信息，从 CPU Cache Line 的数据块中，读取对应的字。\n\n到这里，相信你对直接映射 Cache 有了一定认识，但其实除了直接映射 Cache 之外，还有其他通过内存地址找到 CPU Cache 中的数据的策略，比如全相连 Cache （*Fully Associative Cache*）、组相连 Cache （*Set Associative Cache*）等，这几种策策略的数据结构都比较相似，我们理解了直接映射 Cache 的工作方式，其他的策略如果你有兴趣去看，相信很快就能理解的了。\n\n---\n\n## 如何写出让 CPU 跑得更快的代码？\n\n我们知道 CPU 访问内存的速度，比访问 CPU Cache 的速度慢了 100 多倍，所以如果 CPU 所要操作的数据在 CPU Cache 中的话，这样将会带来很大的性能提升。访问的数据在 CPU Cache 中的话，意味着**缓存命中**，缓存命中率越高的话，代码的性能就会越好，CPU 也就跑的越快。\n\n于是，「如何写出让 CPU 跑得更快的代码？」这个问题，可以改成「如何写出 CPU 缓存命中率高的代码？」。\n\n\n在前面我也提到， L1 Cache 通常分为「数据缓存」和「指令缓存」，这是因为 CPU 会分别处理数据和指令，比如 `1+1=2` 这个运算，`+` 就是指令，会被放在「指令缓存」中，而输入数字 `1` 则会被放在「数据缓存」里。\n\n因此，**我们要分开来看「数据缓存」和「指令缓存」的缓存命中率**。\n\n\n### 如何提升数据缓存的命中率？\n\n假设要遍历二维数组，有以下两种形式，虽然代码执行结果是一样，但你觉得哪种形式效率最高呢？为什么高呢？\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/遍历数组.png)\n\n经过测试，形式一 `array[i][j]`  执行时间比形式二 `array[j][i]` 快好几倍。\n\n之所以有这么大的差距，是因为二维数组 `array` 所占用的内存是连续的，比如长度 `N` 的值是 `2` 的话，那么内存中的数组元素的布局顺序是这样的：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/数组内存布局顺序.png)\n\n\n\n形式一用 `array[i][j]`  访问数组元素的顺序，正是和内存中数组元素存放的顺序一致。当 CPU 访问 `array[0][0]` 时，由于该数据不在 Cache 中，于是会「顺序」把跟随其后的 3 个元素从内存中加载到 CPU Cache，这样当 CPU 访问后面的 3 个数组元素时，就能在 CPU Cache 中成功地找到数据，这意味着缓存命中率很高，缓存命中的数据不需要访问内存，这便大大提高了代码的性能。\n\n而如果用形式二的 `array[j][i]` 来访问，则访问的顺序就是：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/形式二访问顺序.png)\n\n\n你可以看到，访问的方式跳跃式的，而不是顺序的，那么如果 N 的数值很大，那么操作 `array[j][i]` 时，是没办法把 `array[j+1][i]` 也读入到 CPU Cache 中的，既然 `array[j+1][i]` 没有读取到 CPU Cache，那么就需要从内存读取该数据元素了。很明显，这种不连续性、跳跃式访问数据元素的方式，可能不能充分利用到了 CPU Cache 的特性，从而代码的性能不高。\n\n\n那访问 `array[0][0]` 元素时，CPU 具体会一次从内存中加载多少元素到 CPU Cache 呢？这个问题，在前面我们也提到过，这跟 CPU Cache Line 有关，它表示 **CPU Cache 一次性能加载数据的大小**，可以在 Linux 里通过 `coherency_line_size` 配置查看 它的大小，通常是 64 个字节。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/查看CPULine大小.png)\n\n\n也就是说，当 CPU 访问内存数据时，如果数据不在 CPU Cache 中，则会一次性会连续加载 64 字节大小的数据到 CPU Cache，那么当访问 `array[0][0]` 时，由于该元素不足 64 字节，于是就会往后**顺序**读取 `array[0][0]~array[0][15]` 到 CPU Cache 中。顺序访问的 `array[i][j]` 因为利用了这一特点，所以就会比跳跃式访问的 `array[j][i]` 要快。\n\n**因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升，**\n\n\n### 如何提升指令缓存的命中率？\n\n提升数据的缓存命中率的方式，是按照内存布局顺序访问，那针对指令的缓存该如何提升呢？\n\n我们以一个例子来看看，有一个元素为 0 到 100 之间随机数字组成的一维数组：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/随机数数组.png)\n\n接下来，对这个数组做两个操作：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E6%8E%92%E5%BA%8Fand%E6%95%B0%E7%BB%84%E9%81%8D%E5%8E%86.png)\n\n- 第一个操作，循环遍历数组，把小于 50 的数组元素置为 0；\n- 第二个操作，将数组排序；\n\n那么问题来了，你觉得先遍历再排序速度快，还是先排序再遍历速度快呢？\n\n\n\n在回答这个问题之前，我们先了解 CPU 的**分支预测器**。对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，**如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快**。\n\n当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。\n\n因此，先排序再遍历速度会更快，这是因为排序之后，数字是从小到大的，那么前几次循环命中 `if \u003c 50` 的次数会比较多，于是分支预测就会缓存 `if` 里的 `array[i] = 0` 指令到 Cache 中，后续 CPU 执行该指令就只需要从 Cache 读取就好了。\n\n如果你肯定代码中的 `if` 中的表达式判断为 `true` 的概率比较高，我们可以使用显示分支预测工具，比如在 C/C++ 语言中编译器提供了 `likely` 和 `unlikely` 这两种宏，如果 `if` 条件为 `ture` 的概率大，则可以用 `likely` 宏把 `if` 里的表达式包裹起来，反之用 `unlikely` 宏。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/likely.png)\n\n实际上，CPU 自身的动态分支预测已经是比较准的了，所以只有当非常确信 CPU 预测的不准，且能够知道实际的概率情况时，才建议使用这两种宏。\n\n\n### 如何提升多核 CPU 的缓存命中率？\n\n\n在单核 CPU，虽然只能执行一个线程，但是操作系统给每个线程分配了一个时间片，时间片用完了，就调度下一个线程，于是各个线程就按时间片交替地占用 CPU，从宏观上看起来各个线程同时在执行。\n\n而现代 CPU 都是多核心的，线程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的，**如果一个线程在不同核心来回切换，各个核心的缓存命中率就会受到影响**，相反如果线程都在同一个核心上执行，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以减少访问 内存的频率。\n\n当有多个同时执行「计算密集型」的线程，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把**线程绑定在某一个 CPU 核心上**，这样性能可以得到非常可观的提升。\n\n在 Linux 上提供了 `sched_setaffinity` 方法，来实现将线程绑定到某个 CPU 核心这一功能。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/CPU缓存/sched_setaffinity.png)\n\n---\n\n## 总结\n\n由于随着计算机技术的发展，CPU 与 内存的访问速度相差越来越多，如今差距已经高达好几百倍了，所以 CPU 内部嵌入了 CPU Cache 组件，作为内存与 CPU 之间的缓存层，CPU Cache 由于离 CPU 核心很近，所以访问速度也是非常快的，但由于所需材料成本比较高，它不像内存动辄几个 GB 大小，而是仅有几十 KB 到 MB 大小。\n\n当 CPU 访问数据的时候，先是访问 CPU Cache，如果缓存命中的话，则直接返回数据，就不用每次都从内存读取数据了。因此，缓存命中率越高，代码的性能越好。\n\n但需要注意的是，当 CPU 访问数据时，如果 CPU Cache 没有缓存该数据，则会从内存读取数据，但是并不是只读一个数据，而是一次性读取一块一块的数据存放到 CPU Cache 中，之后才会被 CPU 读取。\n\n内存地址映射到 CPU Cache 地址里的策略有很多种，其中比较简单是直接映射 Cache，它巧妙的把内存地址拆分成「索引 + 组标记 + 偏移量」的方式，使得我们可以将很大的内存地址，映射到很小的 CPU Cache 地址里。\n\n要想写出让 CPU 跑得更快的代码，就需要写出缓存命中率高的代码，CPU L1 Cache 分为数据缓存和指令缓存，因而需要分别提高它们的缓存命中率：\n\n- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；\n- 对于指令缓存，有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率；\n\n另外，对于多核 CPU 系统，线程可能在不同 CPU 核心来回切换，这样各个核心的缓存命中率就会受到影响，于是要想提高线程的缓存命中率，可以考虑把线程绑定 CPU 到某一个 CPU 核心。 \n\n----\n\n## 关注作者\n\n分享个喜事，小林平日里忙着输出文章，今天收到一份特别的快递，是 CSDN 寄来的奖状。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20201017154037.jpg)\n\n骄傲的说，你们关注的是 CSDN 首届技术原创第一名的博主，以后简历又可以吹牛逼了\n\n没有啦，其实主要还是**谢谢你们不离不弃的支持**。\n\n\nsao\n\n*哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF*","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/soft_interrupt":{"title":"soft_interrupt","content":"# 2.6 什么是软中断？\n\n今日的技术主题：**什么是软中断？**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/软中断/软中断提纲.png)\n\n---\n\n## 中断是什么？\n\n先来看看什么是中断？在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。\n\n这样的解释可能过于学术了，容易云里雾里，我就举个生活中取外卖的例子。\n\n小林中午搬完砖，肚子饿了，点了份白切鸡外卖，这次我带闪了，没有被某团大数据杀熟。虽然平台上会显示配送进度，但是我也不能一直傻傻地盯着呀，时间很宝贵，当然得去干别的事情，等外卖到了配送员会通过「电话」通知我，电话响了，我就会停下手中地事情，去拿外卖。\n\n\n这里的打电话，其实就是对应计算机里的中断，没接到电话的时候，我可以做其他的事情，只有接到了电话，也就是发生中断，我才会停下当前的事情，去进行另一个事情，也就是拿外卖。\n\n从这个例子，我们可以知道，中断是一种异步的事件处理机制，可以提高系统的并发处理能力。\n\n操作系统收到了中断请求，会打断其他进程的运行，所以**中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。**\n\n而且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说中断有可能会丢失，所以中断处理程序要短且快。\n\n\n还是回到外卖的例子，小林到了晚上又点起了外卖，这次为了犒劳自己，共点了两份外卖，一份小龙虾和一份奶茶，并且是由不同地配送员来配送，那么问题来了，当第一份外卖送到时，配送员给我打了长长的电话，说了一些杂七杂八的事情，比如给个好评等等，但如果这时另一位配送员也想给我打电话。\n\n很明显，这时第二位配送员因为我在通话中（相当于关闭了中断响应），自然就无法打通我的电话，他可能尝试了几次后就走掉了（相当于丢失了一次中断）。\n\n---\n\n## 什么是软中断？\n\n前面我们也提到了，中断请求的处理程序应该要短且快，这样才能减少对正常进程运行调度地影响，而且中断处理程序可能会暂时关闭中断，这时如果中断处理程序执行时间过长，可能在还未执行完中断处理程序前，会丢失当前其他设备的中断请求。\n\n那 Linux 系统**为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」**。\n\n- **上半部用来快速处理中断**，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。\n- **下半部用来延迟处理上半部未完成的工作**，一般以「内核线程」的方式运行。\n\n前面的外卖例子，由于第一个配送员长时间跟我通话，则导致第二位配送员无法拨通我的电话，其实当我接到第一位配送员的电话，可以告诉配送员说我现在下楼，剩下的事情，等我们见面再说（上半部），然后就可以挂断电话，到楼下后，在拿外卖，以及跟配送员说其他的事情（下半部）。\n\n这样，第一位配送员就不会占用我手机太多时间，当第二位配送员正好过来时，会有很大几率拨通我的电话。\n\n再举一个计算机中的例子，常见的网卡接收网络包的例子。\n\n网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。\n\n上部分要做的事情很少，会先禁止网卡中断，避免频繁硬中断，而降低内核的工作效率。接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。\n\n\n所以，中断处理程序的上部分和下半部可以理解为：\n\n- **上半部直接处理硬件请求，也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；\n- **下半部是由内核触发，也就说软中断**，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；\n\n\n还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 `ksoftirqd/0`\n\n不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。\n\n---\n\n### 系统里有哪些软中断？\n\n在 Linux 系统里，我们可以通过查看 `/proc/softirqs` 的 内容来知晓「软中断」的运行情况，以及 `/proc/interrupts` 的 内容来知晓「硬中断」的运行情况。\n\n接下来，就来简单的解析下  `/proc/softirqs` 文件的内容，在我服务器上查看到的文件内容如下：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/软中断/softirqs.png)\n\n你可以看到，每一个 CPU 都有自己对应的不同类型软中断的**累计运行次数**，有 3 点需要注意下。\n\n第一点，要注意第一列的内容，它是代表着软中断的类型，在我的系统里，软中断包括了 10 个类型，分别对应不同的工作类型，比如 `NET_RX` 表示网络接收中断，`NET_TX` 表示网络发送中断、`TIMER` 表示定时中断、`RCU` 表示 RCU 锁中断、`SCHED` 表示内核调度中断。\n\n\n第二点，要注意同一种类型的软中断在不同 CPU 的分布情况，正常情况下，同一种中断在不同 CPU 上的累计次数相差不多，比如我的系统里，`NET_RX` 在 CPU0 、CPU1、CPU2、CPU3 上的中断次数基本是同一个数量级，相差不多。\n\n第三点，这些数值是系统运行以来的累计中断次数，数值的大小没什么参考意义，但是系统的**中断次数的变化速率**才是我们要关注的，我们可以使用 `watch -d cat /proc/softirqs` 命令查看中断次数的变化速率。\n\n\n前面提到过，软中断是以内核线程的方式执行的，我们可以用 `ps` 命令可以查看到，下面这个就是在我的服务器上查到软中断内核线程的结果：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/软中断/ksoftirqd.png)\n\n可以发现，内核线程的名字外面都有有中括号，这说明 ps 无法获取它们的命令行参数，所以一般来说，名字在中括号里的都可以认为是内核线程。\n\n而且，你可以看到有 4 个 `ksoftirqd` 内核线程，这是因为我这台服务器的 CPU 是 4 核心的，每个 CPU 核心都对应着一个内核线程。\n\n---\n\n## 如何定位软中断 CPU 使用率过高的问题？\n\n要想知道当前的系统的软中断情况，我们可以使用 `top` 命令查看，下面是一台服务器上的 top 的数据：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/软中断/top_si.png)\n\n上图中的黄色部分 `si`，就是 CPU 在软中断上的使用率，而且可以发现，每个 CPU 使用率都不高，两个 CPU 的使用率虽然只有 3% 和 4% 左右，但是都是用在软中断上了。\n\n另外，也可以看到 CPU 使用率最高的进程也是软中断 `ksoftirqd`，因此可以认为此时系统的开销主要来源于软中断。\n\n如果要知道是哪种软中断类型导致的，我们可以使用 `watch -d cat /proc/softirqs` 命令查看每个软中断类型的中断次数的变化速率。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/软中断/watch.png)\n\n一般对于网络 I/O 比较高的 Web 服务器，`NET_RX` 网络接收中断的变化速率相比其他中断类型快很多。\n\n如果发现 `NET_RX` 网络接收中断次数的变化速率过快，接下来就可以使用 `sar -n DEV` 查看网卡的网络包接收速率情况，然后分析是哪个网卡有大量的网络包进来。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/软中断/sar_dev.png)\n\n\n接着，在通过 `tcpdump` 抓包，分析这些包的来源，如果是非法的地址，可以考虑加防火墙，如果是正常流量，则要考虑硬件升级等。\n\n---\n\n## 总结\n\n为了避免由于中断处理程序执行时间过长，而影响正常进程的调度，Linux 将中断处理程序分为上半部和下半部：\n\n- 上半部，对应硬中断，由硬件触发中断，用来快速处理中断；\n- 下半部，对应软中断，由内核触发中断，用来异步处理上半部未完成的工作；\n\nLinux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使用 watch -d cat /proc/softirqs 命令。\n\n每一个 CPU 都有各自的软中断内核线程，我们还可以用 ps 命令来查看内核线程，一般名字在中括号里面到，都认为是内核线程。\n\n如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断 ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。\n\n这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/1_hardware/storage":{"title":"storage","content":"# 2.2 磁盘比内存慢几万倍？\n\n大家如果想自己组装电脑的话，肯定需要购买一个 CPU，但是存储器方面的设备，分类比较多，那我们肯定不能只买一种存储器，比如你除了要买内存，还要买硬盘，而针对硬盘我们还可以选择是固态硬盘还是机械硬盘。\n\n相信大家都知道内存和硬盘都属于计算机的存储设备，断电后内存的数据是会丢失的，而硬盘则不会，因为硬盘是持久化存储设备，同时也是一个 I/O 设备。\n\n但其实 CPU 内部也有存储数据的组件，这个应该比较少人注意到，比如**寄存器、CPU L1/L2/L3 Cache** 也都是属于存储设备，只不过它们能存储的数据非常小，但是它们因为靠近 CPU 核心，所以访问速度都非常快，快过硬盘好几个数量级别。\n\n问题来了，**那机械硬盘、固态硬盘、内存这三个存储器，到底和 CPU L1 Cache 相比速度差多少倍呢？**\n\n在回答这个问题之前，我们先来看看「**存储器的层次结构**」，好让我们对存储器设备有一个整体的认识。\n\n![](https://raw.githubusercontent.com/xiaolincoder/ImageHost3/main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E5%99%A8/%E5%AD%98%E5%82%A8%E5%99%A8%E6%8F%90%E7%BA%B2.png)\n\n---\n\n## 存储器的层次结构\n\n\n我们想象中一个场景，大学期末准备考试了，你前去图书馆临时抱佛脚。那么，在看书的时候，我们的大脑会思考问题，也会记忆知识点，另外我们通常也会把常用的书放在自己的桌子上，当我们要找一本不常用的书，则会去图书馆的书架找。\n\n就是这么一个小小的场景，已经把计算机的存储结构基本都涵盖了。\n\n我们可以把 CPU 比喻成我们的大脑，大脑正在思考的东西，就好比 CPU 中的**寄存器**，处理速度是最快的，但是能存储的数据也是最少的，毕竟我们也不能一下同时思考太多的事情，除非你练过。\n\n我们大脑中的记忆，就好比 **CPU Cache**，中文称为 CPU 高速缓存，处理速度相比寄存器慢了一点，但是能存储的数据也稍微多了一些。\n\nCPU Cache 通常会分为 **L1、L2、L3 三层**，其中 L1 Cache 通常分成「数据缓存」和「指令缓存」，L1 是距离 CPU 最近的，因此它比 L2、L3 的读写速度都快、存储空间都小。我们大脑中短期记忆，就好比 L1 Cache，而长期记忆就好比 L2/L3 Cache。\n\n寄存器和 CPU Cache 都是在 CPU 内部，跟 CPU 挨着很近，因此它们的读写速度都相当的快，但是能存储的数据很少，毕竟 CPU 就这么丁点大。\n\n知道 CPU 内部的存储器的层次分布，我们放眼看看 CPU 外部的存储器。\n\n当我们大脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌子上的书，就好比**内存**，我们虽然可以一伸手就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好比**硬盘**，能存储的数据非常大，但是读写速度相比内存差好几个数量级，更别说跟寄存器的差距了。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/学习与存储层次关系.png)\n\n\n我们从图书馆书架取书，把书放到桌子上，再阅读书，我们大脑就会记忆知识点，然后再经过大脑思考，这一系列过程相当于，数据从硬盘加载到内存，再从内存加载到 CPU 的寄存器和 Cache 中，然后再通过 CPU 进行处理和计算。\n\n**对于存储器，它的速度越快、能耗会越高、而且材料的成本也是越贵的，以至于速度快的存储器的容量都比较小。**\n\nCPU 里的寄存器和 Cache，是整个计算机存储器中价格最贵的，虽然存储空间很小，但是读写速度是极快的，而相对比较便宜的内存和硬盘，速度肯定比不上 CPU 内部的存储器，但是能弥补存储空间的不足。\n\n\n存储器通常可以分为这么几个级别：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储区分级.png)\n\n- 寄存器；\n- CPU Cache；\n  1. L1-Cache；\n  2. L2-Cache；\n  3. L3-Cahce；\n- 内存；\n- SSD/HDD 硬盘\n\n### 寄存器\n\n最靠近 CPU 的控制单元和逻辑计算单元的存储器，就是寄存器了，它使用的材料速度也是最快的，因此价格也是最贵的，那么数量不能很多。\n\n存储器的数量通常在几十到几百之间，每个寄存器可以用来存储一定的字节（byte）的数据。比如：\n\n- 32 位 CPU 中大多数寄存器可以存储 `4` 个字节；\n- 64 位 CPU 中大多数寄存器可以存储 `8` 个字节。\n\n寄存器的访问速度非常快，一般要求在半个 CPU 时钟周期内完成读写，CPU 时钟周期跟 CPU 主频息息相关，比如 2 GHz 主频的 CPU，那么它的时钟周期就是 1/2G，也就是 0.5ns（纳秒）。\n\nCPU 处理一条指令的时候，除了读写寄存器，还需要解码指令、控制指令执行和计算。如果寄存器的速度太慢，则会拉长指令的处理周期，从而给用户的感觉，就是电脑「很慢」。\n\n### CPU Cache\n\nCPU Cache 用的是一种叫 **SRAM（*Static Random-Access* Memory，静态随机存储器）** 的芯片。\n\nSRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，而一旦断电，数据就会丢失了。\n\n在 SRAM 里面，一个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不高，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度非常快。\n\n\nCPU 的高速缓存，通常可以分为 L1、L2、L3 这样的三层高速缓存，也称为一级缓存、二级缓存、三级缓存。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/CPU-Cache.png)\n\n\n#### L1 高速缓存\n\nL1 高速缓存的访问速度几乎和寄存器一样快，通常只需要 `2~4` 个时钟周期，而大小在几十 KB 到几百 KB 不等。\n\n每个 CPU 核心都有一块属于自己的 L1 高速缓存，指令和数据在 L1 是分开存放的，所以 L1 高速缓存通常分成**指令缓存**和**数据缓存**。\n\n在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L1 Cache 「数据」缓存的容量大小：\n\n```bash\n$ cat /sys/devices/system/cpu/cpu0/cache/index0/size\n32K\n```\n\n而查看 L1 Cache 「指令」缓存的容量大小，则是：\n\n```bash\n$ cat /sys/devices/system/cpu/cpu0/cache/index1/size\n32K\n```\n\n\n#### L2 高速缓存\n\nL2 高速缓存同样每个 CPU 核心都有，但是 L2 高速缓存位置比 L1 高速缓存距离 CPU 核心 更远，它大小比 L1 高速缓存更大，CPU 型号不同大小也就不同，通常大小在几百 KB 到几 MB 不等，访问速度则更慢，速度在 `10~20` 个时钟周期。\n\n\n在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L2 Cache 的容量大小：\n\n```bash\n$ cat /sys/devices/system/cpu/cpu0/cache/index2/size\n256K\n```\n\n\n#### L3 高速缓存\n\nL3 高速缓存通常是多个 CPU 核心共用的，位置比 L2 高速缓存距离 CPU 核心 更远，大小也会更大些，通常大小在几 MB 到几十 MB 不等，具体值根据 CPU 型号而定。\n\n访问速度相对也比较慢一些，访问速度在 `20~60`个时钟周期。\n\n在 Linux 系统，我们可以通过这条命令，查看 CPU 里的 L3 Cache 的容量大小：\n\n\n```bash\n$ cat /sys/devices/system/cpu/cpu0/cache/index3/size \n3072K\n```\n\n\n### 内存\n\n内存用的芯片和 CPU Cache 有所不同，它使用的是一种叫作 **DRAM （*Dynamic Random Access Memory*，动态随机存取存储器）** 的芯片。\n\n相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。\n\n\nDRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。\n\n\nDRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问的速度会更慢，内存速度大概在 `200~300` 个 时钟周期之间。\n\n\n### SSD/HDD 硬盘\n\nSSD（*Solid-state disk*） 就是我们常说的固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。内存的读写速度比 SSD 大概快 `10~1000` 倍。\n\n当然，还有一款传统的硬盘，也就是机械硬盘（*Hard Disk Drive, HDD*），它是通过物理读写的方式来访问数据的，因此它访问速度是非常慢的，它的速度比内存慢 `10W` 倍左右。\n\n由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。\n\n---\n\n\n## 存储器的层次关系\n\n现代的一台计算机，都用上了 CPU Cahce、内存、到 SSD 或 HDD 硬盘这些存储器设备了。\n\n其中，存储空间越大的存储器设备，其访问速度越慢，所需成本也相对越少。\n\nCPU 并不会直接和每一种存储器设备直接打交道，而是每一种存储器设备只和它相邻的存储器设备打交道。\n\n比如，CPU Cache 的数据是从内存加载过来的，写回数据的时候也只写回到内存，CPU Cache 不会直接把数据写到硬盘，也不会直接从硬盘加载数据，而是先加载到内存，再从内存加载到 CPU Cache 中。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储器的层次关系图.png)\n\n所以，**每个存储器只和相邻的一层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更高，也正因为成本太高，所以 CPU 内部的寄存器、L1\\L2\\L3 Cache 只好用较小的容量，相反内存、硬盘则可用更大的容量，这就我们今天所说的存储器层次结构**。\n\n另外，当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即可，如果寄存器没有这个数据，CPU 就会查询 L1 高速缓存，如果 L1 没有，则查询 L2 高速缓存，L2 还是没有的话就查询 L3 高速缓存，L3 依然没有的话，才去内存中取数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/缓存体系1.png)\n\n所以，存储层次结构也形成了**缓存**的体系。\n\n\n----\n\n## 存储器之间的实际价格和性能差距\n\n前面我们知道了，速度越快的存储器，造价成本往往也越高，那我们就以实际的数据来看看，不同层级的存储器之间的性能和价格差异。\n\n下面这张表格是不同层级的存储器之间的成本对比图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/存储结构/存储器成本的对比.png)\n\n你可以看到 L1 Cache 的访问延时是 1 纳秒，而内存已经是 100 纳秒了，相比 L1 Cache 速度慢了 `100` 倍。另外，机械硬盘的访问延时更是高达 10 毫秒，相比 L1 Cache 速度慢了 `10000000` 倍，差了好几个数量级别。\n\n在价格上，每生成 MB 大小的 L1 Cache 相比内存贵了 `466` 倍，相比机械硬盘那更是贵了 `175000` 倍。\n\n我在某东逛了下各个存储器设备的零售价，8G 内存 + 1T 机械硬盘 + 256G 固态硬盘的总价格，都不及一块 Intle i5-10400 的 CPU 的价格，这款 CPU 的高速缓存的总大小也就十多 MB。\n\n\n---\n\n## 总结\n\n各种存储器之间的关系，可以用我们在图书馆学习这个场景来理解。\n\nCPU 可以比喻成我们的大脑，我们当前正在思考和处理的知识的过程，就好比 CPU 中的**寄存器**处理数据的过程，速度极快，但是容量很小。而 CPU 中的 **L1-L3 Cache** 好比我们大脑中的短期记忆和长期记忆，需要小小花费点时间来调取数据并处理。\n\n我们面前的桌子就相当于**内存**，能放下更多的书（数据），但是找起来和看起来就要花费一些时间，相比 CPU Cache 慢不少。而图书馆的书架相当于**硬盘**，能放下比内存更多的数据，但找起来就更费时间了，可以说是最慢的存储器设备了。\n\n从 寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，访问速度越来越慢，存储容量越来越大，价格也越来越便宜，而且每个存储器只和相邻的一层存储器设备打交道，于是这样就形成了存储器的层次结构。\n\n\n再来回答，开头的问题：那机械硬盘、固态硬盘、内存这三个存储器，到底和 `CPU L1 Cache` 相比速度差多少倍呢？\n\nCPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 **CPU L1 Cache 比内存快 `100` 倍左右**。\n\nSSD 随机访问延时是 150 微秒，所以 **CPU L1 Cache 比 SSD 快 `150000` 倍左右**。\n\n最慢的机械硬盘随机访问延时已经高达 10 毫秒，我们来看看机械硬盘到底有多「龟速」：\n\n- **SSD 比机械硬盘快 70 倍左右；**\n- **内存比机械硬盘快 100000 倍左右；**\n- **CPU L1 Cache 比机械硬盘快 10000000 倍左右；**\n\n\n我们把上述的时间比例差异放大后，就能非常直观感受到它们的性能差异了。如果 CPU 访问 L1 Cache 的缓存时间是 1 秒，那访问内存则需要大约 2 分钟，随机访问 SSD 里的数据则需要 1.7 天，访问机械硬盘那更久，长达近 4 个月。\n\n可以发现，不同的存储器之间性能差距很大，构造存储器分级很有意义，分级的目的是要构造**缓存**体系。\n\n\n---\n\n\n## 关注作者\n\n\n新的**技术交流群**已经慢慢人多起来了，群里的大牛真的多，大家交流都很踊跃，也有很多热心分享和回答问题的小伙伴，是你交朋友好地方，更是你上班划水的好入口。\n\n准备入冬了，一起来抱团取暖吧，加群方式很简单，只需要加我的微信二维码，备注「**加群**」即可。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n**哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF**","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/2_os_structure/linux_vs_windows":{"title":"linux_vs_windows","content":"# 3.1 Linux 内核 vs Windows 内核\n\nWindows 和 Linux 可以说是我们比较常见的两款操作系统的。\n\nWindows 基本占领了电脑时代的市场，商业上取得了很大成就，但是它并不开源，所以要想接触源码得加入 Windows 的开发团队中。\n\n对于服务器使用的操作系统基本上都是 Linux，而且内核源码也是开源的，任何人都可以下载，并增加自己的改动或功能，Linux 最大的魅力在于，全世界有非常多的技术大佬为它贡献代码。\n\n这两个操作系统各有千秋，不分伯仲。\n\n操作系统核心的东西就是内核，这次我们就来看看，**Linux 内核和 Windows 内核有什么区别？**\n\n---\n\n## 内核\n\n什么是内核呢？\n\n计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了，所以这个中间人就由内核来负责，**让内核作为应用连接硬件设备的桥梁**，应用程序只需关心与内核交互，不用关心硬件的细节。\n\n![内核](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/内核/Kernel_Layout.png)\n\n\n内核有哪些能力呢？\n\n现代操作系统，内核一般会提供 4 个基本能力：\n\n- 管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力；\n- 管理内存，决定内存的分配和回收，也就是内存管理的能力；\n- 管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力；\n- 提供系统调用，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。\n\n\n内核是怎么工作的？\n\n内核具有很高的权限，可以控制 cpu、内存、硬盘等硬件，而应用程序具有的权限很小，因此大多数操作系统，把内存分成了两个区域：\n\n- 内核空间，这个内存空间只有内核程序可以访问；\n- 用户空间，这个内存空间专门给应用程序使用；\n\n用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在**用户态**执行，而当程序使内核空间时，程序则在**内核态**执行。\n\n\n应用程序如果需要进入内核空间，就需要通过系统调用，下面来看看系统调用的过程：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/内核/systemcall.png)\n\n内核程序执行在内核态，用户程序执行在用户态。当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作。\n\n\n---\n\n## Linux 的设计\n\nLinux 的开山始祖是来自一位名叫 Linus Torvalds 的芬兰小伙子，他在 1991 年用 C 语言写出了第一版的 Linux 操作系统，那年他 22 岁。\n\n完成第一版 Linux 后，Linux Torvalds 就在网络上发布了 Linux 内核的源代码，每个人都可以免费下载和使用。\n\n\nLinux 内核设计的理念主要有这几个点：\n\n- *MultiTask*，多任务\n- *SMP*，对称多处理\n- *ELF*，可执行文件链接格式\n- *Monolithic Kernel*，宏内核\n\n####  MultiTask\n\nMultiTask 的意思是**多任务**，代表着 Linux 是一个多任务的操作系统。\n\n多任务意味着可以有多个任务同时执行，这里的「同时」可以是并发或并行：\n\n- 对于单核 CPU 时，可以让每个任务执行一小段时间，时间到就切换另外一个任务，从宏观角度看，一段时间内执行了多个任务，这被称为并发。\n- 对于多核 CPU 时，多个任务可以同时被不同核心的 CPU 同时执行，这被称为并行。\n\n### SMP\n\n\nSMP 的意思是**对称多处理**，代表着每个 CPU 的地位是相等的，对资源的使用权限也是相同的，多个 CPU 共享同一个内存，每个 CPU 都可以访问完整的内存和硬件资源。\n\n这个特点决定了 Linux 操作系统不会有某个 CPU 单独服务应用程序或内核程序，而是每个程序都可以被分配到任意一个 CPU 上被执行。\n\n### ELF\n\nELF 的意思是**可执行文件链接格式**，它是 Linux 操作系统中可执行文件的存储格式，你可以从下图看到它的结构：\n\n\n![ELF 文件格式](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/内核/Elf.png)\n\n\nELF 把文件分成了一个个分段，每一个段都有自己的作用，具体每个段的作用这里我就不详细说明了，感兴趣的同学可以去看《程序员的自我修养——链接、装载和库》这本书。\n\n另外，ELF 文件有两种索引，Program header table 中记录了「运行时」所需的段，而 Section header table 记录了二进制文件中各个「段的首地址」。\n\n那 ELF 文件怎么生成的呢？\n\n我们编写的代码，首先通过「编译器」编译成汇编代码，接着通过「汇编器」变成目标代码，也就是目标文件，最后通过「链接器」把多个目标文件以及调用的各种函数库链接起来，形成一个可执行文件，也就是 ELF 文件。\n\n那 ELF 文件是怎么被执行的呢？\n\n执行 ELF 文件的时候，会通过「装载器」把 ELF 文件装载到内存里，CPU 读取内存中的指令和数据，于是程序就被执行起来了。\n\n\n### Monolithic Kernel\n\nMonolithic Kernel 的意思是**宏内核**，Linux 内核架构就是宏内核，意味着 Linux 的内核是一个完整的可执行程序，且拥有最高的权限。\n\n\n宏内核的特征是系统内核的所有模块，比如进程调度、内存管理、文件系统、设备驱动等，都运行在内核态。\n\n不过，Linux 也实现了动态加载内核模块的功能，例如大部分设备驱动是以可加载模块的形式存在的，与内核其他模块解耦，让驱动开发和驱动加载更为方便、灵活。\n\n\n![分别为宏内核、微内核、混合内核的操作系统结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/内核/OS-structure2.png)\n\n与宏内核相反的是**微内核**，微内核架构的内核只保留最基本的能力，比如进程调度、虚拟机内存、中断等，把一些应用放到了用户空间，比如驱动程序、文件系统等。这样服务与服务之间是隔离的，单个服务出现故障或者完全攻击，也不会导致整个操作系统挂掉，提高了操作系统的稳定性和可靠性。 \n\n微内核内核功能少，可移植性高，相比宏内核有一点不好的地方在于，由于驱动程序不在内核中，而且驱动程序一般会频繁调用底层能力的，于是驱动和硬件设备交互就需要频繁切换到内核态，这样会带来性能损耗。华为的鸿蒙操作系统的内核架构就是微内核。\n\n\n还有一种内核叫**混合类型内核**，它的架构有点像微内核，内核里面会有一个最小版本的内核，然后其他模块会在这个基础上搭建，然后实现的时候会跟宏内核类似，也就是把整个内核做成一个完整的程序，大部分服务都在内核中，这就像是宏内核的方式包裹着一个微内核。\n\n---\n\n## Windows 设计\n\n\n当今 Windows 7、Windows 10 使用的内核叫 Windows NT，NT 全称叫 New Technology。\n\n下图是 Windows NT 的结构图片：\n\n![Windows NT 的结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/内核/windowNT.png)\n\nWindows 和 Linux 一样，同样支持 MutiTask 和 SMP，但不同的是，**Window 的内核设计是混合型内核**，在上图你可以看到内核中有一个 *MicroKernel* 模块，这个就是最小版本的内核，而整个内核实现是一个完整的程序，含有非常多模块。\n\nWindows 的可执行文件的格式与 Linux 也不同，所以这两个系统的可执行文件是不可以在对方上运行的。\n\nWindows 的可执行文件格式叫 PE，称为**可移植执行文件**，扩展名通常是`.exe`、`.dll`、`.sys`等。\n\nPE 的结构你可以从下图中看到，它与 ELF 结构有一点相似。\n\n![PE 文件结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/内核/pe.png)\n\n---\n\n## 总结\n\n对于内核的架构一般有这三种类型：\n\n- 宏内核，包含多个模块，整个内核像一个完整的程序；\n- 微内核，有一个最小版本的内核，一些模块和服务则由用户态管理；\n- 混合内核，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序；\n\nLinux 的内核设计是采用了宏内核，Window 的内核设计则是采用了混合内核。\n\n这两个操作系统的可执行文件格式也不一样， Linux 可执行文件格式叫作 ELF，Windows 可执行文件格式叫作 PE。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/3_memory/alloc_mem":{"title":"alloc_mem","content":"# 4.4 在 4GB 物理内存的机器上，申请 8G 内存会怎么样？\n\n大家好，我是小林。\n\n看到读者在群里讨论这些面试题：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/读者提问.png)\n\n其中，第一个问题「**在 4GB 物理内存的机器上，申请 8G 内存会怎么样？**」存在比较大的争议，有人说会申请失败，有的人说可以申请成功。\n\n这个问题在没有前置条件下，就说出答案就是耍流氓。这个问题要考虑三个前置条件：\n\n- 操作系统是 32 位的，还是 64 位的？\n- 申请完 8G 内存后会不会被使用？\n- 操作系统有没有使用 Swap 机制？\n\n所以，我们要分场景讨论。\n\n## 操作系统虚拟内存大小\n\n应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。\n\n当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生**缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。\n\n缺页中断处理函数会看是否有空闲的物理内存：\n\n- 如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。\n- 如果没有空闲的物理内存，那么内核就会开始进行[回收内存](https://xiaolincoding.com/os/3_memory/mem_reclaim.html)的工作，如果回收内存工作结束后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了触发 OOM （Out of Memory）机制。\n\n32 位操作系统和 64 位操作系统的虚拟地址空间大小是不同的，在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，如下所示：\n\n![](https://img-blog.csdnimg.cn/3a6cb4e3f27241d3b09b4766bb0b1124.png)\n\n通过这里可以看出：\n\n- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；\n- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。\n\n### 32 位系统的场景\n\n\u003e 现在可以回答这个问题了：在 32 位操作系统、4GB 物理内存的机器上，申请 8GB 内存，会怎么样？\n\n因为 32 位操作系统，进程最多只能申请 3 GB 大小的虚拟内存空间，所以进程申请 8GB 内存的话，在申请虚拟内存阶段就会失败（我手上没有 32 位操作系统测试，我估计失败的错误是 cannot allocate memory，也就是无法申请内存失败）。\n\n### 64 位系统的场景\n\n\u003e 在 64 位操作系统、4GB 物理内存的机器上，申请 8G 内存，会怎么样？\n\n64 位操作系统，进程可以使用 128 TB 大小的虚拟内存空间，所以进程申请 8GB 内存是没问题的，因为进程申请内存是申请虚拟内存，只要不读写这个虚拟内存，操作系统就不会分配物理内存。\n\n我们可以简单做个测试，我的服务器是 64 位操作系统，但是物理内存只有 2 GB：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/2gb.png)\n\n现在，我在机器上，连续申请 4 次 1 GB 内存，也就是一共申请了 4 GB 内存，注意下面代码只是单纯分配了虚拟内存，并没有使用该虚拟内存：\n\n```c\n#include \u003cstdio.h\u003e\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cerrno.h\u003e\n\n#define MEM_SIZE 1024 * 1024 * 1024\n\nint main() {\n    char* addr[4];\n    int i = 0;\n    for(i = 0; i \u003c 4; ++i) {\n        addr[i] = (char*) malloc(MEM_SIZE);\n        if(!addr[i]) {\n            printf(\"执行 malloc 失败, 错误：%s\\n\",strerror(errno));\n\t\t        return -1;\n        }\n        printf(\"主线程调用malloc后，申请1gb大小得内存，此内存起始地址：0X%p\\n\", addr[i]);\n    }\n    \n    //输入任意字符后，才结束\n    getchar();\n    return 0;\n}\n```\n\n然后运行这个代码，可以看到，我的物理内存虽然只有 2GB，但是程序正常分配了 4GB 大小的虚拟内存：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/虚拟内存4g.png)\n\n我们可以通过下面这条命令查看进程（test）的虚拟内存大小：\n\n```shell\n# ps aux | grep test\nUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot      7797  0.0  0.0 4198540  352 pts/1    S+   16:58   0:00 ./test\n```\n\n其中，VSZ 就代表进程使用的虚拟内存大小，RSS 代表进程使用的物理内存大小。可以看到，VSZ 大小为 4198540，也就是 4GB 的虚拟内存。\n\n\u003e 之前有读者跟我反馈，说他自己也做了这个实验，然后发现 64 位操作系统，在申请 4GB 虚拟内存的时候失败了，这是为什么呢？\n\n失败的错误：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-1.png)\n\n我当时帮他排查了下，发现跟 Linux 中的 [overcommit_memory](http://linuxperf.com/?p=102) 参数有关，可以使用 `cat /proc/sys/vm/overcommit_memory` 来查看这个参数，这个参数接受三个值：\n\n- 如果值为 0（默认值），代表：Heuristic overcommit handling，它允许overcommit，但过于明目张胆的overcommit会被拒绝，比如malloc一次性申请的内存大小就超过了系统总内存。Heuristic的意思是“试探式的”，内核利用某种算法猜测你的内存申请是否合理，大概可以理解为单次申请不能超过free memory + free swap + pagecache的大小 + SLAB中可回收的部分 ，超过了就会拒绝overcommit。\n- 如果值为 1，代表：Always overcommit. 允许overcommit，对内存申请来者不拒。\n- 如果值为 2，代表：Don’t overcommit. 禁止overcommit。\n\n当时那位读者的 overcommit_memory 参数是默认值 0 ，所以申请失败的原因可能是内核认为我们申请的内存太大了，它认为不合理，所以 malloc() 返回了 Cannot allocate memory 错误，这里申请 4GB 虚拟内存失败的同学可以将这个 overcommit_memory 设置为1，就可以 overcommit 了。\n\n```shell\necho 1 \u003e /proc/sys/vm/overcommit_memory \n```\n\n设置完为 1 后，读者的机子就可以正常申请 4GB 虚拟内存了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-2.png)\n\n**不过我的环境 overcommit_memory 是 0，在 64 系统、2 G 物理内存场景下，也是可以成功申请 4 G 内存的，我怀疑可能是不同版本的内核在 overcommit_memory 为 0 时，检测内存申请是否合理的算法可能是不同的。**\n\n**总之，如果你申请大内存的时候，不想被内核检测内存申请是否合理的算法干扰的话，将 overcommit_memory 设置为 1 就行。**\n\n\u003e 那么将这个 overcommit_memory 设置为 1 之后，64 位的主机就可以申请接近 128T 虚拟内存了吗？\n\n不一定，还得看你服务器的物理内存大小。\n\n读者的服务器物理内存是 2 GB，实验后发现，进程还没有申请到 128T 虚拟内存的时候就被杀死了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-3.png)\n\n注意，这次是 killed，而不是 Cannot Allocate Memory，说明并不是内存申请有问题，而是触发 OOM 了。\n\n但是为什么会触发 OOM 呢？\n\n那得看你的主机的「物理内存」够不够大了，即使 malloc 申请的是虚拟内存，只要不去访问就不会映射到物理内存，但是申请虚拟内存的过程中，还是使用到了物理内存（比如内核保存虚拟内存的数据结构，也是占用物理内存的），如果你的主机是只有 2GB 的物理内存的话，大概率会触发 OOM。\n\n可以使用 top 命令，点击两下 m，通过进度条观察物理内存使用情况。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-4.png)\n\n可以看到申请虚拟内存的过程中**物理内存使用量一直在增长**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-5.png)\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-6.png)\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-7.png)\n\n直到直接内存回收之后，也无法回收出一块空间供这个进程使用，这个时候就会触发 OOM，给所有能杀死的进程打分，分数越高的进程越容易被杀死。\n\n在这里当然是这个进程得分最高，那么操作系统就会将这个进程杀死，所以最后会出现 killed，而不是Cannot allocate memory。\n\n\u003e 那么 2GB 的物理内存的 64 位操作系统，就不能申请128T的虚拟内存了吗？\n\n其实可以，上面的情况是还没开启 swap 的情况。\n\n使用 swapfile 的方式开启了 1GB 的 swap 空间之后再做实验：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-8.png)\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-9.png)\n\n发现出现了 Cannot allocate memory，但是其实到这里已经成功了，\n\n打开计算器计算一下，发现已经申请了 127.998T 虚拟内存了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-10.png)\n\n实际上我们是不可能申请完整个 128T 的用户空间的，因为程序运行本身也需要申请虚拟空间\n\n申请 127T 虚拟内存试试：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-11.png)\n\n发现进程没有被杀死，也没有 Cannot allocate memory，也正好是 127T 虚拟内存空间。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-12.png)\n\n在 top 中我们可以看到这个申请了127T虚拟内存的进程。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/033读者-13.png)\n\n## Swap 机制的作用\n\n前面讨论在 32 位/64 位操作系统环境下，申请的虚拟内存超过物理内存后会怎么样？\n\n- 在 32 位操作系统，因为进程最大只能申请 3 GB 大小的虚拟内存，所以直接申请 8G 内存，会申请失败。\n- 在 64 位操作系统，因为进程最大只能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。\n\n程序申请的虚拟内存，如果没有被使用，它是不会占用物理空间的。当访问这块虚拟内存后，操作系统才会进行物理内存分配。\n\n如果申请物理内存大小超过了空闲物理内存大小，就要看操作系统有没有开启 Swap 机制：\n\n- 如果没有开启 Swap 机制，程序就会直接 OOM；\n- 如果有开启 Swap 机制，程序可以正常运行。\n\n\u003e 什么是 Swap 机制？\n\n当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间会被临时保存到磁盘，等到那些程序要运行时，再从磁盘中恢复保存的数据到内存中。\n\n另外，当内存使用存在压力的时候，会开始触发内存回收行为，会把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。\n\n这种，将内存数据换出磁盘，又从磁盘中恢复数据到内存的过程，就是 Swap 机制负责的。\n\nSwap 就是把一块磁盘空间或者本地文件，当成内存来使用，它包含换出和换入两个过程：\n\n- **换出（Swap Out）** ，是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存；\n- **换入（Swap In）**，是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来；\n\nSwap 换入换出的过程如下图：\n\n![](https://img-blog.csdnimg.cn/388a29f45fe947e5a49240e4eff13538.png)\n\n使用 Swap 机制优点是，应用程序实际可以使用的内存空间将远远超过系统的物理内存。由于硬盘空间的价格远比内存要低，因此这种方式无疑是经济实惠的。当然，频繁地读写硬盘，会显著降低操作系统的运行速率，这也是 Swap 的弊端。\n\nLinux 中的 Swap 机制会在内存不足和内存闲置的场景下触发：\n\n- **内存不足**：当系统需要的内存超过了可用的物理内存时，内核会将内存中不常使用的内存页交换到磁盘上为当前进程让出内存，保证正在执行的进程的可用性，这个内存回收的过程是强制的直接内存回收（Direct Page Reclaim）。直接内存回收是同步的过程，会阻塞当前申请内存的进程。\n- **内存闲置**：应用程序在启动阶段使用的大量内存在启动后往往都不会使用，通过后台运行的守护进程（kSwapd），我们可以将这部分只使用一次的内存交换到磁盘上为其他内存的申请预留空间。kSwapd 是 Linux 负责页面置换（Page replacement）的守护进程，它也是负责交换闲置内存的主要进程，它会在[空闲内存低于一定水位](https://xiaolincoding.com/os/3_memory/mem_reclaim.html#%E5%B0%BD%E6%97%A9%E8%A7%A6%E5%8F%91-kSwapd-%E5%86%85%E6%A0%B8%E7%BA%BF%E7%A8%8B%E5%BC%82%E6%AD%A5%E5%9B%9E%E6%94%B6%E5%86%85%E5%AD%98)时，回收内存页中的空闲内存保证系统中的其他进程可以尽快获得申请的内存。kSwapd 是后台进程，所以回收内存的过程是异步的，不会阻塞当前申请内存的进程。\n\nLinux 提供了两种不同的方法启用 Swap，分别是 Swap 分区（Swap Partition）和 Swap 文件（Swapfile），开启方法可以看[这个资料](https://support.huaweicloud.com/trouble-ecs/ecs_trouble_0322.html)：\n\n- Swap 分区是硬盘上的独立区域，该区域只会用于交换分区，其他的文件不能存储在该区域上，我们可以使用 `swapon -s` 命令查看当前系统上的交换分区；\n- Swap 文件是文件系统中的特殊文件，它与文件系统中的其他文件也没有太多的区别；\n\n\u003e Swap 换入换出的是什么类型的内存？\n\n内核缓存的文件数据，因为都有对应的磁盘文件，所以在回收文件数据的时候， 直接写回到对应的文件就可以了。\n\n但是像进程的堆、栈数据等，它们是没有实际载体，这部分内存被称为匿名页。而且这部分内存很可能还要再次被访问，所以不能直接释放内存，于是就需要有一个能保存匿名页的磁盘载体，这个载体就是 Swap 分区。\n\n匿名页回收的方式是通过 Linux 的 Swap 机制，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。\n\n接下来，通过两个实验，看看申请的物理内存超过物理内存会怎样？\n\n- 实验一：没有开启 Swap 机制\n- 实验二：有开启 Swap 机制\n\n### 实验一：没有开启 Swap 机制\n\n我的服务器是 64 位操作系统，但是物理内存只有 2 GB，而且没有 Swap 分区：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/2gb.png)\n\n我们改一下前面的代码，使得在申请完 4GB 虚拟内存后，通过 memset 函数访问这个虚拟内存，看看在没有 Swap 分区的情况下，会发生什么？\n\n```c\n#include \u003cstdio.h\u003e\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cerrno.h\u003e\n\n#define MEM_SIZE 1024 * 1024 * 1024\n\nint main() {\n    char* addr[4];\n    int i = 0;\n    for(i = 0; i \u003c 4; ++i) {\n        addr[i] = (char*) malloc(MEM_SIZE);\n        if(!addr[i]) {\n            printf(\"执行 malloc 失败, 错误：%s\\n\",strerror(errno));\n            return -1;\n        }\n        printf(\"主线程调用malloc后，申请1gb大小得内存，此内存起始地址：0X%p\\n\", addr[i]);\n    }\n\n    for(i = 0; i \u003c 4; ++i) {\n        printf(\"开始访问第 %d 块虚拟内存(每一块虚拟内存为 1 GB)\\n\", i + 1);\n        memset(addr[i], 0, MEM_SIZE);\n    }\n    \n    //输入任意字符后，才结束\n    getchar();\n    return 0;\n}\n```\n\n运行结果：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/发生oom.png)\n\n可以看到，在访问第 2 块虚拟内存（每一块虚拟内存是 1 GB）的时候，因为超过了机器的物理内存（2GB），进程（test）被操作系统杀掉了。\n\n通过查看 message 系统日志，可以发现该进程是被操作系统 OOM killer 机制杀掉了，日志里报错了 Out of memory，也就是发生 OOM（内存溢出错误）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/oom日志.png)\n\n\u003e 什么是 OOM?\n\n内存溢出(Out Of Memory，简称OOM)是指应用系统中存在无法回收的内存或使用的内存过多，最终使得程序运行要用到的内存大于能提供的最大内存。此时程序就运行不了，系统会提示内存溢出。\n\n### 实验二：有开启 Swap 机制\n\n我用我的 mac book pro 笔记本做测试，我的笔记本是 64 位操作系统，物理内存是 8 GB， 目前 Swap 分区大小为 1 GB（注意这个大小不是固定不变的，Swap 分区总大小是会动态变化的，当没有使用 Swap 分区时，Swap 分区总大小是 0；当使用了 Swap 分区，Swap 分区总大小会增加至 1 GB；当 Swap 分区已使用的大小超过 1 GB 时；Swap 分区总大小就会增加到至 2 GB；当 Swap 分区已使用的大小超过 2 GB 时；Swap 分区总大小就增加至 3GB，如此往复。这个估计是 macos 自己实现的，Linux 的分区则是固定大小的，Swap 分区不会根据使用情况而自动增长）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/swap分区大小.png)\n\n为了方便观察磁盘 I/O 情况，我们改进一下前面的代码，分配完 32 GB虚拟内存后（笔记本物理内存是 8 GB），通过一个 while 循环频繁访问虚拟内存，代码如下：\n\n```c\n#include \u003cstdio.h\u003e\n#include \u003cstdlib.h\u003e\n#include \u003cstring.h\u003e\n\n#define MEM_SIZE 32 * 1024 * 1024 * 1024\n\nint main() {\n    char* addr = (char*) malloc((long)MEM_SIZE);\n    printf(\"主线程调用malloc后，目前共申请了 32gb 的虚拟内存\\n\");\n    \n    //循环频繁访问虚拟内存\n    while(1) {\n          printf(\"开始访问 32gb 大小的虚拟内存...\\n\");\n          memset(addr, 0, (long)MEM_SIZE);\n    }\n    return 0;\n}\n```\n\n运行结果如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/代码3运行结果.png)\n\n可以看到，在有 Swap 分区的情况下，即使笔记本物理内存是 8 GB，申请并使用 32 GB 内存是没问题，程序正常运行了，并没有发生 OOM。\n\n从下图可以看到，进程的内存显示 32 GB（这个不要理解为占用的物理内存，理解为已被访问的虚拟内存大小，也就是在物理内存呆过的内存大小），系统已使用的 Swap 分区达到 2.3 GB。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/test进程内存情况.png)\n\n此时我的笔记本电脑的磁盘开始出现“沙沙”的声音，通过查看磁盘的 I/O 情况，可以看到磁盘 I/O 达到了一个峰值，非常高：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/磁盘io.png)\n\n\u003e 有了 Swap 分区，是不是意味着进程可以使用的内存是无上限的？\n\n当然不是，我把上面的代码改成了申请 64GB 内存后，当进程申请完 64GB 虚拟内存后，使用到 56 GB （这个不要理解为占用的物理内存，理解为已被访问的虚拟内存大小，也就是在物理内存呆过的内存大小）的时候，进程就被系统 kill 掉了，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/被kill掉.png)\n\n当系统多次尝试回收内存，还是无法满足所需使用的内存大小，进程就会被系统 kill 掉了，意味着发生了 OOM （*PS：我没有在 macos 系统找到像 linux 系统里的 /var/log/message 系统日志文件，所以无法通过查看日志确认是否发生了 OOM*）。\n\n## 总结\n\n至此， 验证完成了。简单总结下：\n\n- 在 32 位操作系统，因为进程理论上最大能申请 3 GB 大小的虚拟内存，所以直接申请 8G 内存，会申请失败。\n- 在 64位 位操作系统，因为进程理论上最大能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。如果这块虚拟内存被访问了，要看系统有没有 Swap 分区：\n  - 如果没有 Swap 分区，因为物理空间不够，进程会被操作系统杀掉，原因是 OOM（内存溢出）；\n  - 如果有 Swap 分区，即使物理内存只有 4GB，程序也能正常使用 8GB 的内存，进程可以正常运行；\n\n---\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n\n\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/3_memory/cache_lru":{"title":"cache_lru","content":"# 4.5 如何避免预读失效和缓存污染的问题？\n\n大家好，我是小林。\n\n上周群里看到有位小伙伴面试时，被问到这两个问题：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/操作系统/缓存/提问.png)\n\n咋一看，以为是在问操作系统的问题，其实这两个题目都是在问**如何改进 LRU 算法**。\n\n因为传统的 LRU 算法存在这两个问题：\n\n- **「预读失效」导致缓存命中率下降（对应第一个题目）**\n- **「缓存污染」导致缓存命中率下降（对应第二个题目）**\n\nRedis 的缓存淘汰算法则是通过**实现 LFU 算法**来避免「缓存污染」而导致缓存命中率下降的问题（Redis 没有预读机制）。\n\nMySQL 和 Linux 操作系统是通过**改进 LRU 算法**来避免「预读失效和缓存污染」而导致缓存命中率下降的问题。\n\n这次，就重点讲讲 **MySQL 和 Linux 操作系统是如何改进 LRU 算法的？**\n\n好了，开始发车，坐稳了！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/操作系统/缓存/缓存污染提纲.png)\n\n## Linux 和 MySQL 的缓存\n\n### Linux 操作系统的缓存\n\n在应用程序读取文件的数据的时候，Linux 操作系统是会对读取的文件数据进行缓存的，会缓存在文件系统中的 **Page Cache**（如下图中的页缓存）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png)\n\nPage Cache 属于内存空间里的数据，由于内存访问比磁盘访问快很多，在下一次访问相同的数据就不需要通过磁盘 I/O 了，命中缓存就直接返回数据即可。\n\n因此，Page Cache 起到了加速访问数据的作用。\n\n### MySQL 的缓存\n\nMySQL 的数据是存储在磁盘里的，为了提升数据库的读写性能，Innodb 存储引擎设计了一个**缓冲池**（Buffer Pool），Buffer Pool 属于内存空间里的数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/%E7%BC%93%E5%86%B2%E6%B1%A0.drawio.png)\n\n有了缓冲池后：\n\n- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。\n- 当修改数据时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，最后由后台线程将脏页写入到磁盘。\n\n## 传统 LRU 是如何管理内存数据的？\n\nLinux 的 Page Cache 和  MySQL 的 Buffer Pool 的大小是有限的，并不能无限的缓存数据，对于一些频繁访问的数据我们希望可以一直留在内存中，而一些很少访问的数据希望可以在某些时机可以淘汰掉，从而保证内存不会因为满了而导致无法再缓存新的数据，同时还能保证常用数据留在内存中。\n\n要实现这个，最容易想到的就是 LRU（Least recently used）算法。\n\nLRU 算法一般是用「链表」作为数据结构来实现的，链表头部的数据是最近使用的，而链表末尾的数据是最久没被使用的。那么，当空间不够了，就淘汰最久没被使用的节点，也就是链表末尾的数据，从而腾出内存空间。\n\n因为 Linux 的 Page Cache 和  MySQL 的 Buffer Pool 缓存的**基本数据单位都是页（Page）单位**，所以**后续以「页」名称代替「数据」**。\n\n传统的 LRU 算法的实现思路是这样的：\n\n- 当访问的页在内存里，就直接把该页对应的 LRU 链表节点移动到链表的头部。\n- 当访问的页不在内存里，除了要把该页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的页。\n\n比如下图，假设 LRU 链表长度为 5，LRU 链表从左到右有编号为 1，2，3，4，5 的页。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lru.png)\n\n如果访问了 3 号页，因为 3 号页已经在内存了，所以把 3 号页移动到链表头部即可，表示最近被访问了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lru2.png)\n\n而如果接下来，访问了 8 号页，因为 8 号页不在内存里，且 LRU 链表长度为 5，所以必须要淘汰数据，以腾出内存空间来缓存 8 号页，于是就会淘汰末尾的 5 号页，然后再将 8 号页加入到头部。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lru3.png)\n\n传统的 LRU 算法并没有被 Linux 和 MySQL 使用，因为传统的 LRU 算法无法避免下面这两个问题：\n\n- 预读失效导致缓存命中率下降；\n- 缓存污染导致缓存命中率下降；\n\n## 预读失效，怎么办？\n\n### 什么是预读机制？\n\nLinux 操作系统为基于 Page Cache 的读缓存机制提供**预读机制**，一个例子是：\n\n- 应用程序只想读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至少会读 0-4KB 的内容，这恰好可以在一个 page 中装下。\n- 但是操作系统出于空间局部性原理（靠近当前被访问数据的数据，在未来很大概率会被访问到），会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是额外在内存中申请了 3 个 page；\n\n下图代表了操作系统的预读机制：\n\n![](https://img-blog.csdnimg.cn/img_convert/ae8252378169c8c14b8b9907983f7d8b.png)\n\n上图中，应用程序利用 read 系统调动读取 4KB 数据，实际上内核使用预读机制（ReadaHead） 机制完成了 16KB 数据的读取，也就是通过一次磁盘顺序读将多个 Page 数据装入 Page Cache。\n\n这样下次读取 4KB 数据后面的数据的时候，就不用从磁盘读取了，直接在 Page Cache 即可命中数据。因此，预读机制带来的好处就是**减少了 磁盘 I/O 次数，提高系统磁盘 I/O 吞吐量**。\n\nMySQL Innodb 存储引擎的 Buffer Pool 也有类似的预读机制，MySQL 从磁盘加载页时，会提前把它相邻的页一并加载进来，目的是为了减少磁盘 IO。\n\n### 预读失效会带来什么问题？\n\n如果**这些被提前加载进来的页，并没有被访问**，相当于这个预读工作是白做了，这个就是**预读失效**。\n\n如果使用传统的 LRU 算法，就会把「预读页」放到 LRU 链表头部，而当内存空间不够的时候，还需要把末尾的页淘汰掉。\n\n如果这些「预读页」如果一直不会被访问到，就会出现一个很奇怪的问题，**不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是热点数据，这样就大大降低了缓存命中率** 。\n\n### 如何避免预读失效造成的影响？\n\n我们不能因为害怕预读失效，而将预读机制去掉，大部分情况下，空间局部性原理还是成立的。\n\n要避免预读失效带来影响，最好就是**让预读页停留在内存里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在内存里的时间尽可能长**。\n\n那到底怎么才能避免呢？\n\nLinux 操作系统和 MySQL Innodb 通过改进传统 LRU 链表来避免预读失效带来的影响，具体的改进分别如下：\n\n- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active_list）和非活跃 LRU 链表（inactive_list）**；\n- MySQL 的 Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**。\n\n这两个改进方式，设计思想都是类似的，**都是将数据分为了冷数据和热数据，然后分别进行 LRU 算法**。不再像传统的 LRU 算法那样，所有数据都只用一个 LRU 算法管理。\n\n接下来，具体聊聊 Linux 和 MySQL 是如何避免预读失效带来的影响？\n\n\u003e Linux 是如何避免预读失效带来的影响？\n\nLinux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active_list）和非活跃 LRU 链表（inactive_list）**。\n\n- **active list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；\n- **inactive list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；\n\n有了这两个 LRU 链表后，**预读页就只需要加入到 inactive list 区域的头部，当页被真正访问的时候，才将页插入 active list 的头部**。如果预读的页一直没有被访问，就会从 inactive list 移除，这样就不会影响 active list 中的热点数据。\n\n接下来，给大家举个例子。\n\n假设 active list 和 inactive list 的长度为 5，目前内存中已经有如下 10 个页：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/操作系统/缓存/active_inactive_list.drawio.png)\n\n现在有个编号为 20 的页被预读了，这个页只会被插入到 inactive list 的头部，而 inactive list 末尾的页（10号）会被淘汰掉。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/操作系统/缓存/active_inactive_list1.drawio.png)\n\n**即使编号为 20 的预读页一直不会被访问，它也没有占用到  active list 的位置**，而且还会比 active list 中的页更早被淘汰出去。\n\n如果 20 号页被预读后，立刻被访问了，那么就会将它插入到  active list 的头部， active list 末尾的页（5号），会被**降级**到 inactive list ，作为 inactive list 的头部，这个过程并不会有数据被淘汰。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/操作系统/缓存/active_inactive_list2.drawio.png)\n\n\u003e MySQL 是如何避免预读失效带来的影响？\n\nMySQL 的 Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域，**young 区域 和 old 区域**。\n\nyoung 区域在 LRU 链表的前半部分，old 区域则是在后半部分，这两个区域都有各自的头和尾节点，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/young%2Bold.png)\n\nyoung 区域与 old 区域在 LRU 链表中的占比关系并不是一比一的关系，而是是 7 比 3 （默认比例）的关系。\n\n**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。\n\n接下来，给大家举个例子。\n\n假设有一个长度为 10 的 LRU 链表，其中 young 区域占比 70 %，old 区域占比 30 %。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo.drawio.png)\n\n现在有个编号为 20 的页被预读了，这个页只会被插入到 old 区域头部，而 old 区域末尾的页（10号）会被淘汰掉。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo2.png)\n\n如果 20 号页一直不会被访问，它也没有占用到 young 区域的位置，而且还会比 young 区域的数据更早被淘汰出去。\n\n如果 20 号页被预读后，立刻被访问了，那么就会将它插入到 young 区域的头部，young 区域末尾的页（7号），会被挤到 old 区域，作为 old 区域的头部，这个过程并不会有页被淘汰。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lrutwo3.png)\n\n## 缓存污染，怎么办？\n\n### 什么是缓存污染？\n\n虽然 Linux （实现两个 LRU 链表）和 MySQL （划分两个区域）通过改进传统的 LRU 数据结构，避免了预读失效带来的影响。\n\n但是如果还是使用「只要数据被访问一次，就将数据加入到活跃 LRU 链表头部（或者 young 区域）」这种方式的话，那么**还存在缓存污染的问题**。\n\n当我们在批量读取数据的时候，由于数据被访问了一次，这些大量数据都会被加入到「活跃 LRU 链表」里，然后之前缓存在活跃 LRU 链表（或者 young 区域）里的热点数据全部都被淘汰了，**如果这些大量的数据在很长一段时间都不会被访问的话，那么整个活跃 LRU 链表（或者 young 区域）就被污染了**。\n\n### 缓存污染会带来什么问题？\n\n缓存污染带来的影响就是很致命的，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 I/O，系统性能就会急剧下降。\n\n我以 MySQL 举例子，Linux 发生缓存污染的现象也是类似。\n\n当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 I/O，MySQL 性能就会急剧下降。\n\n注意， 缓存污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成缓存污染。\n\n比如，在一个数据量非常大的表，执行了这条语句：\n\n```sql\nselect * from t_user where name like \"%xiaolin%\";\n```\n\n可能这个查询出来的结果就几条记录，但是由于这条语句会发生索引失效，所以这个查询过程是全表扫描的，接着会发生如下的过程：\n\n- 从磁盘读到的页加入到 LRU 链表的 old 区域头部；\n- 当从页里读取行记录时，也就是**页被访问的时候，就要将该页放到 young 区域头部**；\n- 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；\n- 如此往复，直到扫描完表中的所有记录。\n\n经过这一番折腾，由于这条 SQL 语句访问的页非常多，每访问一个页，都会将其加入 young 区域头部，那么**原本 young 区域的热点数据都会被替换掉，导致缓存命中率下降**。那些在批量扫描时，而被加入到 young 区域的页，如果在很长一段时间都不会再被访问的话，那么就污染了 young 区域。\n\n举个例子，假设需要批量扫描：21，22，23，24，25 这五个页，这些页都会被逐一访问（读取页里的记录）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lruthree.drawio.png)\n\n在批量访问这些页的时候，会被逐一插入到 young 区域头部。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/lruthree1.png)\n\n可以看到，原本在 young 区域的 6 和 7 号页都被淘汰了，而批量扫描的页基本占满了 young 区域，如果这些页在很长一段时间都不会被访问，那么就对 young 区域造成了污染。\n\n如果 6 和 7 号页是热点数据，那么在被淘汰后，后续有 SQL 再次读取  6 和 7 号页时，由于缓存未命中，就要从磁盘中读取了，降低了 MySQL 的性能，这就是缓存污染带来的影响。\n\n### 怎么避免缓存污染造成的影响？\n\n前面的 LRU 算法只要数据被访问一次，就将数据加入活跃 LRU 链表（或者 young 区域），**这种 LRU 算法进入活跃 LRU 链表的门槛太低了**！正式因为门槛太低，才导致在发生缓存污染的时候，很容就将原本在活跃 LRU 链表里的热点数据淘汰了。\n\n所以，**只要我们提高进入到活跃 LRU 链表（或者 young 区域）的门槛，就能有效地保证活跃 LRU 链表（或者 young 区域）里的热点数据不会被轻易替换掉**。\n\nLinux 操作系统和 MySQL Innodb 存储引擎分别是这样提高门槛的：\n\n- **Linux 操作系统**：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。\n- **MySQL Innodb**：在内存页被访问**第二次**的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行**停留在 old 区域的时间判断**：\n  - 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；\n  - 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；\n\n提高了进入活跃 LRU 链表（或者 young 区域）的门槛后，就很好了避免缓存污染带来的影响。\n\n在批量读取数据时候，**如果这些大量数据只会被访问一次，那么它们就不会进入到活跃 LRU 链表（或者 young 区域）**，也就不会把热点数据淘汰，只会待在非活跃 LRU 链表（或者 old 区域）中，后续很快也会被淘汰。\n\n## 总结\n\n传统的 LRU 算法法无法避免下面这两个问题：\n\n- 预读失效导致缓存命中率下降；\n- 缓存污染导致缓存命中率下降；\n\n为了避免「预读失效」造成的影响，Linux 和 MySQL 对传统的 LRU 链表做了改进：\n\n- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active list）和非活跃 LRU 链表（inactive list）**。\n- MySQL  Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**。\n\n但是如果还是使用「只要数据被访问一次，就将数据加入到活跃 LRU 链表头部（或者 young 区域）」这种方式的话，那么**还存在缓存污染的问题**。\n\n为了避免「缓存污染」造成的影响，Linux 操作系统和 MySQL Innodb 存储引擎分别提高了升级为热点数据的门槛：\n\n- Linux 操作系统：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。\n- MySQL Innodb：在内存页被访问**第二次**的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行**停留在 old 区域的时间判断**：\n  - 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；\n  - 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；\n\n通过提高了进入 active list  （或者 young 区域）的门槛后，就很好了避免缓存污染带来的影响。\n\n完！\n\n------\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/3_memory/malloc":{"title":"malloc","content":"# 4.2 malloc 是如何分配内存的？\n\n大家好，我是小林。\n\n这次我们就以 malloc 动态内存分配为切入点，我在文中也做了小实验：\n\n- malloc 是如何分配内存的？\n- malloc 分配的是物理内存吗？\n- malloc(1)  会分配多大的内存？\n- free 释放内存，会归还给操作系统吗？\n- free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？\n\n发车！\n\n## Linux 进程的内存分布长什么样？\n\n在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/1db038e1d2e5325b05e2bb80475d962a.png)\n\n\n\n通过这里可以看出：\n\n- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；\n- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。\n\n再来说说，内核空间与用户空间的区别：\n\n- 进程在用户态时，只能访问用户空间内存；\n- 只有进入内核态后，才可以访问内核空间的内存；\n\n虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c88bda5db60029f3ea57e4306e7da936.png)\n\n\n\n接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。\n\n我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：\n\n通过这张图你可以看到，用户空间内存从**低到高**分别是 6 种不同的内存段：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/7b5b6b3728acde8df019350df3cb85c1.png)\n\n\n\n- 程序文件段，包括二进制可执行代码；\n- 已初始化数据段，包括静态常量；\n- 未初始化数据段，包括未初始化的静态变量；\n- 堆段，包括动态分配的内存，从低地址开始向上增长；\n- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关 ）；\n- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；\n\n在这 6 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。\n\n## malloc 是如何分配内存的？\n\n实际上，malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。\n\nmalloc 申请内存的时候，会有两种方式向操作系统申请堆内存。\n\n- 方式一：通过 brk() 系统调用从堆分配内存\n- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；\n\n方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/brk申请.png)\n\n方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/mmap申请.png)\n\n\u003e 什么场景下 malloc()  会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？\n\nmalloc() 源码里默认定义了一个阈值：\n\n- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；\n- 如果用户分配的内存大于 128 KB，则通过 mmap()  申请内存；\n\n注意，不同的 glibc 版本定义的阈值也是不同的。\n\n## malloc()  分配的是物理内存吗？\n\n不是的，**malloc() 分配的是虚拟内存**。\n\n如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。\n\n只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。\n\n## malloc(1)  会分配多大的虚拟内存？\n\nmalloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。\n\n具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系，我们就以 malloc 默认的内存管理器（Ptmalloc2）来分析。\n\n接下里，我们做个实验，用下面这个代码，通过 malloc 申请 1 字节的内存时，看看操作系统实际分配了多大的内存空间。\n\n```c\n#include \u003cstdio.h\u003e\n#include \u003cmalloc.h\u003e\n\nint main() {\n  printf(\"使用cat /proc/%d/maps查看内存分配\\n\",getpid());\n  \n  //申请1字节的内存\n  void *addr = malloc(1);\n  printf(\"此1字节的内存起始地址：%x\\n\", addr);\n  printf(\"使用cat /proc/%d/maps查看内存分配\\n\",getpid());\n \n  //将程序阻塞，当输入任意字符时才往下执行\n  getchar();\n\n  //释放内存\n  free(addr);\n  printf(\"释放了1字节的内存，但heap堆并不会释放\\n\");\n  \n  getchar();\n  return 0;\n}\n```\n\n执行代码（**先提前说明，我使用的 glibc 库的版本是 2.17**）：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/080ee187c8c92db45092b6688774e8da.png)\n\n我们可以通过 /proc//maps 文件查看进程的内存分布情况。我在 maps 文件通过此 1 字节的内存起始地址过滤出了内存地址的范围。\n\n```shell\n[root@xiaolin ~]# cat /proc/3191/maps | grep d730\n00d73000-00d94000 rw-p 00000000 00:00 0                                  [heap]\n```\n\n这个例子分配的内存小于 128 KB，所以是通过 brk() 系统调用向堆空间申请的内存，因此可以看到最右边有 [heap] 的标识。\n\n可以看到，堆空间的内存地址范围是 00d73000-00d94000，这个范围大小是 132KB，也就说明了 **malloc(1) 实际上预分配 132K 字节的内存**。\n\n可能有的同学注意到了，程序里打印的内存起始地址是 `d73010`，而 maps 文件显示堆内存空间的起始地址是 `d73000`，为什么会多出来 `0x10` （16字节）呢？这个问题，我们先放着，后面会说。\n\n## free 释放内存，会归还给操作系统吗？\n\n我们在上面的进程往下执行，看看通过 free() 函数释放内存后，堆内存还在吗？\n\n![图片](https://img-blog.csdnimg.cn/img_convert/1a9337f8f6b83fbc186f257511b5ce67.png)\n\n从下图可以看到，通过 free 释放内存后，堆内存还是存在的，并没有归还给操作系统。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/2b8f63892830553ec04c5f05f336ae8b.png)\n\n这是因为与其把这 1 字节释放给操作系统，不如先缓存着放进 malloc 的内存池里，当进程再次申请 1 字节的内存时就可以直接复用，这样速度快了很多。\n\n当然，当进程退出后，操作系统就会回收进程的所有资源。\n\n上面说的 free 内存后堆内存还存在，是针对 malloc 通过  brk() 方式申请的内存的情况。\n\n如果 malloc 通过 mmap 方式申请的内存，free 释放内存后就会归归还给操作系统。\n\n我们做个实验验证下， 通过 malloc 申请 128 KB 字节的内存，来使得 malloc 通过 mmap 方式来分配内存。\n\n```c\n#include \u003cstdio.h\u003e\n#include \u003cmalloc.h\u003e\n\nint main() {\n  //申请1字节的内存\n  void *addr = malloc(128*1024);\n  printf(\"此128KB字节的内存起始地址：%x\\n\", addr);\n  printf(\"使用cat /proc/%d/maps查看内存分配\\n\",getpid());\n\n  //将程序阻塞，当输入任意字符时才往下执行\n  getchar();\n\n  //释放内存\n  free(addr);\n  printf(\"释放了128KB字节的内存，内存也归还给了操作系统\\n\");\n\n  getchar();\n  return 0;\n}\n```\n\n执行代码：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/500fdc021d956f60963f308760f511d0.png)\n\n查看进程的内存的分布情况，可以发现最右边没有 [head] 标志，说明是通过 mmap 以匿名映射的方式从文件映射区分配的匿名内存。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/501f458b8d35abe5e378a0f14c667797.png)\n\n然后我们释放掉这个内存看看：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/fcdbe91cc03b6a2f6e93dd1971d1b438.png)\n\n再次查看该 128 KB 内存的起始地址，可以发现已经不存在了，说明归还给了操作系统。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/3f63c56b131d92806b5aabca29d33a38.png)\n\n对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」这个问题，我们可以做个总结了：\n\n- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；\n- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。\n\n## 为什么不全部使用 mmap 来分配内存？\n\n因为向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间。\n\n所以，申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。\n\n另外，因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次  mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。\n\n也就是说，**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。\n\n为了改进这两个问题，malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。\n\n**等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**。\n\n## 既然 brk 那么牛逼，为什么不全部使用 brk 来分配？\n\n前面我们提到通过 brk 从堆空间分配的内存，并不会归还给操作系统，那么我们那考虑这样一个场景。\n\n如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/75edee0cb75450e7987a8a482b975bda.png)\n\n但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。\n\n因此，随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。\n\n所以，malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。\n\n## free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？\n\n还记得，我前面提到， malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节吗？\n\n这个多出来的 16 字节就是保存了该内存块的描述信息，比如有该内存块的大小。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/cb6e3ce4532ff0a6bfd60fe3e52a806e.png)\n\n这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。\n\n---\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/3_memory/mem_reclaim":{"title":"mem_reclaim","content":"# 4.3 内存满了，会发生什么？\n\n大家好，我是小林。\n\n前几天有位读者留言说，面腾讯时，被问了两个内存管理的问题：\n\n![](https://img-blog.csdnimg.cn/cbe38428e4e644dd81ab5e85545cacf7.png)\n\n![](https://img-blog.csdnimg.cn/90a7216d65b4454ba185db2a2d6c2b8a.png)\n\n先来说说第一个问题：虚拟内存有什么作用？\n\n- 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。\n- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。\n- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。\n\n然后今天主要是聊聊第二个问题，「**系统内存紧张时，会发生什么？**」\n\n发车！\n\n![](https://img-blog.csdnimg.cn/e069da38c4b54ee98a585a176e2c342f.png)\n\n## 内存分配的过程是怎样的？\n\n应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。\n\n当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生**缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。\n\n缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。\n\n如果没有空闲的物理内存，那么内核就会开始进行**回收内存**的工作，回收的方式主要是两种：直接内存回收和后台内存回收。\n\n- **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。\n- **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。\n\n如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——**触发 OOM （Out of Memory）机制**。\n\nOOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。\n\n申请物理内存的过程如下图：\n\n![](https://img-blog.csdnimg.cn/2f61b0822b3c4a359f99770231981b07.png)\n\n## 哪些内存可以被回收？\n\n系统内存紧张的时候，就会进行回收内存的工作，那具体哪些内存是可以被回收的呢？\n\n主要有两类内存可以被回收，而且它们的回收方式也不同。\n\n- **文件页**（File-backed Page）：内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，**回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存**。\n- **匿名页**（Anonymous Page）：这部分内存没有实际载体，不像文件缓存有硬盘文件这样一个载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们**回收的方式是通过 Linux 的 Swap 机制**，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。\n\n文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：\n\n- **active_list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；\n- **inactive_list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；\n\n越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。\n\n活跃和非活跃的内存页，按照类型的不同，又分别分为文件页和匿名页。可以从 /proc/meminfo 中，查询它们的大小，比如：\n\n```shell\n# grep表示只保留包含active的指标（忽略大小写）\n# sort表示按照字母顺序排序\n[root@xiaolin ~]# cat /proc/meminfo | grep -i active | sort\nActive:           901456 kB\nActive(anon):     227252 kB\nActive(file):     674204 kB\nInactive:         226232 kB\nInactive(anon):    41948 kB\nInactive(file):   184284 kB\n```\n\n## 回收内存带来的性能影响\n\n在前面我们知道了回收内存有两种方式。\n\n- 一种是后台内存回收，也就是唤醒 kswapd 内核线程，这种方式是异步回收的，不会阻塞进程。\n- 一种是直接内存回收，这种方式是同步回收的，会阻塞进程，这样就会造成很长时间的延迟，以及系统的 CPU 利用率会升高，最终引起系统负荷飙高。\n\n可被回收的内存类型有文件页和匿名页：\n\n- 文件页的回收：对于干净页是直接释放内存，这个操作不会影响性能，而对于脏页会先写回到磁盘再释放内存，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。\n- 匿名页的回收：如果开启了 Swap 机制，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会影响系统性能的。\n\n可以看到，回收内存的操作基本都会发生磁盘 I/O 的，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能，整个系统给人的感觉就是很卡。\n\n下面针对回收内存导致的性能影响，说说常见的解决方式。\n\n### 调整文件页和匿名页的回收倾向\n\n从文件页和匿名页的回收操作来看，文件页的回收操作对系统的影响相比匿名页的回收操作会少一点，因为文件页对于干净页回收是不会发生磁盘 I/O 的，而匿名页的 Swap 换入换出这两个操作都会发生磁盘 I/O。\n\nLinux 提供了一个 `/proc/sys/vm/swappiness` 选项，用来调整文件页和匿名页的回收倾向。\n\nswappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。\n\n```shell\n[root@xiaolin ~]# cat /proc/sys/vm/swappiness\n0\n```\n\n一般建议 swappiness 设置为 0（默认值是 60），这样在回收内存的时候，会更倾向于文件页的回收，但是并不代表不会回收匿名页。\n\n### 尽早触发 kswapd 内核线程异步回收内存\n\n\u003e 如何查看系统的直接内存回收和后台内存回收的指标？\n\n我们可以使用 `sar -B 1` 命令来观察：\n\n![](https://img-blog.csdnimg.cn/8acb6b28d0fc4858bd57be147d087def.png)\n\n图中红色框住的就是后台内存回收和直接内存回收的指标，它们分别表示：\n\n- pgscank/s : kswapd(后台回收线程) 每秒扫描的 page 个数。\n- pgscand/s: 应用程序在内存申请过程中每秒直接扫描的 page 个数。\n- pgsteal/s: 扫描的 page 中每秒被回收的个数（pgscank+pgscand）。\n\n如果系统时不时发生抖动，并且在抖动的时间段里如果通过 sar -B 观察到 pgscand 数值很大，那大概率是因为「直接内存回收」导致的。\n\n针对这个问题，解决的办法就是，可以通过尽早的触发「后台内存回收」来避免应用程序进行直接内存回收。\n\n\u003e 什么条件下才能触发 kswapd 内核线程回收内存呢？\n\n内核定义了三个内存阈值（watermark，也称为水位），用来衡量当前剩余内存（pages_free）是否充裕或者紧张，分别是：\n\n- 页最小阈值（pages_min）；\n- 页低阈值（pages_low）；\n- 页高阈值（pages_high）；\n\n这三个内存阈值会划分为四种内存使用情况，如下图：\n\n![](https://img-blog.csdnimg.cn/166bc9f5b7c545d89f1e36ab8dd772cf.png)\n\nkswapd 会定期扫描内存的使用情况，根据剩余内存（pages_free）的情况来进行内存回收的工作。\n\n- 图中绿色部分：如果剩余内存（pages_free）大于 页高阈值（pages_high），说明剩余内存是充足的；\n\n- 图中蓝色部分：如果剩余内存（pages_free）在页高阈值（pages_high）和页低阈值（pages_low）之间，说明内存有一定压力，但还可以满足应用程序申请内存的请求；\n- 图中橙色部分：如果剩余内存（pages_free）在页低阈值（pages_low）和页最小阈值（pages_min）之间，说明内存压力比较大，剩余内存不多了。**这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值（pages_high）为止**。虽然会触发内存回收，但是不会阻塞应用程序，因为两者关系是异步的。\n- 图中红色部分：如果剩余内存（pages_free）小于页最小阈值（pages_min），说明用户可用内存都耗尽了，此时就会**触发直接内存回收**，这时应用程序就会被阻塞，因为两者关系是同步的。\n\n可以看到，当剩余内存页（pages_free）小于页低阈值（pages_low），就会触发 kswapd 进行后台回收，然后 kswapd 会一直回收到剩余内存页（pages_free）大于页高阈值（pages_high）。\n\n也就是说 kswapd 的活动空间只有 pages_low 与 pages_min 之间的这段区域，如果剩余内存低于了 pages_min 会触发直接内存回收，高于了 pages_high 又不会唤醒 kswapd。\n\n页低阈值（pages_low）可以通过内核选项  `/proc/sys/vm/min_free_kbytes` （该参数代表系统所保留空闲内存的最低限）来间接设置。\n\nmin_free_kbytes 虽然设置的是页最小阈值（pages_min），但是页高阈值（pages_high）和页低阈值（pages_low）都是根据页最小阈值（pages_min）计算生成的，它们之间的计算关系如下：\n\n```\npages_min = min_free_kbytes\npages_low = pages_min*5/4\npages_high = pages_min*3/2\n```\n\n如果系统时不时发生抖动，并且通过 sar -B 观察到 pgscand 数值很大，那大概率是因为直接内存回收导致的，这时可以增大 min_free_kbytes 这个配置选项来及早地触发后台回收，然后继续观察 pgscand 是否会降为 0。\n\n增大了 min_free_kbytes 配置后，这会使得系统预留过多的空闲内存，从而在一定程度上降低了应用程序可使用的内存量，这在一定程度上浪费了内存。极端情况下设置 min_free_kbytes 接近实际物理内存大小时，留给应用程序的内存就会太少而可能会频繁地导致 OOM 的发生。\n\n所以在调整 min_free_kbytes 之前，需要先思考一下，应用程序更加关注什么，如果关注延迟那就适当地增大 min_free_kbytes，如果关注内存的使用量那就适当地调小 min_free_kbytes。\n\n### NUMA 架构下的内存回收策略\n\n\u003e 什么是 NUMA 架构？\n\n再说 NUMA 架构前，先给大家说说 SMP 架构，这两个架构都是针对 CPU 的。\n\n SMP 指的是一种**多个 CPU 处理器共享资源的电脑硬件架构**，也就是说每个 CPU 地位平等，它们共享相同的物理资源，包括总线、内存、IO、操作系统等。每个 CPU 访问内存所用时间都是相同的，因此，这种系统也被称为一致存储访问结构（UMA，Uniform Memory Access）。\n\n随着 CPU 处理器核数的增多，多个 CPU 都通过一个总线访问内存，这样总线的带宽压力会越来越大，同时每个 CPU 可用带宽会减少，这也就是 SMP 架构的问题。\n\n![SMP 与 NUMA 架构](https://img-blog.csdnimg.cn/img_convert/feec409868070d8cd79aecad2895b531.png)\n\n为了解决 SMP 架构的问题，就研制出了 NUMA 结构，即非一致存储访问结构（Non-uniform memory access，NUMA）。\n\nNUMA 架构将每个 CPU  进行了分组，每一组 CPU 用 Node 来表示，一个 Node 可能包含多个 CPU 。\n\n**每个 Node 有自己独立的资源，包括内存、IO 等**，每个 Node 之间可以通过互联模块总线（QPI）进行通信，所以，也就意味着每个 Node 上的 CPU 都可以访问到整个系统中的所有内存。但是，访问远端 Node 的内存比访问本地内存要耗时很多。\n\n\u003e NUMA 架构跟回收内存有什么关系？\n\n在 NUMA 架构下，当某个 Node 内存不足时，系统可以从其他 Node 寻找空闲内存，也可以从本地内存中回收内存。\n\n具体选哪种模式，可以通过 /proc/sys/vm/zone_reclaim_mode 来控制。它支持以下几个选项：\n\n- 0 （默认值）：在回收本地内存之前，在其他 Node 寻找空闲内存；\n- 1：只回收本地内存；\n- 2：只回收本地内存，在本地回收内存时，可以将文件页中的脏页写回硬盘，以回收内存。\n- 4：只回收本地内存，在本地回收内存时，可以用 swap 方式回收内存。\n\n在使用 NUMA 架构的服务器，如果系统出现还有一半内存的时候，却发现系统频繁触发「直接内存回收」，导致了影响了系统性能，那么大概率是因为 zone_reclaim_mode 没有设置为 0 ，导致当本地内存不足的时候，只选择回收本地内存的方式，而不去使用其他 Node 的空闲内存。\n\n虽然说访问远端 Node 的内存比访问本地内存要耗时很多，但是相比内存回收的危害而言，访问远端 Node 的内存带来的性能影响还是比较小的。因此，zone_reclaim_mode 一般建议设置为 0。\n\n## 如何保护一个进程不被 OOM 杀掉呢？\n\n在系统空闲内存不足的情况，进程申请了一个很大的内存，如果直接内存回收都无法回收出足够大的空闲内存，那么就会触发 OOM 机制，内核就会根据算法选择一个进程杀掉。\n\nLinux 到底是根据什么标准来选择被杀的进程呢？这就要提到一个在 Linux 内核里有一个 `oom_badness()` 函数，它会把系统中可以被杀掉的进程扫描一遍，并对每个进程打分，得分最高的进程就会被首先杀掉。\n\n进程得分的结果受下面这两个方面影响：\n\n- 第一，进程已经使用的物理内存页面数。\n- 第二，每个进程的 OOM 校准值 oom_score_adj。它是可以通过 `/proc/[pid]/oom_score_adj` 来配置的。我们可以在设置 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。\n\n函数 oom_badness() 里的最终计算方法是这样的：\n\n```c\n// points 代表打分的结果\n// process_pages 代表进程已经使用的物理内存页面数\n// oom_score_adj 代表 OOM 校准值\n// totalpages 代表系统总的可用页面数\npoints = process_pages + oom_score_adj*totalpages/1000\n```\n\n**用「系统总的可用页面数」乘以 「OOM 校准值 oom_score_adj」再除以 1000，最后再加上进程已经使用的物理页面数，计算出来的值越大，那么这个进程被 OOM Kill 的几率也就越大**。\n\n每个进程的 oom_score_adj 默认值都为 0，所以最终得分跟进程自身消耗的内存有关，消耗的内存越大越容易被杀掉。我们可以通过调整 oom_score_adj 的数值，来改成进程的得分结果：\n\n- 如果你不想某个进程被首先杀掉，那你可以调整该进程的 oom_score_adj，从而改变这个进程的得分结果，降低该进程被 OOM 杀死的概率。\n- 如果你想某个进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000。\n\n我们最好将一些很重要的系统服务的 oom_score_adj 配置为 -1000，比如 sshd，因为这些系统服务一旦被杀掉，我们就很难再登陆进系统了。\n\n但是，不建议将我们自己的业务程序的 oom_score_adj 设置为 -1000，因为业务程序一旦发生了内存泄漏，而它又不能被杀掉，这就会导致随着它的内存开销变大，OOM killer 不停地被唤醒，从而把其他进程一个个给杀掉。\n\n参考资料：\n\n- https://time.geekbang.org/column/article/277358\n- https://time.geekbang.org/column/article/75797\n- https://www.jianshu.com/p/e40e8813842f\n\n## 总结\n\n内核在给应用程序分配物理内存的时候，如果空闲物理内存不够，那么就会进行内存回收的工作，主要有两种方式：\n\n- 后台内存回收：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程异步的，不会阻塞进程的执行。\n- 直接内存回收：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是同步的，会阻塞进程的执行。\n\n可被回收的内存类型有文件页和匿名页：\n\n- 文件页的回收：对于干净页是直接释放内存，这个操作不会影响性能，而对于脏页会先写回到磁盘再释放内存，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。\n- 匿名页的回收：如果开启了 Swap 机制，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会影响系统性能的。\n\n文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。回收内存的操作基本都会发生磁盘 I/O 的，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能。\n\n针对回收内存导致的性能影响，常见的解决方式。\n\n- 设置 /proc/sys/vm/swappiness，调整文件页和匿名页的回收倾向，尽量倾向于回收文件页；\n- 设置 /proc/sys/vm/min_free_kbytes，调整 kswapd 内核线程异步回收内存的时机；\n- 设置  /proc/sys/vm/zone_reclaim_mode，调整 NUMA 架构下内存回收策略，建议设置为 0，这样在回收本地内存之前，会在其他 Node 寻找空闲内存，从而避免在系统还有很多空闲内存的情况下，因本地 Node 的本地内存不足，发生频繁直接内存回收导致性能下降的问题；\n\n在经历完直接内存回收后，空闲的物理内存大小依然不够，那么就会触发 OOM 机制，OOM killer 就会根据每个进程的内存占用情况和 oom_score_adj 的值进行打分，得分最高的进程就会被首先杀掉。\n\n我们可以通过调整进程的 /proc/[pid]/oom_score_adj 值，来降低被 OOM killer 杀掉的概率。\n\n完！\n\n---\n\n新的图解文章都在公众号首发，别忘记关注了哦！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/3_memory/vmem":{"title":"vmem","content":"# 4.1 为什么要有虚拟内存？\n\n本篇跟大家说说**内存管理**，内存管理还是比较重要的一个环节，理解了它，至少对整个操作系统的工作会有一个初步的轮廓，这也难怪面试的时候常问内存管理。\n\n干就完事，本文的提纲：\n\n![](https://img-blog.csdnimg.cn/970ec527d1c1417eab0d3246e77405f9.png)\n\n\n\n\n## 虚拟内存\n\n如果你是电子相关专业的，肯定在大学里捣鼓过单片机。\n\n单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。\n\n另外，**单片机的 CPU 是直接操作内存的「物理地址」**。\n\n![](https://img-blog.csdnimg.cn/019f1f0d2d30469cbda2b8fe2cf5e622.png)\n\n\n\n在这种情况下，要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。\n\n\u003e 操作系统是如何解决这个问题呢？\n\n这里关键的问题是这两个程序都引用了绝对物理地址，而这正是我们最需要避免的。\n\n我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。\n\n![进程的中间层](https://img-blog.csdnimg.cn/img_convert/298fb68e3da94d767b02f2ed81ebf2c4.png)\n\n**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**\n\n如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。\n\n于是，这里就引出了两种地址的概念：\n\n- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）\n- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。\n\n操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示：\n\n![](https://img-blog.csdnimg.cn/72ab76ba697e470b8ceb14d5fc5688d9.png)\n\n\n\u003e 操作系统是如何管理虚拟地址与物理地址之间的关系？\n\n主要有两种方式，分别是**内存分段和内存分页**，分段是比较早提出的，我们先来看看内存分段。\n\n\n\n## 内存分段\n\n\n程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（*Segmentation*）的形式把这些段分离出来。**\n\n\u003e 分段机制下，虚拟地址和物理地址是如何映射的？\n\n分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。\n\n![](https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png)\n\n段选择因子和段内偏移量：\n\n- **段选择因子**就保存在段寄存器里面。段选择因子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。\n\n- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。\n\n在上面，知道了虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图：\n\n![](https://img-blog.csdnimg.cn/c5e2ab63e6ee4c8db575f3c7c9c85962.png)\n\n\n\n如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。\n\n分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：\n\n- 第一个就是**内存碎片**的问题。\n- 第二个就是**内存交换的效率低**的问题。\n\n接下来，说说为什么会有这两个问题。\n\n\u003e 我们先来看看，分段为什么会产生内存碎片的问题？\n\n\n我们来看看这样一个例子。假设有 1G 的物理内存，用户执行了多个程序，其中：\n\n- 游戏占用了 512MB 内存\n- 浏览器占用了 128MB 内存\n- 音乐占用了 256 MB 内存。\n\n这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。\n\n如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。\n\n![](https://img-blog.csdnimg.cn/6142bc3c917e4a6298bdb62936e0d332.png)\n\n\u003e 内存分段会出现内存碎片吗？\n\n内存碎片主要分为，内部内存碎片和外部内存碎片。\n\n内存分段管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部内存碎片**。\n\n但是由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部内存碎片**的问题。\n\n解决「外部内存碎片」的问题就是**内存交换**。\n\n可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。\n\n这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。\n\n\u003e 再来看看，分段为什么会导致内存交换效率低的问题？\n\n对于多进程的系统来说，用分段的方式，外部内存碎片是很容易产生的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。\n\n因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。\n\n所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**\n\n为了解决内存分段的「外部内存碎片和内存交换效率低」的问题，就出现了内存分页。\n\n## 内存分页\n\n分段的好处就是能产生连续的内存空间，但是会出现「外部内存碎片和内存交换的空间太大」的问题。\n\n要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。\n\n**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。\n\n虚拟地址与物理地址之间通过**页表**来映射，如下图：\n\n![](https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png)\n\n\n页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。\n\n\n而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。\n\n\u003e 分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？\n\n内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而**采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。**\n\n但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。\n\n如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**\n\n![](https://img-blog.csdnimg.cn/388a29f45fe947e5a49240e4eff13538.png)\n\n\n更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**\n\n\n\u003e 分页机制下，虚拟地址和物理地址是如何映射的？\n\n在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。\n\n![](https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png)\n\n总结一下，对于一个内存地址转换，其实就是这样三个步骤：\n\n- 把虚拟内存地址，切分成页号和偏移量；\n- 根据页号，从页表里面，查询对应的物理页号；\n- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。\n\n下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：\n\n![](https://img-blog.csdnimg.cn/8f187878c809414ca2486b0b71e8880e.png)\n\n\n这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。\n\n\u003e 简单的分页有什么缺陷吗？\n\n有空间上的缺陷。\n\n因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。\n\n在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 `4MB` 的内存来存储页表。\n\n这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。\n\n那么，`100` 个进程的话，就需要 `400MB` 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。\n\n\n### 多级页表\n\n要解决上面的问题，就需要采用一种叫作**多级页表**（*Multi-Level Page Table*）的解决方案。\n\n在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 `4KB` 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。\n\n我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。如下图所示：\n\n\n![](https://img-blog.csdnimg.cn/19296e249b2240c29f9c52be70f611d5.png)\n\n\u003e 你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？\n\n当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。\n\n其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的**局部性原理**么？\n\n每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。\n\n如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= `0.804MB`，这对比单级页表的 `4MB` 是不是一个巨大的节约？\n\n那么为什么不分级的页表就做不到这样节约内存呢？\n\n我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。\n\n我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。\n\n对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：\n\n- 全局页目录项 PGD（*Page Global Directory*）；\n- 上层页目录项 PUD（*Page Upper Directory*）；\n- 中间页目录项 PMD（*Page Middle Directory*）；\n- 页表项 PTE（*Page Table Entry*）；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/内存管理/四级分页.png)\n\n\n### TLB\n\n多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。\n\n程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。\n\n![](https://img-blog.csdnimg.cn/edce58534d9342ff89f5261b1929c754.png)\n\n\n我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（*Translation Lookaside Buffer*） ，通常称为页表缓存、转址旁路缓存、快表等。\n\n![](https://img-blog.csdnimg.cn/a3cdf27646b24614a64cfc5d7ccffa35.png)\n\n在 CPU 芯片里面，封装了内存管理单元（*Memory Management Unit*）芯片，它用来完成地址转换和 TLB 的访问与交互。\n\n有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。\n\nTLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。\n\n\n\n## 段页式内存管理\n\n内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为**段页式内存管理**。\n\n\n![](https://img-blog.csdnimg.cn/f19ebd6f70f84083b0d87cc5e9dea8e3.png)\n\n\n\n段页式内存管理实现的方式：\n\n- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；\n- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；\n\n这样，地址结构就由**段号、段内页号和页内位移**三部分组成。\n\n\n用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：\n\n![](https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png)\n\n\n\n段页式地址变换中要得到物理地址须经过三次内存访问：\n\n- 第一次访问段表，得到页表起始地址；\n- 第二次访问页表，得到物理页号；\n- 第三次将物理页号与页内位移组合，得到物理地址。\n\n可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。\n\n\n## Linux 内存管理\n\n那么，Linux 操作系统采用了哪种方式来管理内存呢？\n\n\u003e 在回答这个问题前，我们得先看看 Intel 处理器的发展历史。\n\n早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。\n\n但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，**页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。**\n\n由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。\n\n![](https://img-blog.csdnimg.cn/bc0aaaf379fc4bc8882efd94b9052b64.png)\n\n这里说明下逻辑地址和线性地址：\n\n- 程序所使用的地址，通常是没被段式内存管理映射的地址，称为逻辑地址；\n- 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址；\n\n逻辑地址是「段式内存管理」转换前的地址，线性地址则是「页式内存管理」转换前的地址。\n\n\u003e 了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采用了什么方式管理内存？\n\n**Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制**。\n\n这主要是上面 Intel 处理器发展历史导致的，因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。\n\n但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作用。也就是说，“上有政策，下有对策”，若惹不起就躲着走。\n\n\n**Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。**\n\n\u003e 我们再来瞧一瞧，Linux 的虚拟地址空间是如何分布的？\n\n在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示：\n\n![](https://img-blog.csdnimg.cn/3a6cb4e3f27241d3b09b4766bb0b1124.png)\n\n通过这里可以看出：\n\n- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；\n- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。\n\n再来说说，内核空间与用户空间的区别：\n\n- 进程在用户态时，只能访问用户空间内存；\n- 只有进入内核态后，才可以访问内核空间的内存；\n\n虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。\n\n![](https://img-blog.csdnimg.cn/48403193b7354e618bf336892886bcff.png)\n\n\n接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。\n\n我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系：\n\n\n![虚拟内存空间划分](https://img-blog.csdnimg.cn/img_convert/b4f882b9447760ce5321de109276ec23.png)\n\n通过这张图你可以看到，用户空间内存，从**低到高**分别是 6 种不同的内存段：\n\n- 程序文件段（.text），包括二进制可执行代码；\n- 已初始化数据段（.data），包括静态常量；\n- 未初始化数据段（.bss），包括未初始化的静态变量；\n- 堆段，包括动态分配的内存，从低地址开始向上增长；\n- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关](http://lishiwen4.github.io/linux/linux-process-memory-location)）；\n- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；\n\n在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。\n\n## 总结\n\n为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套**虚拟地址空间**，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。作为程序，也不用关心物理地址的事情。\n\n每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。\n\n那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。\n\n那么对于虚拟地址与物理地址的映射关系，可以有**分段**和**分页**的方式，同时两者结合都是可以的。\n\n内存分段是根据程序的逻辑角度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是一块连续的空间。但是每个段的大小都不是统一的，这就会导致外部内存碎片和内存交换效率低的问题。\n\n于是，就出现了内存分页，把虚拟空间和物理空间分成大小固定的页，如在 Linux 系统中，每一页的大小为 `4KB`。由于分了页后，就不会产生细小的内存碎片，解决了内存分段的外部内存碎片问题。同时在内存交换的时候，写入硬盘也就一个页或几个页，这就大大提高了内存交换的效率。\n\n再来，为了解决简单分页产生的页表过大的问题，就有了**多级页表**，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加大了时间上的开销。于是根据程序的**局部性原理**，在 CPU 芯片中加入了 **TLB**，负责缓存最近常被访问的页表项，大大提高了地址的转换速度。\n\n**Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理**。于是 Linux 就把所有段的基地址设为 `0`，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。\n\n另外，Linux 系统中虚拟空间分布可分为**用户态**和**内核态**两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。\n\n\u003e 最后，说下虚拟内存有什么作用？\n\n- 第一，虚拟内存可以使得进程的运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。\n- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。\n- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。\n\n---\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/create_thread_max":{"title":"create_thread_max","content":"# 5.6 一个进程最多可以创建多少个线程？\n\n大家好，我是小林。\n\n昨天有位读者问了我这么个问题：\n\n![](https://img-blog.csdnimg.cn/20210715092002563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n![](https://img-blog.csdnimg.cn/20210715092015507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n大致意思就是，他看了一个面经，说虚拟内存是 2G 大小，然后他看了我的图解系统 PDF 里说虚拟内存是 4G，然后他就懵逼了。\n\n其实他看这个面经很有问题，没有说明是什么操作系统，以及是多少位操作系统。\n\n因为不同的操作系统和不同位数的操作系统，虚拟内存可能是不一样多。\n\nWindows 系统我不了解，我就说说 Linux 系统。 \n\n\n在 Linux 操作系统中，虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同位数的系统，地址 空间的范围也不同。比如最常⻅的 32 位和 64 位系统，如下所示:\n\n![](https://img-blog.csdnimg.cn/20210715092026648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n通过这里可以看出:\n- 32 位系统的内核空间占用 1G ，位于最高处，剩下的 3G 是用户空间;\n- 64 位系统的内核空间和用户空间都是 128T ，分别占据整个内存空间的最高和最低处，剩下的中\n  间部分是未定义的。\n\n---\n\n接着，来看看读者那个面经题目：**一个进程最多可以创建多少个线程？**\n\n这个问题跟两个东西有关系：\n- **进程的虚拟内存空间上限**，因为创建一个线程，操作系统需要为其分配一个栈空间，如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多。\n- **系统参数限制**，虽然 Linux 并没有内核参数来控制单个进程创建的最大线程个数，但是有系统级别的参数来控制整个系统的最大线程个数。\n\n\n我们先看看，在进程里创建一个线程需要消耗多少虚拟内存大小？\n\n我们可以执行 ulimit -a 这条命令，查看进程创建线程时默认分配的栈空间大小，比如我这台服务器默认分配给线程的栈空间大小为 8M。\n\n![](https://img-blog.csdnimg.cn/20210715092041211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n在前面我们知道，在 32 位 Linux 系统里，一个进程的虚拟空间是 4G，内核分走了1G，**留给用户用的只有 3G**。\n\n那么假设创建一个线程需要占用 10M 虚拟内存，总共有 3G 虚拟内存可以使用。于是我们可以算出，最多可以创建差不多 300 个（3G/10M）左右的线程。\n\n\n如果你想自己做个实验，你可以找台 32 位的 Linux 系统运行下面这个代码：\n\n![](https://img-blog.csdnimg.cn/20210715092052531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n由于我手上没有 32 位的系统，我这里贴一个网上别人做的测试结果：\n\n![](https://img-blog.csdnimg.cn/202107150921005.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n\n如果想使得进程创建上千个线程，那么我们可以调整创建线程时分配的栈空间大小，比如调整为 512k：\n\n```\n$ ulimit -s 512\n```\n\n----\n\n说完 32 位系统的情况，我们来看看 64 位系统里，一个进程能创建多少线程呢？\n\n我的测试服务器的配置：\n- 64 位系统；\n- 2G 物理内存；\n- 单核 CPU。\n\n64 位系统意味着用户空间的虚拟内存最大值是 128T，这个数值是很大的，如果按创建一个线程需占用 10M 栈空间的情况来算，那么理论上可以创建 128T/10M 个线程，也就是 1000多万个线程，有点魔幻！\n\n所以按 64 位系统的虚拟内存大小，理论上可以创建无数个线程。\n\n事实上，肯定创建不了那么多线程，除了虚拟内存的限制，还有系统的限制。\n\n比如下面这三个内核参数的大小，都会影响创建线程的上限：\n- ***/proc/sys/kernel/threads-max***，表示系统支持的最大线程数，默认值是 `14553`；\n-  ***/proc/sys/kernel/pid_max***，表示系统全局的 PID 号数值的限制，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败，默认值是 `32768`；\n-  ***/proc/sys/vm/max_map_count***，表示限制一个进程可以拥有的VMA(虚拟内存区域)的数量，具体什么意思我也没搞清楚，反正如果它的值很小，也会导致创建线程失败，默认值是 `65530`。\n\n\n\n那接下针对我的测试服务器的配置，看下一个进程最多能创建多少个线程呢？\n\n我在这台服务器跑了前面的程序，其结果如下：\n\n![](https://img-blog.csdnimg.cn/20210715092109740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n可以看到，创建了 14374 个线程后，就无法在创建了，而且报错是因为资源的限制。\n\n前面我提到的 `threads-max` 内核参数，它是限制系统里最大线程数，默认值是 14553。\n\n\n\n我们可以运行那个测试线程数的程序后，看下当前系统的线程数是多少，可以通过 `top -H` 查看。\n\n![](https://img-blog.csdnimg.cn/20210715092125376.png)\n\n\n左上角的 Threads 的数量显示是 14553，与 `threads-max` 内核参数的值相同，所以我们可以认为是因为这个参数导致无法继续创建线程。\n\n那么，我们可以把 threads-max 参数设置成 `99999`:\n\n\n```\necho 99999 \u003e /proc/sys/kernel/threads-max\n```\n\n设置完 threads-max 参数后，我们重新跑测试线程数的程序，运行后结果如下图：\n\n![](https://img-blog.csdnimg.cn/20210715092138115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n可以看到，当进程创建了 32326 个线程后，就无法继续创建里，且报错是无法继续申请内存。\n\n此时的上限个数很接近 `pid_max` 内核参数的默认值（32768），那么我们可以尝试将这个参数设置为 99999：\n\n\n```\necho 99999 \u003e /proc/sys/kernel/pid_max\n```\n\n设置完 pid_max 参数后，继续跑测试线程数的程序，运行后结果创建线程的个数还是一样卡在了 32768 了。\n\n当时我也挺疑惑的，明明 pid_max 已经调整大后，为什么线程个数还是上不去呢？\n\n后面经过查阅资料发现，`max_map_count` 这个内核参数也是需要调大的，但是它的数值与最大线程数之间有什么关系，我也不太明白，只是知道它的值是会限制创建线程个数的上限。\n\n然后，我把 max_map_count 内核参数也设置成后 99999：\n\n```\necho 99999 \u003e /proc/sys/kernel/max_map_count \n```\n\n继续跑测试线程数的程序，结果如下图：\n\n![](https://img-blog.csdnimg.cn/20210715092151214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n当创建差不多 5 万个线程后，我的服务器就卡住不动了，CPU 都已经被占满了，毕竟这个是单核 CPU，所以现在是 CPU 的瓶颈了。\n\n我只有这台服务器，如果你们有性能更强的服务器来测试的话，有兴趣的小伙伴可以去测试下。\n\n接下来，我们换个思路测试下，把创建线程时分配的栈空间调大，比如调大为 100M，在大就会创建线程失败。\n\n```\nulimit -s 1024000\n```\n\n设置完后，跑测试线程的程序，其结果如下：\n\n![](https://img-blog.csdnimg.cn/20210715092207662.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n总共创建了 26390 个线程，然后就无法继续创建了，而且该进程的虚拟内存空间已经高达 25T，要知道这台服务器的物理内存才 2G。\n\n为什么物理内存只有 2G，进程的虚拟内存却可以使用 25T 呢？\n\n因为虚拟内存并不是全部都映射到物理内存的，程序是有局部性的特性，也就是某一个时间只会执行部分代码，所以只需要映射这部分程序就好。\n\n你可以从上面那个 top 的截图看到，虽然进程虚拟空间很大，但是物理内存（RES）只有使用了 400 多M。\n\n\n好了，简单总结下：\n- 32 位系统，用户态的虚拟空间只有 3G，如果创建线程时分配的栈空间是 10M，那么一个进程最多只能创建 300 个左右的线程。\n- 64 位系统，用户态的虚拟空间大到有 128T，理论上不会受虚拟内存大小的限制，而会受系统的参数或性能限制。\n\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/deadlock":{"title":"deadlock","content":"# 5.4 怎么避免死锁？\n\n面试过程中，死锁也是高频的考点，因为如果线上环境真多发生了死锁，那真的出大事了。\n\n这次，我们就来系统地聊聊死锁的问题。\n\n- 死锁的概念；\n- 模拟死锁问题的产生；\n- 利用工具排查死锁问题；\n- 避免死锁问题的发生；\n\n---\n\n## 死锁的概念\n\n在多线程编程中，我们为了防止多线程竞争共享资源而导致数据错乱，都会在操作共享资源之前加上互斥锁，只有成功获得到锁的线程，才能操作共享资源，获取不到锁的线程就只能等待，直到锁被释放。\n\n那么，当两个线程为了保护两个不同的共享资源而使用了两个互斥锁，那么这两个互斥锁应用不当的时候，可能会造成**两个线程都在等待对方释放锁**，在没有外力的作用下，这些线程会一直相互等待，就没办法继续运行，这种情况就是发生了**死锁**。\n\n举个例子，小林拿了小美房间的钥匙，而小林在自己的房间里，小美拿了小林房间的钥匙，而小美也在自己的房间里。如果小林要从自己的房间里出去，必须拿到小美手中的钥匙，但是小美要出去，又必须拿到小林手中的钥匙，这就形成了死锁。\n\n死锁只有**同时满足**以下四个条件才会发生：\n\n- 互斥条件；\n- 持有并等待条件；\n- 不可剥夺条件；\n- 环路等待条件；\n\n\n### 互斥条件\n\n互斥条件是指**多个线程不能同时使用同一个资源**。\n\n比如下图，如果线程 A 已经持有的资源，不能再同时被线程 B 持有，如果线程 B 请求获取线程 A 已经占用的资源，那线程 B 只能等待，直到线程 A 释放了资源。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/操作系统/死锁/互斥条件.png)\n\n### 持有并等待条件\n\n持有并等待条件是指，当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 C 持有了，所以线程  A 就会处于等待状态，但是**线程  A 在等待资源 2 的同时并不会释放自己已经持有的资源 1**。 \n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/操作系统/死锁/持有并等待条件.png)\n\n\n\n### 不可剥夺条件\n\n不可剥夺条件是指，当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**，线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/操作系统/死锁/不可剥夺条件.png)\n\n### 环路等待条件\n\n环路等待条件指的是，在死锁发生的时候，**两个线程获取资源的顺序构成了环形链**。\n\n比如，线程 A 已经持有资源 2，而想请求资源 1， 线程 B 已经获取了资源 1，而想请求资源 2，这就形成资源请求等待的环形图。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/操作系统/死锁/环路等待条件.png)\n\n\n---\n\n## 模拟死锁问题的产生\n\nTalk is cheap. Show me the code.\n\n下面，我们用代码来模拟死锁问题的产生。\n\n首先，我们先创建 2 个线程，分别为线程 A 和 线程 B，然后有两个互斥锁，分别是 mutex_A 和 mutex_B，代码如下：\n\n```c\npthread_mutex_t mutex_A = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_t mutex_B = PTHREAD_MUTEX_INITIALIZER;\n\nint main()\n{\n    pthread_t tidA, tidB;\n    \n    //创建两个线程\n    pthread_create(\u0026tidA, NULL, threadA_proc, NULL);\n    pthread_create(\u0026tidB, NULL, threadB_proc, NULL);\n    \n    pthread_join(tidA, NULL);\n    pthread_join(tidB, NULL);\n    \n    printf(\"exit\\n\");\n    \n    return 0;\n}\n```\n\n接下来，我们看下线程 A 函数做了什么。\n\n\n```c\n//线程函数 A\nvoid *threadA_proc(void *data)\n{\n    printf(\"thread A waiting get ResourceA \\n\");\n    pthread_mutex_lock(\u0026mutex_A);\n    printf(\"thread A got ResourceA \\n\");\n    \n    sleep(1);\n    \n    printf(\"thread A waiting get ResourceB \\n\");\n    pthread_mutex_lock(\u0026mutex_B);\n    printf(\"thread A got ResourceB \\n\");\n\n    pthread_mutex_unlock(\u0026mutex_B);\n    pthread_mutex_unlock(\u0026mutex_A);\n    return (void *)0;\n}\n```\n\n可以看到，线程 A 函数的过程：\n\n- 先获取互斥锁 A，然后睡眠 1 秒；\n- 再获取互斥锁 B，然后释放互斥锁 B；\n- 最后释放互斥锁 A；\n\n```c\n//线程函数 B\nvoid *threadB_proc(void *data)\n{\n    printf(\"thread B waiting get ResourceB \\n\");\n    pthread_mutex_lock(\u0026mutex_B);\n    printf(\"thread B got ResourceB \\n\");\n    \n    sleep(1);\n    \n    printf(\"thread B waiting  get ResourceA \\n\");\n    pthread_mutex_lock(\u0026mutex_A);\n    printf(\"thread B got ResourceA \\n\");\n    \n    pthread_mutex_unlock(\u0026mutex_A);\n    pthread_mutex_unlock(\u0026mutex_B);\n    return (void *)0;\n}\n```\n\n可以看到，线程 B 函数的过程：\n\n- 先获取互斥锁 B，然后睡眠 1 秒；\n- 再获取互斥锁 A，然后释放互斥锁 A；\n- 最后释放互斥锁 B；\n\n然后，我们运行这个程序，运行结果如下：\n\n```shell\nthread B waiting get ResourceB \nthread B got ResourceB \nthread A waiting get ResourceA \nthread A got ResourceA \nthread B waiting get ResourceA \nthread A waiting get ResourceB \n// 阻塞中。。。\n```\n\n可以看到线程 B 在等待互斥锁 A 的释放，线程 A 在等待互斥锁 B 的释放，双方都在等待对方资源的释放，很明显，产生了死锁问题。\n\n---\n\n## 利用工具排查死锁问题\n\n如果你想排查你的 Java 程序是否死锁，则可以使用 `jstack` 工具，它是 jdk 自带的线程堆栈分析工具。\n\n由于小林的死锁代码例子是 C 写的，在 Linux 下，我们可以使用 `pstack` + `gdb` 工具来定位死锁问题。\n\npstack 命令可以显示每个线程的栈跟踪信息（函数调用过程），它的使用方式也很简单，只需要 `pstack \u003cpid\u003e` 就可以了。\n\n那么，在定位死锁问题时，我们可以多次执行 pstack 命令查看线程的函数调用过程，多次对比结果，确认哪几个线程一直没有变化，且是因为在等待锁，那么大概率是由于死锁问题导致的。\n\n我用 pstack 输出了我前面模拟死锁问题的进程的所有线程的情况，我多次执行命令后，其结果都一样，如下：\n\n```shell\n$ pstack 87746\nThread 3 (Thread 0x7f60a610a700 (LWP 87747)):\n#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0\n#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0\n#3  0x0000000000400725 in threadA_proc ()\n#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0\n#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6\nThread 2 (Thread 0x7f60a5709700 (LWP 87748)):\n#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0\n#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0\n#3  0x0000000000400792 in threadB_proc ()\n#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0\n#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6\nThread 1 (Thread 0x7f60a610c700 (LWP 87746)):\n#0  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0\n#1  0x0000000000400806 in main ()\n\n....\n\n$ pstack 87746\nThread 3 (Thread 0x7f60a610a700 (LWP 87747)):\n#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0\n#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0\n#3  0x0000000000400725 in threadA_proc ()\n#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0\n#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6\nThread 2 (Thread 0x7f60a5709700 (LWP 87748)):\n#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0\n#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0\n#3  0x0000000000400792 in threadB_proc ()\n#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0\n#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6\nThread 1 (Thread 0x7f60a610c700 (LWP 87746)):\n#0  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0\n#1  0x0000000000400806 in main ()\n```\n\n可以看到，Thread 2 和 Thread 3 一直阻塞获取锁（*pthread_mutex_lock*）的过程，而且 pstack 多次输出信息都没有变化，那么可能大概率发生了死锁。\n\n但是，还不能够确认这两个线程是在互相等待对方的锁的释放，因为我们看不到它们是等在哪个锁对象，于是我们可以使用 gdb 工具进一步确认。\n\n整个 gdb 调试过程，如下：\n\n```shell\n// gdb 命令\n$ gdb -p 87746\n\n// 打印所有的线程信息\n(gdb) info thread\n  3 Thread 0x7f60a610a700 (LWP 87747)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n  2 Thread 0x7f60a5709700 (LWP 87748)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n* 1 Thread 0x7f60a610c700 (LWP 87746)  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0\n//最左边的 * 表示 gdb 锁定的线程，切换到第二个线程去查看\n\n// 切换到第2个线程\n(gdb) thread 2\n[Switching to thread 2 (Thread 0x7f60a5709700 (LWP 87748))]#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0 \n\n// bt 可以打印函数堆栈，却无法看到函数参数，跟 pstack 命令一样 \n(gdb) bt\n#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0\n#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0\n#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0\n#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25\n#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0\n#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6\n\n// 打印第三帧信息，每次函数调用都会有压栈的过程，而 frame 则记录栈中的帧信息\n(gdb) frame 3\n#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25\n27    printf(\"thread B waiting get ResourceA \\n\");\n28    pthread_mutex_lock(\u0026mutex_A);\n\n// 打印mutex_A的值 ,  __owner表示gdb中标示线程的值，即LWP\n(gdb) p mutex_A\n$1 = {__data = {__lock = 2, __count = 0, __owner = 87747, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, \n  __size = \"\\002\\000\\000\\000\\000\\000\\000\\000\\303V\\001\\000\\001\", '\\000' \u003crepeats 26 times\u003e, __align = 2}\n\n// 打印mutex_B的值 ,  __owner表示gdb中标示线程的值，即LWP\n(gdb) p mutex_B\n$2 = {__data = {__lock = 2, __count = 0, __owner = 87748, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, \n  __size = \"\\002\\000\\000\\000\\000\\000\\000\\000\\304V\\001\\000\\001\", '\\000' \u003crepeats 26 times\u003e, __align = 2}  \n```\n\n我来解释下，上面的调试过程：\n\n1. 通过 `info thread` 打印了所有的线程信息，可以看到有 3 个线程，一个是主线程（LWP 87746），另外两个都是我们自己创建的线程（LWP 87747 和 87748）；\n2. 通过 `thread 2`，将切换到第 2 个线程（LWP 87748）；\n3. 通过 `bt`，打印线程的调用栈信息，可以看到有 threadB_proc 函数，说明这个是线程 B 函数，也就说 LWP 87748 是线程 B;\n4. 通过 `frame 3`，打印调用栈中的第三个帧的信息，可以看到线程 B 函数，在获取互斥锁 A 的时候阻塞了；\n5. 通过 `p mutex_A`，打印互斥锁 A 对象信息，可以看到它被 LWP 为 87747（线程 A） 的线程持有着；\n6. 通过 `p mutex_B`，打印互斥锁 B 对象信息，可以看到他被 LWP 为 87748 （线程 B） 的线程持有着；\n\n因为线程 B 在等待线程 A 所持有的 mutex_A, 而同时线程 A 又在等待线程 B 所拥有的mutex_B, 所以可以断定该程序发生了死锁。\n\n\n---\n\n## 避免死锁问题的发生\n\n前面我们提到，产生死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。\n\n那么避免死锁问题就只需要破环其中一个条件就可以，最常见的并且可行的就是**使用资源有序分配法，来破环环路等待条件**。\n\n那什么是资源有序分配法呢？\n\n线程 A 和 线程 B 获取资源的顺序要一样，当线程 A 是先尝试获取资源 A，然后尝试获取资源  B 的时候，线程 B 同样也是先尝试获取资源 A，然后尝试获取资源 B。也就是说，线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。\n\n我们使用资源有序分配法的方式来修改前面发生死锁的代码，我们可以不改动线程 A 的代码。\n\n我们先要清楚线程 A 获取资源的顺序，它是先获取互斥锁 A，然后获取互斥锁 B。\n\n所以我们只需将线程 B 改成以相同顺序的获取资源，就可以打破死锁了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/操作系统/死锁/资源有序分配.png)\n\n线程 B 函数改进后的代码如下：\n\n```c\n//线程 B 函数，同线程 A 一样，先获取互斥锁 A，然后获取互斥锁 B\nvoid *threadB_proc(void *data)\n{\n    printf(\"thread B waiting get ResourceA \\n\");\n    pthread_mutex_lock(\u0026mutex_A);\n    printf(\"thread B got ResourceA \\n\");\n    \n    sleep(1);\n    \n    printf(\"thread B waiting  get ResourceB \\n\");\n    pthread_mutex_lock(\u0026mutex_B);\n    printf(\"thread B got ResourceB \\n\");\n    \n    pthread_mutex_unlock(\u0026mutex_B);\n    pthread_mutex_unlock(\u0026mutex_A);\n    return (void *)0;\n}\n```\n\n执行结果如下，可以看，没有发生死锁。\n\n```shell\nthread B waiting get ResourceA \nthread B got ResourceA \nthread A waiting get ResourceA \nthread B waiting  get ResourceB \nthread B got ResourceB \nthread A got ResourceA \nthread A waiting get ResourceB \nthread A got ResourceB\nexit\n```\n\n---\n\n## 总结\n\n简单来说，死锁问题的产生是由两个或者以上线程并行执行的时候，争夺资源而互相等待造成的。\n\n死锁只有同时满足互斥、持有并等待、不可剥夺、环路等待这四个条件的时候才会发生。\n\n所以要避免死锁问题，就是要破坏其中一个条件即可，最常用的方法就是使用资源有序分配法来破坏环路等待条件。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/multithread_sync":{"title":"multithread_sync","content":"# 5.3 多线程冲突了怎么办？\n\n\u003e 先来看看虚构的小故事\n\n已经晚上 11 点了，程序员小明的双手还在键盘上飞舞着，眼神依然注视着的电脑屏幕。\n\n没办法这段时间公司业绩增长中，需求自然也多了起来，加班自然也少不了。\n\n天气变化莫测，这时窗外下起了蓬勃大雨，同时闪电轰鸣。\n\n但这一丝都没有影响到小明，始料未及，突然一道巨大的雷一闪而过，办公楼就这么停电了，随后整栋楼都在回荡着的小明那一声撕心裂肺的「卧槽」。\n\n此时，求小明的心里面积有多大？\n\n等小明心里平复后，突然肚子非常的痛，想上厕所，小明心想肯定是晚上吃的某堡王有问题。\n\n整栋楼都停了电，小明两眼一抹黑，啥都看不见，只能靠摸墙的方法，一步一步的来到了厕所门口。\n\n到了厕所（**共享资源**），由于实在太急，小明直接冲入了厕所里，用手摸索着刚好第一个门没锁门，便夺门而入。\n\n这就荒唐了，这个门里面正好小红在上着厕所，正好这个厕所门是坏了的，没办法锁门。\n\n黑暗中，小红虽然看不见，但靠着声音，发现自己面前的这扇门有动静，觉得不对劲，于是铆足了力气，用她穿着高跟鞋脚，用力地一脚踢了过去。\n\n小明很幸运，被踢中了「命根子」，撕心裂肺地喊出了一个字「痛」！\n\n故事说完了，扯了那么多，实际上是为了说明，**对于共享资源，如果没有上锁，在多线程的环境里，那么就可能会发生翻车现场。**\n\n接下来，用 `30+` 张图，带大家走进操作系统中避免多线程资源竞争的**互斥、同步**的方法。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/2-%E6%8F%90%E7%BA%B2.jpg)\n\n---\n\n## 竞争与协作\n\n在单核 CPU 系统里，为了实现多个程序同时运行的假象，操作系统通常以时间片调度的方式，让每个进程执行每次执行一个时间片，时间片用完了，就切换下一个进程运行，由于这个时间片的时间很短，于是就造成了「并发」的现象。\n\n\n![并发](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/3-%E5%B9%B6%E5%8F%91.jpg)\n\n另外，操作系统也为每个进程创建巨大、私有的虚拟内存的假象，这种地址空间的抽象让每个程序好像拥有自己的内存，而实际上操作系统在背后秘密地让多个地址空间「复用」物理内存或者磁盘。\n\n![虚拟内存管理-换入换出](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/4-%E5%86%85%E5%AD%98%E4%BA%A4%E6%8D%A2.jpg)\n\n\n如果一个程序只有一个执行流程，也代表它是单线程的。当然一个程序可以有多个执行流程，也就是所谓的多线程程序，线程是调度的基本单位，进程则是资源分配的基本单位。\n\n所以，线程之间是可以共享进程的资源，比如代码段、堆空间、数据段、打开的文件等资源，但每个线程都有自己独立的栈空间。\n\n![多线程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/5-%E5%A4%9A%E7%BA%BF%E7%A8%8B.jpg)\n\n那么问题就来了，多个线程如果竞争共享资源，如果不采取有效的措施，则会造成共享数据的混乱。\n\n我们做个小实验，创建两个线程，它们分别对共享变量 `i` 自增 `1` 执行 `10000` 次，如下代码（虽然说是 C++ 代码，但是没学过 C++ 的同学也是看到懂的）：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/互斥与同步/6-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AB%9E%E4%BA%89C%2B%2B%E4%BB%A3%E7%A0%81%E4%BE%8B%E5%AD%90.jpeg)\n\n按理来说，`i` 变量最后的值应该是 `20000`，但很不幸，并不是如此。我们对上面的程序执行一下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/7-%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C.jpg)\n\n运行了两次，发现出现了 `i` 值的结果是 `15173`，也会出现 `20000` 的 i 值结果。\n\n\n每次运行不但会产生错误，而且得到不同的结果。在计算机里是不能容忍的，虽然是小概率出现的错误，但是小概率事件它一定是会发生的，「墨菲定律」大家都懂吧。\n\n\u003e 为什么会发生这种情况？\n\n为了理解为什么会发生这种情况，我们必须了解编译器为更新计数器 `i` 变量生成的代码序列，也就是要了解汇编指令的执行顺序。\n\n在这个例子中，我们只是想给 `i` 加上数字 1，那么它对应的汇编指令执行过程是这样的：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/8-%E6%B1%87%E7%BC%96%E8%AF%AD%E5%8F%A5%E8%B5%8B%E5%80%BC%E8%BF%87%E7%A8%8B.jpg)\n\n可以发现，只是单纯给 `i` 加上数字 1，在 CPU 运行的时候，实际上要执行 `3` 条指令。\n\n设想我们的线程 1 进入这个代码区域，它将 i 的值（假设此时是 50 ）从内存加载到它的寄存器中，然后它向寄存器加 1，此时在寄存器中的 i 值是 51。\n\n现在，一件不幸的事情发生了：**时钟中断发生**。因此，操作系统将当前正在运行的线程的状态保存到线程的线程控制块 TCB。\n\n现在更糟的事情发生了，线程 2 被调度运行，并进入同一段代码。它也执行了第一条指令，从内存获取 i 值并将其放入到寄存器中，此时内存中 i 的值仍为 50，因此线程 2 寄存器中的 i 值也是 50。假设线程 2 执行接下来的两条指令，将寄存器中的 i 值 + 1，然后将寄存器中的 i 值保存到内存中，于是此时全局变量 i 值是 51。\n\n最后，又发生一次上下文切换，线程 1 恢复执行。还记得它已经执行了两条汇编指令，现在准备执行最后一条指令。回忆一下， 线程 1 寄存器中的 i 值是51，因此，执行最后一条指令后，将值保存到内存，全局变量 i 的值再次被设置为 51。\n\n简单来说，增加 i （值为 50 ）的代码被运行两次，按理来说，最后的 i 值应该是 52，但是由于**不可控的调度**，导致最后 i 值却是 51。\n\n针对上面线程 1 和线程 2 的执行过程，我画了一张流程图，会更明确一些：\n\n![蓝色表示线程 1 ，红色表示线程 2](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/9-%E6%B1%87%E7%BC%96%E8%AF%AD%E5%8F%A5-%E8%B5%8B%E5%80%BC%E8%BF%87%E7%A8%8B-%E7%AB%9E%E4%BA%89.jpg)\n\n### 互斥的概念\n\n上面展示的情况称为**竞争条件（*race condition*）**，当多线程相互竞争操作共享变量时，由于运气不好，即在执行过程中发生了上下文切换，我们得到了错误的结果，事实上，每次运行都可能得到不同的结果，因此输出的结果存在**不确定性（*indeterminate*）**。\n\n\n由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为**临界区（*critical section*），它是访问共享资源的代码片段，一定不能给多线程同时执行。**\n\n我们希望这段代码是**互斥（*mutualexclusion*）的，也就说保证一个线程在临界区执行时，其他线程应该被阻止进入临界区**，说白了，就是这段代码执行过程中，最多只能出现一个线程。\n\n\n![互斥](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/10-%E4%B8%B4%E7%95%8C%E5%8C%BA.jpg)\n\n另外，说一下互斥也并不是只针对多线程。在多进程竞争共享资源的时候，也同样是可以使用互斥的方式来避免资源竞争造成的资源混乱。\n\n### 同步的概念\n\n互斥解决了并发进程/线程对临界区的使用问题。这种基于临界区控制的交互作用是比较简单的，只要一个进程/线程进入了临界区，其他试图想进入临界区的进程/线程都会被阻塞着，直到第一个进程/线程离开了临界区。\n\n我们都知道在多线程里，每个线程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个线程能密切合作，以实现一个共同的任务。\n\n例子，线程 1 是负责读入数据的，而线程 2 是负责处理数据的，这两个线程是相互合作、相互依赖的。线程 2 在没有收到线程 1 的唤醒通知时，就会一直阻塞等待，当线程 1 读完数据需要把数据传给线程 2 时，线程 1 会唤醒线程 2，并把数据交给线程 2 处理。\n\n\n**所谓同步，就是并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步**。\n\n举个生活的同步例子，你肚子饿了想要吃饭，你叫妈妈早点做菜，妈妈听到后就开始做菜，但是在妈妈没有做完饭之前，你必须阻塞等待，等妈妈做完饭后，自然会通知你，接着你吃饭的事情就可以进行了。\n\n![吃饭与做菜的同步关系](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/11-%E5%90%83%E9%A5%AD%E5%90%8C%E6%AD%A5.jpg)\n\n注意，同步与互斥是两种不同的概念：\n\n- 同步就好比：「操作 A 应在操作 B 之前执行」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执行」等；\n- 互斥就好比：「操作 A 和操作 B 不能在同一时刻执行」；\n\n---\n\n## 互斥与同步的实现和使用\n\n在进程/线程并发执行的过程中，进程/线程之间存在协作的关系，例如有互斥、同步的关系。\n\n为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种：\n\n- *锁*：加锁、解锁操作；\n- *信号量*：P、V 操作；\n\n这两个都可以方便地实现进程/线程互斥，而信号量比锁的功能更强一些，它还可以方便地实现进程/线程同步。\n\n\n### 锁\n\n使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题。\n\n任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。\n\n![加锁-解锁](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/12-%E4%BA%92%E6%96%A5%E9%94%81.jpg)\n\n根据锁的实现不同，可以分为「忙等待锁」和「无忙等待锁」。\n\n\u003e 我们先来看看「忙等待锁」的实现\n\n在说明「忙等待锁」的实现之前，先介绍现代 CPU 体系结构提供的特殊**原子操作指令 —— 测试和置位（*Test-and-Set*）指令**。\n\n如果用 C 代码表示 Test-and-Set 指令，形式如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/13-TestAndSet.jpg)\n\n测试并设置指令做了下述事情:\n\n- 把 `old_ptr` 更新为 `new` 的新值\n- 返回 `old_ptr` 的旧值；\n\n当然，**关键是这些代码是原子执行**。因为既可以测试旧值，又可以设置新值，所以我们把这条指令叫作「测试并设置」。\n\n那什么是原子操作呢？**原子操作就是要么全部执行，要么都不执行，不能出现执行到一半的中间状态**\n\n我们可以运用 Test-and-Set 指令来实现「忙等待锁」，代码如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/14-%E8%87%AA%E6%97%8B%E9%94%81.jpg)\n\n我们来确保理解为什么这个锁能工作：\n\n- 第一个场景是，首先假设一个线程在运行，调用 `lock()`，没有其他线程持有锁，所以 `flag` 是 0。当调用 `TestAndSet(flag, 1)` 方法，返回 0，线程会跳出 while 循环，获取锁。同时也会原子的设置 flag 为1，标志锁已经被持有。当线程离开临界区，调用 `unlock()` 将 `flag` 清理为 0。\n\n\n- 第二种场景是，当某一个线程已经持有锁（即 `flag` 为1）。本线程调用 `lock()`，然后调用 `TestAndSet(flag, 1)`，这一次返回 1。只要另一个线程一直持有锁，`TestAndSet()` 会重复返回 1，本线程会一直**忙等**。当 `flag` 终于被改为 0，本线程会调用 `TestAndSet()`，返回 0 并且原子地设置为 1，从而获得锁，进入临界区。\n\n很明显，当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**自旋锁（*spin lock*）**。\n\n这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。\n    \n\n\n\u003e 再来看看「无等待锁」的实现\n\n\n无等待锁顾明思议就是获取不到锁的时候，不用自旋。\n\n既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/15-%E6%97%A0%E7%AD%89%E5%BE%85%E9%94%81.jpg)\n\n本次只是提出了两种简单锁的实现方式。当然，在具体操作系统实现中，会更复杂，但也离不开本例子两个基本元素。\n\n如果你想要对锁的更进一步理解，推荐大家可以看《操作系统导论》第 28 章锁的内容，这本书在「微信读书」就可以免费看。\n\n### 信号量\n\n信号量是操作系统提供的一种协调共享资源访问的方法。\n\n通常**信号量表示资源的数量**，对应的变量是一个整型（`sem`）变量。\n\n另外，还有**两个原子操作的系统调用函数来控制信号量的**，分别是：\n\n- *P 操作*：将 `sem` 减 `1`，相减后，如果 `sem \u003c 0`，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；\n- *V 操作*：将 `sem` 加 `1`，相加后，如果 `sem \u003c= 0`，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；\n\nP 操作是用在进入临界区之前，V 操作是用在离开临界区之后，这两个操作是必须成对出现的。\n\n举个类比，2 个资源的信号量，相当于 2 条火车轨道，PV 操作如下图过程：\n\n![信号量与火车轨道](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/16-%E7%81%AB%E8%BD%A6PV%E6%93%8D%E4%BD%9C.jpg)\n\n\n\u003e 操作系统是如何实现 PV 操作的呢？\n\n信号量数据结构与 PV 操作的算法描述如下图：\n\n![PV 操作的算法描述](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/17-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9FPV%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0.jpg)\n\nPV 操作的函数是由操作系统管理和实现的，所以操作系统已经使得执行 PV 函数时是具有原子性的。\n\n\u003e PV 操作如何使用的呢？\n\n信号量不仅可以实现临界区的互斥访问控制，还可以线程间的事件同步。\n\n我们先来说说如何使用**信号量实现临界区的互斥访问**。\n\n为每类共享资源设置一个信号量 `s`，其初值为 `1`，表示该临界资源未被占用。\n\n只要把进入临界区的操作置于 `P(s)` 和 `V(s)` 之间，即可实现进程/线程互斥：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/18-%E4%BA%92%E6%96%A5%E4%BF%A1%E5%8F%B7%E9%87%8F.jpg)\n\n\n此时，任何想进入临界区的线程，必先在互斥信号量上执行 P 操作，在完成对临界资源的访问后再执行 V 操作。由于互斥信号量的初始值为 1，故在第一个线程执行 P 操作后 s 值变为 0，表示临界资源为空闲，可分配给该线程，使之进入临界区。\n\n若此时又有第二个线程想进入临界区，也应先执行 P 操作，结果使 s 变为负值，这就意味着临界资源已被占用，因此，第二个线程被阻塞。\n\n并且，直到第一个线程执行 V 操作，释放临界资源而恢复 s 值为 0 后，才唤醒第二个线程，使之进入临界区，待它完成临界资源的访问后，又执行 V 操作，使 s 恢复到初始值 1。\n\n对于两个并发线程，互斥信号量的值仅取 1、0 和 -1 三个值，分别表示：\n\n- 如果互斥信号量为 1，表示没有线程进入临界区；\n- 如果互斥信号量为 0，表示有一个线程进入临界区；\n- 如果互斥信号量为 -1，表示一个线程进入临界区，另一个线程等待进入。\n\n通过互斥信号量的方式，就能保证临界区任何时刻只有一个线程在执行，就达到了互斥的效果。\n\n再来，我们说说如何使用**信号量实现事件同步**。\n\n同步的方式是设置一个信号量，其初值为 `0`。\n\n我们把前面的「吃饭-做饭」同步的例子，用代码的方式实现一下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/19-%E4%BA%92%E6%96%A5%E4%BF%A1%E5%8F%B7%E9%87%8F%E5%90%8C%E6%AD%A5%E5%AE%9E%E7%8E%B0-%E5%90%83%E9%A5%AD%E4%BE%8B%E5%AD%90.jpg)\n\n妈妈一开始询问儿子要不要做饭时，执行的是 `P(s1)` ，相当于询问儿子需不需要吃饭，由于 `s1` 初始值为 0，此时 `s1` 变成 -1，表明儿子不需要吃饭，所以妈妈线程就进入等待状态。\n\n当儿子肚子饿时，执行了 `V(s1)`，使得 `s1` 信号量从 -1 变成 0，表明此时儿子需要吃饭了，于是就唤醒了阻塞中的妈妈线程，妈妈线程就开始做饭。\n\n接着，儿子线程执行了 `P(s2)`，相当于询问妈妈饭做完了吗，由于 `s2` 初始值是 0，则此时 `s2` 变成 -1，说明妈妈还没做完饭，儿子线程就等待状态。\n\n最后，妈妈终于做完饭了，于是执行 `V(s2)`，`s2` 信号量从 -1 变回了 0，于是就唤醒等待中的儿子线程，唤醒后，儿子线程就可以进行吃饭了。\n\n\n### 生产者-消费者问题\n\n![生产者-消费者模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/20-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85.jpg)\n\n生产者-消费者问题描述：\n\n- **生产者**在生成数据后，放在一个缓冲区中；\n- **消费者**从缓冲区取出数据处理；\n- 任何时刻，**只能有一个**生产者或消费者可以访问缓冲区；\n\n我们对问题分析可以得出：\n\n- 任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，**需要互斥**；\n- 缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者**需要同步**。\n\n那么我们需要三个信号量，分别是：\n\n- 互斥信号量 `mutex`：用于互斥访问缓冲区，初始化值为 1；\n- 资源信号量 `fullBuffers`：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；\n- 资源信号量 `emptyBuffers`：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；\n\n具体的实现代码：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/21-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B.jpg)\n\n如果消费者线程一开始执行 `P(fullBuffers)`，由于信号量 `fullBuffers` 初始值为 0，则此时 `fullBuffers` 的值从 0 变为 -1，说明缓冲区里没有数据，消费者只能等待。\n\n接着，轮到生产者执行 `P(emptyBuffers)`，表示减少 1 个空槽，如果当前没有其他生产者线程在临界区执行代码，那么该生产者线程就可以把数据放到缓冲区，放完后，执行 `V(fullBuffers)` ，信号量 `fullBuffers` 从 -1 变成 0，表明有「消费者」线程正在阻塞等待数据，于是阻塞等待的消费者线程会被唤醒。\n\n消费者线程被唤醒后，如果此时没有其他消费者线程在读数据，那么就可以直接进入临界区，从缓冲区读取数据。最后，离开临界区后，把空槽的个数 + 1。 \n\n\n---\n\n## 经典同步问题\n\n### 哲学家就餐问题\n\n当初我在校招的时候，面试官也问过「哲学家就餐」这道题目，我当时听的一脸懵逼，无论面试官怎么讲述这个问题，我也始终没听懂，就莫名其妙的说这个问题会「死锁」。\n\n当然，我这回答槽透了，所以当场 game over，残酷又悲惨故事，就不多说了，反正当时菜就是菜。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/22-%E4%B8%8D%E9%9A%BE%E8%BF%87-%E8%A1%A8%E6%83%85.jpg)\n\n时至今日，看我来图解这道题。\n\n![哲学家就餐的问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/23-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90%E6%A8%A1%E5%9E%8B.jpg)\n\n先来看看哲学家就餐的问题描述：\n\n- `5` 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面；\n- 巧就巧在，这个桌子只有 `5` 支叉子，每两个哲学家之间放一支叉子；\n- 哲学家围在一起先思考，思考中途饿了就会想进餐；\n- **奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐**；\n- **吃完后，会把两支叉子放回原处，继续思考**；\n\n那么问题来了，如何保证哲    学家们的动作有序进行，而不会出现有人永远拿不到叉子呢？\n\n\u003e 方案一\n\n我们用信号量的方式，也就是 PV 操作来尝试解决它，代码如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/24-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg)\n\n上面的程序，好似很自然。拿起叉子用 P 操作，代表有叉子就直接用，没有叉子时就等待其他哲学家放回叉子。\n\n![方案一的问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/25-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%80%E9%97%AE%E9%A2%98.jpg)\n\n不过，这种解法存在一个极端的问题：**假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了，\n这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会在 `P(fork[(i + 1) % N ])` 这条语句阻塞了，很明显这发生了死锁的现象**。\n\n\u003e 方案二\n\n既然「方案一」会发生同时竞争左边叉子导致死锁的现象，那么我们就在拿叉子前，加个互斥信号量，代码如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/26-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg)\n\n上面程序中的互斥信号量的作用就在于，**只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。**\n\n\n![方案二的问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/27-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E9%97%AE%E9%A2%98.jpg)\n\n\n方案二虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。\n\n\u003e 方案三\n\n\n那既然方案二使用互斥信号量，会导致只能允许一个哲学家就餐，那么我们就不用它。\n\n另外，方案一的问题在于，会出现所有哲学家同时拿左边刀叉的可能性，那我们就避免哲学家可以同时拿左边的刀叉，采用分支结构，根据哲学家的编号的不同，而采取不同的动作。\n\n**即让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。**\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/28-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg)\n\n上面的程序，在 P 操作时，根据哲学家的编号不同，拿起左右两边叉子的顺序不同。另外，V 操作是不需要分支的，因为 V 操作是不会阻塞的。\n\n![方案三可解决问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/29-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89-%E5%9B%BE%E8%A7%A3.jpg)\n\n方案三即不会出现死锁，也可以两人同时进餐。\n\n\u003e 方案四\n\n在这里再提出另外一种可行的解决方案，我们**用一个数组 state 来记录每一位哲学家的三个状态，分别是在进餐状态、思考状态、饥饿状态（正在试图拿叉子）。**\n\n那么，**一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。**\n\n第 `i` 个哲学家的左邻右舍，则由宏 `LEFT` 和 `RIGHT` 定义：\n\n- *LEFT* : ( i + 5  - 1 ) % 5\n- *RIGHT* : ( i + 1 ) % 5\n\n比如 i 为 2，则 `LEFT` 为 1，`RIGHT` 为 3。\n\n具体代码实现如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/30-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E5%9B%9B%E7%A4%BA%E4%BE%8B.jpg)\n\n上面的程序使用了一个信号量数组，每个信号量对应一位哲学家，这样在所需的叉子被占用时，想进餐的哲学家就被阻塞。\n\n注意，每个进程/线程将 `smart_person` 函数作为主代码运行，而其他 `take_forks`、`put_forks` 和 `test` 只是普通的函数，而非单独的进程/线程。\n\n![方案四也可解决问题](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/31-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E5%9B%9B-%E5%9B%BE%E8%A7%A3.jpg)\n\n方案四同样不会出现死锁，也可以两人同时进餐。\n\n### 读者-写者问题\n\n前面的「哲学家进餐问题」对于互斥访问有限的竞争问题（如 I/O 设备）一类的建模过程十分有用。\n\n另外，还有个著名的问题是「读者-写者」，它为数据库访问建立了一个模型。\n\n读者只会读取数据，不会修改数据，而写者即可以读也可以修改数据。\n\n读者-写者的问题描述：\n\n- 「读-读」允许：同一时刻，允许多个读者同时读\n- 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写\n- 「写-写」互斥：没有其他写者时，写者才能写\n\n接下来，提出几个解决方案来分析分析。\n\n\u003e 方案一\n\n使用信号量的方式来尝试解决：\n\n- 信号量 `wMutex`：控制写操作的互斥信号量，初始值为 1 ；\n- 读者计数 `rCount`：正在进行读操作的读者个数，初始化为 0；\n- 信号量 `rCountMutex`：控制对 rCount 读者计数器的互斥修改，初始值为 1；\n\n\n接下来看看代码的实现：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/32-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg)\n\n上面的这种实现，是读者优先的策略，因为只要有读者正在读的状态，后来的读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态。\n\n\u003e 方案二\n\n那既然有读者优先策略，自然也有写者优先策略：\n\n- 只要有写者准备要写入，写者应尽快执行写操作，后来的读者就必须阻塞；\n- 如果有写者持续不断写入，则读者就处于饥饿；\n\n在方案一的基础上新增如下变量：\n\n- 信号量 `rMutex`：控制读者进入的互斥信号量，初始值为 1；\n- 信号量 `wDataMutex`：控制写者写操作的互斥信号量，初始值为 1；\n- 写者计数 `wCount`：记录写者数量，初始值为 0；\n- 信号量 `wCountMutex`：控制 wCount 互斥修改，初始值为 1；\n\n具体实现如下代码：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/33-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg)\n\n注意，这里 `rMutex` 的作用，开始有多个读者读数据，它们全部进入读者队列，此时来了一个写者，执行了 `P(rMutex)` 之后，后续的读者由于阻塞在 `rMutex` 上，都不能再进入读者队列，而写者到来，则可以全部进入写者队列，因此保证了写者优先。\n\n同时，第一个写者执行了 `P(rMutex)` 之后，也不能马上开始写，必须等到所有进入读者队列的读者都执行完读操作，通过 `V(wDataMutex)` 唤醒写者的写操作。 \n\n\u003e 方案三\n\n既然读者优先策略和写者优先策略都会造成饥饿的现象，那么我们就来实现一下公平策略。\n\n\n公平策略：\n\n- 优先级相同；\n- 写者、读者互斥访问；\n- 只能一个写者访问临界区；\n- 可以有多个读者同时访问临界资源；\n\n具体代码实现：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/34-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg)\n\n看完代码不知你是否有这样的疑问，为什么加了一个信号量 `flag`，就实现了公平竞争？\n\n对比方案一的读者优先策略，可以发现，读者优先中只要后续有读者到达，读者就可以进入读者队列， 而写者必须等待，直到没有读者到达。\n\n没有读者到达会导致读者队列为空，即 `rCount==0`，此时写者才可以进入临界区执行写操作。\n\n而这里 `flag` 的作用就是阻止读者的这种特殊权限（特殊权限是只要读者到达，就可以进入读者队列）。 \n\n比如：开始来了一些读者读数据，它们全部进入读者队列，此时来了一个写者，执行 `P(flag)` 操作，使得后续到来的读者都阻塞在 `flag` 上，不能进入读者队列，这会使得读者队列逐渐为空，即 `rCount` 减为 0。\n\n这个写者也不能立马开始写（因为此时读者队列不为空），会阻塞在信号量 `wDataMutex` 上，读者队列中的读者全部读取结束后，最后一个读者进程执行 `V(wDataMutex)`，唤醒刚才的写者，写者则继续开始进行写操作。 \n\n---\n\n## 关注作者\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/pessim_and_optimi_lock":{"title":"pessim_and_optimi_lock","content":"# 5.5 什么是悲观锁、乐观锁？\n\n生活中用到的锁，用途都比较简单粗暴，上锁基本是为了防止外人进来、电动车被偷等等。\n\n但生活中也不是没有 BUG 的，比如加锁的电动车在「广西 - 窃·格瓦拉」面前，锁就是形同虚设，只要他愿意，他就可以轻轻松松地把你电动车给「顺走」，不然打工怎么会是他这辈子不可能的事情呢？牛逼之人，必有牛逼之处。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/其他/窃格瓦拉.jpg)\n\n\n那在编程世界里，「锁」更是五花八门，多种多样，每种锁的加锁开销以及应用场景也可能会不同。\n\n如何用好锁，也是程序员的基本素养之一了。\n\n高并发的场景下，如果选对了合适的锁，则会大大提高系统的性能，否则性能会降低。\n\n所以，知道各种锁的开销，以及应用场景是很有必要的。\n\n接下来，就谈一谈常见的这几种锁：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/锁/锁之提供.png)\n\n\n---\n\n多线程访问共享资源的时候，避免不了资源竞争而导致数据错乱的问题，所以我们通常为了解决这一问题，都会在访问共享资源之前加锁。\n\n最常用的就是互斥锁，当然还有很多种不同的锁，比如自旋锁、读写锁、乐观锁等，不同种类的锁自然适用于不同的场景。\n\n如果选择了错误的锁，那么在一些高并发的场景下，可能会降低系统的性能，这样用户体验就会非常差了。\n\n所以，为了选择合适的锁，我们不仅需要清楚知道加锁的成本开销有多大，还需要分析业务场景中访问的共享资源的方式，再来还要考虑并发访问共享资源时的冲突概率。\n\n对症下药，才能减少锁对高并发性能的影响。\n\n那接下来，针对不同的应用场景，谈一谈「**互斥锁、自旋锁、读写锁、乐观锁、悲观锁**」的选择和使用。\n\n## 互斥锁与自旋锁\n\n最底层的两种就是会「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的，你可以认为它们是各种锁的地基，所以我们必须清楚它俩之间的区别和应用。\n\n加锁的目的就是保证共享资源在任意时间里，只有一个线程访问，这样就可以避免多线程导致共享数据错乱的问题。\n\n当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：\n\n- **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程；\n- **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；\n\n互斥锁是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，**既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞**。\n\n**对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/锁/互斥锁工作流程.png)\n\n所以，互斥锁加锁失败时，会从用户态陷入到内核态，让内核帮我们切换线程，虽然简化了使用锁的难度，但是存在一定的性能开销成本。\n\n那这个开销成本是什么呢？会有**两次线程上下文切换的成本**：\n\n- 当线程加锁失败时，内核会把线程的状态从「运行」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运行；\n- 接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把 CPU 切换给该线程运行。\n\n线程的上下文切换的是什么？当两个线程是属于同一个进程，**因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。**\n\n上下文切换的耗时有大佬统计过，大概在几十纳秒到几微秒之间，如果你锁住的代码执行时间比较短，那可能上下文切换的时间都比你锁住的代码执行时间还要长。\n\n所以，**如果你能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**\n\n自旋锁是通过 CPU 提供的 `CAS` 函数（*Compare And Swap*），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。\n\n一般加锁的过程，包含两个步骤：\n\n- 第一步，查看锁的状态，如果锁是空闲的，则执行第二步；\n- 第二步，将锁设置为当前线程持有；\n\nCAS 函数就把这两个步骤合并成一条硬件级指令，形成**原子指令**，这样就保证了这两个步骤是不可分割的，要么一次性执行完两个步骤，要么两个步骤都不执行。\n\n比如，设锁为变量 lock，整数 0 表示锁是空闲状态，整数 pid 表示线程 ID，那么 CAS(lock, 0, pid) 就表示自旋锁的加锁操作，CAS(lock, pid, 0) 则表示解锁操作。\n\n使用自旋锁的时候，当发生多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这里的「忙等待」可以用 `while` 循环等待实现，不过最好是使用 CPU 提供的 `PAUSE` 指令来实现「忙等待」，因为可以减少循环等待时的耗电量。\n\n自旋锁是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。**需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断一个线程，运行其他线程）。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。**\n\n自旋锁开销少，在多核系统下一般不会主动产生线程切换，适合异步、协程等在用户态切换请求的编程方式，但如果被锁住的代码执行时间过长，自旋的线程会长时间占用 CPU 资源，所以自旋的时间和被锁住的代码执行的时间是成「正比」的关系，我们需要清楚的知道这一点。\n\n自旋锁与互斥锁使用层面比较相似，但实现层面上完全不同：**当加锁失败时，互斥锁用「线程切换」来应对，自旋锁则用「忙等待」来应对**。\n\n它俩是锁的最基本处理方式，更高级的锁都会选择其中一个来实现，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现。\n\n---\n\n## 读写锁\n\n读写锁从字面意思我们也可以知道，它由「读锁」和「写锁」两部分构成，如果只读取共享资源用「读锁」加锁，如果要修改共享资源则用「写锁」加锁。\n\n所以，**读写锁适用于能明确区分读操作和写操作的场景**。\n\n读写锁的工作原理是：\n\n- 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。\n- 但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。\n\n所以说，写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。\n\n知道了读写锁的工作原理后，我们可以发现，**读写锁在读多写少的场景，能发挥出优势**。\n\n另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。\n\n读优先锁期望的是，读锁能被更多的线程持有，以便提高读线程的并发性，它的工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取写锁。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/锁/读优先锁工作流程.png)\n\n\n而「写优先锁」是优先服务写线程，其工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取写锁。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/操作系统/锁/写优先锁工作流程.png)\n\n\n读优先锁对于读线程并发性更好，但也不是没有问题。我们试想一下，如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象。\n\n写优先锁可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被「饿死」。\n\n既然不管优先读锁还是写锁，对方可能会出现饿死问题，那么我们就不偏袒任何一方，搞个「公平读写锁」。\n\n**公平读写锁比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。**\n\n互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。\n\n---\n\n## 乐观锁与悲观锁\n\n前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。\n\n悲观锁做事比较悲观，它认为**多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁**。\n\n那相反的，如果多线程同时修改共享资源的概率比较低，就可以采用乐观锁。\n\n乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：**先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作**。\n\n放弃后如何重试，这跟业务场景息息相关，虽然重试的成本很高，但是冲突的概率足够低的话，还是可以接受的。\n\n可见，乐观锁的心态是，不管三七二十一，先改了资源再说。另外，你会发现**乐观锁全程并没有加锁，所以它也叫无锁编程**。\n\n这里举一个场景例子：在线文档。\n\n我们都知道在线文档可以同时多人编辑的，如果使用了悲观锁，那么只要有一个用户正在编辑文档，此时其他用户就无法打开相同的文档了，这用户体验当然不好了。\n\n那实现多人同时编辑，实际上是用了乐观锁，它允许多个用户打开同一个文档进行编辑，编辑完提交之后才验证修改的内容是否有冲突。\n\n怎么样才算发生冲突？这里举个例子，比如用户 A 先在浏览器编辑文档，之后用户 B 在浏览器也打开了相同的文档进行编辑，但是用户 B 比用户 A 提交早，这一过程用户 A 是不知道的，当 A 提交修改完的内容时，那么 A 和 B 之间并行修改的地方就会发生冲突。\n\n服务端要怎么验证是否冲突了呢？通常方案如下：\n\n- 由于发生冲突的概率比较低，所以先让用户编辑文档，但是浏览器在下载文档时会记录下服务端返回的文档版本号；\n- 当用户提交修改时，发给服务端的请求会带上原始文档版本号，服务器收到后将它与当前版本号进行比较，如果版本号不一致则提交失败，如果版本号一致则修改成功，然后服务端版本号更新到最新的版本号。\n\n实际上，我们常见的 SVN 和 Git 也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。\n\n乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**\n\n---\n\n## 总结\n\n开发过程中，最常见的就是互斥锁的了，互斥锁加锁失败时，会用「线程切换」来应对，当加锁失败的线程再次加锁成功后的这一过程，会有两次线程上下文切换的成本，性能损耗比较大。\n\n如果我们明确知道被锁住的代码的执行时间很短，那我们应该选择开销比较小的自旋锁，因为自旋锁加锁失败时，并不会主动产生线程切换，而是一直忙等待，直到获取到锁，那么如果被锁住的代码执行时间很短，那这个忙等待的时间相对应也很短。\n\n\n如果能区分读操作和写操作的场景，那读写锁就更合适了，它允许多个读线程可以同时持有读锁，提高了读的并发性。根据偏袒读方还是写方，可以分为读优先锁和写优先锁，读优先锁并发性很强，但是写线程会被饿死，而写优先锁会优先服务写线程，读线程也可能会被饿死，那为了避免饥饿的问题，于是就有了公平读写锁，它是用队列把请求锁的线程排队，并保证先入先出的原则来对线程加锁，这样便保证了某种线程不会被饿死，通用性也更好点。\n\n互斥锁和自旋锁都是最基本的锁，读写锁可以根据场景来选择这两种锁其中的一个进行实现。\n\n另外，互斥锁、自旋锁、读写锁都属于悲观锁，悲观锁认为并发访问共享资源时，冲突概率可能非常高，所以在访问共享资源前，都需要先加锁。\n\n相反的，如果并发访问共享资源时，冲突概率非常低的话，就可以使用乐观锁，它的工作方式是，在访问共享资源时，不用先加锁，修改完共享资源后，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。\n\n但是，一旦冲突概率上升，就不适合使用乐观锁了，因为它解决冲突的重试成本非常高。\n\n不管使用的哪种锁，我们的加锁的代码范围应该尽可能的小，也就是加锁的粒度要小，这样执行速度会比较快。再来，使用上了合适的锁，就会快上加快了。\n\n## 读者问答\n\n\u003e CAS 不是乐观锁吗，为什么基于 CAS 实现的自旋锁是悲观锁？\n\n乐观锁是先修改同步资源，再验证有没有发生冲突。\n\n悲观锁是修改共享数据前，都要先加锁，防止竞争。\n\nCAS 是乐观锁没错，但是 CAS 和自旋锁不同之处，自旋锁基于 CAS 加了while 或者睡眠 CPU 的操作而产生自旋的效果，加锁失败会忙等待直到拿到锁，自旋锁是要需要事先拿到锁才能修改数据的，所以算悲观锁。\n\n---\n\n## 关注作者\n\n这周末忙里偷闲了下，看了三部电影，简单说一下感受。\n\n首先看了「利刃出鞘」，这部电影是悬疑类型，也是豆瓣高分电影，电影虽然没有什么大场面，但是单纯靠缜密的剧情铺设，全程无尿点，结尾也各种翻转，如果喜欢悬疑类电影朋友，不妨抽个时间看看。\n\n再来，看了「花木兰」，这电影我特喵无法可说，烂片中的战斗鸡，演员都是中国人却全在说英文（导演是美国迪士尼的），这种感觉就很奇怪很别扭，好比你看西游记、水浒传英文版那样的别扭。别扭也就算了，关键剧情平淡无奇，各种无厘头的地方，反正看完之后，我非常后悔把我生命中非常珍贵的 2 个小时献给了它，如果能重来，我选择用这 2 小时睡觉。\n\n最后，当然看了「信条」，诺兰用巨资拍摄出来的电影，花钱买飞机来撞，画面非常震撼，可以说非常有诚意了。诺兰钟爱时间的概念，这次则以时间倒流方式来呈现，非常的烧脑，反正我看完后脑袋懵懵的，我就是要这种感觉，嘻嘻。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n**大家好，我是小林，一个专为大家图解的工具人，如果觉得文章对你有帮助，欢迎分享给你的朋友，我们下次见！**\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/process_base":{"title":"process_base","content":"# 5.1 进程、线程基础知识\n\n\u003e 先来看看一则小故事\n\n我们写好的一行行代码，为了让其工作起来，我们还得把它送进城（**进程**）里，那既然进了城里，那肯定不能胡作非为了。\n\n城里人有城里人的规矩，城中有个专门管辖你们的城管（**操作系统**），人家让你休息就休息，让你工作就工作，毕竟摊位不多，每个人都要占这个摊位来工作，城里要工作的人多着去了。\n\n所以城管为了公平起见，它使用一种策略（**调度**）方式，给每个人一个固定的工作时间（**时间片**），时间到了就会通知你去休息而换另外一个人上场工作。\n\n另外，在休息时候你也不能偷懒，要记住工作到哪了，不然下次到你工作了，你忘记工作到哪了，那还怎么继续？\n\n有的人，可能还进入了县城（**线程**）工作，这里相对轻松一些，在休息的时候，要记住的东西相对较少，而且还能共享城里的资源。\n\n\u003e “哎哟，难道本文内容是进程和线程？”\n\n可以，聪明的你猜出来了，也不枉费我瞎编乱造的故事了。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/2-开车-表情包.jpg)\n\n\n进程和线程对于写代码的我们，真的天天见、日日见了，但见的多不代表你就熟悉它们，比如简单问你一句，你知道它们的工作原理和区别吗？\n\n\n不知道没关系，今天就要跟大家讨论**操作系统的进程和线程**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/3-提纲.jpg)\n\n::: tip\n\n先强调一下，我们本篇讲的主要都是操作系统理论知识，偏大学计算机专业课上的那种，并不是讲解 Linux 或 Windows 操作系统的实现方式，所以大家要区别一下。\n\n想让了解 Linux 或 Windows 操作系统的具体实现，得去看这些操作系统的实现原理或者源码书籍。\n\n:::\n\n\n---\n\n\n## 进程\n\n我们编写的代码只是一个存储在硬盘的静态文件，通过编译后就会生成二进制可执行文件，当我们运行这个可执行文件后，它会被装载到内存中，接着 CPU 会执行程序中的每一条指令，那么这个**运行中的程序，就被称为「进程」（Process）**。\n\n现在我们考虑有一个会读取硬盘文件数据的程序被执行了，那么当运行到读取文件的指令时，就会去从硬盘读取数据，但是硬盘的读写速度是非常慢的，那么在这个时候，如果 CPU 傻傻的等硬盘返回数据的话，那 CPU 的利用率是非常低的。\n\n做个类比，你去煮开水时，你会傻傻的等水壶烧开吗？很明显，小孩也不会傻等。我们可以在水壶烧开之前去做其他事情。当水壶烧开了，我们自然就会听到“嘀嘀嘀”的声音，于是再把烧开的水倒入到水杯里就好了。\n\n所以，当进程要从硬盘读取数据时，CPU 不需要阻塞等待数据的返回，而是去执行另外的进程。当硬盘数据返回时，CPU 会收到个**中断**，于是 CPU 再继续运行这个进程。\n\n![进程 1 与进程 2 切换](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/4-进程交替运行.jpg)\n\n这种**多个程序、交替执行**的思想，就有 CPU 管理多个进程的初步想法。\n\n对于一个支持多进程的系统，CPU 会从一个进程快速切换至另一个进程，其间每个进程各运行几十或几百个毫秒。\n\n虽然单核的 CPU 在某一个瞬间，只能运行一个进程。但在 1 秒钟期间，它可能会运行多个进程，这样就产生**并行的错觉**，实际上这是**并发**。\n\n\u003e 并发和并行有什么区别？\n\n一图胜千言。\n\n![并发与并行](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/5-并发与并行.jpg)\n\n\u003e 进程与程序的关系的类比\n\n到了晚饭时间，一对小情侣肚子都咕咕叫了，于是男生见机行事，就想给女生做晚饭，所以他就在网上找了辣子鸡的菜谱，接着买了一些鸡肉、辣椒、香料等材料，然后边看边学边做这道菜。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/6-做菜对应进程关系.jpg)\n\n突然，女生说她想喝可乐，那么男生只好把做菜的事情暂停一下，并在手机菜谱标记做到哪一个步骤，把状态信息记录了下来。\n\n然后男生听从女生的指令，跑去下楼买了一瓶冰可乐后，又回到厨房继续做菜。\n\n**这体现了，CPU 可以从一个进程（做菜）切换到另外一个进程（买可乐），在切换前必须要记录当前进程中运行的状态信息，以备下次切换回来的时候可以恢复执行。**\n\n所以，可以发现进程有着「**运行 - 暂停 - 运行**」的活动规律。\n\n### 进程的状态\n\n在上面，我们知道了进程有着「运行 - 暂停 - 运行」的活动规律。一般说来，一个进程并不是自始至终连续不停地运行的，它与并发执行中的其他进程的执行是相互制约的。\n\n它有时处于运行状态，有时又由于某种原因而暂停运行处于等待状态，当使它暂停的原因消失后，它又进入准备运行状态。\n\n所以，**在一个进程的活动期间至少具备三种基本状态，即运行状态、就绪状态、阻塞状态。**\n\n![进程的三种基本状态](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/7-进程三个基本状态.jpg)\n\n上图中各个状态的意义：\n\n- 运行状态（*Running*）：该时刻进程占用 CPU；\n- 就绪状态（*Ready*）：可运行，由于其他进程处于运行状态而暂时停止运行；\n- 阻塞状态（*Blocked*）：该进程正在等待某一事件发生（如等待输入/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；\n\n当然，进程还有另外两个基本状态：\n\n- 创建状态（*new*）：进程正在被创建时的状态；\n- 结束状态（*Exit*）：进程正在从系统中消失时的状态；\n\n于是，一个完整的进程状态的变迁如下图：\n\n![进程五种状态的变迁](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/8-进程五个状态.jpg)\n\n再来详细说明一下进程的状态变迁：\n\n- *NULL -\u003e 创建状态*：一个新进程被创建时的第一个状态；\n- *创建状态 -\u003e 就绪状态*：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的；\n- *就绪态 -\u003e 运行状态*：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程；\n- *运行状态 -\u003e 结束状态*：当进程已经运行完成或出错时，会被操作系统作结束状态处理；\n- *运行状态 -\u003e 就绪状态*：处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行；\n- *运行状态 -\u003e 阻塞状态*：当进程请求某个事件且必须等待时，例如请求 I/O 事件；\n- *阻塞状态 -\u003e 就绪状态*：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；\n\n如果有大量处于阻塞状态的进程，进程可能会占用着物理内存空间，显然不是我们所希望的，毕竟物理内存空间是有限的，被阻塞状态的进程占用着物理内存就是一种浪费物理内存的行为。\n\n所以，在虚拟内存管理的操作系统中，通常会把阻塞状态的进程的物理内存空间换出到硬盘，等需要再次运行的时候，再从硬盘换入到物理内存。\n\n\n![虚拟内存管理-换入换出](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/9-换入换出.jpg)\n\n那么，就需要一个新的状态，来**描述进程没有占用实际的物理内存空间的情况，这个状态就是挂起状态**。这跟阻塞状态是不一样，阻塞状态是等待某个事件的返回。\n\n\n另外，挂起状态可以分为两种：\n\n- 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；\n- 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；\n\n这两种挂起状态加上前面的五种状态，就变成了七种状态变迁（留给我的颜色不多了），见如下图：\n\n![七种状态变迁](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/10-进程七中状态.jpg)\n\n导致进程挂起的原因不只是因为进程所使用的内存空间不在物理内存，还包括如下情况：\n\n- 通过 sleep 让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程。\n- 用户希望挂起一个程序的执行，比如在 Linux 中用 `Ctrl+Z` 挂起进程；\n\n\n\n### 进程的控制结构\n\n在操作系统中，是用**进程控制块**（*process control block，PCB*）数据结构来描述进程的。\n\n那 PCB 是什么呢？打开知乎搜索你就会发现这个东西并不是那么简单。\n\n![知乎搜 PCB 的提示](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/11-PCB嫖娼.jpg)\n\n打住打住，我们是个正经的人，怎么会去看那些问题呢？是吧，回来回来。\n\n**PCB 是进程存在的唯一标识**，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。\n\n\u003e PCB 具体包含什么信息呢？\n\n**进程描述信息：**\n\n- 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；\n- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；\n\n**进程控制和管理信息：**\n\n- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；\n- 进程优先级：进程抢占 CPU 时的优先级；\n\n**资源分配清单：**\n\n- 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。\n\n**CPU 相关信息：**\n\n- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。\n\n可见，PCB 包含信息还是比较多的。\n\n\u003e 每个 PCB 是如何组织的呢？\n\n通常是通过**链表**的方式进行组织，把具有**相同状态的进程链在一起，组成各种队列**。比如：\n\n- 将所有处于就绪状态的进程链在一起，称为**就绪队列**；\n- 把所有因等待某事件而处于等待状态的进程链在一起就组成各种**阻塞队列**；\n- 另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。\n\n那么，就绪队列和阻塞队列链表的组织形式如下图：\n\n![就绪队列和阻塞队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/12-PCB状态链表组织.jpg)\n\n除了链接的组织方式，还有索引方式，它的工作原理：将同一状态的进程组织在一个索引表中，索引表项指向相应的 PCB，不同状态对应不同的索引表。\n\n一般会选择链表，因为可能面临进程创建，销毁等调度导致进程状态发生变化，所以链表能够更加灵活的插入和删除。\n\n### 进程的控制\n\n我们熟知了进程的状态变迁和进程的数据结构 PCB 后，再来看看进程的**创建、终止、阻塞、唤醒**的过程，这些过程也就是进程的控制。\n\n**01 创建进程**\n\n操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源。\n\n创建进程的过程如下：\n\n- 申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等；\n- 为该进程分配运行时所必需的资源，比如内存资源；\n- 将 PCB 插入到就绪队列，等待被调度运行；\n\n**02 终止进程**\n\n进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 `kill` 掉）。\n\n当子进程被终止时，其在父进程处继承的资源应当还给父进程。而当父进程被终止时，该父进程的子进程就变为孤儿进程，会被 1 号进程收养，并由 1 号进程对它们完成状态收集工作。\n\n终止进程的过程如下：\n\n- 查找需要终止的进程的 PCB；\n- 如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；\n- 如果其还有子进程，则应将该进程的子进程交给 1 号进程接管；\n- 将该进程所拥有的全部资源都归还给操作系统；\n- 将其从 PCB 所在队列中删除；\n\n**03 阻塞进程**\n\n当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。\n\n阻塞进程的过程如下：\n\n- 找到将要被阻塞进程标识号对应的 PCB；\n- 如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；\n- 将该 PCB 插入到阻塞队列中去；\n\n**04 唤醒进程**\n\n进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。\n\n如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。\n\n唤醒进程的过程如下：\n\n- 在该事件的阻塞队列中找到相应进程的 PCB；\n- 将其从阻塞队列中移出，并置其状态为就绪状态；\n- 把该 PCB 插入到就绪队列中，等待调度程序调度；\n\n进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句。\n\n### 进程的上下文切换\n\n各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执行，那么这个**一个进程切换到另一个进程运行，称为进程的上下文切换**。\n\n\u003e 在详细说进程上下文切换前，我们先来看看 CPU 上下文切换\n\n大多数操作系统都是多任务，通常支持大于 CPU 数量的任务同时运行。实际上，这些任务并不是同时运行的，只是因为系统在很短的时间内，让各个任务分别在 CPU 运行，于是就造成同时运行的错觉。\n\n任务是交给 CPU 运行的，那么在每个任务运行前，CPU 需要知道任务从哪里加载，又从哪里开始运行。\n\n所以，操作系统需要事先帮 CPU 设置好 **CPU 寄存器和程序计数器**。\n\n\nCPU 寄存器是 CPU 内部一个容量小，但是速度极快的内存（缓存）。我举个例子，寄存器像是你的口袋，内存像你的书包，硬盘则是你家里的柜子，如果你的东西存放到口袋，那肯定是比你从书包或家里柜子取出来要快的多。\n\n再来，程序计数器则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。\n\n\n所以说，CPU 寄存器和程序计数是 CPU 在运行任何任务前，所必须依赖的环境，这些环境就叫做 **CPU 上下文**。\n\n既然知道了什么是 CPU 上下文，那理解 CPU 上下文切换就不难了。\n\nCPU 上下文切换就是先把前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。\n\n\n系统内核会存储保持下来的上下文信息，当此任务再次被分配给 CPU 运行时，CPU 会重新加载这些上下文，这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。\n\n上面说到所谓的「任务」，主要包含进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下文切换分成：**进程上下文切换、线程上下文切换和中断上下文切换**。\n\n\n\u003e 进程的上下文切换到底是切换什么呢？\n\n进程是由内核管理和调度的，所以进程的切换只能发生在内核态。\n\n所以，**进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。**\n\n通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行，如下图所示：\n\n![进程上下文切换](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/13-进程上下文切换.jpg)\n\n大家需要注意，进程的上下文开销是很关键的，我们希望它的开销越小越好，这样可以使得进程可以把更多时间花费在执行程序上，而不是耗费在上下文切换。\n\n\u003e 发生进程上下文切换有哪些场景？\n\n- 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；\n- 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；\n- 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度；\n- 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；\n- 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序；\n\n以上，就是发生进程上下文切换的常见场景了。\n\n----\n\n## 线程\n\n在早期的操作系统中都是以进程作为独立运行的基本单位，直到后面，计算机科学家们又提出了更小的能独立运行的基本单位，也就是**线程。**\n\n### 为什么使用线程？\n\n我们举个例子，假设你要编写一个视频播放器软件，那么该软件功能的核心模块有三个：\n\n- 从视频文件当中读取数据；\n- 对读取的数据进行解压缩；\n- 把解压缩后的视频数据播放出来；\n\n对于单进程的实现方式，我想大家都会是以下这个方式：\n\n![单进程实现方式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/14-单线程mp4代码实例.jpg)\n\n对于单进程的这种方式，存在以下问题：\n\n- 播放出来的画面和声音会不连贯，因为当 CPU 能力不够强的时候，`Read`  的时候可能进程就等在这了，这样就会导致等半天才进行数据解压和播放；\n- 各个函数之间不是并发执行，影响资源的使用效率；\n\n那改进成多进程的方式：\n\n![多进程实现方式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/15-多进程mp4-代码实例.jpg)\n\n对于多进程的这种方式，依然会存在问题：\n\n- 进程之间如何通信，共享数据？\n- 维护进程的系统开销较大，如创建进程时，分配资源、建立 PCB；终止进程时，回收资源、撤销 PCB；进程切换时，保存当前进程的状态信息；\n\n那到底如何解决呢？需要有一种新的实体，满足以下特性：\n\n- 实体之间可以并发运行；\n- 实体之间共享相同的地址空间；\n\n这个新的实体，就是**线程( *Thread* )**，线程之间可以并发运行且共享相同的地址空间。\n\n### 什么是线程？\n\n**线程是进程当中的一条执行流程。**\n\n同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。\n\n![多线程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/16-多线程内存结构.jpg)\n\n\u003e 线程的优缺点？\n\n线程的优点：\n\n- 一个进程中可以同时存在多个线程；\n- 各个线程之间可以并发执行；\n- 各个线程之间可以共享地址空间和文件等资源；\n\n线程的缺点：\n\n- 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线程奔溃不会造成进程崩溃，具体分析原因可以看这篇：[线程崩溃了，进程也会崩溃吗？](https://xiaolincoding.com/os/4_process/thread_crash.html)）。\n\n举个例子，对于游戏的用户设计，则不应该使用多线程的方式，否则一个用户挂了，会影响其他同个进程的线程。\n\n### 线程与进程的比较\n\n线程与进程的比较如下：\n\n- 进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位；\n- 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈；\n- 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；\n- 线程能减少并发执行的时间和空间开销；\n\n\n对于，线程相比进程能减少开销，体现在：\n\n- 线程的创建时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；\n- 线程的终止时间比进程快，因为线程释放的资源相比进程少很多；\n- 同一个进程内的线程切换比进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同一个进程的线程都具有同一个页表，那么在切换的时候不需要切换页表。而对于进程之间的切换，切换的时候要把页表给切换掉，而页表的切换过程开销是比较大的；\n- 由于同一进程的各线程间共享内存和文件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了；\n\n所以，不管是时间效率，还是空间效率线程比进程都要高。\n\n### 线程的上下文切换\n\n在前面我们知道了，线程与进程最大的区别在于：**线程是调度的基本单位，而进程则是资源拥有的基本单位**。\n\n所以，所谓操作系统的任务调度，实际上的调度对象是线程，而进程只是给线程提供了虚拟内存、全局变量等资源。\n\n对于线程和进程，我们可以这么理解：\n\n- 当进程只有一个线程时，可以认为进程就等于线程；\n- 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，这些资源在上下文切换时是不需要修改的；\n\n另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。\n\n\u003e 线程上下文切换的是什么？\n\n这还得看线程是不是属于同一个进程：\n\n- 当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；\n- **当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据**；\n\n所以，线程的上下文切换相比进程，开销要小很多。\n\n### 线程的实现\n\n主要有三种线程的实现方式：\n\n- **用户线程（*User Thread*）**：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；\n- **内核线程（*Kernel Thread*）**：在内核中实现的线程，是由内核管理的线程；\n- **轻量级进程（*LightWeight Process*）**：在内核中来支持用户线程；\n\n那么，这还需要考虑一个问题，用户线程和内核线程的对应关系。\n\n首先，第一种关系是**多对一**的关系，也就是多个用户线程对应同一个内核线程：\n\n![多对一](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/17-内核线程与用户线程-一对多关系.jpg)\n\n\n第二种是**一对一**的关系，也就是一个用户线程对应一个内核线程：\n\n![一对一](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/18-内核线程与用户线程-一对一关系.jpg)\n\n第三种是**多对多**的关系，也就是多个用户线程对应到多个内核线程：\n\n![多对多](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/19-内核线程与用户线程-多对多关系.jpg)\n\n\n\u003e 用户线程如何理解？存在什么优势和缺陷？\n\n用户线程是基于用户态的线程管理库来实现的，那么**线程控制块（*Thread Control Block, TCB*）** 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。\n\n所以，**用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**\n\n用户级线程的模型，也就类似前面提到的**多对一**的关系，即多个用户线程对应同一个内核线程，如下图所示：\n\n![用户级线程模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/20-线程PCB-一对多关系.jpg)\n\n\n用户线程的**优点**：\n\n- 每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；\n- 用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快；\n\n用户线程的**缺点**：\n\n- 由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了。\n- 当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的。\n- 由于时间片分配给进程，故与其他进程比，在多线程执行时，每个线程得到的时间片较少，执行会比较慢；\n\n以上，就是用户线程的优缺点了。\n\n\u003e 那内核线程如何理解？存在什么优势和缺陷？\n\n**内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。**\n\n内核线程的模型，也就类似前面提到的**一对一**的关系，即一个用户线程对应一个内核线程，如下图所示：\n\n![内核线程模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/21-线程PCB-一对一关系.jpg)\n\n内核线程的**优点**：\n\n- 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；\n- 分配给线程，多线程的进程获得更多的 CPU 运行时间；\n\n内核线程的**缺点**：\n\n- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB；\n- 线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大；\n\n以上，就是内核线程的优缺点了。\n\n\u003e 最后的轻量级进程如何理解？\n\n**轻量级进程（*Light-weight process，LWP*）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持，而且 LWP 是由内核管理并像普通进程一样被调度**。\n\n在大多数系统中，**LWP与普通进程的区别也就在于它只有一个最小的执行上下文和调度程序所需的统计信息**。一般来说，一个进程代表程序的一个实例，而 LWP 代表程序的执行线程，因为一个执行线程不像进程那样需要那么多状态信息，所以 LWP 也不带有这样的信息。\n\n在 LWP 之上也是可以使用用户线程的，那么 LWP 与用户线程的对应关系就有三种：\n\n- `1 : 1`，即一个 LWP 对应 一个用户线程；\n- `N : 1`，即一个 LWP 对应多个用户线程；\n- `M : N`，即多个 LWP 对应多个用户线程；\n\n接下来针对上面这三种对应关系说明它们优缺点。先看下图的 LWP 模型：\n\n![LWP 模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/22-LWP.jpg)\n\n**1 : 1 模式**\n\n一个线程对应到一个 LWP 再对应到一个内核线程，如上图的进程 4，属于此模型。\n\n- 优点：实现并行，当一个 LWP 阻塞，不会影响其他 LWP；\n- 缺点：每一个用户线程，就产生一个内核线程，创建线程的开销较大。\n\n**N : 1 模式**\n\n多个用户线程对应一个 LWP 再对应一个内核线程，如上图的进程 2，线程管理是在用户空间完成的，此模式中用户的线程对操作系统不可见。\n\n- 优点：用户线程要开几个都没问题，且上下文切换发生在用户空间，切换的效率较高；\n- 缺点：一个用户线程如果阻塞了，则整个进程都将会阻塞，另外在多核 CPU  中，是没办法充分利用 CPU 的。\n\n**M : N 模式**\n\n根据前面的两个模型混搭一起，就形成 `M:N` 模型，该模型提供了两级控制，首先多个用户线程对应到多个 LWP，LWP 再一一对应到内核线程，如上图的进程 3。\n\n- 优点：综合了前两种优点，大部分的线程上下文发生在用户空间，且多个线程又可以充分利用多核 CPU 的资源。\n\n**组合模式**\n\n如上图的进程 5，此进程结合 `1:1` 模型和 `M:N` 模型。开发人员可以针对不同的应用特点调节内核线程的数目来达到物理并行性和逻辑并行性的最佳方案。\n\n---\n\n## 调度\n\n进程都希望自己能够占用 CPU 进行工作，那么这涉及到前面说过的进程上下文切换。\n\n一旦操作系统把进程切换到运行状态，也就意味着该进程占用着 CPU 在执行，但是当操作系统把进程切换到其他状态时，那就不能在 CPU 中执行了，于是操作系统会选择下一个要运行的进程。\n\n选择一个进程运行这一功能是在操作系统中完成的，通常称为**调度程序**（*scheduler*）。\n\n那到底什么时候调度进程，或以什么原则来调度进程呢？\n\n::: tip\n\n我知道很多人会问，线程不是操作系统的调度单位吗？为什么这里参与调度的是进程？\n\n先提前说明，这里的进程指只有主线程的进程，所以调度主线程就等于调度了整个进程。\n\n那为什么干脆不直接取名线程调度？主要是操作系统相关书籍，都是用进程调度这个名字，所以我也沿用了这个名字。\n\n:::\n\n### 调度时机\n\n在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，其实会触发一次调度。\n\n比如，以下状态的变化都会触发操作系统的调度：\n\n- *从就绪态 -\u003e 运行态*：当进程被创建时，会进入到就绪队列，操作系统会从就绪队列选择一个进程运行；\n- *从运行态 -\u003e 阻塞态*：当进程发生 I/O 事件而阻塞时，操作系统必须选择另外一个进程运行；\n- *从运行态 -\u003e 结束态*：当进程退出结束后，操作系统得从就绪队列选择另外一个进程运行；\n\n\n因为，这些状态变化的时候，操作系统需要考虑是否要让新的进程给 CPU 运行，或者是否让当前进程从 CPU 上退出来而换另一个进程运行。\n\n另外，如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断\n，把调度算法分为两类：\n\n- **非抢占式调度算法**挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。\n- **抢占式调度算法**挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生**时钟中断**，以便把 CPU 控制返回给调度程序进行调度，也就是常说的**时间片机制**。\n\n### 调度原则\n\n*原则一*：如果运行的程序，发生了 I/O 事件的请求，那 CPU 使用率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。所以，**为了提高 CPU 利用率，在这种发送 I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择一个进程来运行。**\n\n*原则二*：有的程序执行某个任务花费的时间会比较长，如果这个程序一直占用着 CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。所以，**要提高系统的吞吐率，调度程序要权衡长任务和短任务进程的运行完成数量。**\n\n*原则三*：从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运行时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越小越好，**如果进程的等待时间很长而运行时间很短，那周转时间就很长，这不是我们所期望的，调度程序应该避免这种情况发生。**\n\n*原则四*：处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执行。所以，**就绪队列中进程的等待时间也是调度程序所需要考虑的原则。**\n\n*原则五*：对于鼠标、键盘这种交互式比较强的应用，我们当然希望它的响应时间越快越好，否则就会影响用户体验了。所以，**对于交互式比较强的应用，响应时间也是调度程序需要考虑的原则。**\n\n![五种调度原则](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/23-五种调度规则.jpg)\n\n针对上面的五种调度原则，总结成如下：\n\n- **CPU 利用率**：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；\n- **系统吞吐量**：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；\n- **周转时间**：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好；\n- **等待时间**：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意；\n- **响应时间**：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。\n\n说白了，这么多调度原则，目的就是要使得进程要「快」。\n\n### 调度算法\n\n不同的调度算法适用的场景也是不同的。\n\n接下来，说说在**单核 CPU 系统**中常见的调度算法。\n\n\n\u003e 01 先来先服务调度算法\n\n最简单的一个调度算法，就是非抢占式的**先来先服务（*First Come First Serve, FCFS*）算法**了。\n\n![FCFS 调度算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/24-先来先服务.jpg)\n\n顾名思义，先来后到，**每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**\n\n这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。\n\nFCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。\n\n\n\u003e 02 最短作业优先调度算法\n\n**最短作业优先（*Shortest Job First, SJF*）调度算法**同样也是顾名思义，它会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量。\n\n![SJF 调度算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/25-最短作业优先算法.jpg)\n\n这显然对长作业不利，很容易造成一种极端现象。\n\n比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。\n\n\u003e 03 高响应比优先调度算法\n\n前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。\n\n那么，**高响应比优先\n（*Highest Response Ratio Next, HRRN*）调度算法**主要是权衡了短作业和长作业。\n\n**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，「响应比优先级」的计算公式：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/26-响应比公式.jpg)\n\n\n从上面的公式，可以发现：\n\n- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；\n- 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；\n\n::: tip\n\n很多人问怎么才能知道一个进程要求服务的时间？这不是不可预知的吗？\n\n对的，这是不可预估的。所以，高响应比优先调度算法是「理想型」的调度算法，现实中是实现不了的。\n\n:::\n\n\n\u003e 04 时间片轮转调度算法\n\n最古老、最简单、最公平且使用最广的算法就是**时间片轮转（*Round Robin, RR*）调度算法**。\n\n![RR 调度算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/27-时间片轮询.jpg)\n\n**每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。**\n\n- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；\n- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；\n\n另外，时间片的长度就是一个很关键的点：\n\n- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；\n- 如果时间片设得太长又可能引起短作业进程的响应时间变长，不利于短作业。\n\n一般来说，时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。\n\n\u003e 05 最高优先级调度算法\n\n前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。\n\n但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（*Highest Priority First，HPF*）调度算法**。\n\n进程的优先级可以分为，静态优先级和动态优先级：\n\n- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；\n- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。\n\n该算法也有两种处理优先级高的方法，非抢占式和抢占式：\n\n- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。\n- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。\n\n但是依然有缺点，可能会导致低优先级的进程永远不会运行。\n\n\u003e 06 多级反馈队列调度算法\n\n**多级反馈队列（*Multilevel Feedback Queue*）调度算法**是「时间片轮转算法」和「最高优先级算法」的综合和发展。\n\n顾名思义：\n\n- 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。\n- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；\n\n![多级反馈队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/28-多级队列.jpg)\n\n来看看，它是如何工作的：\n\n- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；\n- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；\n- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；\n\n可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也变更长了，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**\n\n\u003e 看的迷迷糊糊？那我拿去银行办业务的例子，把上面的调度算法串起来，你还不懂，你锤我！\n\n**办理业务的客户相当于进程，银行窗口工作人员相当于 CPU。**\n\n现在，假设这个银行只有一个窗口（单核 CPU ），那么工作人员一次只能处理一个业务。\n\n![银行办业务](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/29-银行1V1.jpg)\n\n那么最简单的处理方式，就是先来的先处理，后面来的就乖乖排队，这就是**先来先服务（*FCFS*）调度算法**。但是万一先来的这位老哥是来贷款的，这一谈就好几个小时，一直占用着窗口，这样后面的人只能干等，或许后面的人只是想简单的取个钱，几分钟就能搞定，却因为前面老哥办长业务而要等几个小时，你说气不气人？\n\n![先来先服务](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/30-银行-先来先服务.jpg)\n\n有客户抱怨了，那我们就要改进，我们干脆优先给那些几分钟就能搞定的人办理业务，这就是**短作业优先（*SJF*）调度算法**。听起来不错，但是依然还是有个极端情况，万一办理短业务的人非常的多，这会导致长业务的人一直得不到服务，万一这个长业务是个大客户，那不就捡了芝麻丢了西瓜\n\n![最短作业优先](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/31-银行-最短作业优先.jpg)\n\n那就公平起见，现在窗口工作人员规定，每个人我只处理 10 分钟。如果 10 分钟之内处理完，就马上换下一个人。如果没处理完，依然换下一个人，但是客户自己得记住办理到哪个步骤了。这个也就是**时间片轮转（*RR*）调度算法**。但是如果时间片设置过短，那么就会造成大量的上下文切换，增大了系统开销。如果时间片过长，相当于退化成 FCFS 算法了。\n\n![时间片轮转](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/32-银行-时间论片.jpg)\n\n既然公平也可能存在问题，那银行就对客户分等级，分为普通客户、VIP 客户、SVIP 客户。只要高优先级的客户一来，就第一时间处理这个客户，这就是**最高优先级（*HPF*）调度算法**。但依然也会有极端的问题，万一当天来的全是高级客户，那普通客户不是没有被服务的机会，不把普通客户当人是吗？那我们把优先级改成动态的，如果客户办理业务时间增加，则降低其优先级，如果客户等待时间增加，则升高其优先级。\n\n![最高优先级（静态）](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/33-银行-最高优先级.jpg)\n\n\n那有没有兼顾到公平和效率的方式呢？这里介绍一种算法，考虑的还算充分的，**多级反馈队列（*MFQ*）调度算法**，它是时间片轮转算法和优先级算法的综合和发展。它的工作方式：\n\n![多级反馈队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/进程和线程/34-银行-多级反馈.jpg)\n\n- 银行设置了多个排队（就绪）队列，每个队列都有不同的优先级，**各个队列优先级从高到低**，同时每个队列执行时间片的长度也不同，**优先级越高的时间片越短**。\n- 新客户（进程）来了，先进入第一级队列的末尾，按先来先服务原则排队等待被叫号（运行）。如果时间片用完客户的业务还没办理完成，则让客户进入到下一级队列的末尾，以此类推，直至客户业务办理完成。\n- 当第一级队列没人排队时，就会叫号二级队列的客户。如果客户办理业务过程中，有新的客户加入到较高优先级的队列，那么此时办理中的客户需要停止办理，回到原队列的末尾等待再次叫号，因为要把窗口让给刚进入较高优先级队列的客户。\n\n可以发现，对于要办理短业务的客户来说，可以很快的轮到并解决。对于要办理长业务的客户，一下子解决不了，就可以放到下一个队列，虽然等待的时间稍微变长了，但是轮到自己的办理时间也变长了，也可以接受，不会造成极端的现象，可以说是综合上面几种算法的优点。\n\n----\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/process_commu":{"title":"process_commu","content":"# 5.2 进程间有哪些通信方式？\n\n直接开讲！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/3-%E6%8F%90%E7%BA%B2.jpg)\n\n---\n\n每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/4-%E8%BF%9B%E7%A8%8B%E7%A9%BA%E9%97%B4.jpg)\n\nLinux 内核提供了不少进程间通信的机制，我们来一起瞧瞧有哪些？\n\n## 管道\n\n如果你学过 Linux 命令，那你肯定很熟悉「`|`」这个竖线。\n\n```bash\n$ ps auxf | grep mysql\n```\n\n上面命令行里的「`|`」竖线就是一个**管道**，它的功能是将前一个命令（`ps auxf`）的输出，作为后一个命令（`grep mysql`）的输入，从这功能描述，可以看出**管道传输数据是单向的**，如果想相互通信，我们需要创建两个管道才行。\n\n同时，我们得知上面这种管道是没有名字，所以「`|`」表示的管道称为**匿名管道**，用完了就销毁。\n\n管道还有另外一个类型是**命名管道**，也被叫做 `FIFO`，因为数据是先进先出的传输方式。\n\n在使用命名管道前，先需要通过 `mkfifo` 命令来创建，并且指定管道名字：\n\n```bash\n$ mkfifo myPipe\n```\n\nmyPipe 就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用 ls 看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思：\n\n```bash\n$ ls -l\nprw-r--r--. 1 root    root         0 Jul 17 02:45 myPipe\n```\n\n接下来，我们往 myPipe 这个管道写入数据：\n\n```bash\n$ echo \"hello\" \u003e myPipe  // 将数据写进管道\n                         // 停住了 ...\n```\n\n你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。\n\n于是，我们执行另外一个命令来读取这个管道里的数据：\n\n```bash\n$ cat \u003c myPipe  // 读取管道里的数据\nhello\n```\n\n可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。\n\n我们可以看出，**管道这种通信方式效率低，不适合进程间频繁地交换数据**。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。\n\n\u003e 那管道如何创建呢，背后原理是什么？\n\n匿名管道的创建，需要通过下面这个系统调用：\n\n```c\nint pipe(int fd[2])\n```\n\n这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 `fd[0]`，另一个是管道的写入端描述符 `fd[1]`。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/5-%E7%AE%A1%E9%81%93-pipe.jpg)\n\n\n其实，**所谓的管道，就是内核里面的一串缓存**。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。\n\n看到这，你可能会有疑问了，这两个描述符都是在一个进程里面，并没有起到进程间通信的作用，怎么样才能使得管道是跨过两个进程的呢？\n\n我们可以使用 `fork` 创建子进程，**创建的子进程会复制父进程的文件描述符**，这样就做到了两个进程各有两个「 `fd[0]` 与 `fd[1]`」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/6-%E7%AE%A1%E9%81%93-pipe-fork.jpg)\n\n管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是：\n\n- 父进程关闭读取的 fd[0]，只保留写入的 fd[1]；\n- 子进程关闭写入的 fd[1]，只保留读取的 fd[0]；\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/7-%E7%AE%A1%E9%81%93-pipe-fork-%E5%8D%95%E5%90%91%E9%80%9A%E4%BF%A1.jpg)\n\n所以说如果需要双向通信，则应该创建两个管道。\n\n到这里，我们仅仅解析了使用管道进行父进程与子进程之间的通信，但是在我们 shell 里面并不是这样的。\n\n在 shell 里面执行 `A | B`命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/8-%E7%AE%A1%E9%81%93-pipe-shell.jpg)\n\n所以说，在 shell 里通过「`|`」匿名管道将多个命令连接在一起，实际上也就是创建了多个子进程，那么在我们编写 shell 脚本时，能使用一个管道搞定的事情，就不要多用一个管道，这样可以减少创建子进程的系统开销。\n\n我们可以得知，**对于匿名管道，它的通信范围是存在父子关系的进程**。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。\n\n另外，**对于命名管道，它可以在不相关的进程间也能相互通信**。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。\n\n不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。\n\n---\n\n## 消息队列\n\n前面说到管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据。\n\n对于这个问题，**消息队列**的通信模式就可以解决。比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。\n\n再来，**消息队列是保存在内核中的消息链表**，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。\n\n消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。\n\n消息这种模型，两个进程之间的通信就像平时发邮件一样，你来一封，我回一封，可以频繁沟通了。\n\n但邮件的通信方式存在不足的地方有两点，**一是通信不及时，二是附件也有大小限制**，这同样也是消息队列通信不足的点。\n\n**消息队列不适合比较大数据的传输**，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。在 Linux 内核中，会有两个宏定义 `MSGMAX` 和 `MSGMNB`，它们以字节为单位，分别定义了一条消息的最大长度和一个队列的最大长度。\n\n**消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。\n\n---\n\n## 共享内存\n\n消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那**共享内存**的方式，就很好的解决了这一问题。\n\n现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。\n\n**共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中**。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/9-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98.jpg)\n\n---\n\n## 信号量\n\n用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。\n\n为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，**信号量**就实现了这一保护机制。\n\n**信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据**。\n\n信号量表示资源的数量，控制信号量的方式有两种原子操作：\n\n- 一个是 **P 操作**，这个操作会把信号量减去 1，相减后如果信号量 \u003c 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 \u003e= 0，则表明还有资源可使用，进程可正常继续执行。\n- 另一个是 **V 操作**，这个操作会把信号量加上 1，相加后如果信号量 \u003c= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 \u003e 0，则表明当前没有阻塞中的进程；\n\nP 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。\n\n接下来，举个例子，如果要使得两个进程互斥访问共享内存，我们可以初始化信号量为 `1`。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/10-%E4%BF%A1%E5%8F%B7%E9%87%8F-%E4%BA%92%E6%96%A5.jpg)\n\n具体的过程如下：\n\n- 进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。\n- 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。\n- 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。\n\n可以发现，信号初始化为 `1`，就代表着是**互斥信号量**，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。\n\n另外，在多进程里，每个进程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个进程能密切合作，以实现一个共同的任务。\n\n例如，进程 A 是负责生产数据，而进程 B 是负责读取数据，这两个进程是相互合作、相互依赖的，进程 A 必须先生产了数据，进程 B 才能读取到数据，所以执行是有前后顺序的。\n\n那么这时候，就可以用信号量来实现多进程同步的方式，我们可以初始化信号量为 `0`。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/11-%E4%BF%A1%E5%8F%B7%E9%87%8F-%E5%90%8C%E6%AD%A5.jpg)\n\n具体过程：\n\n- 如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没生产数据，于是进程 B 就阻塞等待；\n- 接着，当进程 A 生产完数据后，执行了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B；\n- 最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，于是进程 B 就可以正常读取数据了。\n\n可以发现，信号初始化为 `0`，就代表着是**同步信号量**，它可以保证进程 A 应在进程 B 之前执行。\n\n---\n\n## 信号\n\n上面说的进程间通信，都是常规状态下的工作模式。**对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。**\n\n信号跟信号量虽然名字相似度 66.66%，但两者用途完全不一样，就好像 Java 和 JavaScript 的区别。\n\n在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 `kill -l` 命令，查看所有的信号：\n\n```shell\n$ kill -l\n 1) SIGHUP       2) SIGINT       3) SIGQUIT      4) SIGILL       5) SIGTRAP\n 6) SIGABRT      7) SIGBUS       8) SIGFPE       9) SIGKILL     10) SIGUSR1\n11) SIGSEGV     12) SIGUSR2     13) SIGPIPE     14) SIGALRM     15) SIGTERM\n16) SIGSTKFLT   17) SIGCHLD     18) SIGCONT     19) SIGSTOP     20) SIGTSTP\n21) SIGTTIN     22) SIGTTOU     23) SIGURG      24) SIGXCPU     25) SIGXFSZ\n26) SIGVTALRM   27) SIGPROF     28) SIGWINCH    29) SIGIO       30) SIGPWR\n31) SIGSYS      34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3\n38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8\n43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13\n48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12\n53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7\n58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2\n63) SIGRTMAX-1  64) SIGRTMAX\n```\n\n运行在 shell 终端的进程，我们可以通过键盘输入某些组合键的时候，给进程发送信号。例如\n\n- Ctrl+C 产生 `SIGINT` 信号，表示终止该进程；\n- Ctrl+Z 产生 `SIGTSTP` 信号，表示停止该进程，但还未结束；\n\n如果进程在后台运行，可以通过 `kill` 命令的方式给进程发送信号，但前提需要知道运行中的进程 PID 号，例如：\n\n- kill -9 1050 ，表示给 PID 为 1050 的进程发送 `SIGKILL` 信号，用来立即结束该进程；\n\n所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。\n\n信号是进程间通信机制中**唯一的异步通信机制**，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。\n\n**1.执行默认操作**。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。\n\n**2.捕捉信号**。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。\n\n**3.忽略信号**。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SIGSTOP`，它们用于在任何时候中断或结束某一进程。\n\n---\n\n## Socket\n\n前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想**跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。**\n\n实际上，Socket 通信不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信。\n\n我们来看看创建 socket 的系统调用：\n\n```c\nint socket(int domain, int type, int protocOl)\n```\n\n三个参数分别代表：\n\n- domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机；\n- type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM  表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字；\n- protocOl 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可；\n\n根据创建 socket 类型的不同，通信的方式也就不同：\n\n- 实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM；\n- 实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM；\n- 实现本地进程间通信： 「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和 AF_LOCAL 是等价的，所以 AF_UNIX 也属于本地 socket；\n\n接下来，简单说一下这三种通信的编程模式。\n\n\u003e 针对 TCP 协议通信的 socket 编程模型\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/12-TCP%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.jpg)\n\n- 服务端和客户端初始化 `socket`，得到文件描述符；\n- 服务端调用 `bind`，将绑定在 IP 地址和端口;\n- 服务端调用 `listen`，进行监听；\n- 服务端调用 `accept`，等待客户端连接；\n- 客户端调用 `connect`，向服务器端的地址和端口发起连接请求；\n- 服务端 `accept` 返回用于传输的 `socket` 的文件描述符；\n- 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；\n- 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。\n\n这里需要注意的是，服务端调用 `accept` 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。\n\n所以，监听的 socket 和真正用来传送数据的 socket，是「**两个**」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。\n\n成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。\n\n\u003e 针对 UDP 协议通信的 socket 编程模型\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/13-UDP%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.jpg)\n\nUDP 是没有连接的，所以不需要三次握手，也就不需要像 TCP 调用 listen 和 connect，但是 UDP 的交互仍然需要 IP 地址和端口号，因此也需要 bind。\n\n对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送方和接收方，甚至都不存在客户端和服务端的概念，只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind。\n\n另外，每次通信时，调用 sendto 和 recvfrom，都要传入目标主机的 IP 地址和端口。\n\n\u003e 针对本地进程间通信的 socket 编程模型\n\n本地 socket  被用于在**同一台主机上进程间通信**的场景：\n\n- 本地 socket 的编程接口和 IPv4 、IPv6 套接字编程接口是一致的，可以支持「字节流」和「数据报」两种协议；\n- 本地 socket 的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报 socket 实现；\n\n对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。\n\n对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。\n\n本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是**绑定一个本地文件**，这也就是它们之间的最大区别。\n\n---\n\n## 总结\n\n由于每个进程的用户空间都是独立的，不能相互访问，这时就需要借助内核空间来实现进程间通信，原因很简单，每个进程都是共享一个内核空间。\n\nLinux 内核提供了不少进程间通信的方式，其中最简单的方式就是管道，管道分为「匿名管道」和「命名管道」。\n\n**匿名管道**顾名思义，它没有名字标识，匿名管道是特殊文件只存在于内存，没有存在于文件系统中，shell 命令中的「`|`」竖线就是匿名管道，通信的数据是**无格式的流并且大小受限**，通信的方式是**单向**的，数据只能在一个方向上流动，如果要双向通信，需要创建两个管道，再来**匿名管道是只能用于存在父子关系的进程间通信**，匿名管道的生命周期随着进程创建而建立，随着进程终止而消失。\n\n**命名管道**突破了匿名管道只能在亲缘关系进程间的通信限制，因为使用命名管道的前提，需要在文件系统创建一个类型为 p 的设备文件，那么毫无关系的进程就可以通过这个设备文件进行通信。另外，不管是匿名管道还是命名管道，进程写入的数据都是**缓存在内核**中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。\n\n**消息队列**克服了管道通信的数据是无格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以用户自定义的数据类型，发送数据时，会被分成一个一个独立的消息体，当然接收数据时，也要与发送方发送的消息体的数据类型保持一致，这样才能保证读取的数据是正确的。消息队列通信的速度不是最及时的，毕竟**每次数据的写入和读取都需要经过用户态与内核态之间的拷贝过程。**\n\n**共享内存**可以解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销，**它直接分配一个共享空间，每个进程都可以直接访问**，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，享有**最快**的进程间通信方式之名。但是便捷高效的共享内存通信，**带来新的问题，多进程竞争同个共享资源会造成数据的错乱。**\n\n那么，就需要**信号量**来保护共享资源，以确保任何时刻只能有一个进程访问共享资源，这种方式就是互斥访问。**信号量不仅可以实现访问的互斥性，还可以实现进程间的同步**，信号量其实是一个计数器，表示的是资源个数，其值可以通过两个原子操作来控制，分别是 **P 操作和 V 操作**。\n\n与信号量名字很相似的叫**信号**，它俩名字虽然相似，但功能一点儿都不一样。信号是**异步通信机制**，信号可以在应用进程和内核之间直接交互，内核也可以利用信号来通知用户空间的进程发生了哪些系统事件，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令），一旦有信号发生，**进程有三种方式响应信号 1. 执行默认操作、2. 捕捉信号、3. 忽略信号**。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SIGSTOP`，这是为了方便我们能在任何时候结束或停止某个进程。\n\n前面说到的通信机制，都是工作于同一台主机，如果**要与不同主机的进程间通信，那么就需要 Socket 通信了**。Socket 实际上不仅用于不同的主机进程间通信，还可以用于本地主机进程间通信，可根据创建 Socket 的类型不同，分为三种常见的通信方式，一个是基于 TCP 协议的通信方式，一个是基于 UDP 协议的通信方式，一个是本地进程间通信方式。\n\n以上，就是进程间通信的主要机制了。你可能会问了，那线程通信间的方式呢？\n\n同个进程下的线程之间都是共享进程的资源，只要是共享变量都可以做到线程间通信，比如全局变量，所以对于线程间关注的不是通信方式，而是关注多线程竞争共享资源的问题，信号量也同样可以在线程间实现互斥与同步：\n\n- 互斥的方式，可保证任意时刻只有一个线程访问共享资源；\n- 同步的方式，可保证线程 A 应在线程 B 之前执行；\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n\n----\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/4_process/thread_crash":{"title":"thread_crash","content":"\n\n# 5.7 线程崩溃了，进程也会崩溃吗？\n\n\u003e 来源：公众号@码海\n\u003e\n\u003e 原文地址：[美团一面：线程崩溃了，进程也会崩溃吗？](https://mp.weixin.qq.com/s/easnVQ75Rq-C07W4YWeclQ)\n\n大家好，我是小林。\n\n之前分享这篇文章的时候：[进程和线程基础知识全家桶，30 张图一套带走](https://xiaolincoding.com/os/4_process/process_base.html)，提到说线程的一个缺点：\n\n![](https://img-blog.csdnimg.cn/899ce21f16244826a7e2fb899484b348.png)\n\n很多同学就好奇，**为什么 C/C++ 语言里，线程崩溃后，进程也会崩溃，而 Java 语言里却不会呢？**\n\n刚好看到朋友（[公众号：码海](https://mp.weixin.qq.com/s/JnlTdUk8Jvao8L6FAtKqhQ)）写了一篇：「**美团面试题：为什么线程崩溃崩溃不会导致 JVM 崩溃?**」\n\n我觉得写的很好，所以分享给大家一起拜读拜读，本文分以下几节来探讨：\n\n1. 线程崩溃，进程一定会崩溃吗\n2. 进程是如何崩溃的-信号机制简介\n3. 为什么在 JVM 中线程崩溃不会导致 JVM 进程崩溃\n4. openJDK 源码解析\n\n## 线程崩溃，进程一定会崩溃吗\n\n一般来说如果线程是因为非法访问内存引起的崩溃，那么进程肯定会崩溃，为什么系统要让进程崩溃呢，这主要是因为在进程中，**各个线程的地址空间是共享的**，既然是共享，那么某个线程对地址的非法访问就会导致内存的不确定性，进而可能会影响到其他线程，这种操作是危险的，操作系统会认为这很可能导致一系列严重的后果，于是干脆让整个进程崩溃\n\n![](https://img-blog.csdnimg.cn/17be94f342ea4e49a227b195845880fd.png)\n\n\n线程共享代码段，数据段，地址空间，文件非法访问内存有以下几种情况，我们以 C 语言举例来看看。\n\n1.、针对只读内存写入数据\n\n```c\n   #include \u003cstdio.h\u003e\n   #include \u003cstdlib.h\u003e\n   \n   int main() {\n      char *s = \"hello world\";\n      // 向只读内存写入数据，崩溃\n      s[1] = 'H'; \n   }\n```\n\n2、访问了进程没有权限访问的地址空间（比如内核空间）\n\n```c\n   #include \u003cstdio.h\u003e\n   #include \u003cstdlib.h\u003e\n\n   int main() {\n      int *p = (int *)0xC0000fff;\n      // 针对进程的内核空间写入数据，崩溃\n      *p = 10; \n   }\n```\n\n在 32 位虚拟地址空间中，p 指向的是内核空间，显然不具有写入权限，所以上述赋值操作会导致崩溃\n\n3、访问了不存在的内存，比如：\n\n```c\n   #include \u003cstdio.h\u003e\n   #include \u003cstdlib.h\u003e\n   \n   int main() {\n      int *a = NULL;\n      *a = 1;     \n   }\n```\n\n以上错误都是访问内存时的错误，所以统一会报 Segment Fault 错误（即段错误），这些都会导致进程崩溃\n\n## 进程是如何崩溃的-信号机制简介\n\n那么线程崩溃后，进程是如何崩溃的呢，这背后的机制到底是怎样的，答案是**信号**。\n\n大家想想要干掉一个正在运行的进程是不是经常用 kill -9 pid 这样的命令，这里的 kill 其实就是给指定 pid 发送终止信号的意思，其中的 9 就是信号。\n\n其实信号有很多类型的，在 Linux 中可以通过 `kill -l`查看所有可用的信号：\n\n![](https://img-blog.csdnimg.cn/eba4dce5e59442b8b2b24d9e171bab0d.png)\n\n\n当然了发 kill 信号必须具有一定的权限，否则任意进程都可以通过发信号来终止其他进程，那显然是不合理的，实际上 kill 执行的是系统调用，将控制权转移给了内核（操作系统），由内核来给指定的进程发送信号\n\n那么发个信号进程怎么就崩溃了呢，这背后的原理到底是怎样的？\n\n其背后的机制如下\n\n1. CPU 执行正常的进程指令\n2. 调用 kill 系统调用向进程发送信号\n3. 进程收到操作系统发的信号，CPU 暂停当前程序运行，并将控制权转交给操作系统\n4. 调用 kill 系统调用向进程发送信号（假设为 11，即 SIGSEGV，一般非法访问内存报的都是这个错误）\n5. **操作系统根据情况执行相应的信号处理程序（函数），一般执行完信号处理程序逻辑后会让进程退出**\n\n注意上面的第五步，如果进程没有注册自己的信号处理函数，那么操作系统会执行默认的信号处理程序（一般最后会让进程退出），但如果注册了，则会执行自己的信号处理函数，这样的话就给了进程一个垂死挣扎的机会，它收到 kill 信号后，可以调用 exit() 来退出，**但也可以使用 sigsetjmp，siglongjmp 这两个函数来恢复进程的执行**\n\n```c\n// 自定义信号处理函数示例\n\n#include \u003cstdio.h\u003e\n#include \u003csignal.h\u003e\n#include \u003cstdlib.h\u003e\n// 自定义信号处理函数，处理自定义逻辑后再调用 exit 退出\nvoid sigHandler(int sig) {\n  printf(\"Signal %d catched!\\n\", sig);\n  exit(sig);\n}\nint main(void) {\n  signal(SIGSEGV, sigHandler);\n  int *p = (int *)0xC0000fff;\n  *p = 10; // 针对不属于进程的内核空间写入数据，崩溃\n}\n\n// 以上结果输出: Signal 11 catched!\n```\n\n**如代码所示**：注册信号处理函数后，当收到 SIGSEGV 信号后，先执行相关的逻辑再退出\n\n另外当进程接收信号之后也可以不定义自己的信号处理函数，而是选择忽略信号，如下\n\n```c\n#include \u003cstdio.h\u003e\n#include \u003csignal.h\u003e\n#include \u003cstdlib.h\u003e\n\nint main(void) {\n  // 忽略信号\n  signal(SIGSEGV, SIG_IGN);\n\n  // 产生一个 SIGSEGV 信号\n  raise(SIGSEGV);\n\n  printf(\"正常结束\");\n}\n```\n\n也就是说虽然给进程发送了 kill 信号，但如果进程自己定义了信号处理函数或者无视信号就有机会逃出生天，当然了 kill -9 命令例外，不管进程是否定义了信号处理函数，都会马上被干掉。\n\n说到这大家是否想起了一道经典面试题：**如何让正在运行的 Java 工程的优雅停机？**\n\n通过上面的介绍大家不难发现，其实是 JVM 自己定义了信号处理函数，这样当发送 kill pid 命令（默认会传 15 也就是 SIGTERM）后，JVM 就可以在信号处理函数中执行一些资源清理之后再调用 exit 退出。\n\n这种场景显然不能用 kill -9，不然一下把进程干掉了资源就来不及清除了。\n\n## 为什么线程崩溃不会导致 JVM 进程崩溃\n\n现在我们再来看看开头这个问题，相信你多少会心中有数，想想看在 Java 中有哪些是常见的由于非法访问内存而产生的 Exception 或 error 呢，常见的是大家熟悉的 StackoverflowError 或者 NPE（NullPointerException）,NPE 我们都了解，属于是访问了不存在的内存。\n\n但为什么栈溢出（Stackoverflow）也属于非法访问内存呢，这得简单聊一下进程的虚拟空间，也就是前面提到的共享地址空间。\n\n现代操作系统为了保护进程之间不受影响，所以使用了虚拟地址空间来隔离进程，进程的寻址都是针对虚拟地址，每个进程的虚拟空间都是一样的，而线程会共用进程的地址空间。\n\n以 32 位虚拟空间，进程的虚拟空间分布如下：\n\n![](https://img-blog.csdnimg.cn/8de250fcb055400c94f95c99712a1158.png)\n\n那么 stackoverflow 是怎么发生的呢？\n\n进程每调用一个函数，都会分配一个栈帧，然后在栈帧里会分配函数里定义的各种局部变量。\n\n假设现在调用了一个无限递归的函数，那就会持续分配栈帧，但 stack 的大小是有限的（Linux 中默认为 8 M，可以通过 ulimit -a 查看），如果无限递归很快栈就会分配完了，此时再调用函数试图分配超出栈的大小内存，就会发生段错误，也就是 stackoverflowError。\n\n![](https://img-blog.csdnimg.cn/c54aff1660e34d8a8a83d534c3390954.png)\n\n好了，现在我们知道了 StackoverflowError 怎么产生的。\n\n那问题来了，既然 StackoverflowError 或者 NPE 都属于非法访问内存， JVM 为什么不会崩溃呢？\n\n有了上一节的铺垫，相信你不难回答，其实就是**因为 JVM 自定义了自己的信号处理函数，拦截了 SIGSEGV 信号，针对这两者不让它们崩溃**。\n\n怎么证明这个推测呢，我们来看下 JVM 的源码来一探究竟\n\n## openJDK 源码解析\n\nHotSpot 虚拟机目前使用范围最广的 Java 虚拟机，据 R 大所述， Oracle JDK 与 OpenJDK 里的 JVM 都是 HotSpot VM，从源码层面说，两者基本上是同一个东西。\n\nOpenJDK 是开源的，所以我们主要研究下 Java 8 的 OpenJDK 即可，地址如下：[https://github.com/AdoptOpenJDK/openjdk-jdk8u](https://github.com/AdoptOpenJDK/openjdk-jdk8u)，有兴趣的可以下载来看看。\n\n我们只要研究 Linux 下的 JVM，为了便于说明，也方便大家查阅，我把其中关于信号处理的关键流程整理了下（忽略其中的次要代码）。\n\n![](https://img-blog.csdnimg.cn/474ddf8657a0438da1822e0f6fa59af7.png)\n\n\n可以看到，在启动 JVM 的时候，也设置了信号处理函数，收到 SIGSEGV，SIGPIPE 等信号后最终会调用 JVM_handle_linux_signal 这个自定义信号处理函数，再来看下这个函数的主要逻辑。\n\n```java\nJVM_handle_linux_signal(int sig,\n                        siginfo_t* info,\n                        void* ucVoid,\n                        int abort_if_unrecognized) {\n\n   // Must do this before SignalHandlerMark, if crash protection installed we will longjmp away\n  // 这段代码里会调用 siglongjmp，主要做线程恢复之用\n  os::ThreadCrashProtection::check_crash_protection(sig, t);\n\n  if (info != NULL \u0026\u0026 uc != NULL \u0026\u0026 thread != NULL) {\n    pc = (address) os::Linux::ucontext_get_pc(uc);\n\n    // Handle ALL stack overflow variations here\n    if (sig == SIGSEGV) {\n      // Si_addr may not be valid due to a bug in the linux-ppc64 kernel (see\n      // comment below). Use get_stack_bang_address instead of si_addr.\n      address addr = ((NativeInstruction*)pc)-\u003eget_stack_bang_address(uc);\n\n      // 判断是否栈溢出了\n      if (addr \u003c thread-\u003estack_base() \u0026\u0026\n          addr \u003e= thread-\u003estack_base() - thread-\u003estack_size()) {\n        if (thread-\u003ethread_state() == _thread_in_Java) {            // 针对栈溢出 JVM 的内部处理\n            stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::STACK_OVERFLOW);\n        }\n      }\n    }\n  }\n\n  if (sig == SIGSEGV \u0026\u0026\n               !MacroAssembler::needs_explicit_null_check((intptr_t)info-\u003esi_addr)) {\n         // 此处会做空指针检查\n      stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_NULL);\n  }\n\n\n  // 如果是栈溢出或者空指针最终会返回 true，不会走最后的 report_and_die，所以 JVM 不会退出\n  if (stub != NULL) {\n    // save all thread context in case we need to restore it\n    if (thread != NULL) thread-\u003eset_saved_exception_pc(pc);\n\n    uc-\u003euc_mcontext.gregs[REG_PC] = (greg_t)stub;\n    // 返回 true 代表 JVM 进程不会退出\n    return true;\n  }\n\n  VMError err(t, sig, pc, info, ucVoid);\n  // 生成 hs_err_pid_xxx.log 文件并退出\n  err.report_and_die();\n\n  ShouldNotReachHere();\n  return true; // Mute compiler\n\n}\n```\n\n从以上代码我们可以知道以下信息：\n\n1. 发生 stackoverflow 还有空指针错误，确实都发送了 SIGSEGV，只是虚拟机不选择退出，而是自己内部作了额外的处理，其实是恢复了线程的执行，并抛出 StackoverflowError 和 NPE，这就是为什么 JVM 不会崩溃且我们能捕获这两个错误/异常的原因\n2. 如果针对 SIGSEGV 等信号，在以上的函数中 JVM 没有做额外的处理，那么最终会走到 report_and_die 这个方法，这个方法主要做的事情是生成 hs_err_pid_xxx.log crash 文件（记录了一些堆栈信息或错误），然后退出\n\n至此我相信大家明白了为什么发生了 StackoverflowError 和 NPE 这两个非法访问内存的错误，JVM 却没有崩溃。\n\n**原因其实就是虚拟机内部定义了信号处理函数，而在信号处理函数中对这两者做了额外的处理以让 JVM 不崩溃，另一方面也可以看出如果 JVM 不对信号做额外的处理，最后会自己退出并产生 crash 文件 hs_err_pid_xxx.log（可以通过 -XX:ErrorFile=/var/*log*/hs_err.log 这样的方式指定），这个文件记录了虚拟机崩溃的重要原因**。\n\n所以也可以说，虚拟机是否崩溃只要看它是否会产生此崩溃日志文件\n\n## 总结\n\n正常情况下，操作系统为了保证系统安全，所以针对非法内存访问会发送一个 SIGSEGV 信号，而操作系统一般会调用默认的信号处理函数（一般会让相关的进程崩溃）。\n\n但如果进程觉得\"罪不致死\"，那么它也可以选择自定义一个信号处理函数，这样的话它就可以做一些自定义的逻辑，比如记录 crash 信息等有意义的事。\n\n回过头来看为什么虚拟机会针对 StackoverflowError 和 NullPointerException 做额外处理让线程恢复呢，针对 stackoverflow 其实它采用了一种栈回溯的方法保证线程可以一直执行下去，而捕获空指针错误主要是这个错误实在太普遍了。\n\n为了这一个很常见的错误而让 JVM 崩溃那线上的 JVM 要宕机多少次，所以出于工程健壮性的考虑，与其直接让 JVM 崩溃倒不如让线程起死回生，并且将这两个错误/异常抛给用户来处理。\n\n---\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/5_schedule/schedule":{"title":"schedule","content":"# 6.1 进程调度/页面置换/磁盘调度算法\n\n最近，我偷偷潜伏在各大技术群，因为秋招在即，看到不少小伙伴分享的大厂面经。\n\n然后发现，操作系统的知识点考察还是比较多的，大厂就是大厂就爱问基础知识。其中，关于操作系统的「调度算法」考察也算比较频繁。\n\n所以，我这边总结了操作系统的三大调度机制，分别是「**进程调度/页面置换/磁盘调度算法**」，供大家复习，希望大家在秋招能斩获自己心意的 offer。\n\n![本文提纲](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/调度算法提纲.png)\n\n---\n\n## 进程调度算法\n\n进程调度算法也称 CPU 调度算法，毕竟进程是由 CPU 调度的。\n\n当 CPU 空闲时，操作系统就选择内存中的某个「就绪状态」的进程，并给其分配 CPU。\n\n什么时候会发生 CPU 调度呢？通常有以下情况：\n\n1. 当进程从运行状态转到等待状态；\n2. 当进程从运行状态转到就绪状态；\n3. 当进程从等待状态转到就绪状态；\n4. 当进程从运行状态转到终止状态；\n\n其中发生在 1 和 4 两种情况下的调度称为「非抢占式调度」，2 和 3 两种情况下发生的调度称为「抢占式调度」。\n\n非抢占式的意思就是，当进程正在运行时，它就会一直运行，直到该进程完成或发生某个事件而被阻塞时，才会把 CPU 让给其他进程。\n\n而抢占式调度，顾名思义就是进程正在运行的时，可以被打断，使其把 CPU 让给其他进程。那抢占的原则一般有三种，分别是时间片原则、优先权原则、短作业优先原则。\n\n你可能会好奇为什么第 3 种情况也会发生 CPU 调度呢？假设有一个进程是处于等待状态的，但是它的优先级比较高，如果该进程等待的事件发生了，它就会转到就绪状态，一旦它转到就绪状态，如果我们的调度算法是以优先级来进行调度的，那么它就会立马抢占正在运行的进程，所以这个时候就会发生 CPU 调度。\n\n那第 2 种状态通常是时间片到的情况，因为时间片到了就会发生中断，于是就会抢占正在运行的进程，从而占用 CPU。\n\n调度算法影响的是等待时间（进程在就绪队列中等待调度的时间总和），而不能影响进程真在使用 CPU 的时间和 I/O 时间。\n\n接下来，说说常见的调度算法：\n\n- 先来先服务调度算法\n- 最短作业优先调度算法\n- 高响应比优先调度算法\n- 时间片轮转调度算法\n- 最高优先级调度算法\n- 多级反馈队列调度算法\n\n### 先来先服务调度算法\n\n最简单的一个调度算法，就是非抢占式的**先来先服务（*First Come First Severd, FCFS*）算法**了。\n\n![FCFS 调度算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/操作系统/进程和线程/24-先来先服务.jpg)\n\n顾名思义，先来后到，**每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**\n\n这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。\n\nFCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。\n\n\n### 最短作业优先调度算法\n\n**最短作业优先（*Shortest Job First, SJF*）调度算法**同样也是顾名思义，它会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量。\n\n![SJF 调度算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/操作系统/进程和线程/25-最短作业优先算法.jpg)\n\n这显然对长作业不利，很容易造成一种极端现象。\n\n比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。\n\n### 高响应比优先调度算法\n\n前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。\n\n那么，**高响应比优先\n（*Highest Response Ratio Next, HRRN*）调度算法**主要是权衡了短作业和长作业。\n\n**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**，「响应比优先级」的计算公式：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/操作系统/进程和线程/26-响应比公式.jpg)\n\n\n从上面的公式，可以发现：\n\n- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；\n- 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；\n\n\n### 时间片轮转调度算法\n\n最古老、最简单、最公平且使用最广的算法就是**时间片轮转（*Round Robin, RR*）调度算法**。\n。\n\n![RR 调度算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/操作系统/进程和线程/27-时间片轮询.jpg)\n\n**每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。**\n\n- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；\n- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；\n\n另外，时间片的长度就是一个很关键的点：\n\n- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；\n- 如果时间片设得太长又可能引起短作业进程的响应时间变长，不利于短作业。\n\n通常时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。\n\n### 最高优先级调度算法\n\n前面的「时间片轮转算法」做了个假设，即让所有的进程同等重要，也不偏袒谁，大家的运行时间都一样。\n\n但是，对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能**从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（*Highest Priority First，HPF*）调度算法**。\n\n进程的优先级可以分为，静态优先级或动态优先级：\n\n- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；\n- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。\n\n该算法也有两种处理优先级高的方法，非抢占式和抢占式：\n\n- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。\n- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。\n\n但是依然有缺点，可能会导致低优先级的进程永远不会运行。\n\n### 多级反馈队列调度算法\n\n**多级反馈队列（*Multilevel Feedback Queue*）调度算法**是「时间片轮转算法」和「最高优先级算法」的综合和发展。\n\n顾名思义：\n\n- 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。\n- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；\n\n![多级反馈队列](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/操作系统/进程和线程/28-多级队列.jpg)\n\n来看看，它是如何工作的：\n\n- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；\n- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；\n- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；\n\n可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长了，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**\n\n---\n\n## 内存页面置换算法\n\n在了解内存页面置换算法前，我们得先谈一下**缺页异常（缺页中断）**。\n\n当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：\n\n- 缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。\n- 缺页中断返回到该指令的开始重新执行「该指令」，而一般中断返回回到该指令的「下一个指令」执行。\n\n我们来看一下缺页中断的处理流程，如下图：\n\n![缺页中断的处理流程](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/缺页异常流程.png)\n\n1. 在 CPU 里访问一条 Load M 指令，然后 CPU 会去找 M 所对应的页表项。\n2. 如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。\n3. 操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置。\n4. 找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。\n5. 页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。\n6. 最后，CPU 重新执行导致缺页异常的指令。\n\n上面所说的过程，第 4 步是能在物理内存找到空闲页的情况，那如果找不到呢？\n\n找不到空闲页的话，就说明此时内存已满了，这时候，就需要「页面置换算法」选择一个物理页，如果该物理页有被修改过（脏页），则把它换出到磁盘，然后把该被置换出去的页表项的状态改成「无效的」，最后把正在访问的页面装入到这个物理页中。\n\n这里提一下，页表项通常有如下图的字段：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/页表项字段.png)\n\n那其中：\n\n- *状态位*：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。\n- *访问字段*：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。\n- *修改位*：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。\n- *硬盘地址*：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。\n\n\n这里我整理了虚拟内存的管理整个流程，你可以从下面这张图看到：\n\n![虚拟内存的流程](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/虚拟内存管理流程.png)\n\n\n所以，页面置换算法的功能是，**当出现缺页异常，需调入新页面而内存已满时，选择被置换的物理页面**，也就是说选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。\n\n那其算法目标则是，尽可能减少页面的换入换出的次数，常见的页面置换算法有如下几种：\n\n- 最佳页面置换算法（*OPT*）\n- 先进先出置换算法（*FIFO*）\n- 最近最久未使用的置换算法（*LRU*）\n- 时钟页面置换算法（*Lock*）\n- 最不常用置换算法（*LFU*）\n\n### 最佳页面置换算法\n\n最佳页面置换算法基本思路是，**置换在「未来」最长时间不访问的页面**。\n\n所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。\n\n\n我们举个例子，假设一开始有 3 个空闲的物理页，然后有请求的页面序列，那它的置换过程如下图：\n\n![最佳页面置换算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/最优置换算法.png)\n\n在这个请求的页面序列中，缺页共发生了 `7` 次（空闲页换入 3 次 + 最优页面置换 4 次），页面置换共发生了 `4` 次。\n\n这很理想，但是实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。\n\n所以，最佳页面置换算法作用是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。\n\n### 先进先出置换算法\n\n既然我们无法预知页面在下一次访问前所需的等待时间，那我们可以**选择在内存驻留时间很长的页面进行中置换**，这个就是「先进先出置换」算法的思想。\n\n还是以前面的请求的页面序列作为例子，假设使用先进先出置换算法，则过程如下图：\n\n\n![先进先出置换算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/FIFO置换算法.png)\n\n\n在这个请求的页面序列中，缺页共发生了 `10` 次，页面置换共发生了 `7` 次，跟最佳页面置换算法比较起来，性能明显差了很多。\n\n\n### 最近最久未使用的置换算法\n\n最近最久未使用（*LRU*）的置换算法的基本思路是，发生缺页时，**选择最长时间没有被访问的页面进行置换**，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。\n\n这种算法近似最优置换算法，最优置换算法是通过「未来」的使用情况来推测要淘汰的页面，而 LRU 则是通过「历史」的使用情况来推测要淘汰的页面。\n\n\n还是以前面的请求的页面序列作为例子，假设使用最近最久未使用的置换算法，则过程如下图：\n\n![最近最久未使用的置换算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/LRU置换算法.png)\n\n\n在这个请求的页面序列中，缺页共发生了 `9` 次，页面置换共发生了 `6` 次，跟先进先出置换算法比较起来，性能提高了一些。\n\n\n虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。\n\n困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。\n\n所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。\n\n### 时钟页面置换算法\n\n那有没有一种即能优化置换的次数，也能方便实现的算法呢？\n\n时钟页面置换算法就可以两者兼得，它跟 LRU 近似，又是对 FIFO 的一种改进。\n\n该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。\n\n\n当发生缺页中断时，算法首先检查表针指向的页面：\n\n- 如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；\n- 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；\n\n我画了一副时钟页面置换算法的工作流程图，你可以在下方看到：\n\n![时钟页面置换算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/时钟置换算法.png)\n\n了解了这个算法的工作方式，就明白为什么它被称为时钟（*Clock*）算法了。\n\n### 最不常用算法\n\n最不常用（*LFU*）算法，这名字听起来很调皮，但是它的意思不是指这个算法不常用，而是**当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰**。\n\n它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。\n\n看起来很简单，每个页面加一个计数器就可以实现了，但是在操作系统中实现的时候，我们需要考虑效率和硬件成本的。\n\n要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。\n\n但还有个问题，LFU 算法只考虑了频率问题，没考虑时间的问题，比如有些页面在过去时间里访问的频率很高，但是现在已经没有访问了，而当前频繁访问的页面由于没有这些页面访问的次数高，在发生缺页中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不高的页面。\n\n那这个问题的解决的办法还是有的，可以定期减少访问的次数，比如当发生时间中断时，把过去时间访问的页面的访问次数除以 2，也就说，随着时间的流失，以前的高访问次数的页面会慢慢减少，相当于加大了被置换的概率。\n\n---\n\n## 磁盘调度算法\n\n我们来看看磁盘的结构，如下图：\n\n![磁盘的结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/磁盘结构.jpg)\n\n常见的机械磁盘是上图左边的样子，中间圆的部分是磁盘的盘片，一般会有多个盘片，每个盘面都有自己的磁头。右边的图就是一个盘片的结构，盘片中的每一层分为多个磁道，每个磁道分多个扇区，每个扇区是 `512` 字节。那么，多个具有相同编号的磁道形成一个圆柱，称之为磁盘的柱面，如上图里中间的样子。\n\n磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的。\n\n寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省一些不必要的寻道时间，从而提高磁盘的访问性能。\n\n假设有下面一个请求序列，每个数字代表磁道的位置：\n\n98，183，37，122，14，124，65，67\n\n初始磁头当前的位置是在第 `53` 磁道。\n\n接下来，分别对以上的序列，作为每个调度算法的例子，那常见的磁盘调度算法有：\n\n- 先来先服务算法\n- 最短寻道时间优先算法\n- 扫描算法算法\n- 循环扫描算法\n- LOOK 与 C-LOOK 算法\n\n### 先来先服务\n\n先来先服务（*First-Come，First-Served，FCFS*），顾名思义，先到来的请求，先被服务。\n\n那按照这个序列的话：\n\n98，183，37，122，14，124，65，67\n\n那么，磁盘的写入顺序是从左到右，如下图：\n\n![先来先服务](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E5%85%88%E6%9D%A5%E5%85%88%E6%9C%8D%E5%8A%A1.png)\n\n\n先来先服务算法总共移动了 `640` 个磁道的距离，这么一看这种算法，比较简单粗暴，但是如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过长。\n\n### 最短寻道时间优先\n\n最短寻道时间优先（*Shortest Seek First，SSF*）算法的工作方式是，优先选择从当前磁头位置所需寻道时间最短的请求，还是以这个序列为例子：\n\n98，183，37，122，14，124，65，67\n\n那么，那么根据距离磁头（ 53 位置）最近的请求的算法，具体的请求则会是下列从左到右的顺序：\n\n65，67，37，14，98，122，124，183\n\n![最短寻道时间优先](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%9C%80%E7%9F%AD%E5%AF%BB%E9%81%93%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88.png)\n\n\n磁头移动的总距离是 `236` 磁道，相比先来先服务性能提高了不少。\n\n但这个算法可能存在某些请求的**饥饿**，因为本次例子我们是静态的序列，看不出问题，假设是一个动态的请求，如果后续来的请求都是小于 183 \n磁道的，那么 183 磁道可能永远不会被响应，于是就产生了饥饿现象，这里**产生饥饿的原因是磁头在一小块区域来回移动**。\n\n## 扫描算法\n\n最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。\n\n为了防止这个问题，可以规定：**磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（*Scan*）算法**。\n\n这种算法也叫做电梯算法，比如电梯保持按一个方向移动，直到在那个方向上没有请求为止，然后改变方向。\n\n还是以这个序列为例子，磁头的初始位置是 53：\n\n98，183，37，122，14，124，65，67\n\n那么，假设扫描调度算先朝磁道号减少的方向移动，具体请求则会是下列从左到右的顺序：\n\n37，14，`0`，65，67，98，122，124，183\n\n![扫描算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95.png)\n\n磁头先响应左边的请求，直到到达最左端（ 0 磁道）后，才开始反向移动，响应右边的请求。\n\n扫描调度算法性能较好，不会产生饥饿现象，但是存在这样的问题，中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。\n\n\n### 循环扫描算法\n\n扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。\n\n循环扫描（*Circular Scan, CSCAN* ）规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且**返回中途不处理任何请求**，该算法的特点，就是**磁道只响应一个方向上的请求**。\n\n还是以这个序列为例子，磁头的初始位置是 53：\n\n98，183，37，122，14，124，65，67\n\n那么，假设循环扫描调度算先朝磁道增加的方向移动，具体请求会是下列从左到右的顺序：\n\n65，67，98，122，124，183，`199`，`0`，14，37\n\n![循环扫描算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-C-SCAN%E7%AE%97%E6%B3%95.png)\n\n磁头先响应了右边的请求，直到碰到了最右端的磁道 199，就立即回到磁盘的开始处（磁道 0），但这个返回的途中是不响应任何请求的，直到到达最开始的磁道后，才继续顺序响应右边的请求。\n\n循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。\n\n### LOOK 与 C-LOOK算法\n\n我们前面说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。\n\n那这其实是可以优化的，优化的思路就是**磁头在移动到「最远的请求」位置，然后立即反向移动。**\n\n那针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中会响应请求**。\n\n\n![LOOK 算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/磁盘调度-LOOK算法.png)\n\n\n而针 C-SCAN 算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中不会响应请求**。\n\n![C-LOOK 算法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/调度算法/磁盘调度-C-LOOK算法.png)\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/6_file_system/file_system":{"title":"file_system","content":"# 7.1 文件系统全家桶\n\n不多 BB，直接上「硬菜」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/文件系统/文件系统-提纲.png)\n\n---\n\n\n## 文件系统的基本组成\n\n文件系统是操作系统中负责管理持久数据的子系统，说简单点，就是负责把用户的文件存到磁盘硬件中，因为即使计算机断电了，磁盘里的数据并不会丢失，所以可以持久化的保存文件。\n\n文件系统的基本数据单位是文件，它的目的是对磁盘上的文件进行组织管理，那组织的方式不同，就会形成不同的文件系统。\n\nLinux 最经典的一句话是：「**一切皆文件**」，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的。\n\nLinux 文件系统会为每个文件分配两个数据结构：**索引节点（*index node*）和目录项（*directory entry*）**，它们主要用来记录文件的元信息和目录层次结构。\n\n- 索引节点，也就是 *inode*，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、**数据在磁盘的位置**等等。索引节点是文件的**唯一**标识，它们之间一一对应，也同样都会被存储在硬盘中，所以**索引节点同样占用磁盘空间**。\n- 目录项，也就是 *dentry*，用来记录文件的名字、**索引节点指针**以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，**目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存**。\n\n由于索引节点唯一标识一个文件，而目录项记录着文件的名字，所以目录项和索引节点的关系是多对一，也就是说，一个文件可以有多个别名。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。\n\n注意，目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。\n\n\u003e 目录项和目录是一个东西吗？\n\n虽然名字很相近，但是它们不是一个东西，目录是个文件，持久化存储在磁盘，而目录项是内核一个数据结构，缓存在内存。\n\n如果查询目录频繁从磁盘读，效率会很低，所以内核会把已经读过的目录用目录项这个数据结构缓存在内存，下次再次读到相同的目录时，只需从内存读就可以，大大提高了文件系统的效率。\n\n注意，目录项这个数据结构不只是表示目录，也是可以表示文件的。\n\n\u003e 那文件数据是如何存储在磁盘的呢？\n\n磁盘读写的最小单位是**扇区**，扇区的大小只有 `512B` 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。\n\n所以，文件系统把多个扇区组成了一个**逻辑块**，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 `4KB`，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。\n\n以上就是索引节点、目录项以及文件数据的关系，下面这个图就很好的展示了它们之间的关系：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/文件系统/目录项和索引关系图.png)\n\n索引节点是存储在硬盘上的数据，那么为了加速文件的访问，通常会把索引节点加载到内存中。\n\n另外，磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。\n\n- *超级块*，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。\n- *索引节点区*，用来存储索引节点；\n- *数据块区*，用来存储文件或目录数据；\n\n我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：\n\n- 超级块：当文件系统挂载时进入内存；\n- 索引节点区：当文件被访问时进入内存；\n\n---\n\n## 虚拟文件系统\n\n\n文件系统的种类众多，而操作系统希望**对用户提供一个统一的接口**，于是在用户层与文件系统层引入了中间层，这个中间层就称为**虚拟文件系统（*Virtual File System，VFS*）。**\n\nVFS 定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。\n\n在 Linux 文件系统中，用户空间、系统调用、虚拟文件系统、缓存、文件系统以及存储之间的关系如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png)\n\nLinux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类：\n\n- *磁盘的文件系统*，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。\n- *内存的文件系统*，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 `/proc` 和 `/sys` 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。\n- *网络的文件系统*，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。\n\n文件系统首先要先挂载到某个目录才可以正常使用，比如 Linux 系统在启动时，会把文件系统挂载到根目录。\n\n---\n\n## 文件的使用\n\n我们从用户角度来看文件的话，就是我们要怎么使用文件？首先，我们得通过系统调用来打开一个文件。\n\n![write 的过程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%86%99%E5%88%B0%E7%A3%81%E7%9B%98%E8%BF%87%E7%A8%8B.png)\n\n```c\nfd = open(name, flag); # 打开文件\n...\nwrite(fd,...);         # 写数据\n...\nclose(fd);             # 关闭文件\n```\n\n上面简单的代码是读取一个文件的过程：\n\n- 首先用 `open` 系统调用打开文件，`open` 的参数中包含文件的路径名和文件名。\n- 使用 `write` 写数据，其中 `write` 使用 `open` 所返回的**文件描述符**，并不使用文件名作为参数。\n- 使用完文件后，要用 `close` 系统调用关闭文件，避免资源的泄露。\n\n我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪呢，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「**文件描述符**」，所以说文件描述符是打开文件的标识。\n\n![打开文件表](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E8%A1%A8.png)\n\n操作系统在打开文件表中维护着打开文件的状态和信息：\n\n- 文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；\n- 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目；\n- 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；\n- 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求；\n\n\n在用户视角里，文件就是一个持久化的数据结构，但操作系统并不会关心你想存在磁盘上的任何的数据结构，操作系统的视角是如何把文件数据和磁盘块对应起来。\n\n所以，用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统则是以数据块来读写文件，那屏蔽掉这种差异的工作就是文件系统了。\n\n我们来分别看一下，读文件和写文件的过程：\n\n- 当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。\n- 当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。\n\n所以说，**文件系统的基本操作单位是数据块**。\n\n---\n\n## 文件的存储\n\n\n文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：\n\n- 连续空间存放方式\n- 非连续空间存放方式\n\n其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。\n\n不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。\n\n### 连续空间存放方式\n\n连续空间存放方式顾名思义，**文件存放在磁盘「连续的」物理空间中**。这种模式下，文件的数据都是紧密相连，**读写效率很高**，因为一次磁盘寻道就可以读出整个文件。\n\n使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。\n\n所以，**文件头里需要指定「起始块的位置」和「长度」**，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。\n\n注意，此处说的文件头，就类似于 Linux 的 inode。\n\n\n![连续空间存放方式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F.png)\n\n\n连续空间存放的方式虽然读写效率高，**但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。**\n\n如下图，如果文件 B 被删除，磁盘上就留下一块空缺，这时，如果新来的文件小于其中的一个空缺，我们就可以将其放在相应空缺里。但如果该文件的大小大于所有的空缺，但却小于空缺大小之和，则虽然磁盘上有足够的空缺，但该文件还是不能存放。当然了，我们可以通过将现有文件进行挪动来腾出空间以容纳新的文件，但是这个在磁盘挪动文件是非常耗时，所以这种方式不太现实。\n\n![磁盘碎片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E7%A3%81%E7%9B%98%E7%A2%8E%E7%89%87.png)\n\n\n另外一个缺陷是文件长度扩展不方便，例如上图中的文件 A 要想扩大一下，需要更多的磁盘空间，唯一的办法就只能是挪动的方式，前面也说了，这种方式效率是非常低的。\n\n那么有没有更好的方式来解决上面的问题呢？答案当然有，既然连续空间存放的方式不太行，那么我们就改变存放的方式，使用非连续空间存放方式来解决这些缺陷。\n\n### 非连续空间存放方式\n\n非连续空间存放方式分为「链表方式」和「索引方式」。\n\n\u003e 我们先来看看链表的方式。\n\n链表的方式存放是**离散的，不用连续的**，于是就可以**消除磁盘碎片**，可大大提高磁盘空间的利用率，同时**文件的长度可以动态扩展**。根据实现的方式的不同，链表可分为「**隐式链表**」和「**显式链接**」两种形式。\n\n文件要以「**隐式链表**」的方式存放的话，**实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置**，这样一个数据块连着一个数据块，从链头开始就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。\n\n![隐式链表](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E9%93%BE%E8%A1%A8%E6%96%B9%E5%BC%8F.png)\n\n隐式链表的存放方式的**缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间**。隐式链接分配的**稳定性较差**，系统在运行过程中由于软件或者硬件错误**导致链表中的指针丢失或损坏，会导致文件数据的丢失。**\n\n如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「**显式链接**」，它指**把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中**，该表在整个磁盘仅设置一张，**每个表项中存放链接指针，指向下一个数据块号**。\n\n对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为**文件分配表（*File Allocation Table，FAT*）**。\n\n![显式链接](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E5%88%86%E9%85%8D%E8%A1%A8.png)\n\n\n由于查找记录的过程是在内存中进行的，因而不仅显著地**提高了检索速度**，而且**大大减少了访问磁盘的次数**。但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适用于大磁盘**。\n\n比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。\n\n\n\u003e 接下来，我们来看看索引的方式。\n\n链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。\n\n索引的实现是为每个文件创建一个「**索引数据块**」，里面存放的是**指向文件数据块的指针列表**，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。\n\n另外，**文件头需要包含指向「索引数据块」的指针**，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。\n\n创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。\n\n![索引的方式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F.png)\n\n索引的方式优点在于：\n\n- 文件的创建、增大、缩小很方便；\n- 不会有碎片的问题；\n- 支持顺序读写和随机读写；\n\n由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。\n\n\n如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。\n\n先来看看链表 + 索引的组合，这种组合称为「**链式索引块**」，它的实现方式是**在索引数据块留出一个存放下一个索引数据块的指针**，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。\n\n\n![链式索引块](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%93%BE%E5%BC%8F%E7%B4%A2%E5%BC%95%E5%9D%97.png)\n\n\n还有另外一种组合方式是索引 + 索引的方式，这种组合称为「**多级索引块**」，实现方式是**通过一个索引块来存放多个索引数据块**，一层套一层索引，像极了俄罗斯套娃是吧。\n\n![多级索引块](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95%E5%9D%97.png)\n\n\n### Unix 文件的实现方式\n\n我们先把前面提到的文件实现方式，做个比较：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/文件系统/文件存储方式比较.png)\n\n\n那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图：\n\n![早期 Unix 文件系统](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/Unix%20%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95.png)\n\n它是根据文件的大小，存放的方式会有所变化：\n\n- 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；\n- 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；\n- 如果前面两种方式都不够存放大文件，则采用二级间接索引方式；\n- 如果二级间接索引也不够存放大文件，这采用三级间接索引方式；\n\n那么，文件头（*Inode*）就需要包含 13 个指针：\n\n- 10 个指向数据块的指针；\n- 第 11 个指向索引块的指针；\n- 第 12 个指向二级索引块的指针；\n- 第 13 个指向三级索引块的指针；\n\n所以，这种方式能很灵活地支持小文件和大文件的存放：\n\n- 对于小文件使用直接查找的方式可减少索引数据块的开销；\n- 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；\n\n这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是对于大文件的访问，需要大量的查询，效率比较低。\n\n为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。\n\n---\n\n## 空闲空间管理\n\n前面说到的文件的存储是针对已经被占用的数据块组织和管理，接下来的问题是，如果我要保存一个数据块，我应该放在硬盘上的哪个位置呢？难道需要将所有的块扫描一遍，找个空的地方随便放吗？\n\n那这种方式效率就太低了，所以针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：\n\n- 空闲表法\n- 空闲链表法\n- 位图法\n\n### 空闲表法\n\n空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：\n\n![空闲表法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E8%A1%A8%E6%B3%95.png)\n\n当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。\n\n这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。\n\n\n### 空闲链表法\n\n我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图：\n\n\n![空闲链表法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E5%9D%97%E9%93%BE%E8%A1%A8.png)\n\n当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。\n\n这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。\n\n空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。\n\n### 位图法\n\n\n位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。\n\n当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：\n\n```\n1111110011111110001110110111111100111 ...\n```\n\n在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。\n\n---\n\n## 文件系统的结构\n\n前面提到 Linux 是用位图的方式管理空闲空间，用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。\n\n数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 `4 * 1024 * 8 = 2^15` 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 `2^15 * 4 * 1024 = 2^27` 个 byte，也就是 128M。\n\n\n也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。\n\n在 Linux 文件系统，把这个结构称为一个**块组**，那么有 N 多的块组，就能够表示 N 大的文件。\n\n下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/文件系统/块组.png)\n\n最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：\n\n- *超级块*，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。\n- *块组描述符*，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。\n- *数据位图和 inode 位图*， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。\n- *inode 列表*，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。\n- *数据块*，包含文件的有用数据。\n\n你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因：\n\n- 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。\n- 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。\n\n不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。\n\n---\n\n## 目录的存储\n\n在前面，我们知道了一个普通文件是如何存储的，但还有一个特殊的文件，经常用到的目录，它是如何保存的呢？\n\n基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 `vim` 打开它，它也有 inode，inode 里面也是指向一些块。\n\n和普通文件不同的是，**普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。**\n\n在目录文件的块中，最简单的保存格式就是**列表**，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。\n\n列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。\n\n![目录格式哈希表](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E5%93%88%E5%B8%8C%E8%A1%A8.png)\n\n通常，第一项是「`.`」，表示当前目录，第二项是「`..`」，表示上一级目录，接下来就是一项一项的文件名和 inode。\n\n如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。\n\n于是，保存目录的格式改成**哈希表**，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。\n\nLinux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。\n\n目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。\n\n---\n\n## 软链接和硬链接\n\n有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过**硬链接（*Hard Link*）** 和**软链接（*Symbolic Link*）** 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。\n\n硬链接是**多个目录项中的「索引节点」指向一个文件**，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以**硬链接是不可用于跨文件系统的**。由于多个目录项都是指向一个 inode，那么**只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。**\n\n![硬链接](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A1%AC%E9%93%BE%E6%8E%A5-2.png)\n\n\n软链接相当于重新创建一个文件，这个文件有**独立的 inode**，但是这个**文件的内容是另外一个文件的路径**，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以**软链接是可以跨文件系统的**，甚至**目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。**\n\n![软链接](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BD%AF%E9%93%BE%E6%8E%A5.png)\n\n---\n\n## 文件 I/O\n\n文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有\n\n- 缓冲与非缓冲 I/O\n- 直接与非直接 I/O\n- 阻塞与非阻塞 I/O VS 同步与异步 I/O\n\n接下来，分别对这些分类讨论讨论。\n\n### 缓冲与非缓冲 I/O\n\n文件操作的标准库是可以实现数据的缓存，那么**根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O**：\n\n- 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。\n- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。\n\n这里所说的「缓冲」特指标准库内部实现的缓冲。\n\n比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。\n\n### 直接与非直接 I/O\n\n我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。\n\n那么，**根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O**：\n\n- 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。\n- 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。\n\n如果你在使用文件操作类的系统调用函数时，指定了 `O_DIRECT` 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。\n\n\u003e 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？\n\n以下几种场景会触发内核缓存的数据写入磁盘：\n\n- 在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；\n- 用户主动调用 `sync`，内核缓存会刷到磁盘上；\n- 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；\n- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；\n\n###  阻塞与非阻塞 I/O VS 同步与异步 I/O\n\n为什么把阻塞 / 非阻塞与同步与异步放一起说的呢？因为它们确实非常相似，也非常容易混淆，不过它们之间的关系还是有点微妙的。\n\n\n先来看看**阻塞 I/O**，当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。\n\n注意，**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。过程如下图：\n\n![阻塞 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png)\n\n\n知道了阻塞 I/O ，来看看**非阻塞 I/O**，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。过程如下图：\n\n![非阻塞 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png)\n\n\n注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**\n\n\n举个例子，访问管道或 socket 时，如果设置了 `O_NONBLOCK` 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。\n\n应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。\n\n为了解决这种傻乎乎轮询方式，于是 **I/O 多路复用**技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。\n\n这个做法大大改善了 CPU 的利用率，因为当调用了 I/O 多路复用接口，如果没有事件发生，那么当前线程就会发生阻塞，这时 CPU 会切换其他线程执行任务，等内核发现有事件到来的时候，会唤醒阻塞在 I/O 多路复用接口的线程，然后用户可以进行后续的事件处理。\n\n整个流程要比阻塞 IO 要复杂，似乎也更浪费性能。但 **I/O 多路复用接口最大的优势在于，用户可以在一个线程内同时处理多个 socket 的 IO 请求**（参见：[I/O 多路复用：select/poll/epoll](https://xiaolincoding.com/os/8_network_system/selete_poll_epoll.html)）。用户可以注册多个 socket，然后不断地调用  I/O 多路复用接口读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。\n\n下图是使用 select I/O 多路复用过程。注意，`read` 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个**同步的过程**，需要等待：\n\n![I/O 多路复用](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png)\n\n\n实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用**都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。**\n\n而真正的**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。\n\n当我们发起 `aio_read` 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图：\n\n![异步 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png)\n\n下面这张图，总结了以上几种 I/O 模型：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/操作系统/文件系统/同步VS异步IO.png)\n\n\n在前面我们知道了，I/O 是分为两个过程的：\n\n1. 数据准备的过程\n2. 数据从内核空间拷贝到用户进程缓冲区的过程\n\n阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。\n\n异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。\n\n\u003e 用故事去理解这几种 I/O 模型\n\n\n举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。\n\n阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。\n\n\n非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。\n\n基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（`select` 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。\n\n异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/6_file_system/pagecache":{"title":"pagecache","content":"# 7.2 进程写文件时，进程发生了崩溃，已写入的数据会丢失吗？\n\n大家好，我是小林。\n\n前几天，有位读者问了我这么个问题：\n\n![](https://img-blog.csdnimg.cn/img_convert/23eb7000c28fb8135e0947620a75d946.png)\n\n大概就是，进程写文件（使用缓冲 IO）过程中，写一半的时候，进程发生了崩溃，已写入的数据会丢失吗？\n\n答案，是不会的。\n\n![](https://img-blog.csdnimg.cn/img_convert/1541c881598f554920355f0a3c5780fd.png)\n\n因为进程在执行 write （使用缓冲 IO）系统调用的时候，实际上是将文件数据写到了内核的 page cache，它是文件系统中用于缓存文件数据的缓冲，所以即使进程崩溃了，文件数据还是保留在内核的 page cache，我们读数据的时候，也是从内核的 page cache 读取，因此还是依然读的进程崩溃前写入的数据。\n\n内核会找个合适的时机，将  page cache 中的数据持久化到磁盘。但是如果 page cache 里的文件数据，在持久化到磁盘化到磁盘之前，系统发生了崩溃，那这部分数据就会丢失了。\n\n当然， 我们也可以在程序里调用 fsync 函数，在写文文件的时候，立刻将文件数据持久化到磁盘，这样就可以解决系统崩溃导致的文件数据丢失的问题。\n\n我在网上看到一篇介绍 page cache 很好的文章， 分享给大家一起学习。\n\n\u003e 作者：spongecaptain\n\u003e\n\u003e 原文地址：[Linux 的 Page Cache](https://spongecaptain.cool/SimpleClearFileIO/1.%20page%20cache.html)\n\n## Page Cache\n\n### Page Cache 是什么？\n\n为了理解 Page Cache，我们不妨先看一下 Linux 的文件 I/O 系统，如下图所示：\n\n![](https://img-blog.csdnimg.cn/img_convert/72568a29816fa9b505f15edac68adee2.png)\n\n上图中，红色部分为 Page Cache。可见 Page Cache 的本质是由 Linux 内核管理的内存区域。我们通过 mmap 以及 buffered I/O 将文件读取到内存空间实际上都是读取到 Page Cache 中。\n\n### 如何查看系统的 Page Cache？\n\n通过读取 `/proc/meminfo` 文件，能够实时获取系统内存情况：\n\n```shell\n$ cat /proc/meminfo\n...\nBuffers:            1224 kB\nCached:           111472 kB\nSwapCached:        36364 kB\nActive:          6224232 kB\nInactive:         979432 kB\nActive(anon):    6173036 kB\nInactive(anon):   927932 kB\nActive(file):      51196 kB\nInactive(file):    51500 kB\n...\nShmem:             10000 kB\n...\nSReclaimable:      43532 kB\n...\n```\n\n根据上面的数据，你可以简单得出这样的公式（等式两边之和都是 112696 KB）：\n\n```\nBuffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached\n```\n\n两边等式都是 Page Cache，即：\n\n```\nPage Cache = Buffers + Cached + SwapCached\n```\n\n通过阅读下面的小节，就能够理解为什么 SwapCached 与 Buffers 也是 Page Cache 的一部分。\n\n### page 与 Page Cache\n\npage 是内存管理分配的基本单位， Page Cache 由多个 page 构成。page 在操作系统中通常为 4KB 大小（32bits/64bits），而 Page Cache 的大小则为 4KB 的整数倍。\n\n**另一方面，并不是所有 page 都被组织为 Page Cache**。\n\nLinux 系统上供用户可访问的内存分为两个类型，即：\n\n- File-backed pages：文件备份页也就是 Page Cache 中的 page，对应于磁盘上的若干数据块；对于这些页最大的问题是脏页回盘；\n- Anonymous pages：匿名页不对应磁盘上的任何磁盘数据块，它们是进程的运行是内存空间（例如方法栈、局部变量表等属性）；\n\n**为什么 Linux 不把 Page Cache 称为 block cache，这不是更好吗？**\n\n这是因为从磁盘中加载到内存的数据不仅仅放在 Page Cache 中，还放在 buffer cache 中。\n\n例如通过 Direct I/O 技术的磁盘文件就不会进入 Page Cache 中。当然，这个问题也有 Linux 历史设计的原因，毕竟这只是一个称呼，含义随着 Linux 系统的演进也逐渐不同。\n\n下面比较一下 File-backed pages 与 Anonymous pages 在 Swap 机制下的性能。\n\n内存是一种珍惜资源，当内存不够用时，内存管理单元（Memory Mangament Unit）需要提供调度算法来回收相关内存空间。内存空间回收的方式通常就是 swap，即交换到持久化存储设备上。\n\nFile-backed pages（Page Cache）的内存回收代价较低。Page Cache 通常对应于一个文件上的若干顺序块，因此可以通过顺序 I/O 的方式落盘。另一方面，如果 Page Cache 上没有进行写操作（所谓的没有脏页），甚至不会将 Page Cache 回盘，因为数据的内容完全可以通过再次读取磁盘文件得到。\n\nPage Cache 的主要难点在于脏页回盘，这个内容会在后面进行详细说明。\n\nAnonymous pages 的内存回收代价较高。这是因为 Anonymous pages 通常随机地写入持久化交换设备。另一方面，无论是否有写操作，为了确保数据不丢失，Anonymous pages 在 swap 时必须持久化到磁盘。\n\n### Swap 与缺页中断\n\nSwap 机制指的是当物理内存不够用，内存管理单元（Memory Mangament Unit，MMU）需要提供调度算法来回收相关内存空间，然后将清理出来的内存空间给当前内存申请方。\n\nSwap 机制存在的本质原因是 Linux 系统提供了虚拟内存管理机制，每一个进程认为其独占内存空间，因此所有进程的内存空间之和远远大于物理内存。所有进程的内存空间之和超过物理内存的部分就需要交换到磁盘上。\n\n操作系统以 page 为单位管理内存，当进程发现需要访问的数据不在内存时，操作系统可能会将数据以页的方式加载到内存中。上述过程被称为**缺页中断**，当操作系统发生缺页中断时，就会通过系统调用将 page 再次读到内存中。\n\n但主内存的空间是有限的，当主内存中不包含可以使用的空间时，操作系统会从选择合适的物理内存页驱逐回磁盘，为新的内存页让出位置，**选择待驱逐页的过程在操作系统中叫做页面替换（Page Replacement）**，替换操作又会触发 swap 机制。\n\n如果物理内存足够大，那么可能不需要 Swap 机制，但是 Swap 在这种情况下还是有一定优势：对于有发生内存泄漏几率的应用程序（进程），Swap 交换分区更是重要，这可以确保内存泄露不至于导致物理内存不够用，最终导致系统崩溃。但内存泄露会引起频繁的 swap，此时非常影响操作系统的性能。\n\nLinux 通过一个 swappiness 参数来控制 Swap 机制：这个参数值可为 0-100，控制系统 swap 的优先级：\n\n- 高数值：较高频率的 swap，进程不活跃时主动将其转换出物理内存。\n- 低数值：较低频率的 swap，这可以确保交互式不因为内存空间频繁地交换到磁盘而提高响应延迟。\n\n**最后，为什么 SwapCached 也是 Page Cache 的一部分？**\n\n这是因为当匿名页（Inactive(anon) 以及 Active(anon)）先被交换（swap out）到磁盘上后，然后再加载回（swap in）内存中，由于读入到内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属于 Page Cache。这个过程如下图所示。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/cbba24cac4668625c4e32d8cf641cf9c.png)\n\n### Page Cache 与 buffer cache\n\n执行 free 命令，注意到会有两列名为 buffers 和 cached，也有一行名为 “-/+ buffers/cache”。\n\n```\n~ free -m\n             total       used       free     shared    buffers     cached\nMem:        128956      96440      32515          0       5368      39900\n-/+ buffers/cache:      51172      77784\nSwap:        16002          0      16001\n```\n\n其中，cached 列表示当前的页缓存（Page Cache）占用量，buffers 列表示当前的块缓存（buffer cache）占用量。\n\n用一句话来解释：**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。**\n\n- 页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；\n- 块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。\n\nPage Cache 与 buffer cache 的**共同目的都是加速数据 I/O**：\n\n- 写数据时首先写到缓存，将写入的页标记为 dirty，然后向外部存储 flush，也就是缓存写机制中的 write-back（另一种是 write-through，Linux 默认情况下不采用）；\n- 读数据时首先读取缓存，如果未命中，再去外部存储读取，并且将读取来的数据也加入缓存。操作系统总是积极地将所有空闲内存都用作 Page Cache 和 buffer cache，当内存不够用时也会用 LRU 等算法淘汰缓存页。\n\n在 Linux 2.4 版本的内核之前，Page Cache 与 buffer cache 是完全分离的。但是，块设备大多是磁盘，磁盘上的数据又大多通过文件系统来组织，这种设计导致很多数据被缓存了两次，浪费内存。\n\n**所以在 2.4 版本内核之后，两块缓存近似融合在了一起：如果一个文件的页加载到了 Page Cache，那么同时 buffer cache 只需要维护块指向页的指针就可以了**。只有那些没有文件表示的块，或者绕过了文件系统直接操作（如dd命令）的块，才会真正放到 buffer cache 里。\n\n因此，**我们现在提起 Page Cache，基本上都同时指 Page Cache 和 buffer cache 两者，本文之后也不再区分，直接统称为 Page Cache**。\n\n下图近似地示出 32-bit Linux 系统中可能的一种 Page Cache 结构，其中 block size 大小为 1KB，page size 大小为 4KB。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c81ffa0b7d11506ffad3c33001385444.png)\n\nPage Cache 中的每个文件都是一棵基数树（radix tree，本质上是多叉搜索树），树的每个节点都是一个页。根据文件内的偏移量就可以快速定位到所在的页，如下图所示。关于基数树的原理可以参见英文维基，这里就不细说了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/cfda154558181c4af27a34c1d4a97552.png)\n\n### Page Cache 与预读\n\n操作系统为基于 Page Cache 的读缓存机制提供**预读机制**（PAGE_READAHEAD），一个例子是：\n\n- 用户线程仅仅请求读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至少会读 0-4KB 的内容，这恰好可以在一个 page 中装下。\n- 但是操作系统出于局部性原理会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是额外在内存中申请了 3 个 page；\n\n下图代表了操作系统的预读机制：\n\n![](https://img-blog.csdnimg.cn/img_convert/ae8252378169c8c14b8b9907983f7d8b.png)\n\n上图中，应用程序利用 read 系统调动读取 4KB 数据，实际上内核使用 readahead 机制完成了 16KB 数据的读取。\n\n## Page Cache 与文件持久化的一致性\u0026可靠性\n\n现代 Linux 的 Page Cache 正如其名，是对磁盘上 page（页）的内存缓存，同时可以用于读/写操作。\n\n任何系统引入缓存，就会引发一致性问题：内存中的数据与磁盘中的数据不一致，例如常见后端架构中的 Redis 缓存与 MySQL 数据库就存在一致性问题。\n\nLinux 提供多种机制来保证数据一致性，但无论是单机上的内存与磁盘一致性，还是分布式组件中节点 1 与节点 2 、节点 3 的数据一致性问题，理解的关键是 trade-off：吞吐量与数据一致性保证是一对矛盾。\n\n首先，需要我们理解一下文件的数据。**文件 = 数据 + 元数据**。元数据用来描述文件的各种属性，也必须存储在磁盘上。因此，我们说保证文件一致性其实包含了两个方面：数据一致+元数据一致。\n\n\u003e 文件的元数据包括：文件大小、创建时间、访问时间、属主属组等信息。\n\n我们考虑如下一致性问题：如果发生写操作并且对应的数据在 Page Cache 中，那么写操作就会直接作用于 Page Cache 中，此时如果数据还没刷新到磁盘，那么内存中的数据就领先于磁盘，此时对应 page 就被称为 Dirty page。\n\n当前 Linux 下以两种方式实现文件一致性：\n\n1. **Write Through（写穿）**：向用户层提供特定接口，应用程序可主动调用接口来保证文件一致性；\n2. **Write back（写回）**：系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块，这是默认的 Linux 一致性方案；\n\n上述两种方式最终都依赖于系统调用，主要分为如下三种系统调用：\n\n| 方法              | 含义                                                         |\n| :---------------- | :----------------------------------------------------------- |\n| fsync(intfd)      | fsync(fd)：将 fd 代表的文件的脏数据和脏元数据全部刷新至磁盘中。 |\n| fdatasync(int fd) | fdatasync(fd)：将 fd 代表的文件的脏数据刷新至磁盘，同时对必要的元数据刷新至磁盘中，这里所说的必要的概念是指：对接下来访问文件有关键作用的信息，如文件大小，而文件修改时间等不属于必要信息 |\n| sync()            | sync()：则是对系统中所有的脏的文件数据元数据刷新至磁盘中     |\n\n上述三种系统调用可以分别由用户进程与内核进程发起。下面我们研究一下内核线程的相关特性。\n\n1. 创建的针对回写任务的内核线程数由系统中持久存储设备决定，为每个存储设备创建单独的刷新线程；\n\n2. 关于多线程的架构问题，Linux 内核采取了 Lighthttp 的做法，即系统中存在一个管理线程和多个刷新线程（每个持久存储设备对应一个刷新线程）。管理线程监控设备上的脏页面情况，若设备一段时间内没有产生脏页面，就销毁设备上的刷新线程；若监测到设备上有脏页面需要回写且尚未为该设备创建刷新线程，那么创建刷新线程处理脏页面回写。而刷新线程的任务较为单调，只负责将设备中的脏页面回写至持久存储设备中。\n\n3. 刷新线程刷新设备上脏页面大致设计如下：\n   - 每个设备保存脏文件链表，保存的是该设备上存储的脏文件的 inode 节点。所谓的回写文件脏页面即回写该 inode 链表上的某些文件的脏页面；\n    - 系统中存在多个回写时机，第一是应用程序主动调用回写接口（fsync，fdatasync 以及 sync 等），第二管理线程周期性地唤醒设备上的回写线程进行回写，第三是某些应用程序/内核任务发现内存不足时要回收部分缓存页面而事先进行脏页面回写，设计一个统一的框架来管理这些回写任务非常有必要。\n\nWrite Through 与 Write back 在持久化的可靠性上有所不同：\n\n- Write Through 以牺牲系统 I/O 吞吐量作为代价，向上层应用确保一旦写入，数据就已经落盘，不会丢失；\n- Write back 在系统发生宕机的情况下无法确保数据已经落盘，因此存在数据丢失的问题。不过，在程序挂了，例如被 kill -9，Page Cache 中的数据操作系统还是会确保落盘；\n\n## Page Cache 的优劣势\n\n### Page Cache 的优势\n\n**1.加快数据访问**\n\n如果数据能够在内存中进行缓存，那么下一次访问就不需要通过磁盘 I/O 了，直接命中内存缓存即可。\n\n由于内存访问比磁盘访问快很多，因此加快数据访问是 Page Cache 的一大优势。\n\n**2.减少 I/O 次数，提高系统磁盘 I/O 吞吐量**\n\n得益于 Page Cache 的缓存以及预读能力，而程序又往往符合局部性原理，因此通过一次 I/O 将多个 page 装入 Page Cache 能够减少磁盘 I/O 次数， 进而提高系统磁盘 I/O 吞吐量。\n\n### Page Cache 的劣势\n\npage cache 也有其劣势，最直接的缺点是需要占用额外物理内存空间，物理内存在比较紧俏的时候可能会导致频繁的 swap 操作，最终导致系统的磁盘 I/O 负载的上升。\n\nPage Cache 的另一个缺陷是对应用层并没有提供很好的管理 API，几乎是透明管理。应用层即使想优化 Page Cache 的使用策略也很难进行。因此一些应用选择在用户空间实现自己的 page 管理，而不使用 page cache，例如 MySQL InnoDB 存储引擎以 16KB 的页进行管理。\n\nPage Cache 最后一个缺陷是在某些应用场景下比 Direct I/O 多一次磁盘读 I/O 以及磁盘写 I/O。\n\nDirect I/O 即直接 I/O。其名字中的”直接”二字用于区分使用 page cache 机制的缓存 I/O。\n\n- 缓存文件 I/O：用户空间要读写一个文件并**不直接**与磁盘交互，而是中间夹了一层缓存，即 page cache；\n- 直接文件 I/O：用户空间读取的文件**直接**与磁盘交互，没有中间 page cache 层；\n\n“直接”在这里还有另一层语义：其他所有技术中，数据至少需要在内核空间存储一份，但是在 Direct I/O 技术中，数据直接存储在用户空间中，绕过了内核。\n\nDirect I/O 模式如下图所示：\n\n![directIO](https://img-blog.csdnimg.cn/img_convert/503d7d5d3f330d64fcade48b312f767d.png)\n\n此时用户空间直接通过 DMA 的方式与磁盘以及网卡进行数据拷贝。\n\n**Direct I/O 的读写非常有特点**：\n\n- Write 操作：由于其不使用 page cache，所以其进行写文件，如果返回成功，数据就真的落盘了（不考虑磁盘自带的缓存）；\n- Read 操作：由于其不使用 page cache，每次读操作是真的从磁盘中读取，不会从文件系统的缓存中读取。\n\n---\n\n参考资料\n\n- [Linux内核技术实战课](https://time.geekbang.org/column/intro/337)\n- [Reconsidering swapping](https://lwn.net/Articles/690079/)\n- [访问局部性](https://zh.wikipedia.org/wiki/访问局部性)\n- [DMA 与零拷贝技术](https://spongecaptain.cool/SimpleClearFileIO/2.%20DMA%20%E4%B8%8E%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF.html)\n\n---\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/7_device/device":{"title":"device","content":"# 8.1 键盘敲入 A 字母时，操作系统期间发生了什么？\n\n键盘可以说是我们最常使用的输入硬件设备了，但身为程序员的你，你知道「**键盘敲入A 字母时，操作系统期间发生了什么吗**」？\n\n那要想知道这个发生的过程，我们得先了解了解「操作系统是如何管理多种多样的的输入输出设备」的，等了解完这个后，我们再来看看这个问题，你就会发现问题已经被迎刃而解了。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86.png)\n\n\n\n---\n\n## 设备控制器\n\n我们的电脑设备可以接非常多的输入输出设备，比如键盘、鼠标、显示器、网卡、硬盘、打印机、音响等等，每个设备的用法和功能都不同，那操作系统是如何把这些输入输出设备统一管理的呢?\n\n为了屏蔽设备之间的差异，每个设备都有一个叫**设备控制器（*Device Control*）** 的组件，比如硬盘有硬盘控制器、显示器有视频控制器等。\n\n![计算机 I/O 系统结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84.png)\n\n因为这些控制器都很清楚的知道对应设备的用法和功能，所以 CPU 是通过设备控制器来和设备打交道的。\n\n设备控制器里有芯片，它可执行自己的逻辑，也有自己的寄存器，用来与 CPU 进行通信，比如：\n\n- 通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作。\n- 通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等。\n\n实际上，控制器是有三类寄存器，它们分别是**状态寄存器（*Status Register*）**、 **命令寄存器（*Command Register*）**以及**数据寄存器（*Data Register*）**，如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E8%AE%BE%E5%A4%87%E6%8E%A7%E5%88%B6%E5%99%A8.png)\n\n这三个寄存器的作用：\n\n- *数据寄存器*，CPU 向 I/O 设备写入需要传输的数据，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。\n- *命令寄存器*，CPU 发送一个命令，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。\n- *状态寄存器*，目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。\n\nCPU 通过读写设备控制器中的寄存器控制设备，这可比 CPU 直接控制输入输出设备，要方便和标准很多。\n\n另外， 输入输出设备可分为两大类 ：**块设备（*Block Device*）**和**字符设备（*Character Device*）**。\n\n- *块设备*，把数据存储在固定大小的块中，每个块有自己的地址，硬盘、USB 是常见的块设备。\n- *字符设备*，以字符为单位发送或接收一个字符流，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。\n\n\n块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**。\n\n- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。\n- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。\n\n这样做是为了，减少对设备的频繁操作。\n\n那 CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？存在两个方法：\n\n- *端口 I/O*，每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 `in/out` 类似的指令。\n- *内存映射 I/O*，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。\n\n---\n\n## I/O 控制方式\n\n在前面我知道，每种设备都有一个设备控制器，控制器相当于一个小 CPU，它可以自己处理一些事情，但有个问题是，当 CPU 给设备发送了一个指令，让设备控制器去读设备的数据，它读完的时候，要怎么通知 CPU 呢？\n\n控制器的寄存器一般会有状态标记位，用来标识输入或输出操作是否完成。于是，我们想到第一种**轮询等待**的方法，让 CPU 一直查寄存器的状态，直到状态标记为完成，很明显，这种方式非常的傻瓜，它会占用 CPU 的全部时间。\n\n那我们就想到第二种方法 —— **中断**，通知操作系统数据已经准备好了。我们一般会有一个硬件的**中断控制器**，当设备完成任务后触发中断到中断控制器，中断控制器就通知 CPU，一个中断产生了，CPU 需要停下当前手里的事情来处理中断。\n\n另外，中断有两种，一种**软中断**，例如代码调用 `INT` 指令触发，一种是**硬件中断**，就是硬件通过中断控制器触发的。\n\n但中断的方式对于频繁读写数据的磁盘，并不友好，这样 CPU 容易经常被打断，会占用 CPU 大量的时间。对于这一类设备的问题的解决方法是使用 **DMA（*Direct Memory Access*）** 功能，它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存。那要实现 DMA 功能要有 「DMA 控制器」硬件的支持。\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/DMA%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png)\n\nDMA 的工作方式如下：\n\n- CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了；\n- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；\n- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；\n- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；\n\n可以看到， CPU 当要读取磁盘数据的时候，只需给 DMA 控制器发送指令，然后返回去做其他事情，当磁盘数据拷贝到内存后，DMA 控制机器通过中断的方式，告诉 CPU 数据已经准备好了，可以从内存读数据了。仅仅在传送开始和结束时需要 CPU 干预。\n\n----\n\n## 设备驱动程序\n\n虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使用模式都是不同的，所以为了屏蔽「设备控制器」的差异，引入了**设备驱动程序**。\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F.png)\n\n设备控制器不属于操作系统范畴，它是属于硬件，而设备驱动程序属于操作系统的一部分，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口，而设备驱动程序是面向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。\n\n不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统一的接口给操作系统**，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3%E4%B8%80%E8%87%B4%E6%80%A7.png)\n\n前面提到了不少关于中断的事情，设备完成了事情，则会发送中断来通知操作系统。那操作系统就需要有一个地方来处理这个中断，这个地方也就是在设备驱动程序里，它会及时响应控制器发来的中断请求，并根据这个中断的类型调用响应的**中断处理程序**进行处理。\n\n通常，设备驱动程序初始化的时候，要先注册一个该设备的中断处理函数。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E4%B8%AD%E6%96%AD%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B.png)\n\n我们来看看，中断处理程序的处理流程：\n\n1. 在 I/O 时，设备控制器如果已经准备好数据，则会通过中断控制器向 CPU 发送中断请求；\n2. 保护被中断进程的 CPU 上下文；\n3. 转入相应的设备中断处理函数；\n4. 进行中断处理；\n5. 恢复被中断进程的上下文；\n\n----\n\n## 通用块层\n\n对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。\n\n通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层，它主要有两个功能：\n\n- 第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；\n- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。\n\nLinux 内存支持 5 种 I/O 调度算法，分别是：\n\n- 没有调度算法\n- 先入先出调度算法\n- 完全公平调度算法\n- 优先级调度\n- 最终期限调度算法\n\n第一种，没有调度算法，是的，你没听错，它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。\n\n第二种，先入先出调度算法，这是最简单的 I/O 调度算法，先进入 I/O 调度队列的 I/O 请求先发生。\n\n第三种，完全公平调度算法，大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。\n\n第四种，优先级调度算法，顾名思义，优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。\n\n第五种，最终期限调度算法，分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等。\n\n---\n\n## 存储系统 I/O 软件分层\n\n前面说到了不少东西，设备、设备控制器、驱动程序、通用块层，现在再结合文件系统原理，我们来看看 Linux 存储系统的 I/O 软件分层。\n\n可以把 Linux 存储系统的 I/O 由上到下可以分为三个层次，分别是文件系统层、通用块层、设备层。他们整个的层次关系如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82.png)\n\n这三个层次的作用是：\n\n- 文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。\n- 通用块层，包括块设备的 I/O 队列和 I/O 调度器，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。\n- 设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。\n\n有了文件系统接口之后，不但可以通过文件系统的命令行操作设备，也可以通过应用程序，调用 `read`、`write` 函数，就像读写文件一样操作设备，所以说设备在 Linux 下，也只是一个特殊的文件。\n\n但是，除了读写操作，还需要有检查特定于设备的功能和属性。于是，需要 `ioctl` 接口，它表示输入输出控制接口，是用于配置和修改特定设备属性的通用接口。\n\n\n另外，存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。\n\n- 为了提高文件访问的效率，会使用**页缓存、索引节点缓存、目录项缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。\n- 为了提高块设备的访问效率， 会使用**缓冲区**，来缓存块设备的数据。\n\n---\n\n## 键盘敲入字母时，期间发生了什么？\n\n看完前面的内容，相信你对输入输出设备的管理有了一定的认识，那接下来就从操作系统的角度回答开头的问题「键盘敲入字母时，操作系统期间发生了什么？」\n\n我们先来看看 CPU 的硬件架构图：\n\n![CPU 的硬件架构图](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/CPU%20%E7%A1%AC%E4%BB%B6%E6%80%BB%E7%BA%BF%E5%9B%BE.png)\n\nCPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器，这个 I/O 桥接器，另一边接入了内存总线，使得 CPU 和内存通信。再另一边，又接入了一个 I/O 总线，用来连接 I/O 设备，比如键盘、显示器等。\n\n那当用户输入了键盘字符，**键盘控制器**就会产生扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送**中断请求**。\n\nCPU 收到中断请求后，操作系统会**保存被中断进程的 CPU 上下文**，然后调用键盘的**中断处理程序**。\n\n键盘的中断处理程序是在**键盘驱动程序**初始化时注册的，那键盘**中断处理函数**的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的  ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。\n\n得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。\n\n显示出结果后，**恢复被中断进程的上下文**。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/8_network_system/hash":{"title":"hash","content":"# 9.4 什么是一致性哈希？\n\n大家好，我是小林。\n\n在逛牛客网的面经的时候，发现有位同学在面微信的时候，被问到这个问题：\n\n![](https://img-blog.csdnimg.cn/img_convert/2ad888cd9ca79d8d68fbd7ff29a6e088.png)\n\n第一个问题就是：**一致性哈希是什么，使用场景，解决了什么问题？**\n\n这个问题还挺有意思的，所以今天就来聊聊这个。\n\n发车！\n\n![](https://img-blog.csdnimg.cn/img_convert/7de125e1b754aa50132e1fa385ad5c0a.png)\n\n## 如何分配请求？\n\n大多数网站背后肯定不是只有一台服务器提供服务，因为单机的并发量和数据量都是有限的，所以都会用多台服务器构成集群来对外提供服务。\n\n但是问题来了，现在有那么多个节点（后面统称服务器为节点，因为少一个字），要如何分配客户端的请求呢？\n\n![](https://img-blog.csdnimg.cn/img_convert/b752a4f8dcaab8ed4d941ebcc6f606c5.png)\n\n其实这个问题就是「负载均衡问题」。解决负载均衡问题的算法很多，不同的负载均衡算法，对应的就是不同的分配策略，适应的业务场景也不同。\n\n最简单的方式，引入一个中间的负载均衡层，让它将外界的请求「轮流」的转发给内部的集群。比如集群有 3 个节点，外界请求有 3 个，那么每个节点都会处理 1 个请求，达到了分配请求的目的。\n\n![](https://img-blog.csdnimg.cn/img_convert/d3279ad754257977f98e702cb156e9cf.png)\n\n考虑到每个节点的硬件配置有所区别，我们可以引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求，这种算法叫做加权轮询。\n\n加权轮询算法使用场景是建立在每个节点存储的数据都是相同的前提。所以，每次读数据的请求，访问任意一个节点都能得到结果。\n\n但是，加权轮询算法是无法应对「分布式系统」的，因为分布式系统中，每个节点存储的数据是不同的。\n\n当我们想提高系统的容量，就会将数据水平切分到不同的节点来存储，也就是将数据分布到了不同的节点。比如**一个分布式 KV（key-valu）  缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的**，不是说任意访问一个节点都可以得到缓存结果的。\n\n因此，我们要想一个能应对分布式系统的负载均衡算法。\n\n## 使用哈希算法有什么问题？\n\n有的同学可能很快就想到了：**哈希算法**。因为对同一个关键字进行哈希计算，每次计算都是相同的值，这样就可以将某个 key 确定到一个节点了，可以满足分布式系统的负载均衡需求。\n\n哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于 `hash(key) % 3` 公式对数据进行了映射。\n\n如果客户端要获取指定 key 的数据，通过下面的公式可以定位节点：\n\n```\nhash(key) % 3\n```\n\n如果经过上面这个公式计算后得到的值是 0，就说明该 key 需要去第一个节点获取。\n\n但是有一个很致命的问题，**如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据**，否则会出现查询不到数据的问题。\n\n举个例子，假设我们有一个由 A、B、C 三个节点组成分布式 KV 缓存系统，基于计算公式 `hash(key) % 3` 将数据进行了映射，每个节点存储了不同的数据：\n\n\n\n![](https://img-blog.csdnimg.cn/img_convert/025ddcaabece1f4b5823dfb1fb7340ef.png)\n\n\n\n现在有 3 个查询 key 的请求，分别查询 key-01，key-02，key-03 的数据，这三个 key 分别经过 hash() 函数计算后的值为 hash( key-01) = 6、hash( key-02) = 7、hash(key-03) = 8，然后再对这些值进行取模运算。\n\n通过这样的哈希算法，每个 key 都可以定位到对应的节点。\n\n![](https://img-blog.csdnimg.cn/img_convert/ed14c96417e08b4f916e0cd23d12b7bd.png)\n\n\n\n当 3 个节点不能满足业务需求了，这时我们增加了一个节点，节点的数量从 3 变化为 4，意味取模哈希函数中基数的变化，这样会导致**大部分映射关系改变**，如下图：\n\n![](https://img-blog.csdnimg.cn/img_convert/392c54cfb9ec47f5191008aa1d27d6b5.png)\n\n\n\n比如，之前的 hash(key-01) % `3` = 0，就变成了 hash(key-01) % `4` = 2，查询 key-01 数据时，寻址到了节点 C，而  key-01 的数据是存储在节点 A 上的，不是在节点 C，所以会查询不到数据。\n\n同样的道理，如果我们对分布式系统进行缩容，比如移除一个节点，也会因为取模哈希函数中基数的变化，可能出现查询不到数据的问题。\n\n要解决这个问题的办法，就需要我们进行**迁移数据**，比如节点的数量从 3 变化为 4 时，要基于新的计算公式 hash(key) % 4 ，重新对数据和节点做映射。\n\n假设总数据条数为 M，哈希算法在面对节点数量变化时，**最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 O(M)**，这样数据的迁移成本太高了。\n\n所以，我们应该要重新想一个新的算法，来避免分布式系统在扩容或者缩容时，发生过多的数据迁移。\n\n## 使用一致性哈希算法有什么问题？\n\n一致性哈希算法就很好地解决了分布式系统在扩容或者缩容时，发生过多的数据迁移的问题。\n\n一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而**一致哈希算法是对 2^32 进行取模运算，是一个固定的值**。\n\n我们可以把一致哈希算法是对 2^32 进行取模运算的结果值组织成一个圆环，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 2^32 个点组成的圆，这个圆环被称为**哈希环**，如下图：\n\n![](https://img-blog.csdnimg.cn/img_convert/0ea3960fef48d4cbaeb4bec4345301e7.png)\n\n一致性哈希要进行两步哈希：\n\n- 第一步：对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希；\n- 第二步：当对数据进行存储或访问时，对数据进行哈希映射；\n\n所以，**一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上**。\n\n问题来了，对「数据」进行哈希映射得到一个结果要怎么找到存储该数据的节点呢？\n\n答案是，映射的结果值往**顺时针的方向的找到第一个节点**，就是存储该数据的节点。\n\n举个例子，有 3 个节点经过哈希计算，映射到了如下图的位置：\n\n![](https://img-blog.csdnimg.cn/img_convert/83d7f363643353c92d252e34f1d4f687.png)\n\n接着，对要查询的 key-01 进行哈希计算，确定此  key-01 映射在哈希环的位置，然后从这个位置往顺时针的方向找到第一个节点，就是存储该  key-01 数据的节点。\n\n比如，下图中的  key-01 映射的位置，往顺时针的方向找到第一个节点就是节点 A。\n\n![](https://img-blog.csdnimg.cn/img_convert/30c2c70721c12f9c140358fbdc5f2282.png)\n\n所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址：\n\n- 首先，对 key 进行哈希计算，确定此 key 在环上的位置；\n- 然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。\n\n知道了一致哈希寻址的方式，我们来看看，如果增加一个节点或者减少一个节点会发生大量的数据迁移吗？\n\n假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置：\n\n![](https://img-blog.csdnimg.cn/img_convert/f8909edef2f3949f8945bb99380baab3.png)\n\n你可以看到，key-01、key-03 都不受影响，只有 key-02  需要被迁移节点 D。\n\n假设节点数量从 3 减少到了 2，比如将节点 A 移除：\n\n![](https://img-blog.csdnimg.cn/img_convert/31485046f1303b57d8aaeaab103ea7ab.png)\n\n你可以看到，key-02 和 key-03 不会受到影响，只有 key-01 需要被迁移节点 B。\n\n因此，**在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响**。\n\n上面这些图中 3 个节点映射在哈希环还是比较分散的，所以看起来请求都会「均衡」到每个节点。\n\n但是**一致性哈希算法并不保证节点能够在哈希环上分布均匀**，这样就会带来一个问题，会有大量的请求集中在一个节点上。\n\n比如，下图中 3 个节点的映射位置都在哈希环的右半边：\n\n![](https://img-blog.csdnimg.cn/img_convert/d528bae6fcec2357ba2eb8f324ad9fd5.png)\n\n这时候有一半以上的数据的寻址都会找节点 A，也就是访问请求主要集中的节点 A 上，这肯定不行的呀，说好的负载均衡呢，这种情况一点都不均衡。\n\n另外，在这种节点分布不均匀的情况下，进行容灾与扩容时，哈希环上的相邻节点容易受到过大影响，容易发生雪崩式的连锁反应。\n\n比如，上图中如果节点 A 被移除了，当节点 A 宕机后，根据一致性哈希算法的规则，其上数据应该全部迁移到相邻的节点 B 上，这样，节点 B 的数据量、访问量都会迅速增加很多倍，一旦新增的压力超过了节点 B 的处理能力上限，就会导致节点 B 崩溃，进而形成雪崩式的连锁反应。\n\n所以，**一致性哈希算法虽然减少了数据迁移量，但是存在节点分布不均匀的问题**。\n\n\n\n## 如何通过虚拟节点提高均衡度？\n\n要想解决节点能在哈希环上分配不均匀的问题，就是要有大量的节点，节点数越多，哈希环上的节点分布的就越均匀。\n\n但问题是，实际中我们没有那么多节点。所以这个时候我们就加入**虚拟节点**，也就是对一个真实节点做多个副本。\n\n具体做法是，**不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。**\n\n比如对每个节点分别设置 3 个虚拟节点：\n\n- 对节点 A 加上编号来作为虚拟节点：A-01、A-02、A-03\n- 对节点 B 加上编号来作为虚拟节点：B-01、B-02、B-03\n- 对节点 C 加上编号来作为虚拟节点：C-01、C-02、C-03\n\n引入虚拟节点后，原本哈希环上只有 3 个节点的情况，就会变成有 9 个虚拟节点映射到哈希环上，哈希环上的节点数量多了 3 倍。\n\n![](https://img-blog.csdnimg.cn/img_convert/dbb57b8d6071d011d05eeadd93269e13.png)\n\n你可以看到，**节点数量多了后，节点在哈希环上的分布就相对均匀了**。这时候，如果有访问请求寻址到「A-01」这个虚拟节点，接着再通过「A-01」虚拟节点找到真实节点 A，这样请求就能访问到真实节点 A 了。\n\n上面为了方便你理解，每个真实节点仅包含 3 个虚拟节点，这样能起到的均衡效果其实很有限。而在实际的工程中，虚拟节点的数量会大很多，比如 Nginx 的一致性哈希算法，每个权重为 1 的真实节点就含有160 个虚拟节点。\n\n另外，虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。**当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高**。\n\n比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点，可能会对应不同的真实节点，即这些不同的真实节点共同分担了节点变化导致的压力。\n\n而且，有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。\n\n因此，**带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景**。\n\n## 总结\n\n不同的负载均衡算法适用的业务场景也不同的。\n\n轮询这类的策略只能适用与每个节点的数据都是相同的场景，访问任意节点都能请求到数据。但是不适用分布式系统，因为分布式系统意味着数据水平切分到了不同的节点上，访问数据的时候，一定要寻址存储该数据的节点。\n\n哈希算法虽然能建立数据和节点的映射关系，但是每次在节点数量发生变化的时候，最坏情况下所有数据都需要迁移，这样太麻烦了，所以不适用节点数量变化的场景。\n\n为了减少迁移的数据量，就出现了一致性哈希算法。\n\n一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。\n\n但是一致性哈希算法不能够均匀的分布节点，会出现大量请求都集中在一个节点的情况，在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。\n\n为了解决一致性哈希算法不能够均匀的分布节点的问题，就需要引入虚拟节点，对一个真实节点做多个副本。不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。\n\n引入虚拟节点后，可以会提高节点的均衡度，还会提高系统的稳定性。所以，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。\n\n完！\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/8_network_system/reactor":{"title":"reactor","content":"# 9.3 高性能网络模式：Reactor 和 Proactor\n\n小林，来了。\n\n这次就来**图解 Reactor 和 Proactor** 这两个高性能网络模式。\n\n别小看这两个东西，特别是 Reactor 模式，市面上常见的开源软件很多都采用了这个方案，比如 Redis、Nginx、Netty 等等，所以学好这个模式设计的思想，不仅有助于我们理解很多开源软件，而且也能在面试时吹逼。\n\n发车！\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/Reactor/reactor提纲.jpeg)\n\n---\n\n## 演进\n\n\n如果要让服务器服务多个客户端，那么最直接的方式就是为每一条连接创建线程。\n\n其实创建进程也是可以的，原理是一样的，进程和线程的区别在于线程比较轻量级些，线程的创建和线程间切换的成本要小些，为了描述简述，后面都以线程为例。\n\n\n处理完业务逻辑后，随着连接关闭后线程也同样要销毁了，但是这样不停地创建和销毁线程，不仅会带来性能开销，也会造成浪费资源，而且如果要连接几万条连接，创建几万个线程去应对也是不现实的。\n\n要这么解决这个问题呢？我们可以使用「资源复用」的方式。\n\n也就是不用再为每个连接创建线程，而是创建一个「线程池」，将连接分配给线程，然后一个线程可以处理多个连接的业务。\n\n不过，这样又引来一个新的问题，线程怎样才能高效地处理多个连接的业务？\n\n当一个连接对应一个线程时，线程一般采用「read -\u003e 业务处理 -\u003e send」的处理流程，如果当前连接没有数据可读，那么线程会阻塞在 `read` 操作上（ socket 默认情况是阻塞 I/O），不过这种阻塞方式并不影响其他线程。\n\n但是引入了线程池，那么一个线程要处理多个连接的业务，线程在处理某个连接的 `read` 操作时，如果遇到没有数据可读，就会发生阻塞，那么线程就没办法继续处理其他连接的业务。\n\n要解决这一个问题，最简单的方式就是将 socket 改成非阻塞，然后线程不断地轮询调用 `read` 操作来判断是否有数据，这种方式虽然该能够解决阻塞的问题，但是解决的方式比较粗暴，因为轮询是要消耗 CPU 的，而且随着一个 线程处理的连接越多，轮询的效率就会越低。\n\n上面的问题在于，线程并不知道当前连接是否有数据可读，从而需要每次通过 `read` 去试探。\n\n那有没有办法在只有当连接上有数据的时候，线程才去发起读请求呢？答案是有的，实现这一技术的就是 I/O 多路复用。\n\nI/O 多路复用技术会用一个系统调用函数来监听我们所有关心的连接，也就说可以在一个监控线程里面监控很多的连接。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/多路复用.png)\n\n我们熟悉的 select/poll/epoll 就是内核提供给用户态的多路复用系统调用，线程可以通过一个系统调用函数从内核中获取多个事件。\n\n\u003e PS：如果想知道 select/poll/epoll 的区别，可以看看小林之前写的这篇文章：[这次答应我，一举拿下 I/O 多路复用！](https://mp.weixin.qq.com/s/Qpa0qXxuIM8jrBqDaXmVNA)\n\nselect/poll/epoll 是如何获取网络事件的呢？\n\n在获取事件时，先把我们要关心的连接传给内核，再由内核检测：\n\n- 如果没有事件发生，线程只需阻塞在这个系统调用，而无需像前面的线程池方案那样轮询调用 read 操作来判断是否有数据。\n- 如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态中再处理这些连接对应的业务即可。\n\n当下开源软件能做到网络高性能的原因就是 I/O 多路复用吗？\n\n是的，基本是基于 I/O 多路复用，用过 I/O 多路复用接口写网络程序的同学，肯定知道是面向过程的方式写代码的，这样的开发的效率不高。\n\n于是，大佬们基于面向对象的思想，对 I/O 多路复用作了一层封装，让使用者不用考虑底层网络 API 的细节，只需要关注应用代码的编写。\n\n大佬们还为这种模式取了个让人第一时间难以理解的名字：**Reactor 模式**。\n\nReactor 翻译过来的意思是「反应堆」，可能大家会联想到物理学里的核反应堆，实际上并不是的这个意思。\n\n这里的反应指的是「**对事件反应**」，也就是**来了一个事件，Reactor 就有相对应的反应/响应**。\n\n事实上，Reactor 模式也叫 `Dispatcher` 模式，我觉得这个名字更贴合该模式的含义，即 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。\n\n\nReactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：\n\n- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；\n- 处理资源池负责处理事件，如 read -\u003e 业务逻辑 -\u003e send；\n\nReactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：\n\n- Reactor 的数量可以只有一个，也可以有多个；\n- 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；\n\n将上面的两个因素排列组设一下，理论上就可以有 4 种方案选择：\n\n- 单 Reactor 单进程 / 线程；\n- 单 Reactor 多进程 / 线程；\n- 多 Reactor 单进程 / 线程；\n- 多 Reactor 多进程 / 线程；\n\n\n其中，「多 Reactor 单进程 / 线程」实现方案相比「单 Reactor 单进程 / 线程」方案，不仅复杂而且也没有性能优势，因此实际中并没有应用。\n\n剩下的 3 个方案都是比较经典的，且都有应用在实际的项目中：\n\n- 单 Reactor 单进程 / 线程；\n- 单 Reactor 多线程 / 进程；\n- 多 Reactor 多进程 / 线程；\n\n方案具体使用进程还是线程，要看使用的编程语言以及平台有关：\n\n- Java 语言一般使用线程，比如 Netty;\n- C 语言使用进程和线程都可以，例如 Nginx 使用的是进程，Memcache 使用的是线程。\n\n接下来，分别介绍这三个经典的 Reactor 方案。\n\n---\n\n## Reactor\n\n\n### 单 Reactor 单进程 / 线程\n\n\n一般来说，C 语言实现的是「**单 Reactor *单进程***」的方案，因为 C 语编写完的程序，运行后就是一个独立的进程，不需要在进程中再创建线程。\n\n而 Java 语言实现的是「**单 Reactor *单线程***」的方案，因为 Java 程序是跑在 Java 虚拟机这个进程上面的，虚拟机中有很多线程，我们写的 Java 程序只是其中的一个线程而已。\n\n我们来看看「**单 Reactor 单进程**」的方案示意图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/Reactor/单Reactor单进程.png)\n\n\n可以看到进程里有 **Reactor、Acceptor、Handler** 这三个对象：\n\n- Reactor 对象的作用是监听和分发事件；\n- Acceptor 对象的作用是获取连接；\n- Handler 对象的作用是处理业务；\n\n对象里的 select、accept、read、send 是系统调用函数，dispatch 和 「业务处理」是需要完成的操作，其中 dispatch 是分发事件操作。\n\n接下来，介绍下「单 Reactor 单进程」这个方案：\n\n- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；\n- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；\n- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；\n- Handler 对象通过 read -\u003e 业务处理 -\u003e send 的流程来完成完整的业务流程。\n\n单 Reactor 单进程的方案因为全部工作都在同一个进程内完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。\n\n但是，这种方案存在 2 个缺点：\n\n- 第一个缺点，因为只有一个进程，**无法充分利用 多核 CPU 的性能**；\n- 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；\n\n所以，单 Reactor 单进程的方案**不适用计算机密集型的场景，只适用于业务处理非常快速的场景**。\n\nRedis 是由 C 语言实现的，它采用的正是「单 Reactor 单进程」的方案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的方案。\n\n### 单 Reactor 多线程 / 多进程\n\n如果要克服「单 Reactor 单线程 / 进程」方案的缺点，那么就需要引入多线程 / 多进程，这样就产生了**单 Reactor 多线程 / 多进程**的方案。\n\n闻其名不如看其图，先来看看「单 Reactor 多线程」方案的示意图如下：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/Reactor/单Reactor多线程.png)\n\n详细说一下这个方案：\n\n- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；\n- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；\n- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；\n\n上面的三个步骤和单 Reactor 单线程方案是一样的，接下来的步骤就开始不一样了：\n\n- Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；\n- 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；\n\n单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。\n\n例如，子线程完成业务处理后，要把结果传递给主线程的 Reactor 进行发送，这里涉及共享数据的竞争。\n\n要避免多线程由于竞争共享资源而导致数据错乱的问题，就需要在操作共享资源前加上互斥锁，以保证任意时间里只有一个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据。\n\n聊完单 Reactor 多线程的方案，接着来看看单 Reactor 多进程的方案。\n\n事实上，单 Reactor 多进程相比单 Reactor 多线程实现起来很麻烦，主要因为要考虑子进程 \u003c-\u003e 父进程的双向通信，并且父进程还得知道子进程要将数据发送给哪个客户端。\n\n而多线程间可以共享数据，虽然要额外考虑并发问题，但是这远比进程间通信的复杂度低得多，因此实际应用中也看不到单 Reactor 多进程的模式。\n\n另外，「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**。\n\n### 多 Reactor 多进程 / 线程\n\n要解决「单 Reactor」的问题，就是将「单 Reactor」实现成「多 Reactor」，这样就产生了第 **多 Reactor 多进程 / 线程**的方案。\n\n老规矩，闻其名不如看其图。多 Reactor 多进程 / 线程方案的示意图如下（以线程为例）：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/Reactor/主从Reactor多线程.png)\n\n方案详细说明如下：\n\n- 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept  获取连接，将新的连接分配给某个子线程；\n- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。\n- 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。\n- Handler 对象通过 read -\u003e 业务处理 -\u003e send 的流程来完成完整的业务流程。\n\n多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：\n\n- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。\n- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。\n\n\n大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。\n\n采用了「多 Reactor 多进程」方案的开源软件是 Nginx，不过方案与标准的多 Reactor 多进程有些差异。\n\n具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接，通过锁来控制一次只有一个子进程进行 accept（防止出现惊群现象），子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程。\n\n---\n\n\n## Proactor\n\n前面提到的 Reactor 是非阻塞同步网络模式，而 **Proactor 是异步网络模式**。\n\n这里先给大家复习下阻塞、非阻塞、同步、异步 I/O 的概念。\n\n先来看看**阻塞 I/O**，当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。\n\n注意，**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。过程如下图：\n\n![阻塞 I/O](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png)\n\n\n知道了阻塞 I/O ，来看看**非阻塞 I/O**，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。过程如下图：\n\n![非阻塞 I/O](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png)\n\n\n注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**\n\n举个例子，如果 socket 设置了 `O_NONBLOCK` 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。\n\n\n因此，无论 read 和 send 是阻塞 I/O，还是非阻塞 I/O 都是同步调用。因为在 read 调用时，内核将数据从内核空间拷贝到用户空间的过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。\n\n而真正的**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这**两个过程都不用等待**。\n\n当我们发起 `aio_read` （异步 I/O） 之后，就立即返回，内核自动将数据从内核空间拷贝到用户空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，**应用程序并不需要主动发起拷贝动作**。过程如下图：\n\n![异步 I/O](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png)\n\n举个你去饭堂吃饭的例子，你好比应用程序，饭堂好比操作系统。\n\n阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。\n\n非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。\n\n异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。\n\n很明显，异步 I/O 比同步 I/O 性能更好，因为异步 I/O 在「内核数据准备好」和「数据从内核空间拷贝到用户空间」这两个过程都不用等待。\n\nProactor 正是采用了异步 I/O 技术，所以被称为异步网络模型。\n\n现在我们再来理解 Reactor 和 Proactor 的区别，就比较清晰了。\n\n- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。\n- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。\n\n\n\n因此，**Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」**，而 **Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」**。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。\n\n举个实际生活中的例子，Reactor 模式就是快递员在楼下，给你打电话告诉你快递到你家小区了，你需要自己下楼来拿快递。而在 Proactor 模式下，快递员直接将快递送到你家门口，然后通知你。\n\n无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 **Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件**。\n\n\n接下来，一起看看 Proactor 模式的示意图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/Reactor/Proactor.png)\n\n介绍一下 Proactor 模式的工作流程：\n\n- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 \n  Asynchronous Operation Processor 注册到内核；\n- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；\n- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；\n- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；\n- Handler 完成业务处理；\n\n\n可惜的是，在 Linux 下的异步 I/O 是不完善的，\n`aio` 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。\n\n\n而 Windows 里实现了一套完整的支持 socket 的异步编程接口，这套接口就是 `IOCP`，是由操作系统级别实现的异步 I/O，真正意义上异步 I/O，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案。\n\n----\n\n## 总结\n\n常见的 Reactor 实现方案有三种。\n\n第一种方案单 Reactor 单进程 / 线程，不用考虑进程间通信以及数据同步的问题，因此实现起来比较简单，这种方案的缺陷在于无法充分利用多核 CPU，而且处理业务逻辑的时间不能太长，否则会延迟响应，所以不适用于计算机密集型的场景，适用于业务处理快速的场景，比如 Redis 采用的是单 Reactor 单进程的方案。\n\n第二种方案单 Reactor 多线程，通过多线程的方式解决了方案一的缺陷，但它离高并发还差一点距离，差在只有一个 Reactor 对象来承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方。\n\n第三种方案多 Reactor 多进程 / 线程，通过多个 Reactor 来解决了方案二的缺陷，主 Reactor 只负责监听事件，响应事件的工作交给了从 Reactor，Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案，Nginx 则采用了类似于 「多 Reactor 多进程」的方案。\n\nReactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。\n\n因此，真正的大杀器还是 Proactor，它是采用异步 I/O 实现的异步网络模型，感知的是已完成的读写事件，而不需要像 Reactor 感知到事件后，还需要调用 read 来从内核中获取数据。\n\n不过，无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件。\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/8_network_system/selete_poll_epoll":{"title":"selete_poll_epoll","content":"# 9.2 I/O 多路复用：select/poll/epoll\n\n我们以最简单 socket 网络模型，一步一步的过渡到 I/O 多路复用。\n\n但我不会具体细节说到每个系统调用的参数，这方面书上肯定比我说的详细。\n\n好了，**发车！**\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/多路复用提纲.png)\n\n---\n\n## 最基本的 Socket 模型\n\n要想客户端和服务器能在网络中通信，那必须得使用 Socket  编程，它是进程间通信里比较特别的方式，特别之处在于它是可以跨主机间通信。\n\nSocket  的中文名叫作插口，咋一看还挺迷惑的。事实上，双方要进行网络通信前，各自得创建一个 Socket，这相当于客户端和服务器都开了一个“口子”，双方读取和发送数据的时候，都通过这个“口子”。这样一看，是不是觉得很像弄了一根网线，一头插在客户端，一头插在服务端，然后进行通信。\n\n创建 Socket 的时候，可以指定网络层使用的是 IPv4 还是 IPv6，传输层使用的是 TCP 还是 UDP。\n\nUDP 的 Socket 编程相对简单些，这里我们只介绍基于 TCP 的 Socket 编程。\n\n服务器的程序要先跑起来，然后等待客户端的连接和数据，我们先来看看服务端的 Socket 编程过程是怎样的。\n\n服务端首先调用 `socket()` 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调用 `bind()` 函数，给这个 Socket 绑定一个 **IP 地址和端口**，绑定这两个的目的是什么？\n\n- 绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。\n- 绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们；\n\n绑定完 IP 地址和端口后，就可以调用 `listen()` 函数进行监听，此时对应 TCP 状态图中的 `listen`，如果我们要判定服务器中一个网络程序有没有启动，可以通过 `netstat` 命令查看对应的端口号是否有被监听。\n\n服务端进入了监听状态后，通过调用 `accept()` 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。\n\n那客户端是怎么发起连接的呢？客户端在创建好 Socket 后，调用 `connect()` 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。\n\n在  TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：\n\n- 一个是还没完全建立连接的队列，称为 **TCP 半连接队列**，这个队列都是没有完成三次握手的连接，此时服务端处于 `syn_rcvd` 的状态；\n- 一个是一件建立连接的队列，称为 **TCP 全连接队列**，这个队列都是完成了三次握手的连接，此时服务端处于 `established` 状态；\n\n当 TCP 全连接队列不为空后，服务端的 `accept()` 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的  Socket 返回应用程序，后续数据传输都用这个 Socket。\n\n注意，监听的 Socket 和真正用来传数据的 Socket 是两个：\n\n- 一个叫作**监听 Socket**；\n- 一个叫作**已连接 Socket**；\n\n\n连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 `read()` 和 `write()` 函数来读写数据。\n\n至此， TCP 协议的 Socket 程序的调用过程就结束了，整个过程如下图：\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/tcp_socket.png)\n\n\n看到这，不知道你有没有觉得读写 Socket  的方式，好像读写文件一样。\n\n是的，基于 Linux 一切皆文件的理念，在内核中 Socket 也是以「文件」的形式存在的，也是有对应的文件描述符。\n\n\u003e PS : 下面会说到内核里的数据结构，不感兴趣的可以跳过这一部分，不会对后续的内容有影响。\n\n文件描述符的作用是什么？每一个进程都有一个数据结构 `task_struct`，该结构体里有一个指向「文件描述符数组」的成员指针。该数组里列出这个进程打开的所有文件的文件描述符。数组的下标是文件描述符，是一个整数，而数组的内容是一个指针，指向内核中所有打开的文件的列表，也就是说内核可以通过文件描述符找到对应打开的文件。\n\n然后每个文件都有一个 inode，Socket 文件的 inode 指向了内核中的 Socket 结构，在这个结构体里有两个队列，分别是**发送队列**和**接收队列**，这个两个队列里面保存的是一个个 `struct sk_buff`，用链表的组织形式串起来。\n\nsk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。\n\n你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。\n\n于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 `data` 的指针，比如：\n\n- 当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb-\u003edata 的值，来逐步剥离协议首部。\n- 当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb-\u003edata 的值来增加协议首部。\n\n你可以从下面这张图看到，当发送报文时，data 指针的移动过程。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/sk_buff.jpg)\n\n---\n\n## 如何服务更多的用户？\n\n前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是同步阻塞的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。\n\n可如果我们服务器只能服务一个客户，那这样就太浪费资源了，于是我们要改进这个网络 I/O 模型，以支持更多的客户端。\n\n在改进网络 I/O 模型前，我先来提一个问题，你知道服务器单机理论最大能连接多少个客户端？\n\n相信你知道 TCP 连接是由四元组唯一确认的，这个四元组就是：**本机IP, 本机端口, 对端IP, 对端端口**。\n\n服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以**最大 TCP 连接数 = 客户端 IP 数×客户端端口数**。\n\n对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是**服务端单机最大 TCP 连接数约为 2 的 48 次方**。\n\n这个理论值相当“丰满”，但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：\n\n- **文件描述符**，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；\n- **系统内存**，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；\n\n那如果服务器的内存只有 2 GB，网卡是千兆的，能支持并发 1 万请求吗？\n\n并发 1 万请求，也就是经典的 C10K 问题 ，C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。\n\n从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。\n\n不过，要想真正实现 C10K 的服务器，要考虑的地方在于服务器的网络 I/O 模型，效率低的模型，会加重系统开销，从而会离 C10K 的目标越来越远。\n\n---\n\n## 多进程模型\n\n基于最原始的阻塞网络 I/O， 如果服务器要支持多个客户端，其中比较传统的方式，就是使用**多进程模型**，也就是为每个客户端分配一个进程来处理请求。\n\n服务器的主进程负责监听客户的连接，一旦与客户端连接完成，`accept()` 函数就会返回一个「已连接 Socket」，这时就通过 `fork()` 函数创建一个子进程，实际上就把父进程所有相关的东西都**复制**一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。\n\n这两个进程刚复制完的时候，几乎一模一样。不过，会根据**返回值**来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。\n\n正因为子进程会**复制父进程的文件描述符**，于是就可以直接使用「已连接 Socket 」和客户端通信了，\n\n\n\n可以发现，子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」。\n\n下面这张图描述了从连接请求到连接建立，父进程创建生子进程为客户服务。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/多进程.png)\n\n另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成**僵尸进程**，随着僵尸进程越多，会慢慢耗尽我们的系统资源。\n\n因此，父进程要“善后”好自己的孩子，怎么善后呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 `wait()` 和 `waitpid()` 函数。\n\n这种用多个进程来应付多个客户端的方式，在应对 100 个客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣。\n\n进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。\n\n----\n\n## 多线程模型\n\n既然进程间上下文切换的“包袱”很重，那我们就搞个比较轻量级的模型来应对多用户的请求 —— **多线程模型**。\n\n线程是运行在进程中的一个“逻辑流”，单进程中可以运行多个线程，同进程里的线程可以共享进程的部分资源，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。\n\n当服务器与客户端 TCP 完成连接后，通过 `pthread_create()` 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。\n\n如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。\n\n那么，我们可以使用**线程池**的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 Socket 进程处理。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/线程池.png)\n\n\n需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。\n\n上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。\n\n---\n\n## I/O 多路复用\n\n既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 **I/O 多路复用**技术。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/多路复用.png)\n\n一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。\n\n我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，**进程可以通过一个系统调用函数从内核中获取多个事件**。\n\nselect/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。\n\nselect/poll/epoll 这是三个多路复用接口，都能实现 C10K 吗？接下来，我们分别说说它们。\n\n---\n\n## select/poll\n\nselect 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 `select()` 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。\n\n所以，对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。\n\nselect 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 `1024`，只能监听 0~1023 的文件描述符。\n\n\npoll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。\n\n但是 poll 和 select 并没有太大的本质区别，**都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长。\n\n---\n\n## epoll\n\nepoll 通过两个方面，很好解决了 select/poll 的问题。\n\n*第一点*，epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)`，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。\n\n*第二点*， epoll 使用事件驱动的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。\n\n从下图你可以看到 epoll 相关的接口作用：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/epoll.png)\n\nepoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，**epoll 被称为解决 C10K 问题的利器**。\n\n插个题外话，网上文章不少说，`epoll_wait` 返回时，对于就绪的事件，epoll 使用的是共享内存的方式，即用户态和内核态都指向了就绪链表，所以就避免了内存拷贝消耗。\n\n这是错的！看过 epoll 内核源码的都知道，**压根就没有使用共享内存这个玩意**。你可以从下面这份代码看到， `epoll_wait` 实现的内核代码中调用了 `__put_user` 函数，这个函数就是将数据从内核拷贝到用户空间。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/操作系统/多路复用/__put_user.png)\n\n好了，这个题外话就说到这了，我们继续！\n\n\nepoll 支持两种事件触发模式，分别是**边缘触发（*edge-triggered，ET*）**和**水平触发（*level-triggered，LT*）**。\n\n这两个术语还挺抽象的，其实它们的区别还是很好理解的。\n\n- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；\n- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；\n\n举个例子，你的快递被放到了一个快递箱里，如果快递箱只会通过短信通知你一次，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是水平触发的方式。\n\n这就是两者的区别，水平触发的意思是只要满足事件的条件，比如内核中有数据需要读，就一直不断地把这个事件传递给用户；而边缘触发的意思是只有第一次满足条件的时候才触发，之后就不会再传递同样的事件了。\n\n如果使用水平触发模式，当内核通知文件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要一次执行尽可能多的读写操作。\n\n如果使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会**循环**从文件描述符读写数据，那么如果文件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那里，程序就没办法继续往下执行。所以，**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。\n\n一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。\n\nselect/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。\n\n另外，使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用，Linux 手册关于 select 的内容中有如下说明：\n\n\u003e Under Linux, select() may report a socket file descriptor as \"ready for reading\", while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block.\n\n我谷歌翻译的结果：\n\n\u003e 在Linux下，select() 可能会将一个 socket 文件描述符报告为 \"准备读取\"，而后续的读取块却没有。例如，当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。因此，在不应该阻塞的 socket 上使用 O_NONBLOCK 可能更安全。\n\n简单点理解，就是**多路复用 API 返回的事件并不一定可读写的**，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞，因此最好搭配非阻塞 I/O，以便应对极少数的特殊情况。\n\n---\n\n## 总结\n\n\n最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。\n\n比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000 个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。\n\n为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的 I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。\n\nselect 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。\n\n在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。\n\n很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。\n\nepoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。\n\n- epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。\n- epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。\n\n而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。\n\n----\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/8_network_system/zero_copy":{"title":"zero_copy","content":"# 9.1 什么是零拷贝？\n\n磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。\n\n这次，我们就以「文件传输」作为切入点，来分析 I/O 工作方式，以及如何优化传输文件的性能。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8F%90%E7%BA%B2.png)\n\n---\n\n## 为什么要有 DMA 技术?\n\n\n在没有 DMA 技术前，I/O 的过程是这样的：\n\n- CPU 发出对应的指令给磁盘控制器，然后返回；\n- 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个**中断**；\n- CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。\n\n为了方便你理解，我画了一副图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png)\n\n可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。\n\n简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。\n\n计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是**直接内存访问（*Direct Memory Access*）** 技术。\n\n什么是 DMA 技术？简单理解就是，**在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**。\n\n那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png)\n\n具体过程：\n\n- 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；\n- 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；\n- DMA 进一步将 I/O 请求发送给磁盘；\n- 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；\n- **DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务**；\n- 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；\n- CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；\n\n可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。\n\n早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。\n\n---\n\n## 传统的文件传输有多糟糕？\n\n如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。\n\n传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。\n\n代码通常如下，一般会需要两个系统调用：\n\n```c\nread(file, tmp_buf, len);\nwrite(socket, tmp_buf, len);\n```\n\n代码很简单，虽然就两行代码，但是这里面发生了不少的事情。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)\n\n首先，期间共**发生了 4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是  `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。\n\n上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。\n\n其次，还**发生了 4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：\n\n- *第一次拷贝*，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。\n- *第二次拷贝*，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。\n- *第三次拷贝*，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。\n- *第四次拷贝*，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。\n\n我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。\n\n这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。\n\n所以，**要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数**。\n\n\n\n---\n\n## 如何优化文件传输的性能？\n\n\n\u003e 先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？\n\n读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。\n\n而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。\n\n所以，**要想减少上下文切换到次数，就要减少系统调用的次数**。\n\n\n\n \u003e 再来看看，如何减少「数据拷贝」的次数？\n\n在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。\n\n因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此**用户的缓冲区是没有必要存在的**。\n\n\n----\n\n## 如何实现零拷贝？\n\n零拷贝技术实现的方式通常有 2 种：\n\n- mmap + write\n- sendfile\n\n下面就谈一谈，它们是如何减少「上下文切换」和「数据拷贝」的次数。\n\n\n### mmap + write\n\n在前面我们知道，`read()` 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 `mmap()` 替换 `read()` 系统调用函数。\n\n```c\nbuf = mmap(file, len);\nwrite(sockfd, buf, len);\n```\n\n`mmap()` 系统调用函数会直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20+%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)\n\n具体过程如下：\n\n- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；\n- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；\n- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。\n\n我们可以得知，通过使用 `mmap()` 来代替 `read()`， 可以减少一次数据拷贝的过程。\n\n但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。\n\n### sendfile\n\n 在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：\n\n```c\n#include \u003csys/socket.h\u003e\nssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);\n```\n\n它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。\n\n首先，它可以替代前面的 `read()` 和 `write()` 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。\n\n其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)\n\n但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（*The Scatter-Gather Direct Memory Access*）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。\n\n你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：\n\n```bash\n$ ethtool -k eth0 | grep scatter-gather\nscatter-gather: on\n```\n\n于是，从 Linux 内核 `2.4` 版本开始起，对于支持 SG-DMA 技术的网卡， `sendfile()` 系统调用的过程发生了点变化，具体过程如下：\n\n- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；\n- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；\n\n\n所以，这个过程之中，只进行了 2 次数据拷贝，如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)\n\n\n这就是所谓的**零拷贝（*Zero-copy*）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。**\n\n零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，**只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。**\n\n\n所以，总体来看，**零拷贝技术可以把文件传输的性能提高至少一倍以上**。\n\n\n\n### 使用零拷贝技术的项目\n\n事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。\n\n如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 `transferTo` 方法：\n\n\n```java\n@Override\npublic long transferFrom(FileChannel fileChannel, long position, long count) throws IOException { \n    return fileChannel.transferTo(position, count, socketChannel);\n}\n```\n\n如果 Linux 系统支持 `sendfile()` 系统调用，那么 `transferTo()` 实际上最后就会使用到 `sendfile()` 系统调用函数。\n\n曾经有大佬专门写过程序测试过，在同样的硬件条件下，传统文件传输和零拷拷贝文件传输的性能差异，你可以看到下面这张测试数据图，使用了零拷贝能够缩短 `65%` 的时间，大幅度提升了机器传输数据的吞吐量。\n\n![数据来源于：https://developer.ibm.com/articles/j-zerocopy/](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.png)\n\n另外，Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：\n\n```\nhttp {\n...\n    sendfile on\n...\n}\n```\n\nsendfile 配置的具体意思: \n\n- 设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。\n- 设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。\n\n当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。\n\n---\n\n## PageCache 有什么作用？\n\n回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是**磁盘高速缓存（*PageCache*）**。\n\n 由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的。\n\n\n读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。\n\n但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。\n\n\n那问题来了，选择哪些磁盘数据拷贝到内存呢？\n\n我们都知道程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 **PageCache 来缓存最近被访问的数据**，当空间不足时淘汰最久未被访问的缓存。\n\n所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。\n\n还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，**PageCache 使用了「预读功能」**。\n\n比如，假设 read 方法每次只会读 `32 KB` 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。\n\n所以，PageCache 的优点主要是两个：\n\n- 缓存最近被访问的数据；\n- 预读功能；\n\n这两个做法，将大大提高读写磁盘的性能。\n\n**但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能**\n\n这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。\n\n另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题：\n\n- PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了；\n- PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次；\n\n所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。\n\n\n---\n\n\n## 大文件传输用什么方式实现？\n\n那针对大文件的传输，我们应该使用什么方式呢？\n\n我们先来看看最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E9%98%BB%E5%A1%9E%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png)\n\n具体过程：\n\n- 当调用 read 方法时，会阻塞着，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好；\n- 内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里；\n- 最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。\n\n对于阻塞的问题，可以用异步 I/O 来解决，它工作方式如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E5%BC%82%E6%AD%A5%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png)\n\n它把读操作分为两部分：\n\n- 前半部分，内核向磁盘发起读请求，但是可以**不等待数据就位就可以返回**，于是进程此时可以处理其他任务；\n- 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的**通知**，再去处理数据；\n\n而且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。\n\n绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。\n\n前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。\n\n于是，**在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术**。\n\n直接 I/O 应用场景常见的两种：\n\n- 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；\n- 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。\n\n另外，由于直接 I/O 绕过了 PageCache，就无法享受内核的这两点的优化：\n\n- 内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「**合并**」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；\n- 内核也会「**预读**」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作；\n\n于是，传输大文件的时候，使用「异步 I/O + 直接 I/O」了，就可以无阻塞地读取文件了。\n\n所以，传输文件的时候，我们要根据文件的大小来使用不同的方式：\n\n- 传输大文件的时候，使用「异步 I/O + 直接 I/O」；\n- 传输小文件的时候，则使用「零拷贝技术」；\n\n在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式：\n\n```\nlocation /video/ { \n    sendfile on; \n    aio on; \n    directio 1024m; \n}\n```\n\n当文件大小大于 `directio` 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。\n\n---\n\n## 总结\n\n早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。\n\n于是，为了解决这一问题，DMA 技术就出现了，每个 I/O 设备都有自己的 DMA 控制器，通过这个 DMA 控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的工作。\n\n传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。\n\n为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次系统调用（`sendfile` 方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。\n\nKafka 和 Nginx 都有实现零拷贝技术，这将大大提高文件传输的性能。\n\n零拷贝技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读比随机读性能好的原因。这些优势，进一步提升了零拷贝的性能。\n\n需要注意的是，零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送。\n\n另外，当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。\n\n在 Nginx 里，可以通过配置，设定一个文件大小阈值，针对大文件使用异步 IO 和直接 IO，而对小文件使用零拷贝。","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/9_linux_cmd/linux_network":{"title":"linux_network","content":"# 10.1 如何查看网络的性能指标？\n\nLinux 网络协议栈是根据 TCP/IP 模型来实现的，TCP/IP 模型由应用层、传输层、网络层和网络接口层，共四层组成，每一层都有各自的职责。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/封装.png)\n\n应用程序要发送数据包时，通常是通过 socket 接口，于是就会发生系统调用，把应用层的数据拷贝到内核里的 socket 层，接着由网络协议栈从上到下逐层处理后，最后才会送到网卡发送出去。\n\n而对于接收网络包时，同样也要经过网络协议逐层处理，不过处理的方向与发送数据时是相反的，也就是从下到上的逐层处理，最后才送到应用程序。\n\n网络的速度往往跟用户体验是挂钩的，那我们又该用什么指标来衡量 Linux 的网络性能呢？以及如何分析网络问题呢？\n\n这次，我们就来说这些。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/网络/网络提纲.png)\n\n---\n\n## 性能指标有哪些？\n\n通常是以 4 个指标来衡量网络的性能，分别是带宽、延时、吞吐率、PPS（Packet Per Second），它们表示的意义如下：\n\n- *带宽*，表示链路的最大传输速率，单位是 b/s （比特 / 秒），带宽越大，其传输能力就越强。\n- *延时*，表示请求数据包发送后，收到对端响应，所需要的时间延迟。不同的场景有着不同的含义，比如可以表示建立 TCP 连接所需的时间延迟，或一个数据包往返所需的时间延迟。\n- *吞吐率*，表示单位时间内成功传输的数据量，单位是 b/s（比特 / 秒）或者 B/s（字节 / 秒），吞吐受带宽限制，带宽越大，吞吐率的上限才可能越高。\n- *PPS*，全称是 Packet Per Second（包 / 秒），表示以网络包为单位的传输速率，一般用来评估系统对于网络的转发能力。\n\n\n当然，除了以上这四种基本的指标，还有一些其他常用的性能指标，比如：\n\n- *网络的可用性*，表示网络能否正常通信；\n- *并发连接数*，表示 TCP 连接数量；\n- *丢包率*，表示所丢失数据包数量占所发送数据组的比率；\n- *重传率*，表示重传网络包的比例；\n\n你可能会问了，如何观测这些性能指标呢？不急，继续往下看。\n\n---\n\n## 网络配置如何看？\n\n要想知道网络的配置和状态，我们可以使用 `ifconfig` 或者 `ip` 命令来查看。\n\n这两个命令功能都差不多，不过它们属于不同的软件包，`ifconfig` 属于 `net-tools` 软件包，`ip` 属于 `iproute2` 软件包，我的印象中 `net-tools` 软件包没有人继续维护了，而 `iproute2` 软件包是有开发者依然在维护，所以更推荐你使用 `ip` 工具。\n\n\n学以致用，那就来使用这两个命令，来查看网口 `eth0` 的配置等信息：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/网络/showeth0.png)\n\n\n虽然这两个命令输出的格式不尽相同，但是输出的内容基本相同，比如都包含了 IP 地址、子网掩码、MAC 地址、网关地址、MTU 大小、网口的状态以及网路包收发的统计信息，下面就来说说这些信息，它们都与网络性能有一定的关系。\n\n第一，网口的连接状态标志。其实也就是表示对应的网口是否连接到交换机或路由器等设备，如果 `ifconfig` 输出中看到有 `RUNNING`，或者 `ip` 输出中有 `LOWER_UP`，则说明物理网路是连通的，如果看不到，则表示网口没有接网线。\n\n第二，MTU 大小。默认值是 `1500` 字节，其作用主要是限制网络包的大小，如果 IP 层有一个数据报要传，而且数据帧的长度比链路层的 MTU 还大，那么 IP 层就需要进行分片，即把数据报分成若干片，这样每一片就都小于 MTU。事实上，每个网络的链路层 MTU 可能会不一样，所以你可能需要调大或者调小 MTU 的数值。\n\n第三，网口的 IP 地址、子网掩码、MAC 地址、网关地址。这些信息必须要配置正确，网络功能才能正常工作。\n\n第四，网路包收发的统计信息。通常有网络收发的字节数、包数、错误数以及丢包情况的信息，如果 `TX`（发送） 和 `RX`（接收） 部分中 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，则说明网络发送或者接收出问题了，这些出错统计信息的指标意义如下：\n\n- *errors* 表示发生错误的数据包数，比如校验错误、帧同步错误等；\n- *dropped* 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer（这个缓冲区是在内核内存中，更具体一点是在网卡驱动程序里），但因为系统内存不足等原因而发生的丢包；\n- *overruns* 表示超限数据包数，即网络接收/发送速度过快，导致 Ring Buffer 中的数据包来不及处理，而导致的丢包，因为过多的数据包挤压在 Ring Buffer，这样 Ring Buffer 很容易就溢出了；\n- *carrier* 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等；\n- *collisions* 表示冲突、碰撞数据包数；\n\n`ifconfig` 和 `ip` 命令只显示的是网口的配置以及收发数据包的统计信息，而看不到协议栈里的信息，那接下来就来看看如何查看协议栈里的信息。\n\n---\n\n## socket 信息如何查看？\n\n我们可以使用 `netstat` 或者 `ss`，这两个命令查看 socket、网络协议栈、网口以及路由表的信息。\n\n虽然 `netstat` 与 `ss` 命令查看的信息都差不多，但是如果在生产环境中要查看这类信息的时候，尽量不要使用 `netstat` 命令，因为它的性能不好，在系统比较繁忙的情况下，如果频繁使用 `netstat` 命令则会对性能的开销雪上加霜，所以更推荐你使用性能更好的 `ss` 命令。\n\n从下面这张图，你可以看到这两个命令的输出内容：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/网络/showsocket.png)\n\n\n\n可以发现，输出的内容都差不多， 比如都包含了 socket 的状态（*State*）、接收队列（*Recv-Q*）、发送队列（*Send-Q*）、本地地址（*Local Address*）、远端地址（*Foreign Address*）、进程 PID 和进程名称（*PID/Program name*）等。\n\n接收队列（*Recv-Q*）和发送队列（*Send-Q*）比较特殊，在不同的 socket 状态。它们表示的含义是不同的。\n\n当 socket 状态处于 `Established `时：\n\n- *Recv-Q* 表示 socket 缓冲区中还没有被应用程序读取的字节数；\n- *Send-Q* 表示 socket 缓冲区中还没有被远端主机确认的字节数；\n\n而当 socket 状态处于 `Listen` 时：\n\n- *Recv-Q* 表示全连接队列的长度；\n- *Send-Q* 表示全连接队列的最大长度；\n\n\n在 TCP 三次握手过程中，当服务器收到客户端的 SYN 包后，内核会把该连接存储到半连接队列，然后再向客户端发送 SYN+ACK 包，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其增加到全连接队列 ，等待进程调用 `accept()` 函数时把连接取出来。\n\n![半连接队列与全连接队列](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/3.jpg)\n\n\n也就说，全连接队列指的是服务器与客户端完了 TCP 三次握手后，还没有被 `accept()` 系统调用取走连接的队列。\n\n\n那对于协议栈的统计信息，依然还是使用 `netstat` 或 `ss`，它们查看统计信息的命令如下：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/网络/showinfo.png)\n\n\n`ss` 命令输出的统计信息相比 `netsat` 比较少，`ss` 只显示已经连接（*estab*）、关闭（*closed*）、孤儿（*orphaned*） socket 等简要统计。\n\n而 `netstat` 则有更详细的网络协议栈信息，比如上面显示了 TCP 协议的主动连接（*active connections openings*）、被动连接（*passive connection openings*）、失败重试（*failed connection attempts*）、发送（*segments send out*）和接收（*segments received*）的分段数量等各种信息。\n\n---\n\n## 网络吞吐率和 PPS 如何查看？\n\n可以使用 `sar` 命令当前网络的吞吐率和 PPS，用法是给 `sar` 增加 `-n` 参数就可以查看网络的统计信息，比如\n\n- sar -n DEV，显示网口的统计数据；\n- sar -n EDEV，显示关于网络错误的统计数据；\n- sar -n TCP，显示 TCP 的统计数据\n\n比如，我通过 `sar` 命令获取了网口的统计信息：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/网络/sar.png)\n\n\n它们的含义：\n\n- `rxpck/s` 和 `txpck/s` 分别是接收和发送的 PPS，单位为包 / 秒。\n- `rxkB/s` 和 `txkB/s` 分别是接收和发送的吞吐率，单位是 KB/ 秒。\n- `rxcmp/s` 和 `txcmp/s` 分别是接收和发送的压缩数据包数，单位是包 / 秒。\n\n对于带宽，我们可以使用 `ethtool` 命令来查询，它的单位通常是 `Gb/s` 或者 `Mb/s`，不过注意这里小写字母 `b` ，表示比特而不是字节。我们通常提到的千兆网卡、万兆网卡等，单位也都是比特（*bit*）。如下你可以看到， eth0 网卡就是一个千兆网卡：\n\n```bash\n$ ethtool eth0 | grep Speed\n  Speed: 1000Mb/s\n```\n\n---\n\n## 连通性和延时如何查看？\n\n要测试本机与远程主机的连通性和延时，通常是使用 `ping` 命令，它是基于 ICMP 协议的，工作在网络层。\n\n比如，如果要测试本机到 `192.168.12.20` IP 地址的连通性和延时：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/操作系统/网络/ping.png)\n\n显示的内容主要包含  `icmp_seq`（ICMP 序列号）、`TTL`（生存时间，或者跳数）以及 `time` （往返延时），而且最后会汇总本次测试的情况，如果网络没有丢包，`packet loss` 的百分比就是 0。\n\n不过，需要注意的是，`ping` 不通服务器并不代表 HTTP 请求也不通，因为有的服务器的防火墙是会禁用 ICMP 协议的。\n\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n\n\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/9_linux_cmd/pv_uv":{"title":"pv_uv","content":"# 10.2  如何从日志分析 PV、UV？\n\n\n\n很多时候，我们观察程序是否如期运行，或者是否有错误，最直接的方式就是看运行**日志**，当然要想从日志快速查到我们想要的信息，前提是程序打印的日志要精炼、精准。\n\n但日志涵盖的信息远不止于此，比如对于 nginx 的 access.log 日志，我们可以根据日志信息**分析用户行为**。\n\n什么用户行为呢？比如分析出哪个页面访问次数（*PV*）最多，访问人数（*UV*）最多，以及哪天访问量最多，哪个请求访问最多等等。\n\n这次，将用一个大概几万条记录的 nginx 日志文件作为案例，一起来看看如何分析出「用户信息」。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/提纲日志.png)\n\n\n\n---\n\n\n## 别急着开始\n\n当我们要分析日志的时候，先用 `ls -lh` 命令查看日志文件的大小，如果日志文件大小非常大，最好不要在线上环境做。\n\n比如我下面这个日志就 6.5M，不算大，在线上环境分析问题不大。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/ls.png)\n\n如果日志文件数据量太大，你直接一个 `cat` 命令一执行，是会影响线上环境，加重服务器的负载，严重的话，可能导致服务器无响应。\n\n当发现日志很大的时候，我们可以使用 `scp` 命令将文件传输到闲置的服务器再分析，scp 命令使用方式如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/scp.png)\n\n\n---\n\n## 慎用 cat\n\n大家都知道 `cat` 命令是用来查看文件内容的，但是日志文件数据量有多少，它就读多少，很显然不适用大文件。\n\n对于大文件，我们应该养成好习惯，用 `less` 命令去读文件里的内容，因为 less 并不会加载整个文件，而是按需加载，先是输出一小页的内容，当你要往下看的时候，才会继续加载。\n\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/less.png)\n\n\n\n可以发现，nginx 的 access.log 日志每一行是一次用户访问的记录，从左到右分别包含如下信息：\n\n- 客户端的 IP 地址；\n- 访问时间；\n- HTTP 请求的方法、路径、协议版本、协议版本、返回的状态码；\n- User Agent，一般是客户端使用的操作系统以及版本、浏览器及版本等；\n\n\n不过，有时候我们想看日志最新部分的内容，可以使用 `tail` 命令，比如当你想查看倒数 5 行的内容，你可以使用这样的命令：\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/tail-n.png)\n\n\n\n如果你想实时看日志打印的内容，你可以使用 `tail -f` 命令，这样你看日志的时候，就会是阻塞状态，有新日志输出的时候，就会实时显示出来。\n\n\n---\n\n## PV  分析\n\n\nPV 的全称叫 *Page View*，用户访问一个页面就是一次 PV，比如大多数博客平台，点击一次页面，阅读量就加 1，所以说 PV 的数量并不代表真实的用户数量，只是个点击量。\n\n对于 nginx 的 `acess.log` 日志文件来说，分析 PV 还是比较容易的，既然日志里的内容是访问记录，那有多少条日志记录就有多少 PV。\n\n我们直接使用 `wc -l` 命令，就可以查看整体的 PV 了，如下图一共有 49903 条 PV。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/wc.png)\n\n\n\n\n---\n\n## PV 分组\n\nnginx 的 `acess.log` 日志文件有访问时间的信息，因此我们可以根据访问时间进行分组，比如按天分组，查看每天的总 PV，这样可以得到更加直观的数据。\n\n要按时间分组，首先我们先「访问时间」过滤出来，这里可以使用 awk 命令来处理，awk 是一个处理文本的利器。\n\nawk 命令默认是以「空格」为分隔符，由于访问时间在日志里的第 4 列，因此可以使用 `awk '{print $4}' access.log` 命令把访问时间的信息过滤出来，结果如下：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/awk日期.png)\n\n\n\n\n\n上面的信息还包含了时分秒，如果只想显示年月日的信息，可以使用 `awk` 的 `substr` 函数，从第 2 个字符开始，截取 11 个字符。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/awk日期2.png)\n\n\n\n\n接着，我们可以使用 `sort` 对日期进行排序，然后使用 `uniq -c` 进行统计，于是按天分组的 PV 就出来了。\n\n可以看到，每天的 PV 量大概在 2000-2800：\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/awkpv.png)\n\n\n注意，**使用 `uniq -c` 命令前，先要进行 `sort` 排序**，因为 uniq 去重的原理是比较相邻的行，然后除去第二行和该行的后续副本，因此在使用 uniq 命令之前，请使用 sort 命令使所有重复行相邻。\n\n---\n\n## UV 分析\n\nUV 的全称是 *Uniq Visitor*，它代表访问人数，比如公众号的阅读量就是以 UV 统计的，不管单个用户点击了多少次，最终只算 1 次阅读量。\n\naccess.log 日志里虽然没有用户的身份信息，但是我们可以用「客户端 IP 地址」来**近似统计** UV。\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/uv.png)\n\n该命令的输出结果是 2589，也就说明 UV 的量为 2589。上图中，从左到右的命令意思如下：\n\n- `awk '{print $1}' access.log`，取日志的第 1 列内容，客户端的 IP 地址正是第 1 列；\n- `sort`，对信息排序；\n- `uniq`，去除重复的记录；\n- `wc -l`，查看记录条数；\n\n---\n\n## UV 分组\n\n假设我们按天来分组分析每天的 UV 数量，这种情况就稍微比较复杂，需要比较多的命令来实现。\n\n既然要按天统计 UV，那就得把「日期 + IP地址」过滤出来，并去重，命令如下：\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/uv分组.png)\n\n\n具体分析如下：\n\n- 第一次 `ack` 是将第 4 列的日期和第 1 列的客户端 IP 地址过滤出来，并用空格拼接起来；\n- 然后 `sort` 对第一次 ack 输出的内容进行排序；\n- 接着用 `uniq` 去除重复的记录，也就说日期 +IP 相同的行就只保留一个；\n\n上面只是把 UV 的数据列了出来，但是并没有统计出次数。\n\n如果需要对当天的 UV 统计，在上面的命令再拼接 `awk '{uv[$1]++;next}END{for (ip in uv) print ip, uv[ip]}'` 命令就可以了，结果如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/awknext.png)\n\nawk 本身是「逐行」进行处理的，当执行完一行后，我们可以用 `next` 关键字来告诉 awk 跳转到下一行，把下一行作为输入。\n\n对每一行输入，awk 会根据第 1 列的字符串（也就是日期）进行累加，这样相同日期的 ip 地址，就会累加起来，作为当天的 uv 数量。\n\n之后的 `END` 关键字代表一个触发器，就是当前面的输入全部完成后，才会执行 END {} 中的语句，END 的语句是通过 foreach 遍历 uv 中所有的 key，打印出按天分组的 uv 数量。\n\n---\n\n## 终端分析\n\nnginx 的 access.log 日志最末尾关于 User Agent 的信息，主要是客户端访问服务器使用的工具，可能是手机、浏览器等。\n\n因此，我们可以利用这一信息来分析有哪些终端访问了服务器。\n\nUser Agent 的信息在日志里的第 12 列，因此我们先使用 `awk` 过滤出第 12 列的内容后，进行 `sort` 排序，再用 `uniq -c` 去重并统计，最后再使用 `sort -rn`（*r 表示逆向排序， n 表示按数值排序*） 对统计的结果排序，结果如下图：\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/terminal.png)\n\n---\n\n## 分析 TOP3 的请求\n\naccess.log 日志中，第 7 列是客户端请求的路径，先使用 `awk` 过滤出第 7 列的内容后，进行 `sort` 排序，再用 `uniq -c` 去重并统计，然后再使用 `sort -rn` 对统计的结果排序，最后使用 `head -n 3` 分析 TOP3 的请求，结果如下图：\n\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost4@main/网络/log/TOP3.png)\n\n---\n\n## 关注作者\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」，关注后，回复「网络」再送你图解网络 PDF***\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/README":{"title":"README","content":"# 图解系统介绍\n\n大家好，我是小林，是《图解系统》的作者，本站的内容都是整理于我[公众号](https://mp.weixin.qq.com/s/FYH1I8CRsuXDSybSGY_AFA)里的图解文章。\n\n还没关注的朋友，可以微信搜索「**小林coding**」，关注我的公众号，**后续最新版本的 PDF 会在我的公众号第一时间发布**，而且会有更多其他系列的图解文章，比如操作系统、计算机组成、数据库、算法等等。\n\n简单介绍下这个《图解系统》，整个内容共有 **`16W` 字 + `400` 张图**，文字都是小林一个字一个字敲出来的，图片都是小林一个点一条线画出来的，非常的不容易。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/38c89e02026a4c1e8b98ed0a9ee6cb44.png)\n\n图解系统不仅仅涉及了操作系统的内容，还涉及一些计算机组成和 Linux 命令的内容，\n\n当然还是操作系统的内容占比较高，基本把操作系统**进程管理、内存管理、文件系统、设备管理、网络系统**这五大结构图解了，其中进程管理和网络系统这两个章节篇幅比较多，进程管理不仅包含了进程与线程的基本知识，还包含了进程间通信，多线程同步、死锁、悲观锁和乐观锁。网络系统包含 I/O 多路复用、零拷贝、Reactor 等等。\n\n计算机组成主要涉及是 CPU 方面的知识，我们不关注 CPU 是怎么设计与实现的，**只关注跟我们开发者有关系的 CPU 知识**，比如 CPU 执行程序的原理，CPU 缓存，CPU 伪共享等等，这些看似跟我们开发者无关，实际上关系挺大的，只有了解 CPU 缓存才能写出更快的代码，只要了解 CPU 伪共享才能避免写出无效缓存的代码。\n\n至于 Linux 命令的章节暂时内容没有很多，主要就写了如何用 Linux 命令「查看网络指标」和「从日志分析 PV、UV」，之所以没有写太多是觉得命令类的文章没办法体现出小林的图解功力，再加上这类命令一般网上资源也很多，工作中遇到需要使用某个命令时，去搜索了解并自己体验了一番后，才会比较深刻，单纯只看文章很容易就忘记这些命令了。\n\n## 小白适合看吗？\n\n《图解系统》不是单纯的面经，而是相对比较系统化的内容，当然小林所写的内容是操作系统的重点知识，也是面试常问的知识点。\n\n我觉得相比背零零散散的面经，更建议你学好整个操作系统的知识体系，后面你在看面经的时候，你会发觉这些只不过是这颗知识树中的一个小分支，而且延展性会更好。\n\n操作系统是很容易让小白畏惧一门课，因为不管哪本操作系统书都是厚厚的，就会觉得操作系统东西太多，而且也不容易看懂，每个字我们能得懂，但是连成一句话就看懵了。\n\n其实小林当时在入门操作系统的时候，也是跟大家感受一样的，谁不是从小白度过过来的呢？\n\n之前我花了很多时间看书和看视频，学好操作系统后，我就在想能不能写一份帮助大家快速入门操作系统系统文章呢，于是就开始踏上了图解之路，**用精美的图片打破大家对操作系统的畏惧感**。\n\n事实证明，图解系列是正确的，在公众号连续写了很多篇图解系统的文章后，收到了非常多读者的支持与认可，有反馈以前大学没学会的，然后看了我的文章突然就醒悟了，也有反馈面试前突击了我的文章，然后拿到了心意的 offer。\n\n所以，这份图解系统适合小白学习，也可以当作面试突击用的手册。\n\n不过，再怎么吹我的《图解系统》，如果大家想要系统化全面的学习操作系统，自然还是离不开书的，《图解系统》的末尾会有我学习操作系统的心得，会推荐我看过并且认为不错的书和视频，大家可以留意一下。\n\n## 要怎么阅读？\n\n很诚恳的告诉你，《图解系统》不是教科书。而是我在公众号里写的图解系统文章的整合，所以肯定是没有教科书那么细致和全面，当然也就不会有很多废话，都是直击重点，不绕弯，而且有的知识点书上看不到。\n\n阅读的顺序可以不用从头读到尾，你可以根据你想要了解的知识点，通过本站的搜索功能，去看哪个章节的文章就好，可以随意阅读任何章节。\n\n本站的左侧菜单就是《图解系统》的目录结构（别看篇章不多，每一章都是很长很长的文章哦 :laughing:）：\n\n- **硬件结构** :point_down:\n  - [CPU 是如何执行程序的？](操作系统/1_hardware/how_cpu_run.md) \n  - [磁盘比内存慢几万倍？](操作系统/1_hardware/storage.md) \n  - [如何写出让 CPU 跑得更快的代码？](操作系统/1_hardware/how_to_make_cpu_run_faster.md) \n  - [CPU 缓存一致性](操作系统/1_hardware/cpu_mesi.md) \n  - [CPU 是如何执行任务的？](操作系统/1_hardware/how_cpu_deal_task.md) \n  - [什么是软中断？](操作系统/1_hardware/soft_interrupt.md) \n  - [为什么 0.1 + 0.2 不等于 0.3 ？](操作系统/1_hardware/float.md) \n- **操作系统结构** :point_down:\n\t- [Linux 内核 vs Windows 内核](操作系统/2_os_structure/linux_vs_windows.md) \n- **内存管理** :point_down:\n\t- [为什么要有虚拟内存？](操作系统/3_memory/vmem.md) \n\t- [malloc是如何分配内存的？](操作系统/3_memory/malloc.md) \n\t- [内存满了，会发生什么？](操作系统/3_memory/mem_reclaim.md) \n\t- [在 4GB 物理内存的机器上，申请 8G 内存会怎么样？](操作系统/3_memory/alloc_mem.md) \n\t- [如何避免预读失效和缓存污染的问题？](操作系统/3_memory/cache_lru.md) \n\t- [深入理解 Linux 虚拟内存管理](/os/3_memory/linux_mem.md) \n- **进程管理** :point_down:\n\t- [进程、线程基础知识](操作系统/4_process/process_base.md) \n\t- [进程间有哪些通信方式？](操作系统/4_process/process_commu.md) \n\t- [多线程冲突了怎么办？](操作系统/4_process/multithread_sync.md) \n\t- [怎么避免死锁？](操作系统/4_process/deadlock.md) \n\t- [什么是悲观锁、乐观锁？](操作系统/4_process/pessim_and_optimi_lock.md) \n\t- [一个进程最多可以创建多少个线程？](操作系统/4_process/create_thread_max.md) \n\t- [线程崩溃了，进程也会崩溃吗？](操作系统/4_process/thread_crash.md) \n- **调度算法** :point_down:\n\t- [进程调度/页面置换/磁盘调度算法](操作系统/5_schedule/schedule.md)\n- **文件系统** :point_down:\n\t- [文件系统全家桶](操作系统/6_file_system/file_system.md) \n\t- [进程写文件时，进程发生了崩溃，已写入的数据会丢失吗？](操作系统/6_file_system/pagecache.md) \t\n- **设备管理** :point_down:\n\t- [键盘敲入 A 字母时，操作系统期间发生了什么？](操作系统/7_device/device.md) \n- **网络系统** :point_down:\n\t- [什么是零拷贝？](操作系统/8_network_system/zero_copy.md) \n\t- [I/O 多路复用：select/poll/epoll](操作系统/8_network_system/selete_poll_epoll.md) \n\t- [高性能网络模式：Reactor 和 Proactor](操作系统/8_network_system/reactor.md) \n\t- [什么是一致性哈希？](操作系统/8_network_system/hash.md) \n- **学习心得** :point_down:\n\t- [如何查看网络的性能指标？](操作系统/9_linux_cmd/linux_network.md) \t\n  - [画图经验分享](操作系统/9_linux_cmd/pv_uv.md) \t\n- **学习心得** :point_down:\n\t- [计算机网络怎么学？](操作系统/10_learn/learn_os.md) \t\n  - [画图经验分享](操作系统/10_learn/draw.md) \t\n\n## 有错误怎么办？\n\n小林是个手残党，时常写出错别字。\n\n如果你在学习的过程中，**如果你发现有任何错误或者疑惑的地方，欢迎你通过邮箱或者底部留言给小林**，勘误邮箱：xiaolincoding@163.com\n\n小林抽时间会逐个修正，然后发布新版本的图解系统 PDF，一起迭代出更好的图解系统！\n\n新的图解文章都在公众号首发，别忘记关注了哦！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/1_base/how_os_deal_network_package":{"title":"how_os_deal_network_package","content":"# 2.3 Linux 系统是如何收发网络包的？\n\n这次，就围绕一个问题来说。\n\n**Linux 系统是如何收发网络包的？**\n\n## 网络模型\n\n为了使得多种设备能通过网络相互通信，和为了解决各种不同设备在网络互联中的兼容性问题，国际标准化组织制定了开放式系统互联通信参考模型（*Open System Interconnection Reference Model*），也就是 OSI 网络模型，该模型主要有 7 层，分别是应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层。\n\n每一层负责的职能都不同，如下：\n\n- 应用层，负责给应用程序提供统一的接口；\n- 表示层，负责把数据转换成兼容另一个系统能识别的格式；\n- 会话层，负责建立、管理和终止表示层实体之间的通信会话；\n- 传输层，负责端到端的数据传输；\n- 网络层，负责数据的路由、转发、分片；\n- 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；\n- 物理层，负责在物理网络中传输数据帧；\n\n由于 OSI 模型实在太复杂，提出的也只是概念理论上的分层，并没有提供具体的实现方案。\n\n事实上，我们比较常见，也比较实用的是四层模型，即 TCP/IP 网络模型，Linux 系统正是按照这套网络模型来实现网络协议栈的。\n\nTCP/IP 网络模型共有 4 层，分别是应用层、传输层、网络层和网络接口层，每一层负责的职能如下：\n\n- 应用层，负责向用户提供一组应用程序，比如 HTTP、DNS、FTP 等;\n- 传输层，负责端到端的通信，比如 TCP、UDP 等；\n- 网络层，负责网络包的封装、分片、路由、转发，比如 IP、ICMP 等；\n- 网络接口层，负责网络包在物理网络中的传输，比如网络包的封帧、 MAC 寻址、差错检测，以及通过网卡传输网络帧等；\n\nTCP/IP 网络模型相比 OSI 网络模型简化了不少，也更加易记，它们之间的关系如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/OSI与TCP.png)\n\n不过，我们常说的七层和四层负载均衡，是用 OSI 网络模型来描述的，七层对应的是应用层，四层对应的是传输层。\n\n---\n\n## Linux 网络协议栈\n\n\n我们可以把自己的身体比作应用层中的数据，打底衣服比作传输层中的 TCP 头，外套比作网络层中 IP 头，帽子和鞋子分别比作网络接口层的帧头和帧尾。\n\n在冬天这个季节，当我们要从家里出去玩的时候，自然要先穿个打底衣服，再套上保暖外套，最后穿上帽子和鞋子才出门，这个过程就好像我们把 TCP 协议通信的网络包发出去的时候，会把应用层的数据按照网络协议栈层层封装和处理。\n\n你从下面这张图可以看到，应用层数据在每一层的封装格式。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/封装.png)\n\n其中：\n\n- 传输层，给应用数据前面增加了 TCP  头；\n- 网络层，给 TCP 数据包前面增加了 IP  头；\n- 网络接口层，给 IP 数据包前后分别增加了帧头和帧尾；\n\n这些新增的头部和尾部，都有各自的作用，也都是按照特定的协议格式填充，这每一层都增加了各自的协议头，那自然网络包的大小就增大了，但物理链路并不能传输任意大小的数据包，所以在以太网中，规定了最大传输单元（MTU）是 `1500` 字节，也就是规定了单次传输的最大 IP 包大小。\n\n当网络包超过 MTU 的大小，就会在网络层分片，以确保分片后的 IP 包不会超过 MTU 大小，如果 MTU 越小，需要的分包就越多，那么网络吞吐能力就越差，相反的，如果 MTU 越大，需要的分包就越少，那么网络吞吐能力就越好。\n\n知道了 TCP/IP 网络模型，以及网络包的封装原理后，那么 Linux 网络协议栈的样子，你想必猜到了大概，它其实就类似于 TCP/IP 的四层结构：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/协议栈.png)\n\n\n从上图的的网络协议栈，你可以看到：\n\n- 应用程序需要通过系统调用，来跟 Socket 层进行数据交互；\n- Socket 层的下面就是传输层、网络层和网络接口层；\n- 最下面的一层，则是网卡驱动程序和硬件网卡设备；\n\n\n## Linux 接收网络包的流程\n\n\n网卡是计算机里的一个硬件，专门负责接收和发送网络包，当网卡接收到一个网络包后，会通过 DMA 技术，将网络包写入到指定的内存地址，也就是写入到 Ring Buffer ，这个是一个环形缓冲区，接着就会告诉操作系统这个网络包已经到达。\n\n\u003e  那应该怎么告诉操作系统这个网络包已经到达了呢？\n\n最简单的一种方式就是触发中断，也就是每当网卡收到一个网络包，就触发一个中断告诉操作系统。\n\n但是，这存在一个问题，在高性能网络场景下，网络包的数量会非常多，那么就会触发非常多的中断，要知道当 CPU  收到了中断，就会停下手里的事情，而去处理这些网络包，处理完毕后，才会回去继续其他事情，那么频繁地触发中断，则会导致 CPU 一直没完没了的处理中断，而导致其他任务可能无法继续前进，从而影响系统的整体效率。\n\n所以为了解决频繁中断带来的性能开销，Linux 内核在 2.6 版本中引入了 **NAPI 机制**，它是混合「中断和轮询」的方式来接收网络包，它的核心概念就是**不采用中断的方式读取数据**，而是首先采用中断唤醒数据接收的服务程序，然后 `poll` 的方法来轮询数据。\n\n因此，当有网络包到达时，会通过 DMA 技术，将网络包写入到指定的内存地址，接着网卡向 CPU 发起硬件中断，当 CPU 收到硬件中断请求后，根据中断表，调用已经注册的中断处理函数。\n\n硬件中断处理函数会做如下的事情：\n\n- 需要先「暂时屏蔽中断」，表示已经知道内存中有数据了，告诉网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停的被中断。\n- 接着，发起「软中断」，然后恢复刚才屏蔽的中断。\n\n至此，硬件中断处理函数的工作就已经完成。\n\n硬件中断处理函数做的事情很少，主要耗时的工作都交给软中断处理函数了。\n\n\u003e 软中断的处理\n\n内核中的 ksoftirqd 线程专门负责软中断的处理，当 ksoftirqd 内核线程收到软中断后，就会来轮询处理数据。\n\nksoftirqd 线程会从 Ring Buffer 中获取一个数据帧，用 sk_buff 表示，从而可以作为一个网络包交给网络协议栈进行逐层处理。\n\n\u003e 网络协议栈\n\n首先，会先进入到网络接口层，在这一层会检查报文的合法性，如果不合法则丢弃，合法则会找出该网络包的上层协议的类型，比如是 IPv4，还是 IPv6，接着再去掉帧头和帧尾，然后交给网络层。\n\n到了网络层，则取出 IP 包，判断网络包下一步的走向，比如是交给上层处理还是转发出去。当确认这个网络包要发送给本机后，就会从 IP 头里看看上一层协议的类型是 TCP 还是 UDP，接着去掉 IP 头，然后交给传输层。\n\n传输层取出 TCP 头或 UDP 头，根据四元组「源 IP、源端口、目的 IP、目的端口」 作为标识，找出对应的 Socket，并把数据放到 Socket 的接收缓冲区。\n\n最后，应用层程序调用 Socket 接口，将内核的 Socket 接收缓冲区的数据「拷贝」到应用层的缓冲区，然后唤醒用户进程。\n\n至此，一个网络包的接收过程就已经结束了，你也可以从下图左边部分看到网络包接收的流程，右边部分刚好反过来，它是网络包发送的流程。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/操作系统/浮点/收发流程.png)\n\n## Linux 发送网络包的流程\n\n如上图的右半部分，发送网络包的流程正好和接收流程相反。\n\n\n首先，应用程序会调用 Socket 发送数据包的接口，由于这个是系统调用，所以会从用户态陷入到内核态中的 Socket 层，内核会申请一个内核态的 sk_buff 内存，**将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区**。\n\n接下来，网络协议栈从 Socket 发送缓冲区中取出 sk_buff，并按照 TCP/IP 协议栈从上到下逐层处理。\n\n如果使用的是 TCP 传输协议发送数据，那么**先拷贝一个新的 sk_buff 副本** ，这是因为 sk_buff 后续在调用网络层，最后到达网卡发送完成的时候，这个 sk_buff 会被释放掉。而 TCP 协议是支持丢失重传的，在收到对方的 ACK 之前，这个 sk_buff 不能被删除。所以内核的做法就是每次调用网卡发送的时候，实际上传递出去的是 sk_buff 的一个拷贝，等收到 ACK 再真正删除。\n\n接着，对 sk_buff 填充 TCP 头。这里提一下，sk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。\n\n你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。\n\n于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 `data` 的指针，比如：\n\n- 当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb-\u003edata 的值，来逐步剥离协议首部。\n- 当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb-\u003edata 的值来增加协议首部。\n\n你可以从下面这张图看到，当发送报文时，data 指针的移动过程。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/sk_buff.jpg)\n\n至此，传输层的工作也就都完成了。\n\n然后交给网络层，在网络层里会做这些工作：选取路由（确认下一跳的 IP）、填充 IP 头、netfilter 过滤、对超过 MTU 大小的数据包进行分片。处理完这些工作后会交给网络接口层处理。\n\n网络接口层会通过 ARP 协议获得下一跳的 MAC 地址，然后对 sk_buff 填充帧头和帧尾，接着将 sk_buff 放到网卡的发送队列中。\n\n这一些工作准备好后，会触发「软中断」告诉网卡驱动程序，这里有新的网络包需要发送，驱动程序会从发送队列中读取 sk_buff，将这个 sk_buff 挂到 RingBuffer 中，接着将 sk_buff 数据映射到网卡可访问的内存 DMA 区域，最后触发真实的发送。\n\n当数据发送完成以后，其实工作并没有结束，因为内存还没有清理。当发送完成的时候，网卡设备会触发一个硬中断来释放内存，主要是释放 sk_buff 内存和清理  RingBuffer 内存。\n\n最后，当收到这个 TCP 报文的 ACK 应答时，传输层就会释放原始的 sk_buff 。\n\n\u003e 发送网络数据的时候，涉及几次内存拷贝操作？\n\n第一次，调用发送数据的系统调用的时候，内核会申请一个内核态的 sk_buff 内存，将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区。\n\n第二次，在使用 TCP 传输协议的情况下，从传输层进入网络层的时候，每一个 sk_buff 都会被克隆一个新的副本出来。副本 sk_buff 会被送往网络层，等它发送完的时候就会释放掉，然后原始的 sk_buff 还保留在传输层，目的是为了实现 TCP 的可靠传输，等收到这个数据包的 ACK 时，才会释放原始的 sk_buff 。\n\n第三次，当 IP 层发现 sk_buff 大于 MTU 时才需要进行。会再申请额外的 sk_buff，并将原来的 sk_buff 拷贝为多个小的 sk_buff。\n\n## 总结\n\n电脑与电脑之间通常都是通过话网卡、交换机、路由器等网络设备连接到一起，那由于网络设备的异构性，国际标准化组织定义了一个七层的 OSI 网络模型，但是这个模型由于比较复杂，实际应用中并没有采用，而是采用了更为简化的 TCP/IP 模型，Linux 网络协议栈就是按照了该模型来实现的。\n\nTCP/IP 模型主要分为应用层、传输层、网络层、网络接口层四层，每一层负责的职责都不同，这也是 Linux 网络协议栈主要构成部分。\n\n当应用程序通过 Socket 接口发送数据包，数据包会被网络协议栈从上到下进行逐层处理后，才会被送到网卡队列中，随后由网卡将网络包发送出去。\n\n而在接收网络包时，同样也要先经过网络协议栈从下到上的逐层处理，最后才会被送到应用程序。\n\n----\n\n参考资料：\n\n- Linux 网络包发送过程：https://mp.weixin.qq.com/s/wThfD9th9e_-YGHJJ3HXNQ\n- Linux 网络数据接收流程（TCP）- NAPI：https://wenfh2020.com/2021/12/29/kernel-tcp-receive/\n- Linux网络-数据包接收过程：https://blog.csdn.net/frank_jb/article/details/115841622\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，别忘记关注我哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/1_base/tcp_ip_model":{"title":"tcp_ip_model","content":"# 2.1 TCP/IP 网络模型有哪几层？\n\n问大家，为什么要有 TCP/IP 网络模型？\n\n对于同一台设备上的进程间通信，有很多种方式，比如有管道、消息队列、共享内存、信号等方式，而对于不同设备上的进程间通信，就需要网络通信，而设备是多样性的，所以要兼容多种多样的设备，就协商出了一套**通用的网络协议**。\n\n这个网络协议是分层的，每一层都有各自的作用和职责，接下来就根据「 TCP/IP 网络模型」分别对每一层进行介绍。\n\n## 应用层\n\n最上层的，也是我们能直接接触到的就是**应用层**（*Application Layer*），我们电脑或手机使用的应用软件都是在应用层实现。那么，当两个不同设备的应用需要通信的时候，应用就把应用数据传给下一层，也就是传输层。\n\n所以，应用层只需要专注于为用户提供应用功能，比如 HTTP、FTP、Telnet、DNS、SMTP等。\n\n应用层是不用去关心数据是如何传输的，就类似于，我们寄快递的时候，只需要把包裹交给快递员，由他负责运输快递，我们不需要关心快递是如何被运输的。\n\n而且应用层是工作在操作系统中的用户态，传输层及以下则工作在内核态。\n\n\n## 传输层\n\n应用层的数据包会传给传输层，**传输层**（*Transport Layer*）是为应用层提供网络支持的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/应用层.png)\n\n\n在传输层会有两个传输协议，分别是 TCP 和 UDP。\n\nTCP 的全称叫传输控制协议（*Transmission Control Protocol*），大部分应用使用的正是 TCP 传输层协议，比如 HTTP 应用层协议。TCP 相比  UDP 多了很多特性，比如流量控制、超时重传、拥塞控制等，这些都是为了保证数据包能可靠地传输给对方。 \n\nUDP 相对来说就很简单，简单到只负责发送数据包，不保证数据包是否能抵达对方，但它实时性相对更好，传输效率也高。当然，UDP 也可以实现可靠传输，把 TCP 的特性在应用层上实现就可以，不过要实现一个商用的可靠 UDP 传输协议，也不是一件简单的事情。\n\n\n应用需要传输的数据可能会非常大，如果直接传输就不好控制，因此当传输层的数据包大小超过 MSS（TCP 最大报文段长度） ，就要将数据包分块，这样即使中途有一个分块丢失或损坏了，只需要重新发送这一个分块，而不用重新发送整个数据包。在 TCP 协议中，我们把每个分块称为一个 **TCP 段**（*TCP Segment*）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/TCP段.png)\n\n当设备作为接收方时，传输层则要负责把数据包传给应用，但是一台设备上可能会有很多应用在接收或者传输数据，因此需要用一个编号将应用区分开来，这个编号就是**端口**。\n\n比如 80 端口通常是 Web 服务器用的，22 端口通常是远程登录服务器用的。而对于浏览器（客户端）中的每个标签栏都是一个独立的进程，操作系统会为这些进程分配临时的端口号。\n\n由于传输层的报文中会携带端口号，因此接收方可以识别出该报文是发送给哪个应用。\n\n\n## 网络层\n\n\n传输层可能大家刚接触的时候，会认为它负责将数据从一个设备传输到另一个设备，事实上它并不负责。\n\n实际场景中的网络环节是错综复杂的，中间有各种各样的线路和分叉路口，如果一个设备的数据要传输给另一个设备，就需要在各种各样的路径和节点进行选择，而传输层的设计理念是简单、高效、专注，如果传输层还负责这一块功能就有点违背设计原则了。\n\n也就是说，我们不希望传输层协议处理太多的事情，只需要服务好应用即可，让其作为应用间数据传输的媒介，帮助实现应用到应用的通信，而实际的传输功能就交给下一层，也就是**网络层**（*Internet Layer*）。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/网络层.png)\n\n网络层最常使用的是 IP 协议（*Internet Protocol*），IP 协议会将传输层的报文作为数据部分，再加上 IP 报头组装成 IP 报文，如果 IP 报文大小超过 MTU（以太网中一般为 1500 字节）就会**再次进行分片**，得到一个即将发送到网络的 IP 报文。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/键入网址过程/12.jpg)\n\n\n网络层负责将数据从一个设备传输到另一个设备，世界上那么多设备，又该如何找到对方呢？因此，网络层需要有区分设备的编号。\n\n我们一般用 IP 地址给设备进行编号，对于 IPv4 协议， IP 地址共 32 位，分成了四段（比如，192.168.100.1），每段是 8 位。只有一个单纯的 IP 地址虽然做到了区分设备，但是寻址起来就特别麻烦，全世界那么多台设备，难道一个一个去匹配？这显然不科学。\n\n因此，需要将 IP 地址分成两种意义：\n\n- 一个是**网络号**，负责标识该 IP 地址是属于哪个「子网」的；\n- 一个是**主机号**，负责标识同一「子网」下的不同主机；\n\n怎么分的呢？这需要配合**子网掩码**才能算出 IP 地址 的网络号和主机号。\n\n举个例子，比如 10.100.122.0/24，后面的`/24`表示就是 `255.255.255.0` 子网掩码，255.255.255.0 二进制是「11111111-11111111-11111111-00000000」，大家数数一共多少个1？不用数了，是 24 个1，为了简化子网掩码的表示，用/24代替255.255.255.0。\n\n知道了子网掩码，该怎么计算出网络地址和主机地址呢？\n\n将 10.100.122.2 和 255.255.255.0 进行**按位与运算**，就可以得到网络号，如下图：\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/16.jpg)\n\n将 255.255.255.0 取反后与IP地址进行进行**按位与运算**，就可以得到主机号。\n\n大家可以去搜索下子网掩码计算器，自己改变下「掩码位」的数值，就能体会到子网掩码的作用了。\n\n![子网掩码计算器](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/子网掩码计算器.png)\n\n那么在寻址的过程中，先匹配到相同的网络号（表示要找到同一个子网），才会去找对应的主机。\n\n除了寻址能力， IP 协议还有另一个重要的能力就是**路由**。实际场景中，两台设备并不是用一条网线连接起来的，而是通过很多网关、路由器、交换机等众多网络设备连接起来的，那么就会形成很多条网络的路径，因此当数据包到达一个网络节点，就需要通过路由算法决定下一步走哪条路径。\n\n路由器寻址工作中，就是要找到目标地址的子网，找到后进而把数据包转发给对应的网络内。\n\n![IP地址的网络号](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/17.jpg)\n\n所以，**IP 协议的寻址作用是告诉我们去往下一个目的地该朝哪个方向走，路由则是根据「下一个目的地」选择路径。寻址更像在导航，路由更像在操作方向盘**。\n\n\n## 网络接口层\n\n生成了 IP 头部之后，接下来要交给**网络接口层**（*Link Layer*）在 IP 头部的前面加上 MAC 头部，并封装成数据帧（Data frame）发送到网络上。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/网络接口层.png)\n\nIP 头部中的接收方 IP 地址表示网络包的目的地，通过这个地址我们就可以判断要将包发到哪里，但在以太网的世界中，这个思路是行不通的。\n\n什么是以太网呢？电脑上的以太网接口，Wi-Fi 接口，以太网交换机、路由器上的千兆，万兆以太网口，还有网线，它们都是以太网的组成部分。以太网就是一种在「局域网」内，把附近的设备连接起来，使它们之间可以进行通讯的技术。\n\n以太网在判断网络包目的地时和 IP 的方式不同，因此必须采用相匹配的方式才能在以太网中将包发往目的地，而 MAC 头部就是干这个用的，所以，在以太网进行通讯要用到 MAC 地址。\n\nMAC 头部是以太网使用的头部，它包含了接收方和发送方的 MAC 地址等信息，我们可以通过 ARP 协议获取对方的 MAC 地址。\n\n所以说，网络接口层主要为网络层提供「链路级别」传输的服务，负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标识网络上的设备。\n\n## 总结\n\n\n综上所述，TCP/IP 网络通常是由上到下分成 4 层，分别是**应用层，传输层，网络层和网络接口层**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpip参考模型.drawio.png)\n\n再给大家贴一下每一层的封装格式：\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E5%B0%81%E8%A3%85.png)\n\n网络接口层的传输单位是帧（frame），IP 层的传输单位是包（packet），TCP 层的传输单位是段（segment），HTTP 的传输单位则是消息或报文（message）。但这些名词并没有什么本质的区分，可以统称为数据包。\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/1_base/what_happen_url":{"title":"what_happen_url","content":"# 2.2 键入网址到网页显示，期间发生了什么？\n\n想必不少小伙伴面试过程中，会遇到「**当键入网址后，到网页显示，其间发生了什么**」的面试题。\n\n还别说，这问题真挺常问的，前几天坐在我旁边的主管电话面试应聘者的时候，也问了这个问题。\n\n接下来以下图较简单的网络拓扑模型作为例子，探究探究其间发生了什么？\n\n![简单的网络模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/2.jpg)\n\n## 孤单小弟 —— HTTP\n\n\u003e 浏览器做的第一步工作是解析 URL\n\n首先浏览器做的第一步工作就是要对 `URL` 进行解析，从而生成发送给 `Web` 服务器的请求信息。\n\n让我们看看一条长长的 URL 里的各个元素的代表什么，见下图：\n\n![URL 解析](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/3.jpg)\n\n所以图中的长长的 URL 实际上是请求服务器里的文件资源。\n\n\u003e 要是上图中的蓝色部分 URL 元素都省略了，那应该是请求哪个文件呢？\n\n当没有路径名时，就代表访问根目录下事先设置的**默认文件**，也就是 `/index.html` 或者 `/default.html` 这些文件，这样就不会发生混乱了。\n\n\u003e 生产 HTTP 请求信息\n\n对 `URL` 进行解析之后，浏览器确定了 Web 服务器和文件名，接下来就是根据这些信息来生成 HTTP 请求消息了。\n\n![HTTP 的消息格式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/4.jpg)\n\n\u003e 一个孤单 HTTP 数据包表示：“我这么一个小小的数据包，没亲没友，直接发到浩瀚的网络，谁会知道我呢？谁能载我一程呢？谁能保护我呢？我的目的地在哪呢？”充满各种疑问的它，没有停滞不前，依然踏上了征途！\n\n---\n\n## 真实地址查询 —— DNS\n\n通过浏览器解析 URL 并生成 HTTP 消息后，需要委托操作系统将消息发送给 `Web`  服务器。\n\n但在发送之前，还有一项工作需要完成，那就是**查询服务器域名对应的 IP 地址**，因为委托操作系统发送消息时，必须提供通信对象的 IP 地址。\n\n比如我们打电话的时候，必须要知道对方的电话号码，但由于电话号码难以记忆，所以通常我们会将对方电话号 + 姓名保存在通讯录里。\n\n所以，有一种服务器就专门保存了 `Web` 服务器域名与 `IP` 的对应关系，它就是 `DNS` 服务器。\n\n\u003e 域名的层级关系\n\nDNS 中的域名都是用**句点**来分隔的，比如 `www.server.com`，这里的句点代表了不同层次之间的**界限**。\n\n在域名中，**越靠右**的位置表示其层级**越高**。\n\n毕竟域名是外国人发明，所以思维和中国人相反，比如说一个城市地点的时候，外国喜欢从小到大的方式顺序说起（如 XX 街道 XX 区 XX 市 XX 省），而中国则喜欢从大到小的顺序（如 XX 省 XX 市 XX 区 XX 街道）。\n\n实际上域名最后还有一个点，比如 `www.server.com.`，这个最后的一个点代表根域名。\n\n也就是，`.` 根域是在最顶层，它的下一层就是 `.com` 顶级域，再下面是 `server.com`。\n\n所以域名的层级关系类似一个树状结构：\n\n- 根 DNS 服务器（.）\n- 顶级域 DNS 服务器（.com）\n- 权威 DNS 服务器（server.com）\n\n![DNS 树状结构](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/5.jpg)\n\n\n根域的 DNS 服务器信息保存在互联网中所有的 DNS 服务器中。\n\n这样一来，任何 DNS 服务器就都可以找到并访问根域 DNS 服务器了。\n\n因此，客户端只要能够找到任意一台 DNS 服务器，就可以通过它找到根域 DNS 服务器，然后再一路顺藤摸瓜找到位于下层的某台目标 DNS 服务器。\n\n\u003e 域名解析的工作流程\n\n1. 客户端首先会发出一个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP/IP 设置中填写的 DNS 服务器地址）。\n2. 本地域名服务器收到客户端的请求后，如果缓存里的表格能找到 www.server.com，则它直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 www.server.com 的 IP 地址吗？” 根域名服务器是最高层次的，它不直接用于域名解析，但能指明一条道路。 \n3. 根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“www.server.com 这个域名归 .com 区域管理”，我把 .com 顶级域名服务器的地址给你，你去问问它吧。”\n4. 本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 www.server.com  的 IP 地址吗？”\n5. 顶级域名服务器说：“我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。\n6. 本地 DNS 于是转向问权威 DNS 服务器：“老三，www.server.com 对应的 IP 是啥呀？” server.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。\n7. 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。\n8. 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。\n\n\n至此，我们完成了 DNS 的解析过程。现在总结一下，整个过程我画成了一个图。\n\n![域名解析的工作流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/6.jpg)\n\nDNS 域名解析的过程蛮有意思的，整个过程就和我们日常生活中找人问路的过程类似，**只指路不带路**。\n\n\u003e 那是不是每次解析域名都要经过那么多的步骤呢？\n\n当然不是了，还有缓存这个东西的嘛。\n\n浏览器会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问操作系统，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 hosts 文件看，也没有，才会去问「本地 DNS 服务器」。\n\n\u003e 数据包表示：“DNS 老大哥厉害呀，找到了目的地了！我还是很迷茫呀，我要发出去，接下来我需要谁的帮助呢?”\n\n----\n\n## 指南好帮手 —— 协议栈\n\n通过 DNS 获取到 IP 后，就可以把 HTTP 的传输工作交给操作系统中的**协议栈**。\n\n协议栈的内部分为几个部分，分别承担不同的工作。上下关系是有一定的规则的，上面的部分会向下面的部分委托工作，下面的部分收到委托的工作并执行。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/7.jpg)\n\n应用程序（浏览器）通过调用 Socket 库，来委托协议栈工作。协议栈的上半部分有两块，分别是负责收发数据的 TCP 和 UDP 协议，这两个传输协议会接受应用层的委托执行收发数据的操作。\n\n协议栈的下面一半是用 IP 协议控制网络包收发操作，在互联网上传数据时，数据会被切分成一块块的网络包，而将网络包发送给对方的操作就是由 IP 负责的。\n\n此外 IP 中还包括 `ICMP` 协议和 `ARP` 协议。\n\n- `ICMP` 用于告知网络包传送过程中产生的错误以及各种控制信息。\n- `ARP` 用于根据 IP 地址查询相应的以太网 MAC 地址。\n\nIP 下面的网卡驱动程序负责控制网卡硬件，而最下面的网卡则负责完成实际的收发操作，也就是对网线中的信号执行发送和接收操作。\n\n\u003e 数据包看了这份指南表示：“原来我需要那么多大佬的协助啊，那我先去找找 TCP 大佬！”\n\n----\n\n## 可靠传输 —— TCP\n\nHTTP 是基于 TCP 协议传输的，所以在这我们先了解下 TCP 协议。\n\n\u003e TCP 包头格式\n\n我们先看看 TCP 报文头部的格式：\n\n![TCP 包头格式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/8.jpg)\n\n首先，**源端口号**和**目标端口**号是不可少的，如果没有这两个端口号，数据就不知道应该发给哪个应用。\n\n接下来有包的**序**号，这个是为了解决包乱序的问题。\n\n还有应该有的是**确认号**，目的是确认发出去对方是否有收到。如果没有收到就应该重新发送，直到送达，这个是为了解决不丢包的问题。\n\n接下来还有一些**状态位**。例如 `SYN` 是发起一个连接，`ACK` 是回复，`RST` 是重新连接，`FIN` 是结束连接等。TCP 是面向连接的，因而双方要维护连接的状态，这些带状态位的包的发送，会引起双方的状态变更。\n\n还有一个重要的就是**窗口大小**。TCP 要做**流量控制**，通信双方各声明一个窗口（缓存大小），标识自己当前能够的处理能力，别发送的太快，撑死我，也别发的太慢，饿死我。\n\n除了做流量控制以外，TCP还会做**拥塞控制**，对于真正的通路堵车不堵车，它无能为力，唯一能做的就是控制自己，也即控制发送的速度。不能改变世界，就改变自己嘛。\n\n\u003e TCP 传输数据之前，要先三次握手建立连接\n\n在 HTTP 传输数据之前，首先需要 TCP 建立连接，TCP 连接的建立，通常称为**三次握手**。\n\n这个所谓的「连接」，只是双方计算机里维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。\n\n![TCP 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/TCP三次握手.drawio.png)\n\n- 一开始，客户端和服务端都处于 `CLOSED` 状态。先是服务端主动监听某个端口，处于 `LISTEN` 状态。\n\n- 然后客户端主动发起连接 `SYN`，之后处于 `SYN-SENT` 状态。\n\n- 服务端收到发起的连接，返回 `SYN`，并且 `ACK` 客户端的 `SYN`，之后处于 `SYN-RCVD` 状态。\n\n- 客户端收到服务端发送的 `SYN` 和 `ACK` 之后，发送对 `SYN` 确认的 `ACK`，之后处于 `ESTABLISHED` 状态，因为它一发一收成功了。\n\n- 服务端收到 `ACK` 的 `ACK` 之后，处于 `ESTABLISHED` 状态，因为它也一发一收了。\n\n所以三次握手目的是**保证双方都有发送和接收的能力**。\n\n\u003e 如何查看 TCP 的连接状态？\n\nTCP 的连接状态查看，在 Linux 可以通过 `netstat -napt` 命令查看。\n\n![TCP 连接状态查看](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/10.jpg)\n\n\u003e TCP 分割数据\n\n如果 HTTP 请求消息比较长，超过了 `MSS` 的长度，这时 TCP 就需要把 HTTP 的数据拆解成一块块的数据发送，而不是一次性发送所有数据。\n\n![MTU 与 MSS](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/11.jpg)\n\n- `MTU`：一个网络包的最大长度，以太网中一般为 `1500` 字节。\n- `MSS`：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度。\n\n数据会被以 `MSS` 的长度为单位进行拆分，拆分出来的每一块数据都会被放进单独的网络包中。也就是在每个被拆分的数据加上 TCP 头信息，然后交给 IP 模块来发送数据。\n\n![数据包分割](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/12.jpg)\n\n\u003e TCP 报文生成\n\nTCP 协议里面会有两个端口，一个是浏览器监听的端口（通常是随机生成的），一个是 Web 服务器监听的端口（HTTP 默认端口号是 `80`， HTTPS 默认端口号是 `443`）。\n\n在双方建立了连接后，TCP 报文中的数据部分就是存放 HTTP 头部 + 数据，组装好 TCP 报文之后，就需交给下面的网络层处理。\n\n至此，网络包的报文如下图。\n\n![TCP 层报文](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/13.jpg)\n\n\u003e 此时，遇上了 TCP 的  数据包激动表示：“太好了，碰到了可靠传输的 TCP 传输，它给我加上 TCP 头部，我不再孤单了，安全感十足啊！有大佬可以保护我的可靠送达！但我应该往哪走呢？”\n\n---\n\n## 远程定位 —— IP\n\nTCP 模块在执行连接、收发、断开等各阶段操作时，都需要委托 IP 模块将数据封装成**网络包**发送给通信对象。\n\n\u003e IP 包头格式\n\n我们先看看 IP 报文头部的格式：\n\n![IP 包头格式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/14.jpg)\n\n在 IP 协议里面需要有**源地址 IP** 和 **目标地址 IP**：\n\n- 源地址 IP，即是客户端输出的 IP 地址；\n- 目标地址，即通过 DNS 域名解析得到的 Web 服务器 IP。\n\n因为 HTTP 是经过 TCP 传输的，所以在 IP 包头的**协议号**，要填写为 `06`（十六进制），表示协议为 TCP。\n\n\u003e 假设客户端有多个网卡，就会有多个 IP 地址，那 IP 头部的源地址应该选择哪个 IP 呢？\n\n当存在多个网卡时，在填写源地址 IP 时，就需要判断到底应该填写哪个地址。这个判断相当于在多块网卡中判断应该使用哪个一块网卡来发送包。\n\n这个时候就需要根据**路由表**规则，来判断哪一个网卡作为源地址 IP。\n\n在 Linux 操作系统，我们可以使用 `route -n` 命令查看当前系统的路由表。\n\n![路由表](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/15.jpg)\n\n举个例子，根据上面的路由表，我们假设 Web 服务器的目标地址是 `192.168.10.200`。\n\n![路由规则判断](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/16.jpg)\n\n1. 首先先和第一条目的子网掩码（`Genmask`）进行 **与运算**，得到结果为 `192.168.10.0`，但是第一个条目的 `Destination` 是 `192.168.3.0`，两者不一致所以匹配失败。\n2. 再与第二条目的子网掩码进行 **与运算**，得到的结果为 `192.168.10.0`，与第二条目的 `Destination 192.168.10.0` 匹配成功，所以将使用 `eth1` 网卡的 IP 地址作为 IP 包头的源地址。\n\n那么假设 Web 服务器的目标地址是 `10.100.20.100`，那么依然依照上面的路由表规则判断，判断后的结果是和第三条目匹配。\n\n第三条目比较特殊，它目标地址和子网掩码都是 `0.0.0.0`，这表示**默认网关**，如果其他所有条目都无法匹配，就会自动匹配这一行。并且后续就把包发给路由器，`Gateway` 即是路由器的 IP 地址。\n\n\u003e IP 报文生成\n\n至此，网络包的报文如下图。\n\n![IP 层报文](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/17.jpg)\n\n\u003e 此时，加上了 IP 头部的数据包表示 ：“有 IP 大佬给我指路了，感谢 IP 层给我加上了 IP 包头，让我有了远程定位的能力，不会害怕在浩瀚的互联网迷茫了！可是目的地好远啊，我下一站应该去哪呢？”\n\n---\n\n## 两点传输 —— MAC\n\n生成了 IP 头部之后，接下来网络包还需要在 IP 头部的前面加上 **MAC 头部**。\n\n\u003e MAC 包头格式\n\nMAC 头部是以太网使用的头部，它包含了接收方和发送方的 MAC 地址等信息。\n\n![MAC 包头格式](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/18.jpg)\n\n在 MAC 包头里需要**发送方 MAC 地址**和**接收方目标 MAC 地址**，用于**两点之间的传输**。\n\n一般在 TCP/IP 通信里，MAC 包头的**协议类型**只使用：\n\n- `0800` ： IP 协议\n- `0806` ： ARP 协议\n\n\n\u003e MAC 发送方和接收方如何确认?\n\n**发送方**的 MAC 地址获取就比较简单了，MAC 地址是在网卡生产时写入到 ROM 里的，只要将这个值读取出来写入到 MAC 头部就可以了。\n\n**接收方**的 MAC 地址就有点复杂了，只要告诉以太网对方的 MAC 的地址，以太网就会帮我们把包发送过去，那么很显然这里应该填写对方的 MAC 地址。\n\n所以先得搞清楚应该把包发给谁，这个只要查一下**路由表**就知道了。在路由表中找到相匹配的条目，然后把包发给 `Gateway` 列中的 IP 地址就可以了。\n\n\u003e 既然知道要发给谁，那如何获取对方的 MAC 地址呢？\n\n不知道对方 MAC 地址？不知道就喊呗。\n\n此时就需要 `ARP` 协议帮我们找到路由器的 MAC 地址。\n\n![ARP 广播](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/19.jpg)\n\nARP 协议会在以太网中以**广播**的形式，对以太网所有的设备喊出：“这个 IP 地址是谁的？请把你的 MAC 地址告诉我”。\n\n然后就会有人回答：“这个 IP 地址是我的，我的 MAC 地址是 XXXX”。\n\n如果对方和自己处于同一个子网中，那么通过上面的操作就可以得到对方的 MAC 地址。然后，我们将这个 MAC 地址写入 MAC 头部，MAC 头部就完成了。\n\n\u003e 好像每次都要广播获取，这不是很麻烦吗？\n\n放心，在后续操作系统会把本次查询结果放到一块叫做 **ARP 缓存**的内存空间留着以后用，不过缓存的时间就几分钟。\n\n也就是说，在发包时：\n\n- 先查询 ARP 缓存，如果其中已经保存了对方的 MAC 地址，就不需要发送 ARP 查询，直接使用 ARP 缓存中的地址。\n- 而当 ARP 缓存中不存在对方 MAC 地址时，则发送 ARP 广播查询。\n\n\n\u003e 查看 ARP 缓存内容\n\n在 Linux 系统中，我们可以使用 `arp -a` 命令来查看 ARP 缓存的内容。\n\n![ARP 缓存内容](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/20.jpg)\n\n\n\u003e MAC 报文生成\n\n至此，网络包的报文如下图。\n\n![MAC 层报文](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/21.jpg)\n\n\u003e 此时，加上了 MAC 头部的数据包万分感谢，说道 ：“感谢 MAC 大佬，我知道我下一步要去哪了！我现在有很多头部兄弟，相信我可以到达最终的目的地！”。\n\u003e 带着众多头部兄弟的数据包，终于准备要出门了。\n\n---\n\n## 出口 —— 网卡\n\n网络包只是存放在内存中的一串二进制数字信息，没有办法直接发送给对方。因此，我们需要将**数字信息转换为电信号**，才能在网线上传输，也就是说，这才是真正的数据发送过程。\n\n负责执行这一操作的是**网卡**，要控制网卡还需要靠**网卡驱动程序**。\n\n网卡驱动获取网络包之后，会将其**复制**到网卡内的缓存区中，接着会在其**开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列**。\n\n![数据包](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/数据包.drawio.png)\n\n- 起始帧分界符是一个用来表示包起始位置的标记\n- 末尾的 `FCS`（帧校验序列）用来检查包传输过程是否有损坏\n\n最后网卡会将包转为电信号，通过网线发送出去。\n\n\u003e 唉，真是不容易，发一个包，真是历经千辛万苦。致此，一个带有许多头部的数据终于踏上寻找目的地的征途了！\n\n---\n\n## 送别者 —— 交换机\n\n下面来看一下包是如何通过交换机的。交换机的设计是将网络包**原样**转发到目的地。交换机工作在 MAC 层，也称为**二层网络设备**。\n\n\u003e 交换机的包接收操作\n\n首先，电信号到达网线接口，交换机里的模块进行接收，接下来交换机里的模块将电信号转换为数字信号。\n\n然后通过包末尾的 `FCS` 校验错误，如果没问题则放到缓冲区。这部分操作基本和计算机的网卡相同，但交换机的工作方式和网卡不同。\n\n计算机的网卡本身具有 MAC 地址，并通过核对收到的包的接收方 MAC 地址判断是不是发给自己的，如果不是发给自己的则丢弃；相对地，交换机的端口不核对接收方 MAC 地址，而是直接接收所有的包并存放到缓冲区中。因此，和网卡不同，**交换机的端口不具有 MAC 地址**。\n\n将包存入缓冲区后，接下来需要查询一下这个包的接收方 MAC 地址是否已经在 MAC 地址表中有记录了。\n\n交换机的 MAC 地址表主要包含两个信息：\n\n- 一个是设备的 MAC 地址，\n- 另一个是该设备连接在交换机的哪个端口上。\n\n![交换机的 MAC 地址表](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/23.jpg)\n\n\n举个例子，如果收到的包的接收方 MAC 地址为 `00-02-B3-1C-9C-F9`，则与图中表中的第 3 行匹配，根据端口列的信息，可知这个地址位于 `3` 号端口上，然后就可以通过交换电路将包发送到相应的端口了。\n\n所以，**交换机根据 MAC 地址表查找 MAC 地址，然后将信号发送到相应的端口**。\n\n\u003e 当 MAC 地址表找不到指定的 MAC 地址会怎么样？\n\n地址表中找不到指定的 MAC 地址。这可能是因为具有该地址的设备还没有向交换机发送过包，或者这个设备一段时间没有工作导致地址被从地址表中删除了。\n\n这种情况下，交换机无法判断应该把包转发到哪个端口，只能将包转发到除了源端口之外的所有端口上，无论该设备连接在哪个端口上都能收到这个包。\n\n这样做不会产生什么问题，因为以太网的设计本来就是将包发送到整个网络的，然后**只有相应的接收者才接收包，而其他设备则会忽略这个包**。\n\n有人会说：“这样做会发送多余的包，会不会造成网络拥塞呢？”\n\n其实完全不用过于担心，因为发送了包之后目标设备会作出响应，只要返回了响应包，交换机就可以将它的地址写入 MAC 地址表，下次也就不需要把包发到所有端口了。\n\n局域网中每秒可以传输上千个包，多出一两个包并无大碍。\n\n此外，如果接收方 MAC 地址是一个**广播地址**，那么交换机会将包发送到除源端口之外的所有端口。\n\n以下两个属于广播地址：\n\n- MAC 地址中的 `FF:FF:FF:FF:FF:FF`\n- IP 地址中的 `255.255.255.255`\n\n\u003e 数据包通过交换机转发抵达了路由器，准备要离开土生土长的子网了。此时，数据包和交换机离别时说道：“感谢交换机兄弟，帮我转发到出境的大门，我要出远门啦！”\n\n---\n\n## 出境大门 —— 路由器\n\n\u003e 路由器与交换机的区别\n\n网络包经过交换机之后，现在到达了**路由器**，并在此被转发到下一个路由器或目标设备。\n\n这一步转发的工作原理和交换机类似，也是通过查表判断包转发的目标。\n\n不过在具体的操作过程上，路由器和交换机是有区别的。\n\n- 因为**路由器**是基于 IP 设计的，俗称**三层**网络设备，路由器的各个端口都具有 MAC 地址和 IP 地址；\n- 而**交换机**是基于以太网设计的，俗称**二层**网络设备，交换机的端口不具有 MAC 地址。\n\n\u003e 路由器基本原理\n\n路由器的端口具有 MAC 地址，因此它就能够成为以太网的发送方和接收方；同时还具有 IP 地址，从这个意义上来说，它和计算机的网卡是一样的。\n\n当转发包时，首先路由器端口会接收发给自己的以太网包，然后**路由表**查询转发目标，再由相应的端口作为发送方将以太网包发送出去。\n\n\u003e 路由器的包接收操作\n\n首先，电信号到达网线接口部分，路由器中的模块会将电信号转成数字信号，然后通过包末尾的 `FCS` 进行错误校验。\n\n如果没问题则检查 MAC 头部中的**接收方 MAC 地址**，看看是不是发给自己的包，如果是就放到接收缓冲区中，否则就丢弃这个包。\n\n总的来说，路由器的端口都具有 MAC 地址，只接收与自身地址匹配的包，遇到不匹配的包则直接丢弃。\n\n\u003e 查询路由表确定输出端口\n\n完成包接收操作之后，路由器就会**去掉**包开头的 MAC 头部。\n\n**MAC 头部的作用就是将包送达路由器**，其中的接收方 MAC 地址就是路由器端口的 MAC 地址。因此，当包到达路由器之后，MAC 头部的任务就完成了，于是 MAC 头部就会**被丢弃**。\n\n接下来，路由器会根据 MAC 头部后方的 `IP` 头部中的内容进行包的转发操作。\n\n转发操作分为几个阶段，首先是查询**路由表**判断转发目标。\n\n![路由器转发](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/24.jpg)\n\n具体的工作流程根据上图，举个例子。\n\n假设地址为 `10.10.1.101` 的计算机要向地址为 `192.168.1.100` 的服务器发送一个包，这个包先到达图中的路由器。\n\n判断转发目标的第一步，就是根据包的接收方 IP 地址查询路由表中的目标地址栏，以找到相匹配的记录。\n\n路由匹配和前面讲的一样，每个条目的子网掩码和 `192.168.1.100` IP 做 **\u0026 与运算**后，得到的结果与对应条目的目标地址进行匹配，如果匹配就会作为候选转发目标，如果不匹配就继续与下个条目进行路由匹配。\n\n如第二条目的子网掩码 `255.255.255.0` 与 `192.168.1.100` IP 做 **\u0026 与运算**后，得到结果是 `192.168.1.0` ，这与第二条目的目标地址 `192.168.1.0` 匹配，该第二条目记录就会被作为转发目标。\n\n实在找不到匹配路由时，就会选择**默认路由**，路由表中子网掩码为 `0.0.0.0` 的记录表示「默认路由」。\n\n\u003e 路由器的发送操作\n\n接下来就会进入包的**发送操作**。\n\n首先，我们需要根据**路由表的网关列**判断对方的地址。\n\n- 如果网关是一个 IP 地址，则这个IP 地址就是我们要转发到的目标地址，**还未抵达终点**，还需继续需要路由器转发。\n- 如果网关为空，则 IP 头部中的接收方 IP 地址就是要转发到的目标地址，也是就终于找到 IP 包头里的目标地址了，说明**已抵达终点**。\n\n知道对方的 IP 地址之后，接下来需要通过 `ARP` 协议根据 IP 地址查询 MAC 地址，并将查询的结果作为接收方 MAC 地址。\n\n路由器也有 ARP 缓存，因此首先会在 ARP 缓存中查询，如果找不到则发送 ARP 查询请求。\n\n接下来是发送方 MAC 地址字段，这里填写输出端口的 MAC 地址。还有一个以太类型字段，填写 `0800` （十六进制）表示 IP 协议。\n\n网络包完成后，接下来会将其转换成电信号并通过端口发送出去。这一步的工作过程和计算机也是相同的。\n\n发送出去的网络包会通过**交换机**到达下一个路由器。由于接收方 MAC 地址就是下一个路由器的地址，所以交换机会根据这一地址将包传输到下一个路由器。\n\n接下来，下一个路由器会将包转发给再下一个路由器，经过层层转发之后，网络包就到达了最终的目的地。\n\n不知你发现了没有，在网络包传输的过程中，**源 IP 和目标 IP 始终是不会变的，一直变化的是 MAC 地址**，因为需要 MAC 地址在以太网内进行**两个设备**之间的包传输。\n\n\u003e 数据包通过多个路由器道友的帮助，在网络世界途经了很多路程，最终抵达了目的地的城门！城门值守的路由器，发现了这个小兄弟数据包原来是找城内的人，于是它就将数据包送进了城内，再经由城内的交换机帮助下，最终转发到了目的地了。数据包感慨万千的说道：“多谢这一路上，各路大侠的相助！”\n\n---\n\n## 互相扒皮 —— 服务器 与 客户端\n\n数据包抵达了服务器，服务器肯定高兴呀，正所谓有朋自远方来，不亦乐乎？\n\n服务器高兴的不得了，于是开始扒数据包的皮！就好像你收到快递，能不兴奋吗？\n\n![网络分层模型](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E9%94%AE%E5%85%A5%E7%BD%91%E5%9D%80%E8%BF%87%E7%A8%8B/25.jpg)\n\n数据包抵达服务器后，服务器会先扒开数据包的 MAC 头部，查看是否和服务器自己的 MAC 地址符合，符合就将包收起来。\n\n接着继续扒开数据包的 IP 头，发现 IP 地址符合，根据 IP 头中协议项，知道自己上层是 TCP 协议。\n\n于是，扒开 TCP 的头，里面有序列号，需要看一看这个序列包是不是我想要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃。TCP 头部里面还有端口号， HTTP 的服务器正在监听这个端口号。\n\n于是，服务器自然就知道是 HTTP 进程想要这个包，于是就将包发给 HTTP 进程。\n\n服务器的 HTTP 进程看到，原来这个请求是要访问一个页面，于是就把这个网页封装在 HTTP 响应报文里。\n\nHTTP 响应报文也需要穿上 TCP、IP、MAC 头部，不过这次是源地址是服务器 IP 地址，目的地址是客户端 IP 地址。\n\n穿好头部衣服后，从网卡出去，交由交换机转发到出城的路由器，路由器就把响应数据包发到了下一个路由器，就这样跳啊跳。\n\n最后跳到了客户端的城门把守的路由器，路由器扒开 IP 头部发现是要找城内的人，于是又把包发给了城内的交换机，再由交换机转发到客户端。\n\n客户端收到了服务器的响应数据包后，同样也非常的高兴，客户能拆快递了！\n\n于是，客户端开始扒皮，把收到的数据包的皮扒剩 HTTP 响应报文后，交给浏览器去渲染页面，一份特别的数据包快递，就这样显示出来了！\n\n最后，客户端要离开了，向服务器发起了 TCP 四次挥手，至此双方的连接就断开了。\n\n----\n\n## 一个数据包臭不要脸的感受\n\n\u003e 下面内容的 「我」，代表「臭美的数据包角色」。注：（括号的内容）代表我的吐槽，三连呸！\n\n我一开始我虽然孤单、不知所措，但没有停滞不前。我依然满怀信心和勇气开始了征途。（**你当然有勇气，你是应用层数据，后面有底层兄弟当靠山，我呸！**）\n\n我很庆幸遇到了各路神通广大的大佬，有可靠传输的 TCP、有远程定位功能的 IP、有指明下一站位置的 MAC 等（**你当然会遇到，因为都被计算机安排好的，我呸！**）。\n\n这些大佬都给我前面加上了头部，使得我能在交换机和路由器的转发下，抵达到了目的地！（**哎，你也不容易，不吐槽了，放过你！**）\n\n这一路上的经历，让我认识到了网络世界中各路大侠协作的重要性，是他们维护了网络世界的秩序，感谢他们！（**我呸，你应该感谢众多计算机科学家！**）\n\n----\n\n参考资料\n\n[1] 户根勤.网络是怎么连接的.人民邮电出版社.\n\n[2] 刘超.趣谈网络协议.极客时间.\n\n----\n\n## 读者问答\n\n\u003e 读者问：“笔记本的是自带交换机的吗？交换机现在我还不知道是什么”\n\n笔记本不是交换机，交换机通常是2个网口以上。\n\n现在家里的路由器其实有了交换机的功能了。交换机可以简单理解成一个设备，三台电脑网线接到这个设备，这三台电脑就可以互相通信了，交换机嘛，交换数据这么理解就可以。\n\n\u003e 读者问：“如果知道你电脑的 Mac 地址，我可以直接给你发消息吗？”\n\nMac 地址只能是两个设备之间传递时使用的，如果你要从大老远给我发消息，是离不开 IP 的。\n\n\u003e 读者问：“请问公网服务器的 Mac 地址是在什么时机通过什么方式获取到的？我看 ARP 获取 Mac 地址只能获取到内网机器的 Mac 地址吧？”\n\n在发送数据包时，如果目标主机不是本地局域网，填入的 Mac 地址是路由器，也就是把数据包转发给路由器，路由器一直转发下一个路由器，直到转发到目标主机的路由器，发现 IP 地址是自己局域网内的主机，就会 ARP 请求获取目标主机的 Mac 地址，从而转发到这个服务器主机。\n\n转发的过程中，源 IP 地址和目标 IP 地址是不会变的（前提：没有使用 NAT 网络的），源 MAC 地址和目标 MAC 地址是会变化的。\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，别忘记关注我哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/http2":{"title":"http2","content":"# 3.6 HTTP/2 牛逼在哪？ \n\n\n不多 BB 了，直接发车！\n\n**一起来看看 HTTP/2 牛逼在哪？**\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/http2提纲.png)\n\n---\n\n## HTTP/1.1 协议的性能问题\n\n我们得先要了解下 HTTP/1.1 协议存在的性能问题，因为 HTTP/2 协议就是把这些性能问题逐个攻破了。\n\n现在的站点相比以前变化太多了，比如：\n\n- *消息的大小变大了*，从几 KB 大小的消息，到几 MB 大小的消息；\n- *页面资源变多了*，从每个页面不到 10 个的资源，到每页超 100 多个资源；\n- *内容形式变多样了*，从单纯到文本内容，到图片、视频、音频等内容；\n- *实时性要求变高了*，对页面的实时性要求的应用越来越多；\n\n这些变化带来的最大性能问题就是 **HTTP/1.1 的高延迟**，延迟高必然影响的就是用户体验。主要原因如下几个：\n\n- *延迟难以下降*，虽然现在网络的「带宽」相比以前变多了，但是延迟降到一定幅度后，就很难再下降了，说白了就是到达了延迟的下限；\n- *并发连接有限*，谷歌浏览器最大并发连接数是 6 个，而且每一个连接都要经过 TCP 和 TLS 握手耗时，以及 TCP 慢启动过程给流量带来的影响；\n- *队头阻塞问题*，同一连接只能在完成一个 HTTP 事务（请求和响应）后，才能处理下一个事务；\n- *HTTP 头部巨大且重复*，由于 HTTP 协议是无状态的，每一个请求都得携带 HTTP 头部，特别是对于有携带 Cookie 的头部，而 Cookie 的大小通常很大；\n- *不支持服务器推送消息*，因此当客户端需要获取通知时，只能通过定时器不断地拉取消息，这无疑浪费大量了带宽和服务器资源。\n\n为了解决 HTTP/1.1 性能问题，具体的优化手段你可以看这篇文章「[HTTP/1.1 如何优化？](https://xiaolincoding.com/network/2_http/http_optimize.html)」，这里我举例几个常见的优化手段：\n\n- 将多张小图合并成一张大图供浏览器 JavaScript 来切割使用，这样可以将多个请求合并成一个请求，但是带来了新的问题，当某张小图片更新了，那么需要重新请求大图片，浪费了大量的网络带宽；\n- 将图片的二进制数据通过 Base64 编码后，把编码数据嵌入到 HTML 或 CSS 文件中，以此来减少网络请求次数；\n- 将多个体积较小的 JavaScript 文件使用 Webpack 等工具打包成一个体积更大的 JavaScript 文件，以一个请求替代了很多个请求，但是带来的问题，当某个 js 文件变化了，需要重新请求同一个包里的所有 js 文件；\n- 将同一个页面的资源分散到不同域名，提升并发连接上限，因为浏览器通常对同一域名的 HTTP 连接最大只能是 6 个；\n\n\n尽管对 HTTP/1.1 协议的优化手段如此之多，但是效果还是不尽人意，因为这些手段都是对 HTTP/1.1 协议的“外部”做优化，**而一些关键的地方是没办法优化的，比如请求-响应模型、头部巨大且重复、并发连接耗时、服务器不能主动推送等，要改变这些必须重新设计 HTTP 协议，于是 HTTP/2 就出来了！**\n\n\n---\n\n## 兼容 HTTP/1.1 \n\nHTTP/2 出来的目的是为了改善 HTTP 的性能。协议升级有一个很重要的地方，就是要**兼容**老版本的协议，否则新协议推广起来就相当困难，所幸 HTTP/2 做到了兼容 HTTP/1.1。\n\n那么，HTTP/2 是怎么做的呢？\n\n第一点，HTTP/2 没有在 URI 里引入新的协议名，仍然用「http://」表示明文协议，用「https://」表示加密协议，于是只需要浏览器和服务器在背后自动升级协议，这样可以让用户意识不到协议的升级，很好的实现了协议的平滑升级。\n\n第二点，只在应用层做了改变，还是基于 TCP 协议传输，应用层方面为了保持功能上的兼容，HTTP/2 把 HTTP 分解成了「语义」和「语法」两个部分，「语义」层不做改动，与 HTTP/1.1 完全一致，比如请求方法、状态码、头字段等规则保留不变。\n\n但是，HTTP/2 在「语法」层面做了很多改造，基本改变了 HTTP 报文的传输格式。\n\n## 头部压缩\n\nHTTP 协议的报文是由「Header + Body」构成的，对于 Body 部分，HTTP/1.1 协议可以使用头字段 「Content-Encoding」指定 Body 的压缩方式，比如用 gzip 压缩，这样可以节约带宽，但报文中的另外一部分 Header，是没有针对它的优化手段。\n\nHTTP/1.1 报文中 Header 部分存在的问题：\n\n- 含很多固定的字段，比如 Cookie、User Agent、Accept 等，这些字段加起来也高达几百字节甚至上千字节，所以有必要**压缩**；\n- 大量的请求和响应的报文里有很多字段值都是重复的，这样会使得大量带宽被这些冗余的数据占用了，所以有必须要**避免重复性**；\n- 字段是 ASCII 编码的，虽然易于人类观察，但效率低，所以有必要改成**二进制编码**；\n\nHTTP/2 对 Header 部分做了大改造，把以上的问题都解决了。\n\n\nHTTP/2 没使用常见的 gzip 压缩方式来压缩头部，而是开发了 **HPACK** 算法，HPACK 算法主要包含三个组成部分：\n\n- 静态字典；\n- 动态字典；\n- Huffman 编码（压缩算法）；\n\n客户端和服务器两端都会建立和维护「**字典**」，用长度较小的索引号表示重复的字符串，再用 Huffman 编码压缩数据，**可达到 50%~90% 的高压缩率**。\n\n### 静态表编码\n\nHTTP/2 为高频出现在头部的字符串和字段建立了一张**静态表**，它是写入到 HTTP/2 框架里的，不会变化的，静态表里共有 `61` 组，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/静态表.png)\n\n表中的 `Index` 表示索引（Key），`Header Value` 表示索引对应的  Value，`Header Name` 表示字段的名字，比如 Index 为 2 代表 GET，Index 为 8 代表状态码 200。\n\n你可能注意到，表中有的 Index 没有对应的 Header Value，这是因为这些 Value 并不是固定的而是变化的，这些 Value 都会经过 Huffman 编码后，才会发送出去。\n\n这么说有点抽象，我们来看个具体的例子，下面这个 `server` 头部字段，在 HTTP/1.1 的形式如下：\n\n```\nserver: nghttpx\\r\\n\n```\n\n算上冒号空格和末尾的`\\r\\n`，共占用了 17 字节，**而使用了静态表和 Huffman 编码，可以将它压缩成 8 字节，压缩率大概 47%**。\n\n我抓了个 HTTP/2 协议的网络包，你可以从下图看到，高亮部分就是 `server` 头部字段，只用了 8 个字节来表示 `server` 头部数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/静态编码.png)\n\n\n根据 RFC7541 规范，如果头部字段属于静态表范围，并且 Value 是变化，那么它的 HTTP/2 头部前 2 位固定为 `01`，所以整个头部格式如下图：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/静态头部.png)\n\n\nHTTP/2 头部由于基于**二进制编码**，就不需要冒号空格和末尾的\\r\\n作为分隔符，于是改用表示字符串长度（Value Length）来分割 Index 和 Value。\n\n接下来，根据这个头部格式来分析上面抓包的 `server` 头部的二进制数据。\n\n首先，从静态表中能查到 `server` 头部字段的 Index 为 54，二进制为 110110，再加上固定 01，头部格式第 1 个字节就是 `01110110`，这正是上面抓包标注的红色部分的二进制数据。        \n\n然后，第二个字节的首个比特位表示 Value 是否经过 Huffman 编码，剩余的 7 位表示 Value 的长度，比如这次例子的第二个字节为 `10000110`，首位比特位为 1 就代表 Value 字符串是经过 Huffman 编码的，经过 Huffman 编码的 Value 长度为 6。\n\n最后，字符串 `nghttpx` 经过 Huffman 编码后压缩成了 6 个字节，Huffman 编码的原理是将高频出现的信息用「较短」的编码表示，从而缩减字符串长度。\n\n于是，在统计大量的 HTTP 头部后，HTTP/2 根据出现频率将 ASCII 码编码为了 Huffman 编码表，可以在 RFC7541 文档找到这张**静态 Huffman 表**，我就不把表的全部内容列出来了，我只列出字符串 `nghttpx` 中每个字符对应的 Huffman 编码，如下图：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/nghttpx.png)\n\n通过查表后，字符串 `nghttpx` 的 Huffman 编码在下图看到，共 6 个字节，每一个字符的 Huffman 编码，我用相同的颜色将他们对应起来了，最后的 7 位是补位的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/nghttpx2.png)\n\n最终，`server` 头部的二进制数据对应的静态头部格式如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/静态头部2.png)\n\n\n### 动态表编码\n\n静态表只包含了 61 种高频出现在头部的字符串，不在静态表范围内的头部字符串就要自行构建**动态表**，它的 Index 从 `62` 起步，会在编码解码的时候随时更新。\n\n\n比如，第一次发送时头部中的「`User-Agent` 」字段数据有上百个字节，经过 Huffman 编码发送出去后，客户端和服务器双方都会更新自己的动态表，添加一个新的 Index 号 62。**那么在下一次发送的时候，就不用重复发这个字段的数据了，只用发 1 个字节的 Index 号就好了，因为双方都可以根据自己的动态表获取到字段的数据**。\n\n所以，使得动态表生效有一个前提：**必须同一个连接上，重复传输完全相同的 HTTP 头部**。如果消息字段在 1 个连接上只发送了 1 次，或者重复传输时，字段总是略有变化，动态表就无法被充分利用了。\n\n因此，随着在同一 HTTP/2 连接上发送的报文越来越多，客户端和服务器双方的「字典」积累的越来越多，理论上最终每个头部字段都会变成 1 个字节的 Index，这样便避免了大量的冗余数据的传输，大大节约了带宽。\n\n理想很美好，现实很骨感。动态表越大，占用的内存也就越大，如果占用了太多内存，是会影响服务器性能的，因此 Web 服务器都会提供类似 `http2_max_requests` 的配置，用于限制一个连接上能够传输的请求数量，避免动态表无限增大，请求数量到达上限后，就会关闭 HTTP/2 连接来释放内存。\n\n综上，HTTP/2 头部的编码通过「静态表、动态表、Huffman 编码」共同完成的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/头部编码.png)\n\n---\n\n## 二进制帧\n\nHTTP/2 厉害的地方在于将 HTTP/1 的文本格式改成二进制格式传输数据，极大提高了 HTTP 传输效率，而且二进制数据使用位运算能高效解析。\n\n你可以从下图看到，HTTP/1.1 的响应和 HTTP/2 的区别：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/二进制帧.png)\n\nHTTP/2 把响应报文划分成了两类**帧（*Frame*）**，图中的 HEADERS（首部）和 DATA（消息负载） 是帧的类型，也就是说一条 HTTP 响应，划分成了两类帧来传输，并且采用二进制来编码。\n\n比如状态码 200 ，在 HTTP/1.1 是用 '2''0''0' 三个字符来表示（二进制：00110010 00110000 00110000），共用了 3 个字节，如下图\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/http1.png)\n\n在 HTTP/2 对于状态码 200 的二进制编码是 10001000，只用了 1 字节就能表示，相比于 HTTP/1.1 节省了 2 个字节，如下图：\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/h2c.png)\n\nHeader: :status: 200 OK 的编码内容为：1000 1000，那么表达的含义是什么呢？\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/index.png)\n\n1. 最前面的 1 标识该 Header 是静态表中已经存在的 KV。\n2. 我们再回顾一下之前的静态表内容，“:status: 200 OK”其静态表编码是 8，即 1000。\n\n因此，整体加起来就是 1000 1000。\n\nHTTP/2 **二进制帧**的结构如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/帧格式.png)\n\n帧头（Frame Header）很小，只有 9 个字节，帧开头的前 3 个字节表示帧数据（Frame Playload）的**长度**。\n\n帧长度后面的一个字节是表示**帧的类型**，HTTP/2 总共定义了 10 种类型的帧，一般分为**数据帧**和**控制帧**两类，如下表格：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/帧类型.png)\n\n帧类型后面的一个字节是**标志位**，可以保存 8 个标志位，用于携带简单的控制信息，比如：\n\n- **END_HEADERS** 表示头数据结束标志，相当于 HTTP/1 里头后的空行（“\\r\\n”）；\n- **END_Stream** 表示单方向数据发送结束，后续不会再有数据帧。\n- **PRIORITY** 表示流的优先级；\n\n\n帧头的最后 4 个字节是**流标识符**（Stream ID），但最高位被保留不用，只有 31 位可以使用，因此流标识符的最大值是 2^31，大约是 21 亿，它的作用是用来标识该 Frame 属于哪个 Stream，接收方可以根据这个信息从乱序的帧里找到相同 Stream ID 的帧，从而有序组装信息。\n\n最后面就是**帧数据**了，它存放的是通过 **HPACK  算法**压缩过的 HTTP 头部和包体。\n\n---\n\n## 并发传输\n\n知道了 HTTP/2 的帧结构后，我们再来看看它是如何实现**并发传输**的。\n\n我们都知道 HTTP/1.1 的实现是基于请求-响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是无法发送的，也造成了**队头阻塞**的问题。\n\n而 HTTP/2 就很牛逼了，通过 Stream 这个设计，**多个 Stream 复用一条 TCP 连接，达到并发的效果**，解决了 HTTP/1.1 队头阻塞的问题，提高了 HTTP 传输的吞吐量。\n\n为了理解 HTTP/2 的并发是怎样实现的，我们先来理解 HTTP/2 中的 Stream、Message、Frame 这 3 个概念。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/stream.png)\n\n你可以从上图中看到：\n\n- 1 个 TCP 连接包含一个或者多个 Stream，Stream 是 HTTP/2 并发的关键技术；\n- Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成；\n- Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）；\n\n因此，我们可以得出个结论：多个 Stream 跑在一条 TCP 连接，同一个 HTTP 请求与响应是跑在同一个 Stream 中，HTTP 消息可以由多个 Frame 构成， 一个 Frame 可以由多个 TCP 报文构成。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/stream2.png)\n\n在 HTTP/2 连接上，**不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ）**，因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，而**同一 Stream 内部的帧必须是严格有序的**。\n\n比如下图，服务端**并行交错地**发送了两个响应： Stream 1 和 Stream 3，这两个 Stream 都是跑在一个 TCP 连接上，客户端收到后，会根据相同的 Stream ID 有序组装成 HTTP 消息。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/http2多路复用.jpeg)\n\n客户端和服务器**双方都可以建立 Stream**，因为服务端可以主动推送资源给客户端， 客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。\n\n比如下图，Stream 1 是客户端向服务端请求的资源，属于客户端建立的 Stream，所以该 Stream 的 ID 是奇数（数字 1）；Stream 2 和 4 都是服务端主动向客户端推送的资源，属于服务端建立的 Stream，所以这两个 Stream 的 ID 是偶数（数字 2 和 4）。\n\n![](https://img-blog.csdnimg.cn/83445581dafe409d8cfd2c573b2781ac.png)\n\n同一个连接中的 Stream ID 是不能复用的，只能顺序递增，所以当 Stream ID 耗尽时，需要发一个控制帧 `GOAWAY`，用来关闭 TCP 连接。 \n\n在 Nginx 中，可以通过 `http2_max_concurrent_Streams` 配置来设置 Stream 的上限，默认是 128 个。\n\nHTTP/2 通过 Stream 实现的并发，比 HTTP/1.1 通过 TCP 连接实现并发要牛逼的多，**因为当 HTTP/2 实现 100 个并发 Stream 时，只需要建立一次 TCP 连接，而 HTTP/1.1 需要建立 100 个 TCP 连接，每个 TCP 连接都要经过 TCP 握手、慢启动以及 TLS 握手过程，这些都是很耗时的。**\n\nHTTP/2 还可以对每个 Stream 设置不同**优先级**，帧头中的「标志位」可以设置优先级，比如客户端访问 HTML/CSS 和图片资源时，希望服务器先传递 HTML/CSS，再传图片，那么就可以通过设置 Stream 的优先级来实现，以此提高用户体验。\n\n## 服务器主动推送资源\n\nHTTP/1.1 不支持服务器主动推送资源给客户端，都是由客户端向服务器发起请求后，才能获取到服务器响应的资源。\n\n比如，客户端通过 HTTP/1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，如下图左边部分：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/push.png)\n\n如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。\n\n在 Nginx 中，如果你希望客户端访问 /test.html 时，服务器直接推送 /test.css，那么可以这么配置：\n\n\n```nginx\nlocation /test.html { \n  http2_push /test.css; \n}\n```\n\n那 HTTP/2 的推送是怎么实现的？\n\n客户端发起的请求，必须使用的是奇数号 Stream，服务器主动的推送，使用的是偶数号 Stream。服务器在推送资源时，会通过 `PUSH_PROMISE` 帧传输 HTTP 头部，并通过帧中的 `Promised Stream ID` 字段告知客户端，接下来会在哪个偶数号 Stream 中发送包体。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/push2.png)\n\n如上图，在 Stream 1 中通知客户端 CSS 资源即将到来，然后在 Stream 2 中发送 CSS 资源，注意 Stream 1 和 2 是可以**并发**的。\n\n---\n\n## 总结\n\nHTTP/2 协议其实还有很多内容，比如流控制、流状态、依赖关系等等。\n\n这次主要介绍了关于 HTTP/2 是如何提升性能的几个方向，它相比 HTTP/1 大大提高了传输效率、吞吐能力。\n\n第一点，对于常见的 HTTP 头部通过**静态表和 Huffman 编码**的方式，将体积压缩了近一半，而且针对后续的请求头部，还可以建立**动态表**，将体积压缩近 90%，大大提高了编码效率，同时节约了带宽资源。\n\n不过，动态表并非可以无限增大， 因为动态表是会占用内存的，动态表越大，内存也越大，容易影响服务器总体的并发能力，因此服务器需要限制 HTTP/2 连接时长或者请求次数。\n\n第二点，**HTTP/2 实现了 Stream 并发**，多个 Stream 只需复用 1 个 TCP 连接，节约了 TCP 和 TLS 握手时间，以及减少了 TCP 慢启动阶段对流量的影响。不同的 Stream ID 可以并发，即使乱序发送帧也没问题，比如发送 A 请求帧 1 -\u003e B 请求帧 1 -\u003e A 请求帧 2 -\u003e B 请求帧2，但是同一个 Stream 里的帧必须严格有序。\n\n另外，可以根据资源的渲染顺序来设置 Stream 的**优先级**，从而提高用户体验。\n\n第三点，**服务器支持主动推送资源**，大大提升了消息的传输性能，服务器推送资源时，会先发送 PUSH_PROMISE 帧，告诉客户端接下来在哪个 Stream 发送资源，然后用偶数号 Stream 发送资源给客户端。\n\nHTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。\n\n**HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。**\n\n有没有什么解决方案呢？既然是 TCP 协议自身的问题，那干脆放弃 TCP 协议，转而使用 UDP 协议作为传输层协议，这个大胆的决定，HTTP/3 协议做了！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/27-HTTP3.png)\n\n---\n\n参考资料：\n\n1. https://developers.google.com/web/fundamentals/performance/http2\n2. https://http2.akamai.com/demo\n3. https://tools.ietf.org/html/rfc7541\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/http3":{"title":"http3","content":"# 3.7 HTTP/3 强势来袭\n\nHTTP/3 现在（2022 年 5 月）还没正式推出，不过自 2017 年起，HTTP/3 已经更新到 34 个草案了，基本的特性已经确定下来了，对于包格式可能后续会有变化。\n\n所以，这次 HTTP/3 介绍不会涉及到包格式，只说它的特性。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/HTTP3提纲.png)\n\n## 美中不足的 HTTP/2 \n\nHTTP/2 通过头部压缩、二进制编码、多路复用、服务器推送等新特性大幅度提升了 HTTP/1.1 的性能，而美中不足的是 HTTP/2 协议是基于 TCP 实现的，于是存在的缺陷有三个。\n\n- 队头阻塞；\n- TCP 与 TLS 的握手时延迟；\n- 网络迁移需要重新连接；\n\n### 队头阻塞\n\nHTTP/2 多个请求是跑在一个 TCP 连接中的，那么当 TCP 丢包时，整个 TCP 都要等待重传，那么就会阻塞该 TCP 连接中的所有请求。\n\n比如下图中，Stream 2 有一个 TCP 报文丢失了，那么即使收到了 Stream 3 和 Stream 4 的 TCP 报文，应用层也是无法读取的，相当于阻塞了 Stream 3 和 Stream 4 请求。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/http2阻塞.jpeg)\n\n因为 TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，从 HTTP 视角看，就是请求被阻塞了。\n\n举个例子，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/tcp队头阻塞.gif)\n\n图中发送方发送了很多个 Packet，每个 Packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 Packet 3 在网络中丢失了，即使 Packet 4-6 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 Packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。\n\n### TCP 与 TLS 的握手时延迟\n\n发起 HTTP 请求时，需要经过 TCP 三次握手和 TLS 四次握手（TLS 1.2）的过程，因此共需要 3 个 RTT 的时延才能发出请求数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/TCP%2BTLS.gif)\n\n另外，TCP 由于具有「拥塞控制」的特性，所以刚建立连接的 TCP 会有个「慢启动」的过程，它会对 TCP 连接产生“减速”效果。\n\n### 网络迁移需要重新连接\n\n一个 TCP 连接是由四元组（源 IP 地址，源端口，目标 IP 地址，目标端口）确定的，这意味着如果 IP 地址或者端口变动了，就会导致需要 TCP 与 TLS 重新握手，这不利于移动设备切换网络的场景，比如 4G 网络环境切换成 WiFi。\n\n这些问题都是 TCP 协议固有的问题，无论应用层的 HTTP/2 在怎么设计都无法逃脱。要解决这个问题，就必须把**传输层协议替换成 UDP**，这个大胆的决定，HTTP/3 做了！\n\n![HTTP/1 ~ HTTP/3](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/27-HTTP3.png)\n\n## QUIC 协议的特点\n\n我们深知，UDP 是一个简单、不可靠的传输协议，而且是 UDP 包之间是无序的，也没有依赖关系。\n\n而且，UDP 是不需要连接的，也就不需要握手和挥手的过程，所以天然的就比 TCP 快。\n\n当然，HTTP/3 不仅仅只是简单将传输协议替换成了 UDP，还基于 UDP 协议在「应用层」实现了 **QUIC 协议**，它具有类似 TCP 的连接管理、拥塞窗口、流量控制的网络特性，相当于将不可靠传输的 UDP 协议变成“可靠”的了，所以不用担心数据包丢失的问题。\n\nQUIC 协议的优点有很多，这里举例几个，比如：\n\n- 无队头阻塞；\n- 更快的连接建立；\n- 连接迁移；\n\n\n### 无队头阻塞\n\nQUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。\n\n由于 QUIC 使用的传输协议是 UDP，UDP 不关心数据包的顺序，如果数据包丢失，UDP 也不关心。\n\n不过 QUIC 协议会保证数据包的可靠性，每个数据包都有一个序号唯一标识。当某个流中的一个数据包丢失了，即使该流的其他数据包到达了，数据也无法被 HTTP/3 读取，直到 QUIC 重传丢失的报文，数据才会交给 HTTP/3。\n\n而其他流的数据报文只要被完整接收，HTTP/3 就可以读取到数据。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n\n所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/quic无阻塞.jpeg)\n\n### 更快的连接建立\n\n\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、OpenSSL 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。\n\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 **QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS 1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果**。\n\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/0-rtt.gif)\n\n\n### 连接迁移\n\n在前面我们提到，基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n\n![TCP 四元组](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEwLmpwZw?x-oss-process=image/format,png)\n\n那么当移动设备的网络从 4G 切换到 WiFi 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接，而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n\n\n而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过**连接 ID** 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了**连接迁移**的功能。\n\n## HTTP/3 协议\n\n了解完 QUIC 协议的特点后，我们再来看看 HTTP/3 协议在 HTTP 这一层做了什么变化。\n\nHTTP/3 同 HTTP/2 一样采用二进制帧的结构，不同的地方在于 HTTP/2 的二进制帧里需要定义 Stream，而  HTTP/3 自身不需要再定义 Stream，直接使用 QUIC 里的 Stream，于是 HTTP/3 的帧的结构也变简单了。 \n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/http3frame.png)\n\n 从上图可以看到，HTTP/3 帧头只有两个字段：类型和长度。\n\n\n根据帧类型的不同，大体上分为数据帧和控制帧两大类，Headers 帧（HTTP 头部）和 DATA 帧（HTTP 包体）属于数据帧。\n\n\nHTTP/3 在头部压缩算法这一方面也做了升级，升级成了 **QPACK**。与 HTTP/2 中的 HPACK 编码方式相似，HTTP/3 中的 QPACK 也采用了静态表、动态表及 Huffman 编码。\n\n对于静态表的变化，HTTP/2 中的 HPACK 的静态表只有 61 项，而 HTTP/3 中的 QPACK 的静态表扩大到 91 项。\n\nHTTP/2 和 HTTP/3 的 Huffman 编码并没有多大不同，但是动态表编解码方式不同。\n\n所谓的动态表，在首次请求-响应后，双方会将未包含在静态表中的 Header 项更新各自的动态表，接着后续传输时仅用 1 个数字表示，然后对方可以根据这 1 个数字从动态表查到对应的数据，就不必每次都传输长长的数据，大大提升了编码效率。\n\n可以看到，**动态表是具有时序性的，如果首次出现的请求发生了丢包，后续的收到请求，对方就无法解码出 HPACK 头部，因为对方还没建立好动态表，因此后续的请求解码会阻塞到首次请求中丢失的数据包重传过来**。\n\n\nHTTP/3 的 QPACK 解决了这一问题，那它是如何解决的呢？\n\nQUIC 会有两个特殊的单向流，所谓的单向流只有一端可以发送消息，双向则指两端都可以发送消息，传输 HTTP 消息时用的是双向流，这两个单向流的用法：\n\n- 一个叫 QPACK Encoder Stream，用于将一个字典（Key-Value）传递给对方，比如面对不属于静态表的 HTTP 请求头部，客户端可以通过这个 Stream 发送字典；\n- 一个叫 QPACK Decoder Stream，用于响应对方，告诉它刚发的字典已经更新到自己的本地动态表了，后续就可以使用这个字典来编码了。\n\n这两个特殊的单向流是用来**同步双方的动态表**，编码方收到解码方更新确认的通知后，才使用动态表编码 HTTP 头部。\n\n## 总结\n\nHTTP/2 虽然具有多个流并发传输的能力，但是传输层是 TCP 协议，于是存在以下缺陷：\n\n- **队头阻塞**，HTTP/2 多个请求跑在一个 TCP 连接中，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，从 HTTP 视角看，就是多个请求被阻塞了；\n- **TCP 和 TLS 握手时延**，TCP 三次握手和 TLS 四次握手，共有 3-RTT 的时延；\n- **连接迁移需要重新连接**，移动设备从 4G 网络环境切换到 WiFi 时，由于 TCP 是基于四元组来确认一条 TCP 连接的，那么网络环境变化后，就会导致 IP 地址或端口变化，于是 TCP 只能断开连接，然后再重新建立连接，切换网络环境的成本高；\n\nHTTP/3 就将传输层从 TCP 替换成了 UDP，并在 UDP 协议上开发了 QUIC 协议，来保证数据的可靠传输。\n\nQUIC 协议的特点：\n\n- **无队头阻塞**，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，也不会有底层协议限制，某个流发生丢包了，只会影响该流，其他流不受影响；\n- **建立连接速度快**，因为 QUIC 内部包含 TLS 1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与 TLS 密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。\n- **连接迁移**，QUIC 协议没有用四元组的方式来“绑定”连接，而是通过「连接 ID 」来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本；\n\n另外 HTTP/3 的 QPACK 通过两个特殊的单向流来同步双方的动态表，解决了 HTTP/2 的 HPACK 队头阻塞问题。\n\n**期待，HTTP/3 正式推出的那一天！**\n\n---\n\n参考资料：\n\n1. https://medium.com/faun/http-2-spdy-and-http-3-quic-bae7d9a3d484\n2. https://developers.google.com/web/fundamentals/performance/http2?hl=zh-cn\n3. https://blog.cloudflare.com/http3-the-past-present-and-future/\n4. https://tools.ietf.org/html/draft-ietf-quic-http-34\n5. https://tools.ietf.org/html/draft-ietf-quic-transport-34#section-17\n6. https://ably.com/topic/http3?amp%3Butm_campaign=evergreen\u0026amp%3Butm_source=reddit\u0026utm_medium=referral\n7. https://www.nginx.org.cn/article/detail/422\n8. https://www.bilibili.com/read/cv793000/\n9. https://www.chinaz.com/2020/1009/1192436.shtml\n\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/http_interview":{"title":"http_interview","content":"# 3.1 HTTP 常见面试题\n\n在面试过程中，HTTP 被提问的概率还是比较高的。\n\n小林我搜集了 6 大类 HTTP 面试常问的题目，同时这 6 大类题跟 **HTTP 的发展和演变**关联性是比较大的，通过**问答 + 图解**的形式**由浅入深**的方式帮助大家进一步的学习和理解 HTTP。\n\n1. HTTP 基本概念\n2. Get 与 Post\n3. HTTP 特性\n4. HTTP 缓存技术\n5. HTTPS 与 HTTP\n6. HTTP/1.1、HTTP/2、HTTP/3 演变\n\n![提纲](https://img-blog.csdnimg.cn/6b9bfd38d2684b3f9843ebabf8771212.png)\n\n\n## HTTP 基本概念\n\n### HTTP 是什么？\n\nHTTP 是超文本传输协议，也就是**H**yper**T**ext **T**ransfer **P**rotocol。\n\n\u003e 能否详细解释「超文本传输协议」？\n\nHTTP 的名字「超文本协议传输」，它可以拆成三个部分：\n\n- 超文本\n- 传输\n- 协议\n\n![三个部分](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/3-HTTP三部分.png)\n\n*1. 「协议」*\n\n在生活中，我们也能随处可见「协议」，例如：\n\n- 刚毕业时会签一个「三方协议」；\n- 找房子时会签一个「租房协议」；\n\n\n![三方协议和租房协议](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/4-租房和三方协议.png)\n\n生活中的协议，本质上与计算机中的协议是相同的，协议的特点:\n\n- 「**协**」字，代表的意思是必须有**两个以上的参与者**。例如三方协议里的参与者有三个：你、公司、学校三个；租房协议里的参与者有两个：你和房东。\n- 「**议**」字，代表的意思是对参与者的一种**行为约定和规范**。例如三方协议里规定试用期期限、毁约金等；租房协议里规定租期期限、每月租金金额、违约如何处理等。\n\n针对 HTTP **协议**，我们可以这么理解。\n\nHTTP 是一个用在计算机世界里的**协议**。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范（**两个以上的参与者**），以及相关的各种控制和错误处理方式（**行为约定和规范**）。\n\n*2. 「传输」*\n\n所谓的「传输」，很好理解，就是把一堆东西从 A 点搬到 B 点，或者从 B 点 搬到 A 点。\n\n别轻视了这个简单的动作，它至少包含两项重要的信息。\n\nHTTP 协议是一个**双向协议**。\n\n我们在上网冲浪时，浏览器是请求方 A，百度网站就是应答方 B。双方约定用 HTTP 协议来通信，于是浏览器把请求数据发送给网站，网站再把一些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图片、视频了。\n\n![请求 - 应答](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/5-请求应答.png)\n\n数据虽然是在 A 和 B 之间传输，但允许中间有**中转或接力**。\n\n就好像第一排的同学想传递纸条给最后一排的同学，那么传递的过程中就需要经过好多个同学（中间人），这样的传输方式就从「A \u003c --- \u003e B」，变成了「A \u003c-\u003e N \u003c-\u003e M \u003c-\u003e B」。\n\n而在 HTTP 里，需要中间人遵从 HTTP 协议，只要不打扰基本的数据传输，就可以添加任意额外的东西。\n\n针对**传输**，我们可以进一步理解了 HTTP。\n\nHTTP 是一个在计算机世界里专门用来在**两点之间传输数据**的约定和规范。\n\n*3. 「超文本」*\n\nHTTP 传输的内容是「超文本」。\n\n我们先来理解「文本」，在互联网早期的时候只是简单的字符文字，但现在「文本」的涵义已经可以扩展为图片、视频、压缩包等，在 HTTP 眼里这些都算作「文本」。\n\n再来理解「超文本」，它就是**超越了普通文本的文本**，它是文字、图片、视频等的混合体，最关键有超链接，能从一个超文本跳转到另外一个超文本。\n\nHTML 就是最常见的超文本了，它本身只是纯文字文件，但内部用很多标签定义了图片、视频等的链接，再经过浏览器的解释，呈现给我们的就是一个文字、有画面的网页了。\n\nOK，经过了对 HTTP 里这三个名词的详细解释，就可以给出比「超文本传输协议」这七个字更准确更有技术含量的答案：\n\n**HTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。**\n\n\u003e 那「HTTP 是用于从互联网服务器传输超文本到本地浏览器的协议」，这种说法正确吗？\n\n这种说法是**不正确**的。因为也可以是「服务器\u003c -- \u003e服务器」，所以采用**两点之间**的描述会更准确。\n\n### HTTP 常见的状态码有哪些？\n\n![ 五大类 HTTP 状态码 ](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/6-五大类HTTP状态码.png)\n\n`1xx` 类状态码属于**提示信息**，是协议处理中的一种中间状态，实际用到的比较少。\n\n- 「**101 Switching Protocols**」协议切换，服务器已经理解了客户端的请求，并将通过Upgrade消息头通知客户端采用不同的协议来完成这个请求。比如切换到一个实时且同步的协议（如WebSocket）以传送利用此类特性的资源。\n\n`2xx` 类状态码表示服务器**成功**处理了客户端的请求，也是我们最愿意看到的状态。\n\n- 「**200 OK**」是最常见的成功状态码，表示一切正常。如果是非 `HEAD` 请求，服务器返回的响应头都会有 body 数据。\n\n- 「**204 No Content**」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。\n\n- 「**206 Partial Content**」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。\n\n`3xx` 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是**重定向**。 \n\n- 「**301 Moved Permanently**」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。\n\n- 「**302 Found**」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。\n\n301 和 302 都会在响应头里使用字段 `Location`，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。\n\n- 「**304 Not Modified**」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。\n\n`4xx` 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义。\n\n- 「**400 Bad Request**」表示客户端请求的报文有错误，但只是个笼统的错误。\n\n- 「**403 Forbidden**」表示服务器禁止访问资源，并不是客户端的请求出错。\n\n- 「**404 Not Found**」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。\n\n`5xx` 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码。\n\n- 「**500 Internal Server Error**」与 400 类似，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。\n\n- 「**501 Not Implemented**」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。\n\n- 「**502 Bad Gateway**」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。\n\n- 「**503 Service Unavailable**」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思。\n\n### HTTP 常见字段有哪些？\n\n*Host* 字段\n\n客户端发送请求时，用来指定服务器的域名。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/7-HOST字段.png)\n\n```\nHost: www.A.com\n```\n\n有了 `Host` 字段，就可以将请求发往「同一台」服务器上的不同网站。\n\n*Content-Length 字段*\n\n服务器在返回数据时，会有 `Content-Length` 字段，表明本次回应的数据长度。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/8-content-length字段.png)\n\n```\nContent-Length: 1000\n```\n\n如上面则是告诉浏览器，本次服务器回应的数据长度是 1000 个字节，后面的字节就属于下一个回应了。\n\n大家应该都知道 HTTP 是基于 TCP 传输协议进行通信的，而使用了 TCP 传输协议，就会存在一个“粘包”的问题，**HTTP 协议通过设置回车符、换行符作为 HTTP header 的边界，通过 Content-Length 字段作为 HTTP body 的边界，这两个方式都是为了解决“粘包”的问题**。具体什么是 TCP 粘包，可以看这篇文章：[如何理解是 TCP 面向字节流协议？](https://xiaolincoding.com/network/3_tcp/tcp_stream.html)\n\n*Connection 字段*\n\n`Connection` 字段最常用于客户端要求服务器使用「HTTP 长连接」机制，以便其他请求复用。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/9-connection字段.png)\n\nHTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n\n![HTTP 长连接](https://img-blog.csdnimg.cn/img_convert/d2b20d1cc03936332adb2a68512eb167.png)\n\nHTTP/1.1 版本的默认连接都是长连接，但为了兼容老版本的 HTTP，需要指定 `Connection` 首部字段的值为 `Keep-Alive`。\n\n```\nConnection: Keep-Alive\n```\n\n开启了 HTTP Keep-Alive 机制后， 连接就不会中断，而是保持连接。当客户端发送另一个请求时，它会使用同一个连接，一直持续到客户端或服务器端提出断开连接。\n\nPS：大家不要把 HTTP  Keep-Alive 和 TCP Keepalive 搞混了，这两个虽然长的像，但是不是一个东西，具体可以看我这篇文章：[TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？](https://xiaolincoding.com/network/3_tcp/tcp_http_keepalive.html)\n\n*Content-Type 字段*\n\n`Content-Type` 字段用于服务器回应时，告诉客户端，本次数据是什么格式。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/10-content-type字段.png)\n\n```\nContent-Type: text/html; Charset=utf-8\n```\n\n上面的类型表明，发送的是网页，而且编码是UTF-8。\n\n客户端请求的时候，可以使用 `Accept` 字段声明自己可以接受哪些数据格式。\n\n```\nAccept: */*\n```\n\n上面代码中，客户端声明自己可以接受任何格式的数据。\n\n*Content-Encoding 字段*\n\n`Content-Encoding` 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/11-content-encoding字段.png)\n\n\n```\nContent-Encoding: gzip\n```\n\n上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。\n\n客户端在请求时，用 `Accept-Encoding` 字段说明自己可以接受哪些压缩方法。\n\n```\nAccept-Encoding: gzip, deflate\n```\n\n---\n\n## GET 与 POST\n\n### GET 和 POST 有什么区别？\n\n根据 RFC 规范，**GET 的语义是从服务器获取指定的资源**，这个资源可以是静态的文本、页面、图片视频等。GET 请求的参数位置一般是写在 URL 中，URL 规定只能支持 ASCII，所以 GET 请求的参数只允许 ASCII 字符 ，而且浏览器会对 URL 的长度有限制（HTTP协议本身对 URL长度并没有做任何规定）。\n\n比如，你打开我的文章，浏览器就会发送 GET 请求给服务器，服务器就会返回文章的所有文字及资源。\n\n![GET 请求](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/12-Get请求.png)\n\n根据 RFC 规范，**POST 的语义是根据请求负荷（报文body）对指定的资源做出处理**，具体的处理方式视资源类型而不同。POST 请求携带数据的位置一般是写在报文 body 中，body 中的数据可以是任意格式的数据，只要客户端与服务端协商好即可，而且浏览器不会对 body 大小做限制。\n\n比如，你在我文章底部，敲入了留言后点击「提交」（**暗示你们留言**），浏览器就会执行一次 POST 请求，把你的留言文字放进了报文 body 里，然后拼接好 POST 请求头，通过 TCP 协议发送给服务器。\n\n![POST 请求](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/13-Post请求.png)\n\n### GET 和 POST 方法都是安全和幂等的吗？\n\n先说明下安全和幂等的概念：\n\n- 在 HTTP 协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。\n- 所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。\n\n如果从 RFC 规范定义的语义来看： \n\n- **GET 方法就是安全且幂等的**，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，**可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），而且在浏览器中 GET 请求可以保存为书签**。\n- **POST** 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是**不安全**的，且多次提交数据就会创建多个资源，所以**不是幂等**的。所以，**浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签**。\n\n做个简要的小结。\n\nGET 的语义是请求获取指定的资源。GET 方法是安全、幂等、可被缓存的。\n\nPOST 的语义是根据请求负荷（报文主体）对指定的资源做出处理，具体的处理方式视资源类型而不同。POST 不安全，不幂等，（大部分实现）不可缓存。\n\n注意， 上面是从 RFC 规范定义的语义来分析的。\n\n但是实际过程中，开发者不一定会按照 RFC 规范定义的语义来实现 GET 和 POST 方法。比如：\n\n- 可以用 GET 方法实现新增或删除数据的请求，这样实现的 GET 方法自然就不是安全和幂等。\n- 可以用 POST 方法实现查询数据的请求，这样实现的  POST 方法自然就是安全和幂等。\n\n曾经有个笑话，有人写了个博客，删除博客用的是 GET 请求，他觉得没人访问就连鉴权都没做。然后 Google 服务器爬虫爬了一遍，他所有博文就没了。。。\n\n如果「安全」放入概念是指信息是否会被泄漏的话，虽然 POST 用 body 传输数据，而 GET 用 URL 传输，这样数据会在浏览器地址拦容易看到，但是并不能说 GET 不如 POST 安全的。\n\n因为 HTTP 传输的内容都是明文的，虽然在浏览器地址拦看不到 POST 提交的 body 数据，但是只要抓个包就都能看到了。\n\n所以，要避免传输过程中数据被窃取，就要使用 HTTPS 协议，这样所有 HTTP 的数据都会被加密传输。\n\n\u003e GET 请求可以带 body 吗？\n\nRFC 规范并没有规定 GET 请求不能带 body 的。理论上，任何请求都可以带 body 的。只是因为 RFC 规范定义的 GET 请求是获取资源，所以根据这个语义不需要用到 body。\n\n另外，URL 中的查询参数也不是 GET 所独有的，POST 请求的 URL 中也可以有参数的。\n\n## HTTP 缓存技术\n\n### HTTP 缓存有哪些实现方式？\n\n对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都**缓存在本地**，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升。\n\n所以，避免发送 HTTP 请求的方法就是通过**缓存技术**，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的头部有不少是针对缓存的字段。\n\nHTTP 缓存有两种实现方式，分别是**强制缓存和协商缓存**。\n\n### 什么是强制缓存？\n\n强缓存指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。\n\n如下图中，返回的是 200 状态码，但在 size 项中标识的是 from disk cache，就是使用了强制缓存。\n\n![](https://img-blog.csdnimg.cn/1cb6bc37597e4af8adfef412bfc57a42.png)\n\n强缓存是利用下面这两个 HTTP 响应头部（Response Header）字段实现的，它们都用来表示资源在客户端缓存的有效期：\n\n- `Cache-Control`， 是一个相对时间；\n- `Expires`，是一个绝对时间；\n\n如果 HTTP 响应头部同时有 Cache-Control 和 Expires 字段的话，**Cache-Control 的优先级高于 Expires** 。\n\nCache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：\n\n- 当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；\n- 浏览器再次请求访问服务器中的该资源时，会先**通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期**，如果没有，则使用该缓存，否则重新请求服务器；\n- 服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。\n\n### 什么是协商缓存？\n\n当我们在浏览器使用开发者工具的时候，你可能会看到过某些请求的响应码是 `304`，这个是告诉浏览器可以使用本地缓存的资源，通常这种通过服务端告知客户端是否可以使用缓存的方式被称为协商缓存。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http1.1%E4%BC%98%E5%8C%96/%E7%BC%93%E5%AD%98etag.png)\n\n上图就是一个协商缓存的过程，所以**协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存**。\n\n协商缓存可以基于两种头部来实现。\n\n第一种：请求头部中的 `If-Modified-Since` 字段与响应头部中的 `Last-Modified` 字段实现，这两个字段的意思是：\n\n- 响应头部中的 `Last-Modified`：标示这个响应资源的最后修改时间；\n- 请求头部中的 `If-Modified-Since`：当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上 Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 HTTP 304 走缓存。\n\n\n第二种：请求头部中的 `If-None-Match` 字段与响应头部中的 `ETag` 字段，这两个字段的意思是：\n  - 响应头部中 `Etag`：唯一标识响应资源；\n  - 请求头部中的 `If-None-Match`：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。\n\n第一种实现方式是基于时间实现的，第二种实现方式是基于一个唯一标识实现的，相对来说后者可以更加准确地判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。\n\n如果在第一次请求资源的时候，服务端返回的 HTTP 响应头部同时有 Etag 和 Last-Modified 字段，那么客户端再下一次请求的时候，如果带上了 ETag 和 Last-Modified 字段信息给服务端，**这时 Etag 的优先级更高**，也就是服务端先会判断 Etag 是否变化了，如果 Etag 有变化就不用在判断 Last-Modified 了，如果 Etag 没有变化，然后再看  Last-Modified。\n\n**为什么 ETag 的优先级更高？** 这是因为 ETag 主要能解决 Last-Modified 几个比较难以解决的问题：\n\n1. 在没有修改文件内容情况下文件的最后修改时间可能也会改变，这会导致客户端认为这文件被改动了，从而重新请求；\n2. 可能有些文件是在秒级以内修改的，`If-Modified-Since` 能检查到的粒度是秒级的，使用 Etag就能够保证这种需求下客户端在 1 秒内能刷新多次；\n3. 有些服务器不能精确获取文件的最后修改时间。\n\n注意，**协商缓存这两个字段都需要配合强制缓存中 Cache-Control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求**。\n\n下图是强制缓存和协商缓存的工作流程：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/http缓存.png)\n\n当使用 ETag 字段实现的协商缓存的过程：\n\n- 当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 ETag 唯一标识，这个唯一标识的值是根据当前请求的资源生成的；\n- 当浏览器再次请求访问服务器中的该资源时，首先会先检查强制缓存是否过期：\n  - 如果没有过期，则直接使用本地缓存；\n  - 如果缓存过期了，会在 Request 头部加上 If-None-Match 字段，该字段的值就是 ETag 唯一标识；\n- 服务器再次收到请求后，**会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较**：\n  - **如果值相等，则返回 304 Not Modified，不会返回资源**；\n  - 如果不相等，则返回 200 状态码和返回资源，并在 Response 头部加上新的 ETag 唯一标识；\n- 如果浏览器收到 304 的请求响应状态码，则会从本地缓存中加载资源，否则更新资源。\n\n## HTTP 特性\n\n到目前为止，HTTP 常见到版本有 HTTP/1.1，HTTP/2.0，HTTP/3.0，不同版本的 HTTP 特性是不一样的。\n\n这里先用  HTTP/1.1 版本给大家介绍，其他版本的后续也会介绍。\n\n###  HTTP/1.1 的优点有哪些？\n\nHTTP 最突出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。\n\n*1. 简单*\n\nHTTP 基本的报文格式就是 `header + body`，头部信息也是 `key-value` 简单文本的形式，**易于理解**，降低了学习和使用的门槛。\n\n*2. 灵活和易于扩展*\n\nHTTP 协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员**自定义和扩充**。\n\n同时 HTTP 由于是工作在应用层（ `OSI` 第七层），则它**下层可以随意变化**，比如：\n\n- HTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层；\n- HTTP/1.1 和 HTTP/2.0 传输协议使用的是 TCP 协议，而到了 HTTP/3.0 传输协议改用了 UDP 协议。\n\n*3. 应用广泛和跨平台*\n\n互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用遍地开花，同时天然具有**跨平台**的优越性。\n\n### HTTP/1.1 的缺点有哪些？\n\nHTTP 协议里有优缺点一体的**双刃剑**，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。\n\n*1. 无状态双刃剑*\n\n无状态的**好处**，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。\n\n无状态的**坏处**，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。\n\n例如登录-\u003e添加购物车-\u003e下单-\u003e结算-\u003e支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有关联的，每次都要问一遍身份信息。\n\n这样每操作一次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是**酸爽**！\n\n对于无状态的问题，解法方案有很多种，其中比较简单的方式用 **Cookie** 技术。\n\n`Cookie` 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。\n\n相当于，**在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，带上「小贴纸」，服务器就能认得了了**，\n\n\n![Cookie 技术](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/14-cookie技术.png)\n\n*2. 明文传输双刃剑*\n\n明文意味着在传输过程中的信息，是可方便阅读的，比如 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。\n\n但是这正是这样，HTTP 的所有信息都暴露在了光天化日下，相当于**信息裸奔**。在传输的漫长的过程中，信息的内容都毫无隐私可言，很容易就能被窃取，如果里面有你的账号密码信息，那**你号没了**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/15-你号没了.png)\n\n*3. 不安全*\n\nHTTP 比较严重的缺点就是不安全：\n\n- 通信使用明文（不加密），内容可能会被窃听。比如，**账号信息容易泄漏，那你号没了。**\n- 不验证通信方的身份，因此有可能遭遇伪装。比如，**访问假的淘宝、拼多多，那你钱没了。**\n- 无法证明报文的完整性，所以有可能已遭篡改。比如，**网页上植入垃圾广告，视觉污染，眼没了。**\n\nHTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。\n\n###  HTTP/1.1 的性能如何？\n\nHTTP 协议是基于 **TCP/IP**，并且使用了「**请求 - 应答**」的通信模式，所以性能的关键就在这**两点**里。\n\n*1. 长连接*\n\n早期 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接（三次握手），而且是串行请求，做了无谓的 TCP 连接建立和断开，增加了通信开销。\n\n为了解决上述 TCP 连接问题，HTTP/1.1 提出了**长连接**的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。\n\n持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n\n![短连接与长连接](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/16-短连接与长连接.png)\n\n当然，如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开这个连接。\n\n*2. 管道网络传输*\n\nHTTP/1.1 采用了长连接的方式，这使得管道（pipeline）网络传输成为了可能。\n\n即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以**减少整体的响应时间。**\n\n举例来说，客户端需要请求两个资源。以前的做法是，在同一个 TCP 连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。那么，管道机制则是允许浏览器同时发出 A 请求和 B 请求，如下图：\n\n![管道网络传输](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/17-管道网络传输.png)\n\n但是**服务器必须按照接收请求的顺序发送对这些管道化请求的响应**。\n\n如果服务端在处理 A 请求时耗时比较长，那么后续的请求的处理都会被阻塞住，这称为「队头堵塞」。\n\n所以，**HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞**。\n\n::: tip\n\n**注意!!!**\n\n实际上 HTTP/1.1 管道化技术不是默认开启，而且浏览器基本都没有支持，所以**后面所有文章讨论 HTTP/1.1 都是建立在没有使用管道化的前提**。大家知道有这个功能，但是没有被使用就行了。\n\n:::\n\n*3. 队头阻塞* \n\n「请求 - 应答」的模式加剧了 HTTP 的性能问题。\n\n因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「**队头阻塞**」，好比上班的路上塞车。\n\n![队头阻塞](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/18-队头阻塞.png)\n\n总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。\n\n## HTTP 与 HTTPS\n\n### HTTP 与 HTTPS 有哪些区别？\n\n- HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。\n\n- HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。\n\n- 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。\n\n- HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。\n\n### HTTPS 解决了 HTTP 的哪些问题？\n\nHTTP 由于是明文传输，所以安全上存在以下三个风险：\n\n- **窃听风险**，比如通信链路上可以获取通信内容，用户号容易没。\n- **篡改风险**，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。\n- **冒充风险**，比如冒充淘宝网站，用户钱容易没。\n\n![HTTP 与 HTTPS 网络层](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/19-HTTPS与HTTP.png)\n\nHTTP**S** 在 HTTP 与 TCP 层之间加入了 `SSL/TLS` 协议，可以很好的解决了上述的风险：\n\n- **信息加密**：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。\n- **校验机制**：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。\n- **身份证书**：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。\n\n可见，只要自身不做「恶」，SSL/TLS 协议是能保证通信是安全的。\n\n\u003e HTTPS 是如何解决上面的三个风险的？\n\n- **混合加密**的方式实现信息的**机密性**，解决了窃听的风险。\n- **摘要算法**的方式来实现**完整性**，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。\n- 将服务器公钥放入到**数字证书**中，解决了冒充的风险。\n\n\n*1. 混合加密*\n\n通过**混合加密**的方式可以保证信息的**机密性**，解决了窃听的风险。\n\n![混合加密](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/20-混合加密.png)\n\nHTTPS 采用的是**对称加密**和**非对称加密**结合的「混合加密」方式：\n\n- 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。\n- 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。\n\n采用「混合加密」的方式的原因：\n\n- **对称加密**只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。\n- **非对称加密**使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。\n\n*2. 摘要算法 + 数字签名*\n\n为了保证传输的内容不被篡改，我们需要对内容计算出一个「指纹」，然后同内容一起传输给对方。\n\n对方收到后，先是对内容也计算出一个「指纹」，然后跟发送方发送的「指纹」做一个比较，如果「指纹」相同，说明内容没有被篡改，否则就可以判断出内容被篡改了。\n\n那么，在计算机里会**用摘要算法（哈希函数）来计算出内容的哈希值**，也就是内容的「指纹」，这个**哈希值是唯一的，且无法通过哈希值推导出内容**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/摘要算法.png)\n\n通过哈希算法可以确保内容不会被篡改，**但是并不能保证「内容 + 哈希值」不会被中间人替换，因为这里缺少对客户端收到的消息是否来源于服务端的证明**。\n\n举个例子，你想向老师请假，一般来说是要求由家长写一份请假理由并签名，老师才能允许你请假。\n\n但是你有模仿你爸爸字迹的能力，你用你爸爸的字迹写了一份请假理由然后签上你爸爸的名字，老师一看到这个请假条，查看字迹和签名，就误以为是你爸爸写的，就会允许你请假。\n\n那作为老师，要如何避免这种情况发生呢？现实生活中的，可以通过电话或视频来确认是否是由父母发出的请假，但是计算机里可没有这种操作。\n\n那为了避免这种情况，计算机里会用**非对称加密算法**来解决，共有两个密钥：\n\n- 一个是公钥，这个是可以公开给所有人的；\n- 一个是私钥，这个必须由本人管理，不可泄露。\n\n这两个密钥可以**双向加解密**的，比如可以用公钥加密内容，然后用私钥解密，也可以用私钥加密内容，公钥解密内容。\n\n流程的不同，意味着目的也不相同：\n\n- **公钥加密，私钥解密**。这个目的是为了**保证内容传输的安全**，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容；\n- **私钥加密，公钥解密**。这个目的是为了**保证消息不会被冒充**，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。\n\n一般我们不会用非对称加密来加密实际的传输内容，因为非对称加密的计算比较耗费性能的。\n\n所以非对称加密的用途主要在于**通过「私钥加密，公钥解密」的方式，来确认消息的身份**，我们常说的**数字签名算法**，就是用的是这种方式，不过私钥加密内容不是内容本身，而是**对内容的哈希值加密**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/数字签名.png)\n\n私钥是由服务端保管，然后服务端会向客户端颁发对应的公钥。如果客户端收到的信息，能被公钥解密，就说明该消息是由服务器发送的。\n\n引入了数字签名算法后，你就无法模仿你爸爸的字迹来请假了，你爸爸手上持有着私钥，你老师持有着公钥。\n\n这样只有用你爸爸手上的私钥才对请假条进行「签名」，老师通过公钥看能不能解出这个「签名」，如果能解出并且确认内容的完整性，就能证明是由你爸爸发起的请假条，这样老师才允许你请假，否则老师就不认。\n\n*3. 数字证书*\n\n前面我们知道：\n\n- 可以通过哈希算法来保证消息的完整性；\n- 可以通过数字签名来保证消息的来源可靠性（能确认消息是由持有私钥的一方发送的）；\n\n但是这还远远不够，**还缺少身份验证的环节**，万一公钥是被伪造的呢？\n\n还是拿请假的例子，虽然你爸爸持有私钥，老师通过是否能用公钥解密来确认这个请假条是不是来源你父亲的。\n\n但是我们还可以自己伪造出一对公私钥啊！\n\n你找了个夜晚，偷偷把老师桌面上和你爸爸配对的公钥，换成了你的公钥，那么下次你在请假的时候，你继续模仿你爸爸的字迹写了个请假条，然后用你的私钥做个了「数字签名」。\n\n但是老师并不知道自己的公钥被你替换过了，所以他还是按照往常一样用公钥解密，由于这个公钥和你的私钥是配对的，老师当然能用这个被替换的公钥解密出来，并且确认了内容的完整性，于是老师就会以为是你父亲写的请假条，又允许你请假了。\n\n好家伙，为了一个请假，真的是斗智斗勇。\n\n后面你的老师和父亲发现了你伪造公私钥的事情后，决定重新商量一个对策来应对你这个臭家伙。\n\n正所谓魔高一丈，道高一尺。\n\n既然伪造公私钥那么随意，所以你爸把他的公钥注册到**警察局**，警察局用他们自己的私钥对你父亲的公钥做了个数字签名，然后把你爸爸的「个人信息 + 公钥 + 数字签名」打包成一个**数字证书，也就是说这个数字证书包含你爸爸的公钥。**\n\n这样，你爸爸如果因为家里确实有事要向老师帮你请假的时候，不仅会用自己的私钥对内容进行签名，还会把数字证书给到老师。\n\n老师拿到了数字证书后，**首先会去警察局验证这个数字证书是否合法**，因为数字证书里有警察局的数字签名，警察局要验证证书合法性的时候，用自己的公钥解密，如果能解密成功，就说明这个数字证书是在警察局注册过的，就认为该数字证书是合法的，然后就会把数字证书里头的公钥（你爸爸的）给到老师。\n\n**由于通过警察局验证了数字证书是合法的，那么就能证明这个公钥就是你父亲的**，于是老师就可以安心的用这个公钥解密出清教条，如果能解密出，就证明是你爸爸写的请假条。\n\n正是通过了一个权威的机构来证明你爸爸的身份，所以你的伪造公私钥这个小伎俩就没用了。\n\n在计算机里，这个权威的机构就是 CA （数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。\n\n数字证书的工作流程，我也画了一张图，方便大家理解：\n\n![数子证书工作流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/22-数字证书工作流程.png)\n\n通过数字证书的方式保证服务器公钥的身份，解决冒充的风险。\n\n### HTTPS  是如何建立连接的？其间交互了什么？\n\nSSL/TLS 协议基本流程：\n\n- 客户端向服务器索要并验证服务器的公钥。\n- 双方协商生产「会话秘钥」。\n- 双方采用「会话秘钥」进行加密通信。\n\n前两步也就是 SSL/TLS 的建立过程，也就是 TLS 握手阶段。\n\nTLS 的「握手阶段」涉及**四次**通信，使用不同的密钥交换算法，TLS 握手流程也会不一样的，现在常用的密钥交换算法有两种：[RSA 算法](https://xiaolincoding.com/network/2_http/https_rsa.html) 和 [ECDHE 算法](https://xiaolincoding.com/network/2_http/https_ecdhe.html)。\n\n基于 RSA 算法的 TLS 握手过程比较容易理解，所以这里先用这个给大家展示 TLS 握手过程，如下图：\n\n![HTTPS 连接建立过程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/23-HTTPS工作流程.png)\n\n\nTLS 协议建立的详细流程：\n\n*1. ClientHello*\n\n首先，由客户端向服务器发起加密通信请求，也就是 `ClientHello` 请求。\n\n在这一步，客户端主要向服务器发送以下信息：\n\n（1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。\n\n（2）客户端生产的随机数（`Client Random`），后面用于生成「会话秘钥」条件之一。\n\n（3）客户端支持的密码套件列表，如 RSA 加密算法。\n\n*2. SeverHello*\n\n服务器收到客户端请求后，向客户端发出响应，也就是 `ServerHello`。服务器回应的内容有如下内容：\n\n（1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。\n\n（2）服务器生产的随机数（`Server Random`），也是后面用于生产「会话秘钥」条件之一。\n\n（3）确认的密码套件列表，如 RSA 加密算法。\n\n（4）服务器的数字证书。\n\n*3.客户端回应*\n\n客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。\n\n如果证书没有问题，客户端会**从数字证书中取出服务器的公钥**，然后使用它加密报文，向服务器发送如下信息：\n\n（1）一个随机数（`pre-master key`）。该随机数会被服务器公钥加密。\n\n（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n\n（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。\n\n上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。\n\n**服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」**。\n\n*4. 服务器的最后回应*\n\n服务器收到客户端的第三个随机数（`pre-master key`）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。\n\n然后，向客户端发送最后的信息：\n\n（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n\n（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。\n\n至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。\n\n::: tip\n\n如果想深入学习基于 RSA 算法的 HTTPS 握手过程，可以看这篇，我通过抓包的方式，逐步分析每一个过程：[HTTPS RSA 握手解析](https://xiaolincoding.com/network/2_http/https_rsa.html)\n\n不过，基于 RSA 算法的 HTTPS 存在「前向安全」的问题：如果服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解。\n\n为了解决这个问题，后面就出现了 ECDHE 密钥协商算法，我们现在大多数网站使用的正是 ECDHE 密钥协商算法，关于 ECDHE 握手的过程可以看这篇文章：[HTTPS ECDHE 握手解析](https://xiaolincoding.com/network/2_http/https_ecdhe.html#%E7%A6%BB%E6%95%A3%E5%AF%B9%E6%95%B0)\n\n:::\n\n\u003e 客户端校验数字证书的流程是怎样的？\n\n接下来，详细说一下实际中数字证书签发和验证流程。\n\n如下图图所示，为数字证书签发和验证流程：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E8%AF%81%E4%B9%A6%E7%9A%84%E6%A0%A1%E9%AA%8C.png)\n\nCA 签发证书的过程，如上图左边部分：\n\n- 首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值；\n- 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；\n- 最后将 Certificate Signature 添加在文件证书上，形成数字证书；\n\n客户端校验服务端的数字证书的过程，如上图右边部分：\n\n- 首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1；\n- 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ；\n- 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。\n\n但事实上，证书的验证过程中**还存在一个证书信任链的问题**，因为我们向 CA 申请的证书一般不是根证书签发的，而是由中间证书签发的，比如百度的证书，从下图你可以看到，证书的层级有三级：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/baidu%E8%AF%81%E4%B9%A6.png)\n\n对于这种三级层级关系的证书的验证过程如下：\n\n- 客户端收到 baidu.com 的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公钥去验证 baidu.com 证书是否可信。于是，客户端根据 baidu.com 证书中的签发者，找到该证书的颁发机构是 “GlobalSign Organization Validation CA - SHA256 - G2”，然后向 CA 请求该中间证书。\n- 请求到证书后发现 “GlobalSign Organization Validation CA - SHA256 - G2” 证书是由 “GlobalSign Root CA” 签发的，由于 “GlobalSign Root CA” 没有再上级签发机构，说明它是根证书，也就是自签证书。应用软件会检查此证书有否已预载于根证书清单上，如果有，则可以利用根证书中的公钥去验证 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，如果发现验证通过，就认为该中间证书是可信的。\n- “GlobalSign Organization Validation CA - SHA256 - G2” 证书被信任后，可以使用 “GlobalSign Organization Validation CA - SHA256 - G2” 证书中的公钥去验证 baidu.com 证书的可信性，如果验证通过，就可以信任 baidu.com 证书。\n\n在这四个步骤中，最开始客户端只信任根证书 GlobalSign Root CA 证书的，然后 “GlobalSign Root CA” 证书信任 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，而 “GlobalSign Organization Validation CA - SHA256 - G2” 证书又信任 baidu.com 证书，于是客户端也信任 baidu.com 证书。\n\n总括来说，由于用户信任 GlobalSign，所以由 GlobalSign 所担保的 baidu.com 可以被信任，另外由于用户信任操作系统或浏览器的软件商，所以由软件商预载了根证书的 GlobalSign 都可被信任。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E7%94%A8%E6%88%B7%E4%BF%A1%E4%BB%BB.png)\n\n操作系统里一般都会内置一些根证书，比如我的 MAC 电脑里内置的根证书有这么多：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E7%B3%BB%E7%BB%9F%E6%A0%B9%E8%AF%81%E4%B9%A6.png)\n\n这样的一层层地验证就构成了一条信任链路，整个证书信任链验证流程如下图所示：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/https/%E8%AF%81%E4%B9%A6%E9%93%BE.png)\n\n最后一个问题，为什么需要证书链这么麻烦的流程？Root CA 为什么不直接颁发证书，而是要搞那么多中间层级呢？\n\n**这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。**\n\n### HTTPS 的应用数据是如何保证完整性的？\n\nTLS 在实现上分为**握手协议**和**记录协议**两层：\n\n- TLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）；\n- TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议；\n\nTLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/记录协议.png)\n\n具体过程如下：\n\n- 首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。\n\n- 接下来，经过压缩的片段会被**加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证**。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。\n\n- 再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。\n\n- 最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。\n\n记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。\n\n如果你想详细了解记录协议是如何分片、压缩、计算 MAC 值、分组加密，可以看这篇：[理解 SSL/TLS 系列 (四) 记录协议](https://blog.csdn.net/zhanyiwp/article/details/105627799)\n\n### HTTPS 一定安全可靠吗？\n\n之前有读者在字节面试的时候，被问到：**HTTPS 一定安全可靠吗？**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/提问.jpeg)\n\n这个问题的场景是这样的：客户端通过浏览器向服务端发起 HTTPS 请求时，被「假基站」转发到了一个「中间人服务器」，于是客户端是和「中间人服务器」完成了 TLS 握手，然后这个「中间人服务器」再与真正的服务端完成 TLS 握手。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/https中间人.drawio.png)\n\n具体过程如下：\n\n- 客户端向服务端发起 HTTPS 建立连接请求时，然后被「假基站」转发到了一个「中间人服务器」，接着中间人向服务端发起 HTTPS 建立连接请求，此时客户端与中间人进行 TLS 握手，中间人与服务端进行 TLS 握手；\n- 在客户端与中间人进行 TLS 握手过程中，中间人会发送自己的公钥证书给客户端，**客户端验证证书的真伪**，然后从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给中间人，中间人使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（A），后续客户端与中间人通信就用这个对称加密密钥来加密数据了。\n- 在中间人与服务端进行 TLS 握手过程中，服务端会发送从 CA 机构签发的公钥证书给中间人，从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给服务端，服务端使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（B），后续中间人与服务端通信就用这个对称加密密钥来加密数据了。\n- 后续的通信过程中，中间人用对称加密密钥（A）解密客户端的 HTTPS 请求的数据，然后用对称加密密钥（B）加密 HTTPS 请求后，转发给服务端，接着服务端发送 HTTPS 响应数据给中间人，中间人用对称加密密钥（B）解密 HTTPS 响应数据，然后再用对称加密密钥（A）加密后，转发给客户端。\n\n从客户端的角度看，其实并不知道网络中存在中间人服务器这个角色。那么中间人就可以解开浏览器发起的 HTTPS 请求里的数据，也可以解开服务端响应给浏览器的 HTTPS 响应数据。相当于，中间人能够 “偷看” 浏览器与服务端之间的 HTTPS 请求和响应的数据。\n\n但是要发生这种场景是有前提的，前提是用户点击接受了中间人服务器的证书。\n\n中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/证书安全提示.png)\n\n如果用户执意点击「继续浏览此网站」，相当于用户接受了中间人伪造的证书，那么后续整个 HTTPS 通信都能被中间人监听了。\n\n所以，这其实并不能说 HTTPS 不够安全，毕竟浏览器都已经提示证书有问题了，如果用户坚决要访问，那不能怪 HTTPS ，得怪自己手贱。\n\n另外，如果你的电脑中毒了，被恶意导入了中间人的根证书，那么在验证中间人的证书的时候，由于你操作系统信任了中间人的根证书，那么等同于中间人的证书是合法的，这种情况下，浏览器是不会弹出证书存在问题的风险提醒的。\n\n这其实也不关 HTTPS 的事情，是你电脑中毒了才导致 HTTPS 数据被中间人劫持的。\n\n所以，**HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全**。\n\n\u003e 为什么抓包工具能截取 HTTPS 数据？\n\n很多抓包工具 之所以可以明文看到 HTTPS 数据，工作原理与中间人一致的。\n\n对于 HTTPS 连接来说，中间人要满足以下两点，才能实现真正的明文代理:\n\n1. 中间人，作为客户端与真实服务端建立连接这一步不会有问题，因为服务端不会校验客户端的身份；\n2. 中间人，作为服务端与真实客户端建立连接，这里会有客户端信任服务端的问题，也就是服务端必须有对应域名的私钥；\n\n中间人要拿到私钥只能通过如下方式：\n\n1. 去网站服务端拿到私钥；\n2. 去 CA 处拿域名签发私钥；\n3. 自己签发受浏览器信任的证书；\n\n不用解释，抓包工具只能使用第三种方式取得中间人的身份。\n\n因此使用抓包工具进行 HTTPS 抓包的时候，抓包工具会生成根证书，导入到客户端系统的 受信任的根证书列表 中，这里的根证书实际上起认证中心（CA）的作用。  \n\n随后抓包工具使用该根证书签发域名的证书，因为根证书受信任，域名的证书同样会被浏览器信任。也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人（抓包工具）签发的证书去中间人（抓包工具）自己的 CA 做认证，这个证书当然被认为是有效的。  \n\n\u003e 如何避免被中间人抓取数据？\n\n我们要保证自己电脑的安全，不要被病毒乘虚而入，而且也不要点击任何证书非法的网站，这样 HTTPS 数据就不会被中间人截取到了。\n\n当然，我们还可以通过 **HTTPS 双向认证**来避免这种问题。\n\n一般我们的 HTTPS 是单向认证，客户端只会验证了服务端的身份，但是服务端并不会验证客户端的身份。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/双向认证.png)\n\n如果用了双向认证方式，不仅客户端会验证服务端的身份，而且服务端也会验证客户端的身份。服务端一旦验证到请求自己的客户端为不可信任的，服务端就拒绝继续通信，客户端如果发现服务端为不可信任的，那么也中止通信。\n\n## HTTP/1.1、HTTP/2、HTTP/3 演变\n\n### HTTP/1.1 相比 HTTP/1.0 提高了什么性能？\n\nHTTP/1.1 相比 HTTP/1.0 性能上的改进：\n\n- 使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。\n- 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。\n\n\n但 HTTP/1.1 还是有性能瓶颈：\n\n- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；\n- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；\n- 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；\n- 没有请求优先级控制；\n- 请求只能从客户端开始，服务器只能被动响应。\n\n### HTTP/2 做了什么优化？\n\nHTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。\n\n![HTT/1 ~ HTTP/2](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/25-HTTP2.png)\n\n那 HTTP/2 相比 HTTP/1.1 性能上的改进：\n\n- 头部压缩\n- 二进制格式\n- 并发传输\n- 服务器主动推送资源\n\n*1. 头部压缩*\n\nHTTP/2 会**压缩头**（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你**消除重复的部分**。\n\n这就是所谓的 `HPACK` 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就**提高速度**了。\n\n*2. 二进制格式*\n\nHTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式**，头信息和数据体都是二进制，并且统称为帧（frame）：**头信息帧（Headers Frame）和数据帧（Data Frame）**。\n\n![HTTP/1 与 HTTP/2 ](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%B8%A7.png)\n\n这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这**增加了数据传输的效率**。\n\n比如状态码 200 ，在 HTTP/1.1 是用 '2''0''0' 三个字符来表示（二进制：00110010 00110000 00110000），共用了 3 个字节，如下图\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/http1.png)\n\n在 HTTP/2 对于状态码 200 的二进制编码是 10001000，只用了 1 字节就能表示，相比于 HTTP/1.1 节省了 2 个字节，如下图：\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/h2c.png)\n\nHeader: :status: 200 OK 的编码内容为：1000 1000，那么表达的含义是什么呢？\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/index.png)\n\n1. 最前面的 1 标识该 Header 是静态表中已经存在的 KV。（至于什么是静态表，可以看这篇：[HTTP/2 牛逼在哪？](https://xiaolincoding.com/network/2_http/http2.html)）\n2. 在静态表理，“:status: 200 ok” 静态表编码是 8，二进制即是 1000。\n\n因此，整体加起来就是 1000 1000。\n\n*3. 并发传输*\n\n我们都知道 HTTP/1.1 的实现是基于请求-响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是无法发送的，也造成了**队头阻塞**的问题。\n\n而 HTTP/2 就很牛逼了，引出了 Stream 概念，多个 Stream 复用在一条 TCP 连接。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http2/stream.png)\n\n从上图可以看到，1 个 TCP 连接包含多个 Stream，Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成。Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）。\n\n**针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应**。\n\n比如下图，服务端**并行交错地**发送了两个响应： Stream 1 和 Stream 3，这两个 Stream 都是跑在一个 TCP 连接上，客户端收到后，会根据相同的 Stream ID 有序组装成 HTTP 消息。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/http/http2多路复用.jpeg)\n\n*4、服务器推送*\n\nHTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以**主动**向客户端发送消息。\n\n客户端和服务器**双方都可以建立 Stream**， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。\n\n比如下图，Stream 1 是客户端向服务端请求的资源，属于客户端建立的 Stream，所以该 Stream 的 ID 是奇数（数字 1）；Stream 2 和 4 都是服务端主动向客户端推送的资源，属于服务端建立的 Stream，所以这两个 Stream 的 ID 是偶数（数字 2 和 4）。\n\n![](https://img-blog.csdnimg.cn/83445581dafe409d8cfd2c573b2781ac.png)\n\n再比如，客户端通过 HTTP/1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，如下图左边部分：\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/push.png)\n\n如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。\n\n\u003e HTTP/2 有什么缺陷？\n\nHTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。\n\n**HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/http2阻塞.jpeg)\n\n举个例子，如下图：\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/tcp%E9%98%9F%E5%A4%B4%E9%98%BB%E5%A1%9E.gif)\n\n图中发送方发送了很多个 packet，每个 packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 packet 3 在网络中丢失了，即使 packet 4-6 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。\n\n所以，一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**。\n\n::: tip\n\n如果想更进一步了解 HTTP/2 协议，可以看我这篇文章：[HTTP/2 牛逼在哪？](https://xiaolincoding.com/network/2_http/http2.html)\n\n:::\n\n### HTTP/3 做了哪些优化？\n\n前面我们知道了 HTTP/1.1 和 HTTP/2 都有队头阻塞的问题：\n\n- HTTP/1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是**没有解决响应的队头阻塞**，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应完这个请求后， 才能处理下一个请求，这属于 HTTP 层队头阻塞。\n- HTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是**一旦发生丢包，就会阻塞住所有的 HTTP 请求**，这属于 TCP 层队头阻塞。\n\nHTTP/2 队头阻塞的问题是因为 TCP，所以 **HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！**\n\n![HTTP/1 ~ HTTP/3](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/27-HTTP3.png)\n\nUDP 发送是不管顺序，也不管丢包的，所以不会出现像 HTTP/2 队头阻塞的问题。大家都知道 UDP 是不可靠传输的，但基于 UDP 的 **QUIC 协议** 可以实现类似 TCP 的可靠性传输。\n\nQUIC 有以下 3 个特点。\n\n- 无队头阻塞\n- 更快的连接建立\n- 连接迁移\n\n*1、无队头阻塞*\n\nQUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。\n\nQUIC 有自己的一套机制可以保证传输的可靠性的。**当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题**。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n\n所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/quic无阻塞.jpeg)\n\n*2、更快的连接建立*\n\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。\n\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，但这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，如下图：\n\n![TCP HTTPS（TLS/1.3） 和 QUIC HTTPS ](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/28-HTTP3交互次数.png)\n\n甚至，在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。\n\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT（下图的右下角）：\n\n![](https://img-blog.csdnimg.cn/4cad213f5125432693e0e2a512c2d1a1.png)\n\n*3、连接迁移*\n\n基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n\n![TCP 四元组](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEwLmpwZw?x-oss-process=image/format,png)\n\n那么**当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接**。而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n\n而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过**连接 ID** 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了**连接迁移**的功能。\n\n所以， QUIC 是一个在 UDP 之上的**伪** TCP + TLS + HTTP/2 的多路复用的协议。\n\nQUIC 是新协议，对于很多网络设备，根本不知道什么是 QUIC，只会当做 UDP，这样会出现新的问题，因为有的网络设备是会丢掉 UDP 包的，而 QUIC 是基于 UDP 实现的，那么如果网络设备无法识别这个是 QUIC 包，那么就会当作 UDP 包，然后被丢弃。\n\nHTTP/3 现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 TCP。\n\n::: tip\n\n如果想更进一步了解 HTTP/3 和 QUIC 协议，可以看我这两篇文章：\n\n- [HTTP/3 强势来袭](https://xiaolincoding.com/network/2_http/http3.html)\n- [如何基于 UDP 协议实现可靠传输？](https://xiaolincoding.com/network/3_tcp/quic.html)\n\n:::\n\n----\n\n参考资料：\n\n[1] 上野 宣.图解HTTP.人民邮电出版社.\n\n[2] 罗剑锋.透视HTTP协议.极客时间.\n\n[3] 陈皓.HTTP的前世今.酷壳CoolShell.https://coolshell.cn/articles/19840.html\n\n[4] 阮一峰.HTTP 协议入门.阮一峰的网络日志.http://www.ruanyifeng.com/blog/2016/08/http.html\n\n----\n\n\n## 读者问答\n\n\n\u003e 读者问：“https 和 http 相比，就是传输的内容多了对称加密，可以这么理解吗？”\n\n1. 建立连接时候：https 比 http 多了 TLS 的握手过程；\n\n2. 传输内容的时候：https 会把数据进行加密，通常是对称加密数据；\n\n\u003e 读者问：“ 我看文中 TLS 和 SSL 没有做区分，这两个需要区分吗？”\n\n这俩实际上是一个东西。\n\nSSL 是洋文 “*Secure Sockets Layer*” 的缩写，中文叫做「安全套接层」。它是在上世纪 90 年代中期，由网景公司设计的。\n\n到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为 TLS（是 “*Transport Layer Security*” 的缩写），中文叫做 「传输层安全协议」。\n\n很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。\n\n\u003e 读者问：“为啥 SSL 的握手是 4 次？”\n\nSSL/TLS 1.2 需要 4 握手，需要 2 个 RTT 的时延，我文中的图是把每个交互分开画了，实际上把他们合在一起发送，就是 4 次握手：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/29-TLS1.2-四次握手.png)\n\n\n另外， SSL/TLS 1.3 优化了过程，只需要 1 个 RTT 往返时延，也就是只需要 3 次握手：\n\n\n![T](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/30-TLS1.3.png)\n\n----\n\n\n本文的 `30` 张图片，都是从一条线两条线画出来，灰常的费劲，深切感受到画图也是个**体力活**啊！ \n\n爱偷懒的我其实不爱画图，但为了让大家能更好的理解，在跟自己无数次斗争后，踏上了耗时耗体力的画图的不归路，希望对你们有帮助！\n\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.860106459Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/http_optimize":{"title":"http_optimize","content":"# 3.2 HTTP/1.1 如何优化？\n\n问你一句：「**你知道 HTTP/1.1 该如何优化吗？**」\n\n我们可以从下面这三种优化思路来优化 HTTP/1.1 协议：\n\n- *尽量避免发送 HTTP 请求*；\n- *在需要发送 HTTP 请求时，考虑如何减少请求次数*；\n- *减少服务器的 HTTP 响应的数据大小*；\n\n下面，就针对这三种思路具体看看有哪些优化方法。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/优化http1.1提纲.png)\n\n\n---\n\n## 如何避免发送 HTTP 请求？\n\n这个思路你看到是不是觉得很奇怪，不发送 HTTP 请求，那客户端还怎么和服务器交互数据？小林你这不是耍流氓嘛？\n\n冷静冷静，你说的没错，客户端当然要向服务器发送请求的。\n\n但是，对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都**缓存在本地**，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升。\n\n所以，避免发送 HTTP 请求的方法就是通过**缓存技术**，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的头部有不少是针对缓存的字段。\n\n那缓存是如何做到的呢？\n\n客户端会把第一次请求以及响应的数据保存在本地磁盘上，其中将请求的 URL 作为 key，而响应作为 value，两者形成映射关系。\n\n\n这样当后续发起相同的请求时，就可以先在本地磁盘上通过 key 查到对应的 value，也就是响应，如果找到了，就直接从本地读取该响应。毋庸置疑，读取本地磁盘的速度肯定比网络请求快得多，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/缓存访问.png)\n\n聪明的你可能想到了，万一缓存的响应不是最新的，而客户端并不知情，那么该怎么办呢？\n\n放心，这个问题 HTTP 设计者早已考虑到。\n\n所以，服务器在发送 HTTP 响应时，会估算一个过期的时间，并把这个信息放到响应头部中，这样客户端在查看响应头部的信息时，一旦发现缓存的响应是过期的，则就会重新发送网络请求。\n\n如果客户端从第一次请求得到的响应头部中发现该响应过期了，客户端重新发送请求，假设服务器上的资源并没有变更，还是老样子，那么你觉得还要在服务器的响应带上这个资源吗？\n\n很显然不带的话，可以提高 HTTP 协议的性能，那具体如何做到呢？ \n\n只需要客户端在重新发送请求时，在请求的 `Etag` 头部带上第一次请求的响应头部中的摘要，这个摘要是唯一标识响应的资源，当服务器收到请求后，会将本地资源的摘要与请求中的摘要做个比较。\n\n如果不同，那么说明客户端的缓存已经没有价值，服务器在响应中带上最新的资源。\n\n如果相同，说明客户端的缓存还是可以继续使用的，那么服务器**仅返回不含有包体的 `304 Not Modified` 响应**，告诉客户端仍然有效，这样就可以减少响应资源在网络中传输的延时，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/缓存etag.png)\n\n缓存真的是性能优化的一把万能钥匙，小到 CPU Cache、Page Cache、Redis Cache，大到 HTTP 协议的缓存。\n\n\n---\n\n## 如何减少 HTTP 请求次数？\n\n减少 HTTP 请求次数自然也就提升了 HTTP 性能，可以从这 3 个方面入手：\n\n- *减少重定向请求次数*；\n- *合并请求*；\n- *延迟发送请求*；\n\n### 减少重定向请求次数\n\n我们先来看看什么是**重定向请求**？\n\n服务器上的一个资源可能由于迁移、维护等原因从 url1 移至 url2 后，而客户端不知情，它还是继续请求 url1，这时服务器不能粗暴地返回错误，而是通过 `302` 响应码和 `Location` 头部，告诉客户端该资源已经迁移至 url2 了，于是客户端需要再发送 url2 请求以获得服务器的资源。\n\n那么，如果重定向请求越多，那么客户端就要多次发起 HTTP 请求，每一次的 HTTP 请求都得经过网络，这无疑会越降低网络性能。\n\n另外，服务端这一方往往不只有一台服务器，比如源服务器上一级是代理服务器，然后代理服务器才与客户端通信，这时客户端重定向就会导致客户端与代理服务器之间需要 2 次消息传递，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/客户端重定向.png)\n\n\n如果**重定向的工作交由代理服务器完成，就能减少 HTTP 请求次数了**，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/代理服务器重定向.png)\n\n而且当代理服务器知晓了重定向规则后，可以进一步减少消息传递次数，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/代理服务器重定向2.png)\n\n除了 `302` 重定向响应码，还有其他一些重定向的响应码，你可以从下图看到：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/重定向响应码.png)\n\n其中，`301` 和 `308` 响应码是告诉客户端可以将重定向响应缓存到本地磁盘，之后客户端就自动用 url2 替代 url1 访问服务器的资源。\n\n\n\n### 合并请求\n\n如果把多个访问小文件的请求合并成一个大的请求，虽然传输的总资源还是一样，但是减少请求，也就意味着**减少了重复发送的 HTTP 头部**。\n\n另外由于 HTTP/1.1 是请求响应模型，如果第一个发送的请求，未收到对应的响应，那么后续的请求就不会发送（PS：HTTP/1.1 管道模式是默认不使用的，所以讨论 HTTP/1.1  的队头阻塞问题，是不考虑管道模式的），于是为了防止单个请求的阻塞，所以**一般浏览器会同时发起 5-6 个请求，每一个请求都是不同的 TCP 连接**，那么如果合并了请求，也就会**减少 TCP 连接的数量，因而省去了  TCP 握手和慢启动过程耗费的时间**。\n\n\n接下来，具体看看合并请求的几种方式。\n\n有的网页会含有很多小图片、小图标，有多少个小图片，客户端就要发起多少次请求。那么对于这些小图片，我们可以考虑使用 `CSS Image Sprites` 技术把它们合成一个大图片，这样浏览器就可以用一次请求获得一个大图片，然后再根据 CSS 数据把大图片切割成多张小图片。\n\n![图来源于：墨染枫林的CSDN](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/css精灵.png)\n\n这种方式就是**通过将多个小图片合并成一个大图片来减少 HTTP 请求的次数，以减少 HTTP 请求的次数，从而减少网络的开销**。\n\n\n除了将小图片合并成大图片的方式，还有服务端使用 `webpack` 等打包工具将 js、css 等资源合并打包成大文件，也是能达到类似的效果。\n\n另外，还可以将图片的二进制数据用 `base64` 编码后，以 URL 的形式嵌入到 HTML 文件，跟随 HTML 文件一并发送.\n\n\n```\n\u003cimage src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPoAAAFKCAIAAAC7M9WrAAAACXBIWXMAA ... /\u003e\n```\n\n\n\n这样客户端收到 HTML 后，就可以直接解码出数据，然后直接显示图片，就不用再发起图片相关的请求，这样便减少了请求的次数。\n\n![图来源于：陈健平的CSDN ](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/base64图片.png)\n\n\n\n可以看到，**合并请求的方式就是合并资源，以一个大资源的请求替换多个小资源的请求**。\n\n但是这样的合并请求会带来新的问题，**当大资源中的某一个小资源发生变化后，客户端必须重新下载整个完整的大资源文件**，这显然带来了额外的网络消耗。\n\n\n### 延迟发送请求\n\n不要一口气吃成大胖子，一般 HTML 里会含有很多 HTTP 的 URL，当前不需要的资源，我们没必要也获取过来，于是可以通过「**按需获取**」的方式，来减少第一时间的 HTTP 请求次数。\n\n请求网页的时候，没必要把全部资源都获取到，而是只获取当前用户所看到的页面资源，当用户向下滑动页面的时候，再向服务器获取接下来的资源，这样就达到了延迟发送请求的效果。\n\n----\n\n\n## 如何减少 HTTP 响应的数据大小？\n\n对于 HTTP 的请求和响应，通常 HTTP 的响应的数据大小会比较大，也就是服务器返回的资源会比较大。\n\n于是，我们可以考虑对响应的资源进行**压缩**，这样就可以减少响应的数据大小，从而提高网络传输的效率。\n\n压缩的方式一般分为 2 种，分别是：\n\n- *无损压缩*；\n- *有损压缩*；\n\n### 无损压缩\n\n无损压缩是指资源经过压缩后，信息不被破坏，还能完全恢复到压缩前的原样，适合用在文本文件、程序可执行文件、程序源代码。\n\n首先，我们针对代码的语法规则进行压缩，因为通常代码文件都有很多换行符或者空格，这些是为了帮助程序员更好的阅读，但是机器执行时并不要这些符，把这些多余的符号给去除掉。\n\n接下来，就是无损压缩了，需要对原始资源建立统计模型，利用这个统计模型，将常出现的数据用较短的二进制比特序列表示，将不常出现的数据用较长的二进制比特序列表示，生成二进制比特序列一般是「霍夫曼编码」算法。\n\ngzip 就是比较常见的无损压缩。客户端支持的压缩算法，会在 HTTP 请求中通过头部中的 `Accept-Encoding` 字段告诉服务器：\n\n\n```\nAccept-Encoding: gzip, deflate, br\n```\n\n服务器收到后，会从中选择一个服务器支持的或者合适的压缩算法，然后使用此压缩算法对响应资源进行压缩，最后通过响应头部中的 `Content-Encoding` 字段告诉客户端该资源使用的压缩算法。\n\n\n```\nContent-Encoding: gzip\n```\n\ngzip 的压缩效率相比 Google 推出的 Brotli 算法还是差点意思，也就是上文中的 br，所以如果可以，服务器应该选择压缩效率更高的 br 压缩算法。\n\n### 有损压缩\n\n与无损压缩相对的就是有损压缩，经过此方法压缩，解压的数据会与原始数据不同但是非常接近。\n\n有损压缩主要将次要的数据舍弃，牺牲一些质量来减少数据量、提高压缩比，这种方法经常用于压缩多媒体数据，比如音频、视频、图片。\n\n可以通过 HTTP 请求头部中的 `Accept` 字段里的「 q 质量因子」，告诉服务器期望的资源质量。\n\n```\nAccept: audio/*; q=0.2, audio/basic\n```\n\n关于图片的压缩，目前压缩比较高的是 Google 推出的 **WebP 格式**，它与常见的 Png 格式图片的压缩比例对比如下图：\n\n![来源于：https://isparta.github.io/compare-webp/index.html](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http1.1优化/webp与png.png)\n\n可以发现，相同图片质量下，WebP 格式的图片大小都比 Png 格式的图片小，所以对于大量图片的网站，可以考虑使用 WebP 格式的图片，这将大幅度提升网络传输的性能。\n\n关于音视频的压缩，音视频主要是动态的，每个帧都有时序的关系，通常时间连续的帧之间的变化是很小的。\n\n比如，一个在看书的视频，画面通常只有人物的手和书桌上的书是会有变化的，而其他地方通常都是静态的，于是只需要在一个静态的关键帧，使用**增量数据**来表达后续的帧，这样便减少了很多数据，提高了网络传输的性能。对于视频常见的编码格式有 H264、H265 等，音频常见的编码格式有 AAC、AC3。\n\n\n---\n\n## 总结\n\n这次主要从 3 个方面介绍了优化 HTTP/1.1 协议的思路。\n\n第一个思路是，通过缓存技术来避免发送 HTTP 请求。客户端收到第一个请求的响应后，可以将其缓存在本地磁盘，下次请求的时候，如果缓存没过期，就直接读取本地缓存的响应数据。如果缓存过期，客户端发送请求的时候带上响应数据的摘要，服务器比对后发现资源没有变化，就发出不带包体的 304 响应，告诉客户端缓存的响应仍然有效。\n\n第二个思路是，减少 HTTP 请求的次数，有以下的方法：\n\n1. 将原本由客户端处理的重定向请求，交给代理服务器处理，这样可以减少重定向请求的次数；\n2. 将多个小资源合并成一个大资源再传输，能够减少 HTTP 请求次数以及 头部的重复传输，再来减少 TCP 连接数量，进而省去 TCP 握手和慢启动的网络消耗；\n3. 按需访问资源，只访问当前用户看得到/用得到的资源，当客户往下滑动，再访问接下来的资源，以此达到延迟请求，也就减少了同一时间的 HTTP 请求次数。\n\n第三思路是，通过压缩响应资源，降低传输资源的大小，从而提高传输效率，所以应当选择更优秀的压缩算法。\n\n\n不管怎么优化 HTTP/1.1 协议都是有限的，不然也不会出现 HTTP/2 和 HTTP/3 协议，后续我们再来介绍 HTTP/2 和 HTTP/3 协议。\n\n好了，此次分享到这就结束了，如果这篇文章对你有帮助，欢迎来个三连，你们的支持就是小林的最大动力，我们下次见！\n\n---\n\n参考资料：\n\n1. https://isparta.github.io/compare-webp/index.html\n2. https://zh.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Lossy_compression\n3. https://en.wikipedia.org/wiki/Lossless_compression\n4. https://time.geekbang.org/column/article/242667\n5. https://www.tutorialrepublic.com/css-tutorial/css-sprites.php\n6. https://blog.csdn.net/weixin_38055381/article/details/81504716\n7. https://blog.csdn.net/weixin_44151887/article/details/106278559\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/http_rpc":{"title":"http_rpc","content":"# 3.8 既然有 HTTP 协议，为什么还要有 RPC？\n\n\u003e来源：公众号@小白debug\n\u003e\n\u003e原文地址：[既然有 HTTP 协议，为什么还要有 RPC？](https://mp.weixin.qq.com/s/qmnfmUCdekEt1xG0hp_4MQ)\n\n我想起了我刚工作的时候，第一次接触 RPC 协议，当时就很懵，**我 HTTP 协议用的好好的，为什么还要用 RPC 协议？**\n\n于是就到网上去搜。\n\n不少解释显得非常官方，我相信大家在各种平台上也都看到过，解释了又好像没解释，都在**用一个我们不认识的概念去解释另外一个我们不认识的概念**，懂的人不需要看，不懂的人看了还是不懂。\n\n这种看了，又好像没看的感觉，云里雾里的很难受，**我懂**。\n\n为了避免大家有强烈的**审丑疲劳**，今天我们来尝试重新换个方式讲一讲。\n\n## 从 TCP 聊起\n\n作为一个程序员，假设我们需要在 A 电脑的进程发一段数据到 B 电脑的进程，我们一般会在代码里使用 Socket 进行编程。\n\n这时候，我们可选项一般也就 TCP 和 UDP 二选一。TCP 可靠，UDP 不可靠。除非是马总这种神级程序员（早期 QQ 大量使用 UDP），否则，只要稍微对可靠性有些要求，普通人一般无脑选 TCP 就对了。\n\n类似下面这样。\n\n```c\nfd = socket(AF_INET,SOCK_STREAM,0);\n```\n\n其中 `SOCK_STREAM`，是指使用**字节流**传输数据，说白了就是 **TCP 协议**。\n\n在定义了 Socket 之后，我们就可以愉快的对这个 Socket 进行操作，比如用 `bind()` 绑定 IP 端口，用 `connect()` 发起建连。\n\n![握手建立连接流程](https://img-blog.csdnimg.cn/img_convert/23cc66a7f4cb06afe13842b4b339e28b.gif)\n\n在连接建立之后，我们就可以使用 `send()` 发送数据，`recv()` 接收数据。\n\n光这样一个纯裸的 TCP 连接，就可以做到收发数据了，那是不是就够了？\n\n不行，这么用会有问题。\n\n## 使用纯裸 TCP 会有什么问题\n\n八股文常背，TCP 是有三个特点，**面向连接**、**可靠**、基于**字节流**。\n\n![TCP 是什么](https://img-blog.csdnimg.cn/img_convert/3fcad07ba7ae92299b32224da8583363.png)\n\n这三个特点真的概括的**非常精辟**，这个八股文我们没白背。\n\n每个特点展开都能聊一篇文章，而今天我们需要关注的是**基于字节流**这一点。\n\n字节流可以理解为一个双向的通道里流淌的数据，这个**数据**其实就是我们常说的二进制数据，简单来说就是一大堆 **01 串**。纯裸 TCP 收发的这些 01 串之间是**没有任何边界**的，你根本不知道到哪个地方才算一条完整消息。\n\n![01 二进制字节流](https://img-blog.csdnimg.cn/img_convert/254d845f9de05c19536d8343d268595a.png)\n\n正因为这个没有**任何边界**的特点，所以当我们选择使用 TCP 发送\"夏洛\"和\"特烦恼\"的时候，接收端收到的就是\"夏洛特烦恼\"，这时候接收端没发区分你是想要表达\"夏洛\"+\"特烦恼\"还是\"夏洛特\"+\"烦恼\"。\n\n![消息对比](https://img-blog.csdnimg.cn/img_convert/cd7c006cb4180bf751c4afd268ed44f0.png)\n\n这就是所谓的**粘包问题**，之前也写过一篇专门的[文章](https://xiaolincoding.com/network/3_tcp/tcp_stream.html)聊过这个问题。\n\n说这个的目的是为了告诉大家，纯裸 TCP 是不能直接拿来用的，你需要在这个基础上加入一些**自定义的规则**，用于区分**消息边界**。\n\n于是我们会把每条要发送的数据都包装一下，比如加入**消息头**，**消息头里写清楚一个完整的包长度是多少**，根据这个长度可以继续接收数据，截取出来后它们就是我们真正要传输的**消息体**。\n\n![消息边界长度标志](https://img-blog.csdnimg.cn/img_convert/9428feed1ff22156fc136d17a129527b.png)\n\n而这里头提到的**消息头**，还可以放各种东西，比如消息体是否被压缩过和消息体格式之类的，只要上下游都约定好了，互相都认就可以了，这就是所谓的**协议。**\n\n每个使用 TCP 的项目都可能会定义一套类似这样的协议解析标准，他们可能**有区别，但原理都类似**。\n\n**于是基于 TCP，就衍生了非常多的协议，比如 HTTP 和 RPC。**\n\n## HTTP 和 RPC\n\n我们回过头来看网络的分层图。\n\n![四层网络协议](https://img-blog.csdnimg.cn/img_convert/da970d16a205fb48d6a8bea14498814d.png)\n\n**TCP 是传输层的协议**，而基于 TCP 造出来的 HTTP 和**各类** RPC 协议，它们都只是定义了不同消息格式的**应用层协议**而已。\n\n**HTTP** 协议（**H**yper **T**ext **T**ransfer **P**rotocol），又叫做**超文本传输协议**。我们用的比较多，平时上网在浏览器上敲个网址就能访问网页，这里用到的就是 HTTP 协议。\n\n![HTTP调用](https://img-blog.csdnimg.cn/img_convert/809c33f7090c08b78d494445e39ae1b4.png)\n\n而 **RPC**（**R**emote **P**rocedure **C**all），又叫做**远程过程调用**。它本身并不是一个具体的协议，而是一种**调用方式**。\n\n举个例子，我们平时调用一个**本地方法**就像下面这样。\n\n```\n res = localFunc(req)\n```\n\n如果现在这不是个本地方法，而是个**远端服务器**暴露出来的一个方法 `remoteFunc`，如果我们还能像调用本地方法那样去调用它，这样就可以**屏蔽掉一些网络细节**，用起来更方便，岂不美哉？\n\n```\n res = remoteFunc(req)\n```\n\n![RPC可以像调用本地方法那样调用远端方法](https://img-blog.csdnimg.cn/img_convert/2b2ea6d26af9ded517043e528b032307.png)\n\n基于这个思路，大佬们造出了非常多款式的 RPC 协议，比如比较有名的`gRPC`，`thrift`。\n\n值得注意的是，虽然大部分 RPC 协议底层使用 TCP，但实际上**它们不一定非得使用 TCP，改用 UDP 或者 HTTP，其实也可以做到类似的功能。**\n\n![基于TCP协议的HTTP和RPC协议](https://img-blog.csdnimg.cn/img_convert/054e9738bc492a6fb6e9a71737d95fc0.png)\n\n到这里，我们回到文章标题的问题。\n\n\u003e 既然有 HTTP 协议，为什么还要有 RPC？\n\n其实，`TCP` 是**70年**代出来的协议，而 `HTTP` 是 **90 年代**才开始流行的。而直接使用裸 TCP 会有问题，可想而知，这中间这么多年有多少自定义的协议，而这里面就有**80年代**出来的 `RPC`。\n\n所以我们该问的不是**既然有 HTTP 协议为什么要有 RPC**，而是**为什么有 RPC 还要有 HTTP 协议**。\n\n\u003e 那既然有 RPC 了，为什么还要有 HTTP 呢？\n\n现在电脑上装的各种**联网**软件，比如 xx管家，xx卫士，它们都作为**客户端（Client）需要跟服务端（Server）建立连接收发消息**，此时都会用到应用层协议，在这种 Client/Server (C/S) 架构下，它们可以使用自家造的 RPC 协议，因为它只管连自己公司的服务器就 ok 了。\n\n但有个软件不同，**浏览器（Browser）**，不管是 Chrome 还是 IE，它们不仅要能访问自家公司的**服务器（Server）**，还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP 就是那个时代用于统一 **Browser/Server (B/S)** 的协议。\n\n也就是说在多年以前，**HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。**很多软件同时支持多端，比如某度云盘，既要支持**网页版**，还要支持**手机端和 PC 端**，如果通信协议都用 HTTP 的话，那服务器只用同一套就够了。而 RPC 就开始退居幕后，一般用于公司内部集群里，各个微服务之间的通讯。\n\n那这么说的话，**都用 HTTP 得了，还用什么 RPC？**\n\n仿佛又回到了文章开头的样子，那这就要从它们之间的区别开始说起。\n\n## HTTP 和 RPC 有什么区别\n\n我们来看看 RPC 和 HTTP 区别比较明显的几个点。\n\n### 服务发现\n\n首先要向某个服务器发起请求，你得先建立连接，而建立连接的前提是，你得知道 **IP 地址和端口**。这个找到服务对应的 IP 端口的过程，其实就是**服务发现**。\n\n在 **HTTP** 中，你知道服务的域名，就可以通过 **DNS 服务**去解析得到它背后的 IP 地址，默认 80 端口。\n\n而 **RPC** 的话，就有些区别，一般会有专门的**中间服务**去保存服务名和IP信息，比如 **Consul 或者 Etcd，甚至是 Redis**。想要访问某个服务，就去这些中间服务去获得 IP 和端口信息。由于 DNS 也是服务发现的一种，所以也有基于 DNS 去做服务发现的组件，比如**CoreDNS**。\n\n可以看出服务发现这一块，两者是有些区别，但不太能分高低。\n\n### 底层连接形式\n\n以主流的 **HTTP/1.1** 协议为例，其默认在建立底层 TCP 连接之后会一直保持这个连接（**Keep Alive**），之后的请求和响应都会复用这条连接。\n\n而 **RPC** 协议，也跟 HTTP 类似，也是通过建立 TCP 长链接进行数据交互，但不同的地方在于，RPC 协议一般还会再建个**连接池**，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，**用完放回去，下次再复用**，可以说非常环保。\n\n![connection_pool](https://img-blog.csdnimg.cn/img_convert/ec5c8e28d3ea308c6db2ac991a12ea80.png)\n\n**由于连接池有利于提升网络请求性能，所以不少编程语言的网络库里都会给 HTTP 加个连接池**，比如 **Go** 就是这么干的。\n\n可以看出这一块两者也没太大区别，所以也不是关键。\n\n### 传输的内容\n\n基于 TCP 传输的消息，说到底，无非都是**消息头 Header 和消息体 Body。**\n\n**Header** 是用于标记一些特殊信息，其中最重要的是**消息体长度**。\n\n**Body** 则是放我们真正需要传输的内容，而这些内容只能是二进制 01 串，毕竟计算机只认识这玩意。所以 TCP 传字符串和数字都问题不大，因为字符串可以转成编码再变成 01 串，而数字本身也能直接转为二进制。但结构体呢，我们得想个办法将它也转为二进制 01 串，这样的方案现在也有很多现成的，比如 **Json，Protobuf。**\n\n这个将结构体转为二进制数组的过程就叫**序列化**，反过来将二进制数组复原成结构体的过程叫**反序列化**。\n\n![序列化和反序列化](https://img-blog.csdnimg.cn/img_convert/dba2bc3af0938d2c087f85acc191fd3f.png)\n\n对于主流的 HTTP/1.1，虽然它现在叫**超文本**协议，支持音频视频，但 HTTP 设计初是用于做网页**文本**展示的，所以它传的内容以字符串为主。Header 和 Body 都是如此。在 Body 这块，它使用 **Json** 来**序列化**结构体数据。\n\n我们可以随便截个图直观看下。\n\n![HTTP 报文](https://img-blog.csdnimg.cn/img_convert/324cbe84c303a3b975e50329f5cdbf8b.png)\n\n可以看到这里面的内容非常多的**冗余**，显得**非常啰嗦**。最明显的，像 `Header` 里的那些信息，其实如果我们约定好头部的第几位是 Content-Type，就**不需要每次都真的把\"Content-Type\"这个字段都传过来**，类似的情况其实在 `body` 的 Json 结构里也特别明显。\n\n而 RPC，因为它定制化程度更高，可以采用体积更小的 Protobuf 或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。**因此性能也会更好一些，这也是在公司内部微服务中抛弃 HTTP，选择使用 RPC 的最主要原因。**\n\n![HTTP 原理](https://img-blog.csdnimg.cn/img_convert/f4cef7331cabcfe56d9d6434f7ef907f.png)\n\n![RPC 原理](https://img-blog.csdnimg.cn/img_convert/12244fb0b19b2e61755fcab799198f68.png)\n\n当然上面说的 HTTP，其实**特指的是现在主流使用的 HTTP/1.1**，`HTTP/2` 在前者的基础上做了很多改进，所以**性能可能比很多 RPC 协议还要好**，甚至连 `gRPC` 底层都直接用的 `HTTP/2`。\n\n\u003e 那么问题又来了，为什么既然有了 HTTP/2，还要有 RPC 协议？\n\n这个是由于 HTTP/2 是 2015 年出来的。那时候很多公司内部的 RPC 协议都已经跑了好些年了，基于历史原因，一般也没必要去换了。\n\n## 总结\n\n- 纯裸 TCP 是能收发数据，但它是个**无边界**的数据流，上层需要定义**消息格式**用于定义**消息边界**。于是就有了各种协议，HTTP 和各类 RPC 协议就是在 TCP 之上定义的应用层协议。\n- **RPC 本质上不算是协议，而是一种调用方式**，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，**不一定非得基于 TCP 协议**。\n- 从发展历史来说，**HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合**。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。\n- RPC 其实比 HTTP 出现的要早，且比目前主流的 HTTP/1.1 **性能**要更好，所以大部分公司内部都还在使用 RPC。\n- **HTTP/2.0** 在 **HTTP/1.1** 的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。\n\n---\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」***\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/http_websocket":{"title":"http_websocket","content":"# 3.9 既然有 HTTP 协议，为什么还要有 WebSocket？\n\n\u003e 来源：公众号@小白debug\n\u003e\n\u003e 原文地址：[既然有 HTTP 协议，为什么还要有 WebSocket？](https://mp.weixin.qq.com/s/jJNdXMNmXcE8wSE0gbtTAQ)\n\n平时我们打开网页，比如购物网站某宝。都是点一下「列表商品」，跳转一下网页就到了「商品详情」。\n\n从 HTTP 协议的角度来看，就是点一下网页上的某个按钮，**前端发一次 HTTP 请求，网站返回一次 HTTP 响应**。这种由客户端主动请求，服务器响应的方式也满足大部分网页的功能场景。\n\n但有没有发现，这种情况下，服务器从来就「不会主动」给客户端发一次消息。就像你喜欢的女生从来不会主动找你一样。\n\n但如果现在，你在刷网页的时候「右下角」突然弹出一个小广告，提示你【一个人在家偷偷才能玩哦】。\n\n**求知，好学，勤奋**，这些刻在你 DNA 里的东西都动起来了。\n\n你点开后发现。\n\n长相平平无奇的古某提示你\"道士 9 条狗，全服横着走\"。\n\n影帝某辉老师跟你说\"系兄弟就来砍我\"。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/b8cca4b1291f25235bc8df3dddbb6da3.png)\n\n来都来了，你就选了个角色进到了游戏界面里。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/95e5b4cee384b182d0e604378c3ca00a.jpeg)\n\n这时候，上来就是一个小怪，从远处走来，然后疯狂拿木棒子抽你。\n\n**你全程没点任何一次鼠标**。服务器就自动将怪物的移动数据和攻击数据源源不断发给你了。\n\n这….太暖心了。\n\n感动之余，问题就来了：\n\n像这种**看起来服务器主动发消息给客户端的场景**，是怎么做到的？\n\n在真正回答这个问题之前，我们先来聊下一些相关的知识背景。\n\n## 使用 HTTP 不断轮询\n\n其实问题的痛点在于，**怎么样才能在用户不做任何操作的情况下，网页能收到消息并发生变更。**\n\n最常见的解决方案是，**网页的前端代码里不断定时发 HTTP 请求到服务器，服务器收到请求后给客户端响应消息。**\n\n这其实时一种「**伪**」服务器推的形式。\n\n它其实并不是服务器主动发消息到客户端，而是客户端自己不断偷偷请求服务器，只是用户无感知而已。\n\n用这种方式的场景也有很多，最常见的就是**扫码登录**。\n\n比如，某信公众号平台，登录页面二维码出现之后，**前端**网页根本不知道用户扫没扫，于是不断去向**后端**服务器询问，看有没有人扫过这个码。而且是以大概 1 到 2 秒的间隔去不断发出请求，这样可以保证用户在扫码后能在 1 到 2 秒内得到及时的反馈，不至于**等太久**。\n\n## 使用 HTTP 定时轮询\n\n但这样，会有两个比较明显的问题：\n\n- 当你打开 F12 页面时，你会发现满屏的 HTTP 请求。虽然很小，但这其实也消耗带宽，同时也会增加下游服务器的负担。\n- 最坏情况下，用户在扫码后，需要等个 1~2 秒，正好才触发下一次 HTTP 请求，然后才跳转页面，用户会感到**明显的卡顿**。\n\n使用起来的体验就是，二维码出现后，手机扫一扫，然后在手机上点个确认，这时候**卡顿等个 1~2 秒**，页面才跳转。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/5e0e0e25e8aca80812c9a2892032111c.png)\n\n那么问题又来了，**有没有更好的解决方案？**\n\n有，而且实现起来成本还非常低。\n\n## 长轮询\n\n我们知道，HTTP 请求发出后，一般会给服务器留一定的时间做响应，比如 3 秒，规定时间内没返回，就认为是超时。\n\n如果我们的 HTTP 请求**将超时设置的很大**，比如 30 秒，**在这 30 秒内只要服务器收到了扫码请求，就立马返回给客户端网页。如果超时，那就立马发起下一次请求。**\n\n这样就减少了 HTTP 请求的个数，并且由于大部分情况下，用户都会在某个 30 秒的区间内做扫码操作，所以响应也是及时的。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/1058a96ba35215c0f30accc3ff5bb824.png)\n\n比如，某度云网盘就是这么干的。所以你会发现一扫码，手机上点个确认，电脑端网页就**秒跳转**，体验很好。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/a3a8c95b97d2bac26cfab123a4da68b2.png)\n\n像这种发起一个请求，在较长时间内等待服务器响应的机制，就是所谓的**长轮询机制**。我们常用的消息队列 RocketMQ 中，消费者去取数据时，也用到了这种方式。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/6173c1d25abc914ef17ee9e534ed6a5f.png)\n\n像这种，在用户不感知的情况下，服务器将数据推送给浏览器的技术，就是所谓的**服务器推送**技术，它还有个毫不沾边的英文名，**comet** 技术，大家听过就好。\n\n上面提到的两种解决方案（不断轮询和长轮询），本质上，其实还是客户端主动去取数据。\n\n对于像扫码登录这样的**简单场景**还能用用。但如果是网页游戏呢，游戏一般会有大量的数据需要从服务器主动推送到客户端。\n\n这就得说下 **WebSocket** 了。\n\n## WebSocket 是什么\n\n我们知道 TCP 连接的两端，**同一时间里**，**双方**都可以**主动**向对方发送数据。这就是所谓的**全双工**。\n\n而现在使用最广泛的 `HTTP/1.1`，也是基于 TCP 协议的，**同一时间里**，客户端和服务器**只能有一方主动**发数据，这就是所谓的**半双工**。\n\n也就是说，好好的全双工 TCP，被 HTTP/1.1 用成了半双工。\n\n为什么？\n\n这是由于 HTTP 协议设计之初，考虑的是看看网页文本的场景，能做到**客户端发起请求再由服务器响应**，就够了，根本就没考虑网页游戏这种，客户端和服务器之间都要互相主动发大量数据的场景。\n\n所以，为了更好的支持这样的场景，我们需要另外一个**基于 TCP 的新协议**。\n\n于是新的应用层协议 **WebSocket** 就被设计出来了。\n\n大家别被这个名字给带偏了。虽然名字带了个 socket，但其实 **socket 和 WebSocket 之间，就跟雷峰和雷峰塔一样，二者接近毫无关系**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/3bbe4c5db972513f912d30ba8cbddd65.png)\n\n### 怎么建立 WebSocket 连接\n\n我们平时刷网页，一般都是在浏览器上刷的，一会刷刷图文，这时候用的是 **HTTP 协议**，一会打开网页游戏，这时候就得切换成我们新介绍的 **WebSocket 协议**。\n\n为了兼容这些使用场景。浏览器在 **TCP 三次握手**建立连接之后，都**统一使用 HTTP 协议**先进行一次通信。\n\n- 如果此时是**普通的 HTTP 请求**，那后续双方就还是老样子继续用普通 HTTP 协议进行交互，这点没啥疑问。\n- 如果这时候是**想建立 WebSocket 连接**，就会在 HTTP 请求里带上一些**特殊的 header 头**，如下：\n\n```http\nConnection: Upgrade\nUpgrade: WebSocket\nSec-WebSocket-Key: T2a6wZlAwhgQNqruZ2YUyg==\\r\\n\n```\n\n这些 header 头的意思是，浏览器想**升级协议（Connection: Upgrade）**，并且**想升级成 WebSocket 协议（Upgrade: WebSocket）**。同时带上一段**随机生成的 Base64 码（Sec-WebSocket-Key）**，发给服务器。\n\n如果服务器正好支持升级成 WebSocket 协议。就会走 WebSocket 握手流程，同时根据客户端生成的 Base64 码，用某个**公开的**算法变成另一段字符串，放在 HTTP 响应的 `Sec-WebSocket-Accept` 头里，同时带上 `101 状态码`，发回给浏览器。HTTP 的响应如下：\n\n```http\nHTTP/1.1 101 Switching Protocols\\r\\n\nSec-WebSocket-Accept: iBJKv/ALIW2DobfoA4dmr3JHBCY=\\r\\n\nUpgrade: WebSocket\\r\\n\nConnection: Upgrade\\r\\n\n```\n\nHTTP 状态码=200（正常响应）的情况，大家见得多了。101 确实不常见，它其实是指**协议切换**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/dea71991b336c876cae2e1ebdf03b62d.png)\n\n之后，浏览器也用同样的**公开算法**将 `Base64 码` 转成另一段字符串，如果这段字符串跟服务器传回来的**字符串一致**，那验证通过。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/117eebe06cc6b35ded3216a95706f080.png)\n\n就这样经历了一来一回两次 HTTP 握手，WebSocket 就建立完成了，后续双方就可以使用 WebSocket 的数据格式进行通信了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f4edd3018914fe6eb38fad6aa3fd2d65.png)\n\n### WebSocket 抓包\n\n我们可以用 WireShark 抓个包，实际看下数据包的情况。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f756ca625523f0f9d40a402465179bbe.png)\n\n上面这张图，注意画了红框的第 `2445` 行报文，是 WebSocket 的**第一次握手**，意思是发起了一次带有 `特殊 Header` 的 HTTP 请求。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/82d65f08dad05e6b537ea06b94224a5f.png)\n\n上面这个图里画了红框的 `4714` 行报文，就是服务器在得到第一次握手后，响应的**第二次握手**，可以看到这也是个 HTTP 类型的报文，返回的状态码是 101。同时可以看到返回的报文 Header 中也带有各种 `WebSocket` 相关的信息，比如 `Sec-WebSocket-Accept`。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/382c7699530ea7e7b22f60bb68af21bd.png)\n\n上面这张图就是全貌了，从截图上的注释可以看出，WebSocket 和 HTTP 一样都是基于 TCP 的协议。**经历了三次 TCP 握手之后，利用 HTTP 协议升级为 WebSocket 协议**。\n\n你在网上可能会看到一种说法：“WebSocket 是基于 HTTP 的新协议”，**其实这并不对**，因为 WebSocket 只有在建立连接时才用到了 HTTP，**升级完成之后就跟 HTTP 没有任何关系了**。\n\n这就好像你喜欢的女生通过你要到了你大学室友的微信，然后他们自己就聊起来了。你能说这个女生是通过你去跟你室友沟通的吗？不能。你跟 HTTP 一样，都只是个**工具人**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/2e9d4b1652bdfa1e3ae4bb24f70a1b5a.png)\n\n这就有点\"**借壳生蛋**\"的那意思。\n\n## HTTP 和 WebSocket 的关系\n\n### WebSocket 的消息格式\n\n上面提到在完成协议升级之后，两端就会用 WebSocket 的数据格式进行通信。\n\n数据包在 WebSocket 中被叫做**帧**，我们来看下它的数据格式长什么样子。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/3a63a86e5d7e72a37b9828fc6e65c21f.png)\n\n这里面字段很多，但我们只需要关注下面这几个。\n\n**opcode 字段**：这个是用来标志这是个**什么类型**的数据帧。比如。\n\n- 等于 1 ，是指text类型（`string`）的数据包\n- 等于 2 ，是二进制数据类型（`[]byte`）的数据包\n- 等于 8 ，是关闭连接的信号\n\n**payload 字段**：存放的是我们**真正想要传输的数据的长度**，单位是**字节**。比如你要发送的数据是 `字符串\"111\"`，那它的长度就是 `3`。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/437a076935f82be1d36960c9a4785fbd.png)\n\n另外，可以看到，我们存放**payload 长度的字段有好几个**，我们既可以用最前面的 `7 bit`, 也可以用后面的 `7+16 bit` 或 `7+64 bit`。\n\n那么问题就来了。\n\n我们知道，在数据层面，大家都是 01 二进制流。我怎么知道**什么情况下应该读 7 bit，什么情况下应该读 7+16 bit 呢？**\n\nWebSocket 会用最开始的 7 bit 做标志位。不管接下来的数据有多大，都**先读最先的 7 个 bit**，根据它的取值决定还要不要再读个 16 bit 或 64 bit。\n\n- 如果 `最开始的 7 bit` 的值是 0~125，那么它就表示了 **payload 全部长度**，只读最开始的 `7 个 bit` 就完事了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/690f5a4deda2de50f3a35eddf0be4d75.png)\n\n- 如果是 `126（0x7E）`。那它表示payload的长度范围在 `126~65535` 之间，接下来还需要**再读 16 bit**。这 16 bit 会包含 payload 的真实长度。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c815c9dabc02fceb42a98c762705af33.png)\n\n- 如果是 `127（0x7F）`。那它表示 payload 的长度范围 `\u003e=65536`，接下来还需要**再读 64 bit**。这 64 bit 会包含 payload 的长度。这能放 2 的 64 次方 byte 的数据，换算一下好多个 TB，肯定够用了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/192b22b4fe46e8dfb7b17549306d5998.png)\n\n**payload data 字段**：这里存放的就是真正要传输的数据，在知道了上面的 payload 长度后，就可以根据这个值去截取对应的数据。\n\n大家有没有发现一个小细节，WebSocket 的数据格式也是**数据头（内含 payload 长度） + payload data** 的形式。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/d449242f1bf41c6f95a5314ec8311d0d.jpeg)\n\n这是因为 TCP 协议本身就是全双工，但直接使用**纯裸 TCP** 去传输数据，会有**粘包**的\"问题\"。为了解决这个问题，上层协议一般会用**消息头+消息体**的格式去重新包装要发的数据。\n\n而**消息头**里一般含有**消息体的长度**，通过这个长度可以去截取真正的消息体。\n\nHTTP 协议和大部分 RPC 协议，以及我们今天介绍的 WebSocket 协议，都是这样设计的。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/b91fedb1856897c231b8fb5932b7b2d2.png)\n\n### WebSocket 的使用场景\n\nWebSocket 完美继承了 TCP 协议的**全双工**能力，并且还贴心的提供了解决粘包的方案。\n\n它适用于**需要服务器和客户端（浏览器）频繁交互**的大部分场景，比如网页/小程序游戏，网页聊天室，以及一些类似飞书这样的网页协同办公软件。\n\n回到文章开头的问题，在使用 WebSocket 协议的网页游戏里，怪物移动以及攻击玩家的行为是**服务器逻辑**产生的，对玩家产生的伤害等数据，都需要由**服务器主动发送给客户端**，客户端获得数据后展示对应的效果。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/31410d2e885aab55c2c588aad754bb5c.png)\n\n## 总结\n\n- TCP 协议本身是**全双工**的，但我们最常用的 HTTP/1.1，虽然是基于 TCP 的协议，但它是**半双工**的，对于大部分需要服务器主动推送数据到客户端的场景，都不太友好，因此我们需要使用支持全双工的 WebSocket 协议。\n- 在 HTTP/1.1 里，只要客户端不问，服务端就不答。基于这样的特点，对于登录页面这样的简单场景，可以使用**定时轮询或者长轮询**的方式实现**服务器推送**(comet)的效果。\n- 对于客户端和服务端之间需要频繁交互的复杂场景，比如网页游戏，都可以考虑使用 WebSocket 协议。\n- WebSocket 和 socket 几乎没有任何关系，只是叫法相似。\n- 正因为各个浏览器都支持 HTTP 协议，所以 WebSocket 会先利用 HTTP 协议加上一些特殊的 Header 头进行握手升级操作，升级成功后就跟 HTTP 没有任何关系了，之后就用 WebSocket 的数据格式进行收发数据。\n\n------\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」关注我***\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/https_ecdhe":{"title":"https_ecdhe","content":"# 3.4 HTTPS ECDHE 握手解析\n\nHTTPS 常用的密钥交换算法有两种，分别是 RSA 和 ECDHE 算法。\n\n其中，RSA 是比较传统的密钥交换算法，它不具备前向安全的性质，因此现在很少服务器使用的。而 ECDHE 算法具有前向安全，所以被广泛使用。\n\n我在上一篇已经介绍了 [RSA 握手的过程](https://mp.weixin.qq.com/s/U9SRLE7jZTB6lUZ6c8gTKg)，今天这一篇就「从理论再到实战抓包」介绍 **ECDHE 算法**。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ecdhe提纲.png)\n\n---\n\n## 离散对数\n\nECDHE 密钥协商算法是 DH 算法演进过来的，所以我们先从 DH 算法说起。\n\nDH 算法是非对称加密算法， 因此它可以用于密钥交换，该算法的核心数学思想是**离散对数**。\n\n是不是听到这个数学概念就怂了？不怕，这次不会说离散对数推导的过程，只简单提一下它的数学公式。\n\n离散对数是「离散 + 对数」的两个数学概念的组合，所以我们先来复习一遍对数。\n\n要说起对数，必然要说指数，因为它们是互为反函数，指数就是幂运算，对数是指数的逆运算。\n\n举个栗子，如果以 2 作为底数，那么指数和对数运算公式，如下图所示：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/指数与对数.png)\n\n那么对于底数为 2 的时候， 32 的对数是 5，64 的对数是 6，计算过程如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/求对数.png)\n\n对数运算的取值是可以连续的，而离散对数的取值是不能连续的，因此也以「离散」得名，\n\n离散对数是在对数运算的基础上加了「模运算」，也就说取余数，对应编程语言的操作符是「%」，也可以用 mod 表示。离散对数的概念如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/离散对数.png)\n\n上图的，底数 a 和模数 p 是离散对数的公共参数，也就说是公开的，b 是真数，i 是对数。知道了对数，就可以用上面的公式计算出真数。但反过来，知道真数却很难推算出对数。\n\n**特别是当模数 p 是一个很大的质数，即使知道底数 a 和真数 b ，在现有的计算机的计算水平是几乎无法算出离散对数的，这就是 DH 算法的数学基础。**\n\n\n\n---\n\n## DH 算法\n\n认识了离散对数，我们来看看 DH 算法是如何密钥交换的。\n\n现假设小红和小明约定使用 DH 算法来交换密钥，那么基于离散对数，小红和小明需要先确定模数和底数作为算法的参数，这两个参数是公开的，用 P 和 G 来代称。\n\n然后小红和小明各自生成一个随机整数作为**私钥**，双方的私钥要各自严格保管，不能泄漏，小红的私钥用 a 代称，小明的私钥用 b 代称。\n\n现在小红和小明双方都有了 P 和 G 以及各自的私钥，于是就可以计算出**公钥**：\n\n- 小红的公钥记作 A，A = G ^ a ( mod P )；\n- 小明的公钥记作 B，B = G ^ b ( mod P )；\n\nA 和 B 也是公开的，因为根据离散对数的原理，从真数（A 和 B）反向计算对数 a 和 b 是非常困难的，至少在现有计算机的计算能力是无法破解的，如果量子计算机出来了，那就有可能被破解，当然如果量子计算机真的出来了，那么密钥协商算法就要做大的升级了。\n\n\n双方交换各自 DH 公钥后，小红手上共有 5 个数：P、G、a、A、B，小明手上也同样共有 5 个数：P、G、b、B、A。\n\n然后小红执行运算：B ^ a ( mod P )，其结果为 K，因为离散对数的幂运算有交换律，所以小明执行运算：A ^ b ( mod P )，得到的结果也是 K。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/dh算法.png)\n\n这个 K 就是小红和小明之间用的**对称加密密钥**，可以作为会话密钥使用。\n\n可以看到，整个密钥协商过程中，小红和小明公开了 4 个信息：P、G、A、B，其中 P、G 是算法的参数，A 和 B 是公钥，而 a、b 是双方各自保管的私钥，黑客无法获取这 2 个私钥，因此黑客只能从公开的 P、G、A、B 入手，计算出离散对数（私钥）。\n\n前面也多次强调， 根据离散对数的原理，如果 P 是一个大数，在现有的计算机的计算能力是很难破解出 私钥 a、b 的，破解不出私钥，也就无法计算出会话密钥，因此 DH 密钥交换是安全的。\n\n---\n\n## DHE 算法\n\n根据私钥生成的方式，DH 算法分为两种实现：\n\n- static DH 算法，这个是已经被废弃了；\n- DHE 算法，现在常用的；\n\nstatic DH 算法里有一方的私钥是静态的，也就说每次密钥协商的时候有一方的私钥都是一样的，一般是服务器方固定，即 a 不变，客户端的私钥则是随机生成的。\n\n于是，DH 交换密钥时就只有客户端的公钥是变化，而服务端公钥是不变的，那么随着时间延长，黑客就会截获海量的密钥协商过程的数据，因为密钥协商的过程有些数据是公开的，黑客就可以依据这些数据暴力破解出服务器的私钥，然后就可以计算出会话密钥了，于是之前截获的加密数据会被破解，所以 **static DH 算法不具备前向安全性**。\n\n既然固定一方的私钥有被破解的风险，那么干脆就让双方的私钥在每次密钥交换通信时，都是随机生成的、临时的，这个方式也就是 DHE 算法，E 全称是 ephemeral（临时性的）。\n\n所以，即使有个牛逼的黑客破解了某一次通信过程的私钥，其他通信过程的私钥仍然是安全的，因为**每个通信过程的私钥都是没有任何关系的，都是独立的，这样就保证了「前向安全」**。\n\n\n----\n\n## ECDHE 算法\n\nDHE 算法由于计算性能不佳，因为需要做大量的乘法，为了提升 DHE 算法的性能，所以就出现了现在广泛用于密钥交换算法 ——  **ECDHE 算法**。\n\nECDHE 算法是在 DHE 算法的基础上利用了 ECC 椭圆曲线特性，可以用更少的计算量计算出公钥，以及最终的会话密钥。\n\n小红和小明使用 ECDHE 密钥交换算法的过程：\n\n- 双方事先确定好使用哪种椭圆曲线，和曲线上的基点 G，这两个参数都是公开的；\n- 双方各自随机生成一个随机数作为**私钥d**，并与基点 G相乘得到**公钥Q**（Q = dG），此时小红的公私钥为 Q1 和 d1，小明的公私钥为 Q2 和 d2；\n- 双方交换各自的公钥，最后小红计算点（x1，y1） = d1Q2，小明计算点（x2，y2） = d2Q1，由于椭圆曲线上是可以满足乘法交换和结合律，所以 d1Q2 = d1d2G = d2d1G = d2Q1 ，因此**双方的 x 坐标是一样的，所以它是共享密钥，也就是会话密钥**。\n\n这个过程中，双方的私钥都是随机、临时生成的，都是不公开的，即使根据公开的信息（椭圆曲线、公钥、基点 G）也是很难计算出椭圆曲线上的离散对数（私钥）。\n\n---\n\n\n## ECDHE 握手过程\n\n知道了 ECDHE 算法基本原理后，我们就结合实际的情况来看看。  \n\n我用 Wireshark 工具抓了用 ECDHE 密钥协商算法的 TSL 握手过程，可以看到是四次握手：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_tls握手.png)\n\n细心的小伙伴应该发现了，**使用了 ECDHE，在 TLS 第四次握手前，客户端就已经发送了加密的 HTTP 数据**，而对于 RSA 握手过程，必须要完成 TLS 四次握手，才能传输应用数据。\n\n所以，**ECDHE 相比 RSA 握手过程省去了一个消息往返的时间**，这个有点「抢跑」的意思，它被称为是「*TLS False Start*」，跟「*TCP Fast Open*」有点像，都是在还没连接完全建立前，就发送了应用数据，这样便提高了传输的效率。\n\n接下来，分析每一个 ECDHE 握手过程。\n\n### TLS 第一次握手\n\n客户端首先会发一个「**Client Hello**」消息，消息里面有客户端使用的 TLS 版本号、支持的密码套件列表，以及生成的**随机数（*Client Random*）**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_clinethello.png)\n\n\n### TLS 第二次握手\n\n服务端收到客户端的「打招呼」，同样也要回礼，会返回「**Server Hello**」消息，消息面有服务器确认的 TLS 版本号，也给出了一个**随机数（*Server Random*）**，然后从客户端的密码套件列表选择了一个合适的密码套件。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_serverhello.png)\n\n不过，这次选择的密码套件就和 RSA 不一样了，我们来分析一下这次的密码套件的意思。\n\n「 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384」\n\n- 密钥协商算法使用 ECDHE；\n- 签名算法使用 RSA；\n- 握手后的通信使用 AES 对称算法，密钥长度 256 位，分组模式是 GCM；\n- 摘要算法使用 SHA384；\n\n接着，服务端为了证明自己的身份，发送「**Certificate**」消息，会把证书也发给客户端。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_certificate.png)\n\n\n这一步就和 RSA 握手过程有很大到区别了，因为服务端选择了 ECDHE 密钥协商算法，所以会在发送完证书后，发送「**Server Key Exchange**」消息。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_serverkey.png)\n\n\n这个过程服务器做了三件事：\n\n- 选择了**名为 x25519 的椭圆曲线**，选好了椭圆曲线相当于椭圆曲线基点 G 也定好了，这些都会公开给客户端；\n- 生成随机数作为服务端椭圆曲线的私钥，保留到本地；\n- 根据基点 G 和私钥计算出**服务端的椭圆曲线公钥**，这个会公开给客户端。\n\n为了保证这个椭圆曲线的公钥不被第三方篡改，服务端会用 RSA 签名算法给服务端的椭圆曲线公钥做个签名。\n\n\n随后，就是「**Server Hello Done**」消息，服务端跟客户端表明：“这些就是我提供的信息，打招呼完毕”。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_serverhellodone.png)\n\n\n至此，TLS 两次握手就已经完成了，目前客户端和服务端通过明文共享了这几个信息：**Client Random、Server Random 、使用的椭圆曲线、椭圆曲线基点 G、服务端椭圆曲线的公钥**，这几个信息很重要，是后续生成会话密钥的材料。\n\n### TLS 第三次握手\n\n客户端收到了服务端的证书后，自然要校验证书是否合法，如果证书合法，那么服务端到身份就是没问题的。校验证书的过程会走证书链逐级验证，确认证书的真实性，再用证书的公钥验证签名，这样就能确认服务端的身份了，确认无误后，就可以继续往下走。\n\n客户端会生成一个随机数作为客户端椭圆曲线的私钥，然后再根据服务端前面给的信息，生成**客户端的椭圆曲线公钥**，然后用「**Client Key Exchange**」消息发给服务端。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_clientkeyexchange.png)\n\n\n至此，双方都有对方的椭圆曲线公钥、自己的椭圆曲线私钥、椭圆曲线基点 G。于是，双方都就计算出点（x，y），其中 x 坐标值双方都是一样的，前面说 ECDHE 算法时候，说  x 是会话密钥，**但实际应用中，x 还不是最终的会话密钥**。\n\n还记得 TLS 握手阶段，客户端和服务端都会生成了一个随机数传递给对方吗？\n\n**最终的会话密钥，就是用「客户端随机数 + 服务端随机数 + x（ECDHE 算法算出的共享密钥） 」三个材料生成的**。\n\n之所以这么麻烦，是因为 TLS 设计者不信任客户端或服务器「伪随机数」的可靠性，为了保证真正的完全随机，把三个不可靠的随机数混合起来，那么「随机」的程度就非常高了，足够让黑客计算不出最终的会话密钥，安全性更高。\n\n\n算好会话密钥后，客户端会发一个「**Change Cipher Spec**」消息，告诉服务端后续改用对称算法加密通信。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_schangecipherspec.png)\n\n\n接着，客户端会发「**Encrypted Handshake Message**」消息，把之前发送的数据做一个摘要，再用对称密钥加密一下，让服务端做个验证，验证下本次生成的对称密钥是否可以正常使用。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/ech_encryptedhandshakemessage.png)\n\n\n### TLS 第四次握手\n\n最后，服务端也会有一个同样的操作，发「**Change Cipher Spec**」和「**Encrypted Handshake Message**」消息，如果双方都验证加密和解密没问题，那么握手正式完成。于是，就可以正常收发加密的 HTTP 请求和响应了。\n\n----\n\n## 总结\n\nRSA  和 ECDHE 握手过程的区别：\n\n- RSA 密钥协商算法「不支持」前向保密，ECDHE 密钥协商算法「支持」前向保密；\n- 使用了 RSA 密钥协商算法，TLS 完成四次握手后，才能进行应用数据传输，而对于 ECDHE 算法，客户端可以不用等服务端的最后一次 TLS 握手，就可以提前发出加密的 HTTP 数据，节省了一个消息的往返时间（这个是 RFC 文档规定的，具体原因文档没有说明，所以这点我也不太明白）；\n- 使用 ECDHE， 在 TLS 第 2 次握手中，会出现服务器端发出的「Server Key Exchange」消息，而 RSA 握手过程没有该消息；\n\n---\n\n参考资料：\n\n1. https://zh.wikipedia.org/wiki/橢圓曲線迪菲-赫爾曼金鑰交換\n2. https://zh.wikipedia.org/wiki/椭圆曲线\n3. https://zh.wikipedia.org/wiki/迪菲-赫爾曼密鑰交換\n4. https://time.geekbang.org/column/article/148188\n5. https://zhuanlan.zhihu.com/p/106967180\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/https_optimize":{"title":"https_optimize","content":"# 3.5 HTTPS 如何优化？\n\n\n由裸数据传输的 HTTP 协议转成加密数据传输的 HTTPS 协议，给应用数据套了个「保护伞」，提高安全性的同时也带来了性能消耗。\n\n因为 HTTPS 相比 HTTP 协议多一个 TLS 协议握手过程，**目的是为了通过非对称加密握手协商或者交换出对称加密密钥**，这个过程最长可以花费掉 2 RTT，接着后续传输的应用数据都得使用对称加密密钥来加密/解密。\n\n为了数据的安全性，我们不得不使用 HTTPS 协议，至今大部分网址都已从 HTTP 迁移至 HTTPS 协议，因此针对 HTTPS 的优化是非常重要的。\n\n这次，就从多个角度来优化 HTTPS。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/优化https提纲.png)\n\n---\n\n## 分析性能损耗\n\n既然要对 HTTPS 优化，那得清楚哪些步骤会产生性能消耗，再对症下药。\n\n产生性能消耗的两个环节：\n\n- 第一个环节，TLS 协议握手过程；\n- 第二个环节，握手后的对称加密报文传输。\n\n对于第二环节，现在主流的对称加密算法 AES、ChaCha20 性能都是不错的，而且一些 CPU 厂商还针对它们做了硬件级别的优化，因此这个环节的性能消耗可以说非常地小。\n\n而第一个环节，TLS 协议握手过程不仅增加了网络延时（最长可以花费掉 2 RTT），而且握手过程中的一些步骤也会产生性能损耗，比如：\n\n- 对于 ECDHE 密钥协商算法，握手过程中会客户端和服务端都需要临时生成椭圆曲线公私钥；\n- 客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，目的是验证服务器的证书是否有被吊销；\n- 双方计算 Pre-Master，也就是对称加密密钥；\n\n为了大家更清楚这些步骤在 TLS 协议握手的哪一个阶段，我画出了这幅图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/tls性能损耗.png)\n\n\n---\n\n## 硬件优化\n\n玩游戏时，如果我们怎么都战胜不了对方，那么有一个最有效、最快的方式来变强，那就是「充钱」，如果还是不行，那说明你充的钱还不够多。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/充钱.jpg)\n\n\n\n对于计算机里也是一样，软件都是跑在物理硬件上，硬件越牛逼，软件跑的也越快，所以如果要优化 HTTPS 优化，最直接的方式就是花钱买性能参数更牛逼的硬件。\n\n但是花钱也要花对方向，**HTTPS 协议是计算密集型，而不是 I/O 密集型**，所以不能把钱花在网卡、硬盘等地方，应该花在 CPU 上。\n\n一个好的 CPU，可以提高计算性能，因为 HTTPS 连接过程中就有大量需要计算密钥的过程，所以这样可以加速 TLS 握手过程。\n\n\n另外，如果可以，应该选择可以**支持 AES-NI 特性的 CPU**，因为这种款式的 CPU 能在指令级别优化了 AES 算法，这样便加速了数据的加解密传输过程。\n\n如果你的服务器是 Linux 系统，那么你可以使用下面这行命令查看 CPU 是否支持 AES-NI 指令集：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/aesni_intel.png)\n\n\n\n如果我们的 CPU 支持 AES-NI 特性，那么对于对称加密的算法应该选择 AES 算法。否则可以选择 ChaCha20 对称加密算法，因为 ChaCha20 算法的运算指令相比 AES 算法会对 CPU 更友好一点。\n\n---\n\n## 软件优化\n\n如果公司预算充足对于新的服务器是可以考虑购买更好的 CPU，但是对于已经在使用的服务器，硬件优化的方式可能就不太适合了，于是就要从软件的方向来优化了。\n\n软件的优化方向可以分层两种，一个是**软件升级**，一个是**协议优化**。\n\n先说第一个软件升级，软件升级就是将正在使用的软件升级到最新版本，因为最新版本不仅提供了最新的特性，也优化了以前软件的问题或性能。比如：\n\n- 将 Linux 内核从 2.x 升级到 4.x；\n- 将 OpenSSL 从 1.0.1 升级到 1.1.1；\n- ...\n\n看似简单的软件升级，对于有成百上千服务器的公司来说，软件升级也跟硬件升级同样是一个棘手的问题，因为要实行软件升级，会花费时间和人力，同时也存在一定的风险，也可能会影响正常的线上服务。\n\n既然如此，我们把目光放到协议优化，也就是在现有的环节下，通过较小的改动，来进行优化。\n\n\n---\n\n## 协议优化\n\n协议的优化就是对「密钥交换过程」进行优化。\n\n### 密钥交换算法优化\n\nTLS 1.2 版本如果使用的是 RSA 密钥交换算法，那么需要 4 次握手，也就是要花费 2 RTT，才可以进行应用数据的传输，而且 RSA 密钥交换算法不具备前向安全性。\n\n总之使用 **RSA 密钥交换算法的 TLS 握手过程，不仅慢，而且安全性也不高**。\n\n因此如果可以，尽量**选用 ECDHE 密钥交换**算法替换 RSA 算法，因为该算法由于支持「False Start」，它是“抢跑”的意思，客户端可以在 TLS 协议的第 3 次握手后，第 4 次握手前，发送加密的应用数据，以此将 **TLS 握手的消息往返由 2 RTT 减少到 1 RTT，而且安全性也高，具备前向安全性**。\n\nECDHE 算法是基于椭圆曲线实现的，不同的椭圆曲线性能也不同，应该尽量**选择 x25519 曲线**，该曲线是目前最快的椭圆曲线。\n\n比如在 Nginx 上，可以使用 ssl_ecdh_curve 指令配置想使用的椭圆曲线，把优先使用的放在前面：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/ssl_ecdh_curve.png)\n\n\n对于对称加密算法方面，如果对安全性不是特别高的要求，可以**选用 AES_128_GCM**，它比 AES_256_GCM 快一些，因为密钥的长度短一些。\n\n比如在 Nginx 上，可以使用 ssl_ciphers 指令配置想使用的非对称加密算法和对称加密算法，也就是密钥套件，而且把性能最快最安全的算法放在最前面：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/ssl_ciphers.png)\n\n\n\n### TLS 升级\n\n当然，如果可以，直接把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，**完成 TLS 握手只要 1 RTT**，而且安全性更高。\n\n在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手），然后计算出最终的会话密钥，下图的左边部分就是 TLS 1.2 的握手过程：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/tls1.2and1.3.png)\n\n\n上图的右边部分就是 TLS 1.3 的握手过程，可以发现 **TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，于是这样就减少到只需 1 RTT 就能完成 TLS 握手**。\n\n怎么合并的呢？具体的做法是，客户端在  Client Hello 消息里带上了支持的椭圆曲线，以及这些椭圆曲线对应的公钥。\n\n服务端收到后，选定一个椭圆曲线等参数，然后返回消息时，带上服务端这边的公钥。经过这 1 个 RTT，双方手上已经有生成会话密钥的材料了，于是客户端计算出会话密钥，就可以进行应用数据的加密传输了。\n\n\n而且，TLS1.3 对密码套件进行“减肥”了，\n**对于密钥交换算法，废除了不支持前向安全性的  RSA 和 DH 算法，只支持 ECDHE 算法**。\n\n对于对称加密和签名算法，只支持目前最安全的几个密码套件，比如 openssl 中仅支持下面 5 种密码套件：\n\n- TLS_AES_256_GCM_SHA384\n- TLS_CHACHA20_POLY1305_SHA256\n- TLS_AES_128_GCM_SHA256\n- TLS_AES_128_CCM_8_SHA256\n- TLS_AES_128_CCM_SHA256\n\n之所以 TLS1.3   仅支持这么少的密码套件，是因为 TLS1.2 由于支持各种古老且不安全的密码套件，中间人可以利用降级攻击，伪造客户端的 Client Hello 消息，替换客户端支持的密码套件为一些不安全的密码套件，使得服务器被迫使用这个密码套件进行 HTTPS 连接，从而破解密文。\n\n---\n\n## 证书优化\n\n为了验证的服务器的身份，服务器会在 TLS 握手过程中，把自己的证书发给客户端，以此证明自己身份是可信的。\n\n对于证书的优化，可以有两个方向：\n\n- 一个是**证书传输**，\n- 一个是**证书验证**；\n\n### 证书传输优化\n\n要让证书更便于传输，那必然是减少证书的大小，这样可以节约带宽，也能减少客户端的运算量。所以，**对于服务器的证书应该选择椭圆曲线（ECDSA）证书，而不是 RSA 证书，因为在相同安全强度下， ECC 密钥长度比 RSA 短的多**。 \n\n### 证书验证优化\n\n客户端在验证证书时，是个复杂的过程，会走证书链逐级验证，验证的过程不仅需要「用 CA 公钥解密证书」以及「用签名算法验证证书的完整性」，而且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性。\n\n这个访问过程是 HTTP 访问，因此又会产生一系列网络通信的开销，如 DNS 查询、建立连接、收发数据等。\n\n#### CRL \n\nCRL 称为证书吊销列表（*Certificate Revocation List*），这个列表是由 CA 定期更新，列表内容都是被撤销信任的证书序号，如果服务器的证书在此列表，就认为证书已经失效，不在的话，则认为证书是有效的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/crl.png)\n\n但是 CRL 存在两个问题：\n\n- 第一个问题，由于 CRL 列表是由 CA 维护的，定期更新，如果一个证书刚被吊销后，客户端在更新 CRL 之前还是会信任这个证书，**实时性较差**；\n- 第二个问题，**随着吊销证书的增多，列表会越来越大，下载的速度就会越慢**，下载完客户端还得遍历这么大的列表，那么就会导致客户端在校验证书这一环节的延时很大，进而拖慢了 HTTPS 连接。\n\n#### OCSP \n\n因此，现在基本都是使用 OCSP ，名为在线证书状态协议（*Online Certificate Status Protocol*）来查询证书的有效性，它的工作方式是**向 CA 发送查询请求，让 CA 返回证书的有效状态**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/ocsp.png)\n\n\n不必像 CRL 方式客户端需要下载大大的列表，还要从列表查询，同时因为可以实时查询每一张证书的有效性，解决了 CRL 的实时性问题。\n\n\nOCSP 需要向  CA 查询，因此也是要发生网络请求，而且还得看  CA 服务器的“脸色”，如果网络状态不好，或者 CA 服务器繁忙，也会导致客户端在校验证书这一环节的延时变大。\n\n#### OCSP Stapling\n\n于是为了解决这一个网络开销，就出现了 OCSP Stapling，其原理是：服务器向 CA 周期性地查询证书状态，获得一个带有时间戳和签名的响应结果并缓存它。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/opscp-stapling.png)\n\n\n当有客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握手过程中发给客户端。由于有签名的存在，服务器无法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询。\n\n\n\n---\n\n## 会话复用\n\nTLS 握手的目的就是为了协商出会话密钥，也就是对称加密密钥，那我们如果我们把首次 TLS 握手协商的对称加密密钥缓存起来，待下次需要建立 HTTPS 连接时，直接「复用」这个密钥，不就减少 TLS 握手的性能损耗了吗？\n\n这种方式就是**会话复用**（*TLS session resumption*），会话复用分两种：\n\n- 第一种叫 Session ID；\n- 第二种叫 Session Ticket；\n\n### Session ID\n\nSession ID 的工作原理是，**客户端和服务器首次  TLS 握手连接后，双方会在内存缓存会话密钥，并用唯一的 Session ID 来标识**，Session ID 和会话密钥相当于 key-value 的关系。\n\n\n当客户端再次连接时，hello 消息里会带上 Session ID，服务器收到后就会从内存找，如果找到就直接用该会话密钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信。当然为了安全性，内存中的会话密钥会定期失效。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/sessionid.png)\n\n\n但是它有两个缺点：\n\n- 服务器必须保持每一个客户端的会话密钥，随着客户端的增多，**服务器的内存压力也会越大**。\n- 现在网站服务一般是由多台服务器通过负载均衡提供服务的，**客户端再次连接不一定会命中上次访问过的服务器**，于是还要走完整的 TLS 握手过程；\n\n\n### Session Ticket\n\n为了解决 Session ID 的问题，就出现了 Session Ticket，**服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端**，类似于 HTTP 的 Cookie。\n\n客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。\n\n客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，如果没问题，就可以恢复会话了，开始加密通信。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/ticket.png)\n\n对于集群服务器的话，**要确保每台服务器加密 「会话密钥」的密钥是一致的**，这样客户端携带 Ticket 访问任意一台服务器时，都能恢复会话。\n\nSession ID 和 Session Ticket **都不具备前向安全性**，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会话密钥」，前面劫持的通信密文都会被破解。\n\n同时应对**重放攻击**也很困难，这里简单介绍下重放攻击工作的原理。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/重放攻击.png)\n\n\n假设 Alice 想向 Bob 证明自己的身份。 Bob 要求 Alice 的密码作为身份证明，爱丽丝应尽全力提供（可能是在经过如哈希函数的转换之后）。与此同时，Eve 窃听了对话并保留了密码（或哈希）。\n\n交换结束后，Eve（冒充 Alice ）连接到 Bob。当被要求提供身份证明时，Eve 发送从 Bob 接受的最后一个会话中读取的 Alice 的密码（或哈希），从而授予 Eve 访问权限。\n\n重放攻击的危险之处在于，如果中间人截获了某个客户端的 Session ID 或 Session Ticket 以及 POST 报文，而一般 POST 请求会改变数据库的数据，中间人就可以利用此截获的报文，不断向服务器发送该报文，这样就会导致数据库的数据被中间人改变了，而客户是不知情的。\n\n避免重放攻击的方式就是需要**对会话密钥设定一个合理的过期时间**。\n\n\n### Pre-shared Key\n\n前面的 Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。\n\n而 TLS1.3  更为牛逼，对于重连 TLS1.3 只需要 **0 RTT**，原理和 Ticket 类似，只不过在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这种方式叫 **Pre-shared Key**。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/0-RTT.png)\n\n同样的，Pre-shared Key 也有重放攻击的危险。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https优化/0-rtt-attack.png)\n\n如上图，假设中间人通过某种方式，截获了客户端使用会话重用技术的 POST 请求，通常 POST 请求是会改变数据库的数据，然后中间人就可以把截获的这个报文发送给服务器，服务器收到后，也认为是合法的，于是就恢复会话，致使数据库的数据又被更改，但是此时用户是不知情的。\n\n所以，应对重放攻击可以给会话密钥设定一个合理的过期时间，以及只针对安全的 HTTP 请求如 GET/HEAD 使用会话重用。\n\n---\n\n\n## 总结\n\n对于硬件优化的方向，因为 HTTPS 是属于计算密集型，应该选择计算力更强的 CPU，而且最好选择**支持 AES-NI 特性的 CPU**，这个特性可以在硬件级别优化 AES 对称加密算法，加快应用数据的加解密。\n\n对于软件优化的方向，如果可以，把软件升级成较新的版本，比如将 Linux 内核 2.X 升级成 4.X，将 openssl 1.0.1 升级到 1.1.1，因为新版本的软件不仅会提供新的特性，而且还会修复老版本的问题。\n\n对于协议优化的方向：\n\n- 密钥交换算法应该选择 **ECDHE 算法**，而不用 RSA 算法，因为 ECDHE 算法具备前向安全性，而且客户端可以在第三次握手之后，就发送加密应用数据，节省了 1 RTT。\n- 将 TLS1.2 升级 **TLS1.3**，因为 TLS1.3 的握手过程只需要 1 RTT，而且安全性更强。\n\n对于证书优化的方向：\n\n- 服务器应该选用 **ECDSA 证书**，而非 RSA 证书，因为在相同安全级别下，ECC 的密钥长度比 RSA 短很多，这样可以提高证书传输的效率；\n- 服务器应该开启 **OCSP Stapling** 功能，由服务器预先获得 OCSP 的响应，并把响应结果缓存起来，这样 TLS 握手的时候就不用再访问 CA 服务器，减少了网络通信的开销，提高了证书验证的效率；\n\n对于重连 HTTPS 时，我们可以使用一些技术让客户端和服务端使用上一次 HTTPS 连接使用的会话密钥，直接恢复会话，而不用再重新走完整的 TLS 握手过程。\n\n常见的**会话重用**技术有 Session ID 和 Session Ticket，用了会话重用技术，当再次重连 HTTPS 时，只需要 1 RTT 就可以恢复会话。对于 TLS1.3 使用 Pre-shared Key 会话重用技术，只需要 0 RTT 就可以恢复会话。\n\n这些会话重用技术虽然好用，但是存在一定的安全风险，它们不仅不具备前向安全，而且有重放攻击的风险，所以应当对会话密钥设定一个合理的过期时间。\n\n---\n\n参考资料：\n\n1. http://www.doc88.com/p-8621583210895.html\n2. https://zhuanlan.zhihu.com/p/33685085\n3. https://en.wikipedia.org/wiki/Replay_attack\n4. https://en.wikipedia.org/wiki/Downgrade_attack\n5. https://www.cnblogs.com/racent-Z/p/14011056.html\n6. http://www.guoyanbin.com/a-detailed-look-at-rfc-8446-a-k-a-tls-1-3/\n7. https://www.thesslstore.com/blog/crl-explained-what-is-a-certificate-revocation-list/\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/2_http/https_rsa":{"title":"https_rsa","content":"# 3.3 HTTPS RSA 握手解析\n\n我前面讲，简单给大家介绍了的 HTTPS 握手过程，但是还不够细！\n\n只讲了比较基础的部分，所以这次我们再来深入一下 HTTPS，用**实战抓包**的方式，带大家再来窥探一次 HTTPS。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/https提纲.png)\n\n对于还不知道对称加密和非对称加密的同学，你先复习我以前的这篇文章[「硬核！30 张图解 HTTP 常见的面试题」，](https://mp.weixin.qq.com/s/bUy220-ect00N4gnO0697A)本篇文章默认大家已经具备了这些知识。\n\n---\n\n## TLS 握手过程\n\nHTTP 由于是明文传输，所谓的明文，就是说客户端与服务端通信的信息都是肉眼可见的，随意使用一个抓包工具都可以截获通信的内容。\n\n所以安全上存在以下三个风险：\n\n- *窃听风险*，比如通信链路上可以获取通信内容，用户号容易没。\n- *篡改风险*，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。\n- *冒充风险*，比如冒充淘宝网站，用户钱容易没。\n\nHTTP**S** 在 HTTP 与 TCP 层之间加入了 TLS 协议，来解决上述的风险。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/19-HTTPS与HTTP.png)\n\nTLS 协议是如何解决 HTTP 的风险的呢？\n\n- *信息加密*： HTTP 交互信息是被加密的，第三方就无法被窃取；\n- *校验机制*：校验信息传输过程中是否有被第三方篡改过，如果被篡改过，则会有警告提示；\n- *身份证书*：证明淘宝是真的淘宝网；\n\n可见，有了 TLS 协议，能保证 HTTP 通信是安全的了，那么在进行 HTTP 通信前，需要先进行 TLS 握手。TLS 的握手过程，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/tls握手.png)\n\n\n上图简要概述了 TLS 的握手过程，其中每一个「框」都是一个记录（*record*），记录是 TLS 收发数据的基本单位，类似于 TCP 里的 segment。多个记录可以组合成一个 TCP 包发送，所以**通常经过「四个消息」就可以完成 TLS 握手，也就是需要 2个 RTT 的时延**，然后就可以在安全的通信环境里发送 HTTP 报文，实现 HTTPS 协议。\n\n所以可以发现，HTTPS 是应用层协议，需要先完成 TCP 连接建立，然后走 TLS 握手过程后，才能建立通信安全的连接。\n\n事实上，不同的密钥交换算法，TLS 的握手过程可能会有一些区别。\n\n这里先简单介绍下密钥交换算法，因为考虑到性能的问题，所以双方在加密应用信息时使用的是对称加密密钥，而对称加密密钥是不能被泄漏的，为了保证对称加密密钥的安全性，所以使用非对称加密的方式来保护对称加密密钥的协商，这个工作就是密钥交换算法负责的。\n\n接下来，我们就以最简单的 `RSA` 密钥交换算法，来看看它的 TLS 握手过程。\n\n---\n\n## RSA 握手过程\n\n传统的 TLS 握手基本都是使用 RSA 算法来实现密钥交换的，在将 TLS 证书部署服务端时，证书文件其实就是服务端的公钥，会在 TLS 握手阶段传递给客户端，而服务端的私钥则一直留在服务端，一定要确保私钥不能被窃取。\n\n在 RSA 密钥协商算法中，客户端会生成随机密钥，并使用服务端的公钥加密后再传给服务端。根据非对称加密算法，公钥加密的消息仅能通过私钥解密，这样服务端解密后，双方就得到了相同的密钥，再用它加密应用消息。\n\n我用 Wireshark 工具抓了用 RSA 密钥交换的 TLS 握手过程，你可以从下面看到，一共经历了四次握手：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/tls四次握手.png)\n\n对应 Wireshark 的抓包，我也画了一幅图，你可以从下图很清晰地看到该过程：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/https_rsa.png)\n\n\n那么，接下来针对每一个 TLS 握手做进一步的介绍。\n\n### TLS 第一次握手\n\n客户端首先会发一个「**Client Hello**」消息，字面意思我们也能理解到，这是跟服务器「打招呼」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/clienthello.png)\n\n消息里面有客户端使用的 TLS 版本号、支持的密码套件列表，以及生成的**随机数（*Client Random*）**，这个随机数会被服务端保留，它是生成对称加密密钥的材料之一。\n\n\n### TLS 第二次握手\n\n当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否支持，和从密码套件列表中选择一个密码套件，以及生成**随机数（*Server Random*）**。\n\n接着，返回「**Server Hello**」消息，消息里面有服务器确认的 TLS 版本号，也给出了随机数（Server Random），然后从客户端的密码套件列表选择了一个合适的密码套件。\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/serverhello.png)\n\n可以看到，服务端选择的密码套件是  “Cipher Suite: TLS_RSA_WITH_AES_128_GCM_SHA256”。\n\n这个密码套件看起来真让人头晕，好一大串，但是其实它是有固定格式和规范的。基本的形式是「**密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法**」， 一般 WITH 单词前面有两个单词，第一个单词是约定密钥交换的算法，第二个单词是约定证书的验证算法。比如刚才的密码套件的意思就是：\n\n- 由于 WITH 单词只有一个 RSA，则说明握手时密钥交换算法和签名算法都是使用 RSA；\n- 握手后的通信使用 AES 对称算法，密钥长度 128 位，分组模式是 GCM；\n- 摘要算法 SHA256 用于消息认证和产生随机数；\n\n\n就前面这两个客户端和服务端相互「打招呼」的过程，客户端和服务端就已确认了 TLS 版本和使用的密码套件，而且你可能发现客户端和服务端都会各自生成一个随机数，并且还会把随机数传递给对方。\n\n那这个随机数有啥用呢？其实这两个随机数是后续作为生成「会话密钥」的条件，所谓的会话密钥就是数据传输时，所使用的对称加密密钥。\n\n然后，服务端为了证明自己的身份，会发送「**Server Certificate**」给客户端，这个消息里含有数字证书。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/certificate.png)\n\n\n随后，服务端发了「**Server Hello Done**」消息，目的是告诉客户端，我已经把该给你的东西都给你了，本次打招呼完毕。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/serverhellodone.png)\n\n\n### 客户端验证证书\n\n在这里刹个车，客户端拿到了服务端的数字证书后，要怎么校验该数字证书是真实有效的呢？\n\n#### 数字证书和 CA 机构\n\n在说校验数字证书是否可信的过程前，我们先来看看数字证书是什么，一个数字证书通常包含了：\n\n- 公钥；\n- 持有者信息；\n- 证书认证机构（CA）的信息；\n- CA 对这份文件的数字签名及使用的算法；\n- 证书有效期；\n- 还有一些其他额外信息；\n\n那数字证书的作用，是用来认证公钥持有者的身份，以防止第三方进行冒充。说简单些，证书就是用来告诉客户端，该服务端是否是合法的，因为只有证书合法，才代表服务端身份是可信的。\n\n我们用证书来认证公钥持有者的身份（服务端的身份），那证书又是怎么来的？又该怎么认证证书呢？\n\n为了让服务端的公钥被大家信任，服务端的证书都是由 CA （*Certificate Authority*，证书认证机构）签名的，CA 就是网络世界里的公安局、公证中心，具有极高的可信度，所以由它来给各个公钥签名，信任的一方签发的证书，那必然证书也是被信任的。\n\n之所以要签名，是因为签名的作用可以避免中间人在获取证书时对证书内容的篡改。\n\n#### 数字证书签发和验证流程\n\n如下图图所示，为数字证书签发和验证流程：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/证书的校验.png)\n\nCA 签发证书的过程，如上图左边部分：\n\n- 首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值；\n- 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；\n- 最后将 Certificate Signature 添加在文件证书上，形成数字证书；\n\n客户端校验服务端的数字证书的过程，如上图右边部分：\n\n- 首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1；\n- 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ；\n- 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。\n\n#### 证书链\n\n但事实上，证书的验证过程中还存在一个证书信任链的问题，因为我们向 CA 申请的证书一般不是根证书签发的，而是由中间证书签发的，比如百度的证书，从下图你可以看到，证书的层级有三级：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/baidu证书.png)\n\n对于这种三级层级关系的证书的验证过程如下：\n\n- 客户端收到 baidu.com 的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公钥去验证 baidu.com 证书是否可信。于是，客户端根据 baidu.com 证书中的签发者，找到该证书的颁发机构是 “GlobalSign Organization Validation CA - SHA256 - G2”，然后向 CA 请求该中间证书。\n- 请求到证书后发现 “GlobalSign Organization Validation CA - SHA256 - G2” 证书是由 “GlobalSign Root CA” 签发的，由于 “GlobalSign Root CA” 没有再上级签发机构，说明它是根证书，也就是自签证书。应用软件会检查此证书是否已预载于根证书清单上，如果有，则可以利用根证书中的公钥去验证 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，如果发现验证通过，就认为该中间证书是可信的。\n- “GlobalSign Organization Validation CA - SHA256 - G2” 证书被信任后，可以使用 “GlobalSign Organization Validation CA - SHA256 - G2” 证书中的公钥去验证 baidu.com 证书的可信性，如果验证通过，就可以信任 baidu.com 证书。\n\n在这四个步骤中，最开始客户端只信任根证书 GlobalSign Root CA 证书的，然后 “GlobalSign Root CA” 证书信任 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，而 “GlobalSign Organization Validation CA - SHA256 - G2” 证书又信任 baidu.com 证书，于是客户端也信任 baidu.com 证书。\n\n总括来说，由于用户信任 GlobalSign，所以由 GlobalSign 所担保的 baidu.com 可以被信任，另外由于用户信任操作系统或浏览器的软件商，所以由软件商预载了根证书的 GlobalSign 都可被信任。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/用户信任.png)\n\n\n操作系统里一般都会内置一些根证书，比如我的 MAC 电脑里内置的根证书有这么多：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/系统根证书.png)\n\n这样的一层层地验证就构成了一条信任链路，整个证书信任链验证流程如下图所示：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/证书链.png)\n\n最后一个问题，为什么需要证书链这么麻烦的流程？Root CA 为什么不直接颁发证书，而是要搞那么多中间层级呢？\n\n这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。\n\n### TLS 第三次握手\n\n客户端验证完证书后，认为可信则继续往下走。\n\n接着，客户端就会生成一个新的**随机数  (*pre-master*)**，用服务器的 RSA 公钥加密该随机数，通过「**Client Key Exchange**」消息传给服务端。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/clietnkeyexchange.png)\n\n服务端收到后，用 RSA 私钥解密，得到客户端发来的随机数 (pre-master)。\n\n至此，**客户端和服务端双方都共享了三个随机数，分别是 Client Random、Server Random、pre-master**。\n\n于是，双方根据已经得到的三个随机数，生成**会话密钥（Master Secret）**，它是对称密钥，用于对后续的 HTTP 请求/响应的数据加解密。\n\n\n生成完「会话密钥」后，然后客户端发一个「**Change Cipher Spec**」，告诉服务端开始使用加密方式发送消息。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/cipherspecmessage.png)\n\n然后，客户端再发一个「**Encrypted Handshake Message（Finishd）**」消息，把之前所有发送的数据做个**摘要**，再用会话密钥（master secret）加密一下，让服务器做个验证，验证加密通信「是否可用」和「之前握手信息是否有被中途篡改过」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/https/encryptd.png)\n\n可以发现，「Change Cipher Spec」之前传输的 TLS 握手数据都是明文，之后都是对称密钥加密的密文。\n\n### TLS 第四次握手\n\n服务器也是同样的操作，发「**Change Cipher Spec**」和「**Encrypted Handshake Message**」消息，如果双方都验证加密和解密没问题，那么握手正式完成。\n\n最后，就用「会话密钥」加解密 HTTP 请求和响应了。\n\n---\n\n## RSA 算法的缺陷\n\n**使用 RSA 密钥协商算法的最大问题是不支持前向保密**。\n\n因为客户端传递随机数（用于生成对称加密密钥的条件之一）给服务端时使用的是公钥加密的，服务端收到后，会用私钥解密得到随机数。所以一旦服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解。\n\n\n为了解决这个问题，后面就出现了 ECDHE 密钥协商算法，我们现在大多数网站使用的正是 ECDHE 密钥协商算法，关于 ECDHE 握手的过程，将在下一篇揭晓。\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/challenge_ack":{"title":"challenge_ack","content":"# 4.9 已建立连接的 TCP，收到 SYN 会发生什么？\n\n大家好，我是小林。\n\n昨晚有位读者问了我这么个问题：\n\n![](https://img-blog.csdnimg.cn/ea1c6e0165f04232ab02046132e63d0f.jpg)\n\n\n大概意思是，一个已经建立的 TCP 连接，客户端中途宕机了，而服务端此时也没有数据要发送，一直处于 Established 状态，客户端恢复后，向服务端建立连接，此时服务端会怎么处理？\n\n看过我的图解网络的读者都知道，TCP 连接是由「四元组」唯一确认的。\n\n然后这个场景中，客户端的 IP、服务端 IP、目的端口并没有变化，所以这个问题关键要看客户端发送的 SYN 报文中的源端口是否和上一次连接的源端口相同。\n\n**1. 客户端的 SYN 报文里的端口号与历史连接不相同**\n\n如果客户端恢复后发送的 SYN 报文中的源端口号跟上一次连接的源端口号不一样，此时服务端会认为是新的连接要建立，于是就会通过三次握手来建立新的连接。\n\n那旧连接里处于 Established 状态的服务端最后会怎么样呢？\n\n如果服务端发送了数据包给客户端，由于客户端的连接已经被关闭了，此时客户的内核就会回 RST 报文，服务端收到后就会释放连接。\n\n如果服务端一直没有发送数据包给客户端，在超过一段时间后，TCP 保活机制就会启动，检测到客户端没有存活后，接着服务端就会释放掉该连接。\n\n**2. 客户端的 SYN 报文里的端口号与历史连接相同**\n\n如果客户端恢复后，发送的 SYN 报文中的源端口号跟上一次连接的源端口号一样，也就是处于 Established 状态的服务端收到了这个 SYN 报文。\n\n大家觉得服务端此时会做什么处理呢？\n- 丢掉 SYN 报文？\n- 回复 RST 报文？\n- 回复 ACK 报文？\n\n刚开始我看到这个问题的时候，也是没有思路的，因为之前没关注过，然后这个问题不能靠猜，所以我就看了 RFC 规范和看了 Linux 内核源码，最终知道了答案。\n\n我不卖关子，先直接说答案。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/est_syn.png)\n\n**处于 Established 状态的服务端，如果收到了客户端的 SYN 报文（注意此时的 SYN 报文其实是乱序的，因为 SYN 报文的初始化序列号其实是一个随机数），会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK。**\n\n**接着，客户端收到这个 Challenge ACK，发现确认号（ack num）并不是自己期望收到的，于是就会回 RST 报文，服务端收到后，就会释放掉该连接。**\n\n## RFC 文档解释\n\nRFC 793 文档里的第 34 页里，有说到这个例子。\n\n![](https://img-blog.csdnimg.cn/873ad18443c040708c415bab6592ae41.png)\n\n原文的解释我也贴出来给大家看看。\n\n- When the SYN arrives at line 3, TCP B, being in a synchronized state,\nand the incoming segment outside the window, responds with an\nacknowledgment indicating what sequence it next expects to hear (ACK\n100).\n- TCP A sees that this segment does not acknowledge anything it\nsent and, being unsynchronized, sends a reset (RST) because it has\ndetected a half-open connection.\n- TCP B aborts at line 5.  \n- TCP A willcontinue to try to Established the connection;\n\n我就不瞎翻译了，意思和我在前面用中文说的解释差不多。\n\n## 源码分析\n处于 Established 状态的服务端如果收到了客户端的 SYN 报文时，内核会调用这些函数：\n\n```csharp\ntcp_v4_rcv\n  -\u003e tcp_v4_do_rcv\n    -\u003e tcp_rcv_Establisheded\n      -\u003e tcp_validate_incoming\n        -\u003e tcp_send_ack\n```\n\n\n我们只关注 tcp_validate_incoming 函数是怎么处理 SYN 报文的，精简后的代码如下：\n\n![](https://img-blog.csdnimg.cn/780bc02c8fa940c0a320a5916b216c21.png)\n\n从上面的代码实现可以看到，处于 Established 状态的服务端，在收到报文后，首先会判断序列号是否在窗口内，如果不在，则看看 RST 标记有没有被设置，如果有就会丢掉。然后如果没有 RST 标志，就会判断是否有 SYN 标记，如果有 SYN 标记就会跳转到 syn_challenge 标签，然后执行 tcp_send_challenge_ack 函数。\n\ntcp_send_challenge_ack 函数里就会调用 tcp_send_ack 函数来回复一个携带了正确序列号和确认号的 ACK 报文。\n\n## 如何关闭一个 TCP 连接？\n\n这里问题大家这么一个问题，如何关闭一个 TCP 连接？\n\n可能大家第一反应是「杀掉进程」不就行了吗？\n\n是的，这个是最粗暴的方式，杀掉客户端进程和服务端进程影响的范围会有所不同：\n- 在客户端杀掉进程的话，就会发送 FIN 报文，来断开这个客户端进程与服务端建立的所有 TCP 连接，这种方式影响范围只有这个客户端进程所建立的连接，而其他客户端或进程不会受影响。\n- 而在服务端杀掉进程影响就大了，此时所有的 TCP 连接都会被关闭，服务端无法继续提供访问服务。\n\n所以，关闭进程的方式并不可取，最好的方式要精细到关闭某一条 TCP 连接。\n\n有的小伙伴可能会说，伪造一个四元组相同的 RST 报文不就行了？\n\n这个思路很好，但是不要忘了还有个序列号的问题，你伪造的 RST 报文的序列号一定能被对方接受吗？\n\n如果 RST 报文的序列号不是对方期望收到的序列号，这个 RST 报文会被对方丢弃的，就达不到关闭的连接的效果。\n\n举个例子，下面这个场景，客户端发送了一个长度为 100 的 TCP 数据报文，服务端收到后响应了 ACK 报文，表示收到了这个 TCP 数据报文。**服务端响应的这个 ACK 报文中的确认号（ack = x + 100）就是表明服务端下一次期望收到的序列号是 x + 100**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/rst合法.png)\n\n所以，**要伪造一个能关闭 TCP 连接的 RST 报文，必须同时满足「四元组相同」和「序列号是对方期望的」这两个条件。**\n\n直接伪造符合预期的序列号是比较困难，因为如果一个正在传输数据的 TCP 连接，序列号都是时刻都在变化，因此很难刚好伪造一个正确序列号的 RST 报文。\n\n### killcx 的工具\n\n办法还是有的，**我们可以伪造一个四元组相同的 SYN 报文，来拿到“合法”的序列号！**\n\n正如我们最开始学到的，如果处于 Established 状态的服务端，收到四元组相同的 SYN 报文后，**会回复一个 Challenge ACK，这个 ACK 报文里的「确认号」，正好是服务端下一次想要接收的序列号，说白了，就是可以通过这一步拿到服务端下一次预期接收的序列号。**\n\n**然后用这个确认号作为 RST 报文的序列号，发送给服务端，此时服务端会认为这个 RST 报文里的序列号是合法的，于是就会释放连接！**\n\n在 Linux 上有个叫 killcx 的工具，就是基于上面这样的方式实现的，它会主动发送 SYN 包获取 SEQ/ACK 号，然后利用 SEQ/ACK 号伪造两个 RST 报文分别发给客户端和服务端，这样双方的 TCP 连接都会被释放，这种方式活跃和非活跃的 TCP 连接都可以杀掉。\n\n\nkillcx 的工具使用方式也很简单，如果在服务端执行 killcx 工具，只需指明客户端的 IP 和端口号，如果在客户端执行 killcx 工具，则就指明服务端的  IP 和端口号。\n\n```csharp\n./killcx \u003cIP地址\u003e:\u003c端口号\u003e\n```\nkillcx 工具的工作原理，如下图，下图是在客户端执行 killcx 工具。\n\n![](https://img-blog.csdnimg.cn/95592346a9a747819cd27741a660213c.png)\n\n它伪造客户端发送 SYN 报文，服务端收到后就会回复一个携带了正确「序列号和确认号」的 ACK 报文（Challenge ACK），然后就可以利用这个 ACK 报文里面的信息，伪造两个 RST 报文：\n- 用 Challenge ACK 里的确认号伪造 RST 报文发送给服务端，服务端收到 RST 报文后就会释放连接。\n- 用 Challenge ACK 里的序列号伪造 RST 报文发送给客户端，客户端收到 RST 也会释放连接。\n\n正是通过这样的方式，成功将一个 TCP 连接关闭了！\n\n这里给大家贴一个使用 killcx 工具关闭连接的抓包图，大家多看看序列号和确认号的变化。\n\n![](https://img-blog.csdnimg.cn/71cbefee5ab741018386b6a37f492614.png?)\n\n所以，以后抓包中，如果莫名奇妙出现一个 SYN 包，有可能对方接下来想要对你发起的 RST 攻击，直接将你的 TCP 连接断开！\n\n怎么样，很巧妙吧！\n\n### tcpkill 的工具\n\n除了 killcx 工具能关闭 TCP 连接，还有 tcpkill 工具也可以做到。\n\n这两个工具都是通过伪造 RST 报文来关闭指定的 TCP 连接，但是它们拿到正确的序列号的实现方式是不同的。\n\n- tcpkill 工具是在双方进行 TCP 通信时，拿到对方下一次期望收到的序列号，然后将序列号填充到伪造的 RST 报文，并将其发送给对方，达到关闭 TCP 连接的效果。\n- killcx 工具是主动发送一个 SYN 报文，对方收到后会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK，这时就可以拿到对方下一次期望收到的序列号，然后将序列号填充到伪造的 RST 报文，并将其发送给对方，达到关闭 TCP 连接的效果。\n\n可以看到， 这两个工具在获取对方下一次期望收到的序列号的方式是不同的。\n\ntcpkill 工具属于被动获取，就是在双方进行 TCP 通信的时候，才能获取到正确的序列号，很显然**这种方式无法关闭非活跃的 TCP 连接**，只能用于关闭活跃的 TCP 连接。因为如果这条 TCP 连接一直没有任何数据传输，则就永远获取不到正确的序列号。\n\nkillcx 工具则是属于主动获取，它是主动发送一个 SYN 报文，通过对方回复的 Challenge ACK 来获取正确的序列号，所以这种方式**无论 TCP 连接是否活跃，都可以关闭**。\n\n接下来，我就用这 tcpkill 工具来做个实验。\n\n在这里， 我用 nc 工具来模拟一个 TCP 服务端，监听 8888 端口。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill1.png)\n\n接着，在客户端机子上，用 nc 工具模拟一个 TCP 客户端，连接我们刚才启动的服务端，并且指定了客户端的端口为 11111。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill2.png)\n\n这时候， 服务端就可以看到这条 TCP 连接了。\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill3.png)\n\n注意，我这台服务端的公网 IP 地址是 121.43.173.240，私网 IP 地址是 172.19.11.21，在服务端通过 netstat 命令查看 TCP 连接的时候，则会将服务端的地址显示成私网 IP 地址 。至此，我们前期工作就做好了。\n\n接下来，我们在服务端执行 tcpkill 工具，来关闭这条 TCP 连接，看看会发生什么？\n\n在这里，我指定了要关闭的客户端 IP 为 114.132.166.90 和端口为 11111 的 TCP 连接。\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill4.png)\n\n可以看到，tcpkill 工具阻塞中，没有任何输出，而且此时的 TCP 连接还是存在的，并没有被干掉。\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill5.png)\n\n为什么 TCP 连接没用被干掉？\n\n因为在执行 tcpkill 工具后，这条 TCP 连接并没有传输任何数据，而 tcpkill 工具是需要拦截双方的 TCP 通信，才能获取到正确的序列号，从而才能伪装出正确的序列号的 RST 报文。\n\n所以，从这里也说明了，**tcpkill 工具不适合关闭非活跃的 TCP 连接**。\n\n接下来，我们尝试在客户端发送一个数据。\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill8.png)\n\n可以看到，在发送了「hi」数据后，客户端就断开了，并且错误提示连接被对方关闭了。\n\n此时，服务端已经查看不到刚才那条 TCP 连接了。\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill7.png)\n\n然后，我们在服务端看看 tcpkill 工具输出的信息。\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill8.png)\n\n可以看到， **tcpkill 工具给服务端和客户端都发送了伪造的 RST 报文，从而达到关闭一条 TCP 连接的效果**。\n\n到这里我们知道了， 运行 tcpkill 工具后，只有目标连接有新 TCP 包发送/接收的时候，才能关闭一条 TCP 连接。因此，**tcpkill 只适合关闭活跃的 TCP 连接，不适合用来关闭非活跃的 TCP 连接**。\n\n上面的实验过程，我也抓了数据包，流程如下：\n\n![图片](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/tcpkill/tcpkill9.png)\n\n最后一个 RST 报文就是 tcpkill 工具伪造的 RST 报文。\n\n## 总结\n\n要伪造一个能关闭 TCP 连接的 RST 报文，必须同时满足「四元组相同」和「序列号是对方期望的」这两个条件。\n\n今天给大家介绍了两种关闭 TCP 连接的工具：tcpkill 和 killcx 工具。\n\n这两种工具都是通过伪造 RST 报文来关闭 TCP 连接的，但是它们获取「对方下一次期望收到的序列号的方式是不同的，也正因此，造就了这两个工具的应用场景有区别。\n\n- tcpkill 工具只能用来关闭活跃的 TCP 连接，无法关闭非活跃的 TCP 连接，因为 tcpkill 工具是等双方进行 TCP 通信后，才去获取正确的序列号，如果这条 TCP 连接一直没有任何数据传输，则就永远获取不到正确的序列号。\n- killcx 工具可以用来关闭活跃和非活跃的 TCP 连接，因为 killcx 工具是主动发送 SYN 报文，这时对方就会回复  Challenge ACK ，然后  killcx 工具就能从这个 ACK 获取到正确的序列号。\n\n完！\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/isn_deff":{"title":"isn_deff","content":"# 4.7 为什么 TCP 每次建立连接时，初始化序列号都要不一样呢？\n\n大家好，我是小林。\n\n**为什么 TCP 每次建立连接时，初始化序列号都要不一样呢？**\n\n接下来，我一步一步给大家讲明白，我觉得应该有不少人会有类似的问题，所以今天在肝一篇！\n\n\u003e 为什么 TCP 每次建立连接时，初始化序列号都要不一样呢？\n\n主要原因是为了防止历史报文被下一个相同四元组的连接接收。\n\n\u003e TCP 四次挥手中的 TIME_WAIT 状态不是会持续 2 MSL 时长，历史报文不是早就在网络中消失了吗？\n\n是的，如果能正常四次挥手，由于 TIME_WAIT 状态会持续  2 MSL 时长，历史报文会在下一个连接之前就会自然消失。\n\n但是来了，我们并不能保证每次连接都能通过四次挥手来正常关闭连接。\n\n假设每次建立连接，客户端和服务端的初始化序列号都是从 0 开始：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/isn相同.png)\n\n过程如下：\n\n- 客户端和服务端建立一个 TCP 连接，在客户端发送数据包被网络阻塞了，然后超时重传了这个数据包，而此时服务端设备断电重启了，之前与客户端建立的连接就消失了，于是在收到客户端的数据包的时候就会发送 RST 报文。\n- 紧接着，客户端又与服务端建立了与上一个连接相同四元组的连接；\n- 在新连接建立完成后，上一个连接中被网络阻塞的数据包正好抵达了服务端，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱。\n\n可以看到，如果每次建立连接，客户端和服务端的初始化序列号都是一样的话，很容易出现历史报文被下一个相同四元组的连接接收的问题。\n\n\u003e 客户端和服务端的初始化序列号不一样不是也会发生这样的事情吗？\n\n是的，即使客户端和服务端的初始化序列号不一样，也会存在收到历史报文的可能。\n\n但是我们要清楚一点，历史报文能否被对方接收，还要看该历史报文的序列号是否正好在对方接收窗口内，如果不在就会丢弃，如果在才会接收。\n\n如果每次建立连接客户端和服务端的初始化序列号都「不一样」，就有大概率因为历史报文的序列号「不在」对方接收窗口，从而很大程度上避免了历史报文，比如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/isn不相同.png)\n\n相反，如果每次建立连接客户端和服务端的初始化序列号都「一样」，就有大概率遇到历史报文的序列号刚「好在」对方的接收窗口内，从而导致历史报文被新连接成功接收。\n\n所以，每次初始化序列号不一样能够很大程度上避免历史报文被下一个相同四元组的连接接收，注意是很大程度上，并不是完全避免了。\n\n\u003e 那客户端和服务端的初始化序列号都是随机的，那还是有可能随机成一样的呀？\n\nRFC793 提到初始化序列号 ISN 随机生成算法：ISN = M + F(localhost, localport, remotehost, remoteport)。\n\n- M是一个计时器，这个计时器每隔 4 微秒加1。\n- F 是一个 Hash 算法，根据源IP、目的IP、源端口、目的端口生成一个随机数值，要保证 hash 算法不能被外部轻易推算得出。\n\n可以看到，随机数是会基于时钟计时器递增的，基本不可能会随机成一样的初始化序列号。\n\n\u003e 懂了，客户端和服务端初始化序列号都是随机生成的话，就能避免连接接收历史报文了。\n\n是的，但是也不是完全避免了。\n\n为了能更好的理解这个原因，我们先来了解序列号（SEQ）和初始序列号（ISN）。\n\n- **序列号**，是 TCP 一个头部字段，标识了 TCP 发送端到 TCP 接收端的数据流的一个字节，因为 TCP 是面向字节流的可靠协议，为了保证消息的顺序性和可靠性，TCP 为每个传输方向上的每个字节都赋予了一个编号，以便于传输成功后确认、丢失后重传以及在接收端保证不会乱序。**序列号是一个 32 位的无符号数，因此在到达 4G 之后再循环回到 0**。\n- **初始序列号**，在 TCP 建立连接的时候，客户端和服务端都会各自生成一个初始序列号，它是基于时钟生成的一个随机数，来保证每个连接都拥有不同的初始序列号。**初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时**。\n\n给大家抓了一个包，下图中的 Seq 就是序列号，其中红色框住的分别是客户端和服务端各自生成的初始序列号。\n\n![](https://img-blog.csdnimg.cn/img_convert/ed84bb4aa742a33f50d8035da2867ca2.png)\n\n通过前面我们知道，**序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据**。\n\n不要以为序列号的上限值是 4GB，就以为很大，很难发生回绕。在一个速度足够快的网络中传输大量数据时，序列号的回绕时间就会变短。如果序列号回绕的时间极短，我们就会再次面临之前延迟的报文抵达后序列号依然有效的问题。\n\n为了解决这个问题，就需要有 TCP 时间戳。tcp_timestamps 参数是默认开启的，开启了 tcp_timestamps 参数，TCP 头部就会使用时间戳选项，它有两个好处，**一个是便于精确计算 RTT ，另一个是能防止序列号回绕（PAWS）**。\n\n试看下面的示例，假设 TCP 的发送窗口是 1 GB，并且使用了时间戳选项，发送方会为每个 TCP 报文分配时间戳数值，我们假设每个报文时间加 1，然后使用这个连接传输一个 6GB 大小的数据流。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/1d497c38621ebc44ee3d8763fd03da67.png)\n\n32 位的序列号在时刻 D 和 E 之间回绕。假设在时刻B有一个报文丢失并被重传，又假设这个报文段在网络上绕了远路并在时刻 F 重新出现。如果 TCP 无法识别这个绕回的报文，那么数据完整性就会遭到破坏。\n\n使用时间戳选项能够有效的防止上述问题，如果丢失的报文会在时刻 F 重新出现，由于它的时间戳为 2，小于最近的有效时间戳（5 或 6），因此防回绕序列号算法（PAWS）会将其丢弃。\n\n防回绕序列号算法要求连接双方维护最近一次收到的数据包的时间戳（Recent TSval），每收到一个新数据包都会读取数据包中的时间戳值跟 Recent TSval 值做比较，**如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包**。\n\n\u003e 懂了，客户端和服务端的初始化序列号都是随机生成，能很大程度上避免历史报文被下一个相同四元组的连接接收，然后又引入时间戳的机制，从而完全避免了历史报文被接收的问题。\n\n嗯嗯，没错。\n\n\u003e 如果时间戳也回绕了怎么办？\n\n时间戳的大小是 32 bit，所以理论上也是有回绕的可能性的。\n\n时间戳回绕的速度只与对端主机时钟频率有关。\n\nLinux 以本地时钟计数（jiffies）作为时间戳的值，不同的增长时间会有不同的问题：\n\n- 如果时钟计数加 1 需要1ms，则需要约 24.8 天才能回绕一半，只要报文的生存时间小于这个值的话判断新旧数据就不会出错。\n- 如果时钟计数提高到 1us 加1，则回绕需要约71.58分钟才能回绕，这时问题也不大，因为网络中旧报文几乎不可能生存超过70分钟，只是如果70分钟没有报文收发则会有一个包越过PAWS（这种情况会比较多见，相比之下 24 天没有数据传输的TCP连接少之又少），但除非这个包碰巧是序列号回绕的旧数据包而被放入接收队列（太巧了吧），否则也不会有问题；\n- 如果时钟计数提高到 0.1 us 加 1 回绕需要 7 分钟多一点，这时就可能会有问题了，连接如果 7 分钟没有数据收发就会有一个报文越过 PAWS，对于TCP连接而言这么短的时间内没有数据交互太常见了吧！这样的话会频繁有包越过 PAWS 检查，从而使得旧包混入数据中的概率大大增加；\n\nLinux 在 PAWS 检查做了一个特殊处理，如果一个 TCP 连接连续 24 天不收发数据则在接收第一个包时基于时间戳的 PAWS 会失效，也就是可以 PAWS 函数会放过这个特殊的情况，认为是合法的，可以接收该数据包。\n\n```c\n// tcp_paws_check 函数如果返回 true 则 PAWS 通过：\nstatic inline bool tcp_paws_check(const struct tcp_options_received *rx_opt, int paws_win)\n{\n......\n    \n   //从上次收到包到现在经历的时间多于24天，返回true\n if (unlikely(get_seconds() \u003e= rx_opt-\u003ets_recent_stamp + TCP_PAWS_24DAYS))\n    return true;\n\n.....\n    return false;\n}\n```\n\n要解决时间戳回绕的问题，可以考虑以下解决方案：\n\n1）增加时间戳的大小，由32 bit扩大到64bit\n\n这样虽然可以在能够预见的未来解决时间戳回绕的问题，但会导致新旧协议兼容性问题，像现在的IPv4与IPv6一样\n\n2）将一个与时钟频率无关的值作为时间戳，时钟频率可以增加但时间戳的增速不变\n\n随着时钟频率的提高，TCP在相同时间内能够收发的包也会越来越多。如果时间戳的增速不变，则会有越来越多的报文使用相同的时间戳。这种趋势到达一定程度则时间戳就会失去意义，除非在可预见的未来这种情况不会发生。\n\n3）暂时没想到\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/out_of_order_fin":{"title":"out_of_order_fin","content":"# 4.10 四次挥手中收到乱序的 FIN 包会如何处理？\n\n大家好，我是小林。\n\n收到个读者的问题，他在面试鹅厂的时候，被搞懵了，因为面试官问了他这么一个网络问题：\n\n![](https://img-blog.csdnimg.cn/39f790ee7a45473587c8fe3e08e01ba4.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_17,color_FFFFFF,t_70,g_se,x_16)\n\n不得不说，鹅厂真的很喜欢问网络问题，而且爱问异常情况下的网络问题，之前也有篇另外一个读者面试鹅厂的网络问题：「[被鹅厂面怕了！](https://blog.csdn.net/qq_34827674/article/details/117922761)」。\n\n\n不过这道鹅厂的网络题可能是提问的读者表述有问题，**因为如果 FIN 报文比数据包先抵达客户端，此时 FIN 报文其实是一个乱序的报文，此时客户端的 TCP 连接并不会从 FIN_WAIT_2 状态转换到 TIME_WAIT 状态**。\n\n![](https://img-blog.csdnimg.cn/ccabc2f21b014c6c9118cd29ae11c18c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n因此，我们要关注到点是看「**在 FIN_WAIT_2 状态下，是如何处理收到的乱序到 FIN 报文，然后 TCP 连接又是什么时候才进入到 TIME_WAIT 状态?**」。\n\n我这里先直接说结论：\n\n**在 FIN_WAIT_2 状态时，如果收到乱序的 FIN 报文，那么就被会加入到「乱序队列」，并不会进入到 TIME_WAIT 状态。**\n\n**等再次收到前面被网络延迟的数据包时，会判断乱序队列有没有数据，然后会检测乱序队列中是否有可用的数据，如果能在乱序队列中找到与当前报文的序列号保持的顺序的报文，就会看该报文是否有 FIN 标志，如果发现有 FIN 标志，这时才会进入 TIME_WAIT 状态。**\n\n我也画了一张图，大家可以结合着图来理解。\n\n![](https://img-blog.csdnimg.cn/4effcf2a9e7e4adeb892da98ee21694b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n## TCP 源码分析\n接下来，我带大家看看源码，听到要源码分析，可能有的同学就怂了。\n\n其实要分析我们今天这个问题，只要懂 if else 就行了，我也会用中文来表述代码的逻辑，所以单纯看我的文字也是可以的。\n\n这次我们重点分析的是，在 FIN_WAIT_2 状态下，收到 FIN 报文是如何处理的。\n\n在 Linux 内核里，当 IP 层处理完消息后，会通过回调 tcp_v4_rcv 函数将消息转给 TCP 层，所以这个函数就是 TCP 层收到消息的入口。\n\n![](https://img-blog.csdnimg.cn/ad39a3204f914df89aa6c6138cfc31aa.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n处于 FIN_WAIT_2 状态下的客户端，在收到服务端的报文后，最终会调用 tcp_v4_do_rcv 函数。\n\n\n![](https://img-blog.csdnimg.cn/c5ca5b3fea0e4ad6baa2ab370358f03e.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n接下来，tcp_v4_do_rcv 方法会调用 tcp_rcv_state_process，在这里会根据 TCP 状态做对应的处理，这里我们只关注 FIN_WAIT_2 状态。\n\n![](https://img-blog.csdnimg.cn/f76b7e2167544fec859700f55138e95f.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n在上面这个代码里，可以看到如果 shutdown 关闭了读方向，那么在收到对方发来的数据包，则会回复 RST 报文。\n\n而我们这次的题目里， shutdown 只关闭了写方向，所以会继续往下调用 tcp_data_queue 函数（因为 case TCP_FIN_WAIT2 代码块里并没有 break 语句，所以会走到该函数）。\n\n![](https://img-blog.csdnimg.cn/4ff161a34408447fa38b120b014b29f4.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n在上面的 tcp_data_queue 函数里，如果收到的报文的序列号是我们预期的，也就是有序的话：\n- 会判断该报文有没有 FIN 标志，如果有的话就会调用 tcp_fin 函数，这个函数负责将 FIN_WAIT_2 状态转换为 TIME_WAIT。\n- 接着还会看乱序队列有没有数据，如果有的话会调用 tcp_ofo_queue 函数，这个函数负责检查乱序队列中是否有数据包可用，即能不能在乱序队列找到与当前数据包保持序列号连续的数据包。\n\n而当收到的报文的序列号不是我们预期的，也就是乱序的话，则调用 tcp_data_queue_ofo 函数，将报文加入到乱序队列，这个队列的数据结构是红黑树。\n\n我们的题目里，客户端收到的 FIN 报文实际上是一个乱序的报文，因此此时并不会调用 tcp_fin 函数进行状态转换，而是将报文通过 tcp_data_queue_ofo 函数加入到乱序队列。\n\n然后当客户端收到被网络延迟的数据包后，此时因为该数据包的序列号是期望的，然后又因为上一次收到的乱序 FIN 报文被加入到了乱序队列，表明乱序队列是有数据的，于是就会调用 tcp_ofo_queue 函数。\n\n我们来看看 tcp_ofo_queue 函数。\n\n![](https://img-blog.csdnimg.cn/dd51b407245d45549eeae64d24634133.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n在上面的 tcp_ofo_queue 函数里，在乱序队列中找到能与当前报文的序列号保持的顺序的报文后，会看该报文是否有 FIN 标志，如果有的话，就会调用 tcp_fin() 函数。\n\n最后，我们来看看 tcp_fin 函数的处理。\n\n![](https://img-blog.csdnimg.cn/67b33007fcd04d2fa98e79d19823fc95.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n可以看到，如果当前的 TCP 状态为 TCP_FIN_WAIT2，就会发送第四次挥手 ack，然后调用 tcp_time_wait 函数，这个函数里会将 TCP 状态变更为 TIME_WAIT，并启动 TIME_WAIT 的定时器。\n\n## 怎么看 TCP 源码？\n之前有不少同学问我，我是怎么看 TCP 源码的？\n\n其实我看 TCP 源码，并不是直接打开 Linux 源码直接看，因为 Linux 源码实在太庞大了，如果我不知道 TCP 入口函数在哪，那简直就是大海捞针。\n\n\n\n所以，在看 TCP 源码，我们可以去网上搜索下别人的源码分析，网上已经有很多前辈帮我们分析了 TCP 源码了，而且各个函数的调用链路，他们都有写出来了。\n\n\n比如，你想了解 TCP 三次握手/四次挥手的源码实现，你就可以以「TCP 三次握手/四次挥手的源码分析」这样关键字来搜索，大部分文章的注释写的还是很清晰，我最开始就按这种方式来学习 TCP 源码的。\n\n网上的文章一般只会将重点的部分，很多代码细节没有贴出来，如果你想完整的看到函数的所有代码，那就得看内核代码了。\n\n\n这里推荐个看 Linux 内核代码的在线网站：\n\nhttps://elixir.bootlin.com/linux/latest/source\n\n![](https://img-blog.csdnimg.cn/c56e69f998e747208abb82897edc2629.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP5p6XY29kaW5n,size_20,color_FFFFFF,t_70,g_se,x_16)\n\n\n我觉得还是挺好用的，左侧各个版本的代码都有，右上角也可以搜索函数。\n\n所以，我看 TCP 源码的经验就是，先在网上找找前辈写的 TCP 源码分析，然后知道整个函数的调用链路后，如果想具体了解某个函数的具体实现，可以在我说的那个看 Linux 内核代码的在线网站上搜索该函数，就可以看到完整的函数的实现。如果中途遇到看不懂的代码，也可以将这个代码复制到百度或者谷歌搜索，一般也能找到别人分析的过程。\n\n学会了看 TCP 源码其实有助于我们分析一些异常问题，就比如今天这道网络题目，在网上其实是搜索不出答案的，而且我们也很难用实验的方式来模拟。\n\n所以要想知道答案，只能去看源码。\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/port":{"title":"port","content":"# 4.18 TCP 和 UDP 可以使用同一个端口吗？\n\n大家好，我是小林。\n\n之前有读者在字节面试的时候，被问到：**TCP 和 UDP 可以同时监听相同的端口吗？**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/提问.png)\n\n关于端口的知识点，还是挺多可以讲的，比如还可以牵扯到这几个问题：\n\n- 多个 TCP 服务进程可以同时绑定同一个端口吗？\n- 重启 TCP 服务进程时，为什么会出现“Address in use”的报错信息？又该怎么避免？\n- 客户端的端口可以重复使用吗？\n- 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？\n\n所以，这次就跟大家盘一盘这些问题。\n\n## TCP 和 UDP 可以同时绑定相同的端口吗？\n\n其实我感觉这个问题「TCP 和 UDP 可以同时监听相同的端口吗？」表述有问题，这个问题应该表述成「**TCP 和 UDP 可以同时绑定相同的端口吗？**」\n\n因为「监听」这个动作是在 TCP 服务端网络编程中才具有的，而 UDP 服务端网络编程中是没有「监听」这个动作的。\n\nTCP 和 UDP 服务端网络相似的一个地方，就是会调用 bind 绑定端口。\n\n给大家贴一下  TCP 和 UDP 网络编程的区别就知道了。\n\nTCP 网络编程如下，服务端执行 listen() 系统调用就是监听端口的动作。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp网络编程.png)\n\nUDP 网络编程如下，服务端是没有监听这个动作的，只有执行  bind()  系统调用来绑定端口的动作。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/udp网络编程.png)\n\n\u003e TCP 和 UDP 可以同时绑定相同的端口吗？\n\n答案：**可以的**。\n\n在数据链路层中，通过 MAC 地址来寻找局域网中的主机。在网际层中，通过 IP 地址来寻找网络中互连的主机或路由器。在传输层中，需要通过端口进行寻址，来识别同一计算机中同时通信的不同应用程序。\n\n所以，传输层的「端口号」的作用，是为了区分同一个主机上不同应用程序的数据包。\n\n传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。\n\n当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp和udp模块.jpeg)\n\n因此， TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。\n\n\u003e 验证结果\n\n我简单写了 TCP 和 UDP 服务端的程序，它们都绑定同一个端口号 8888。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp和udp服务端程序.png)\n\n运行这两个程序后，通过 netstat 命令可以看到，TCP 和 UDP 是可以同时绑定同一个端口号的。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/1.png)\n\n## 多个 TCP 服务进程可以绑定同一个端口吗？\n\n还是以前面的 TCP 服务端程序作为例子，启动两个同时绑定同一个端口的 TCP 服务进程。\n\n运行第一个  TCP 服务进程之后，netstat 命令可以查看，8888 端口已经被一个 TCP 服务进程绑定并监听了，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/2.png)\n\n接着，运行第二个 TCP 服务进程的时候，就报错了“Address already in use”，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/3.png)\n\n我上面的测试案例是两个 TCP 服务进程同时绑定地址和端口是：0.0.0.0 地址和8888端口，所以才出现的错误。\n\n如果两个 TCP 服务进程绑定的 IP 地址不同，而端口相同的话，也是可以绑定成功的，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/4.png)\n\n所以，默认情况下，针对「多个 TCP 服务进程可以绑定同一个端口吗？」这个问题的答案是：**如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”**。\n\n注意，如果 TCP 服务进程 A 绑定的地址是  0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。\n\n这是因为 0.0.0.0  地址比较特殊，代表任意地址，意味着绑定了 0.0.0.0  地址，相当于把主机上的所有 IP 地址都绑定了。\n\n::: tip\n\n如果想多个进程绑定相同的 IP 地址和端口，也是有办法的，就是对 socket 设置 SO_REUSEPORT 属性（内核 3.9 版本提供的新特性），本文不对 SO_REUSEPORT 做具体介绍，感兴趣的同学自行去学习。\n\n:::\n\n\u003e 重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？\n\nTCP 服务进程需要绑定一个 IP 地址和一个端口，然后就监听在这个地址和端口上，等待客户端连接的到来。\n\n然后在实践中，我们可能会经常碰到一个问题，当 TCP 服务进程重启之后，总是碰到“Address in use”的报错信息，TCP 服务进程不能很快地重启，而是要过一会才能重启成功。\n\n这是为什么呢？\n\n当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/四次挥手.png)\n\n**当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误**。\n\n而等 TIME_WAIT 状态的连接结束后，重启 TCP 服务进程就能成功。\n\n\u003e 重启 TCP 服务进程时，如何避免“Address in use”的报错信息？\n\n我们可以在调用 bind 前，对 socket 设置 SO_REUSEADDR 属性，可以解决这个问题。\n\n```c\nint on = 1;\nsetsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, \u0026on, sizeof(on));\n```\n\n因为 SO_REUSEADDR 作用是：**如果当前启动进程绑定的 IP+PORT 与处于TIME_WAIT 状态的连接占用的 IP+PORT 存在冲突，但是新启动的进程使用了 SO_REUSEADDR 选项，那么该进程就可以绑定成功**。\n\n举个例子，服务端有个监听 0.0.0.0 地址和 8888 端口的 TCP 服务进程。‍\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/5.png)\n\n有个客户端（IP地址：192.168.1.100）已经和服务端（IP 地址：172.19.11.200）建立了 TCP 连接，那么在 TCP 服务进程重启时，服务端会与客户端经历四次挥手，服务端的 TCP 连接会短暂处于 TIME_WAIT 状态：\n\n```bash\n客户端地址:端口           服务端地址:端口        TCP 连接状态\n192.168.1.100:37272     172.19.11.200:8888    TIME_WAI\n```\n\n如果 TCP 服务进程没有对 socket 设置 SO_REUSEADDR 属性，那么在重启时，由于存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，那么在执行 bind() 函数的时候，就会返回了 Address already in use 的错误。\n\n如果 TCP 服务进程对 socket 设置 SO_REUSEADDR 属性了，那么在重启时，即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。\n\n因此，在所有 TCP 服务器程序中，调用 bind 之前最好对 socket 设置 SO_REUSEADDR 属性，这不会产生危害，相反，它会帮助我们在很快时间内重启服务端程序。‍\n\n**前面我提到过这个问题**：如果 TCP 服务进程 A 绑定的地址是  0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。\n\n这个问题也可以由 SO_REUSEADDR 解决，因为它的**另外一个作用**：绑定的 IP地址 + 端口时，只要 IP 地址不是正好(exactly)相同，那么允许绑定。\n\n比如，0.0.0.0:8888 和192.168.1.100:8888，虽然逻辑意义上前者包含了后者，但是 0.0.0.0 泛指所有本地 IP，而 192.168.1.100 特指某一IP，两者并不是完全相同，所以在对 socket 设置 SO_REUSEADDR 属性后，那么执行 bind() 时候就会绑定成功。\n\n## 客户端的端口可以重复使用吗？\n\n客户端在执行 connect 函数的时候，会在内核里随机选择一个端口，然后向服务端发起 SYN 报文，然后与服务端进行三次握手。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp编程.png)\n\n所以，客户端的端口选择的发生在 connect 函数，内核在选择端口的时候，会从 `net.ipv4.ip_local_port_range` 这个内核参数指定的范围来选取一个端口作为客户端端口。\n\n该参数的默认值是 32768 61000，意味着端口总可用的数量是 61000 - 32768 = 28232 个。\n\n当客户端与服务端完成 TCP 连接建立后，我们可以通过 netstat 命令查看 TCP 连接。\n\n```bash\n$ netstat -napt\n协议  源ip地址:端口            目的ip地址：端口         状态\ntcp  192.168.110.182.64992   117.147.199.51.443     ESTABLISHED\n```\n\n\u003e 那问题来了，上面客户端已经用了 64992 端口，那么还可以继续使用该端口发起连接吗？\n\n这个问题，很多同学都会说不可以继续使用该端口了，如果按这个理解的话， 默认情况下客户端可以选择的端口是 28232 个，那么意味着客户端只能最多建立  28232 个 TCP 连接，如果真是这样的话，那么这个客户端并发连接也太少了吧，所以这是错误理解。\n\n正确的理解是，**TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。所以如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元组信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。**\n\n比如下面这张图，有 2 个 TCP 连接，左边是客户端，右边是服务端，客户端使用了相同的端口 50004 与两个服务端建立了 TCP 连接。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/6.jpeg)\n\n仔细看，上面这两条 TCP 连接的四元组信息中的「目的 IP 地址」是不同的，一个是 180.101.49.12 ，另外一个是 180.101.49.11。\n\n\u003e 多个客户端可以 bind 同一个端口吗？\n\nbind 函数虽然常用于服务端网络编程中，但是它也是用于客户端的。\n\n前面我们知道，客户端是在调用 connect 函数的时候，由内核随机选取一个端口作为连接的端口。\n\n而如果我们想自己指定连接的端口，就可以用 bind 函数来实现：客户端先通过 bind 函数绑定一个端口，然后调用 connect 函数就会跳过端口选择的过程了，转而使用 bind 时确定的端口。\n\n针对这个问题：多个客户端可以 bind 同一个端口吗？\n\n要看多个客户端绑定的 IP + PORT 是否都相同，如果都是相同的，那么在执行 bind() 时候就会出错，错误是“Address already in use”。\n\n如果一个绑定在 192.168.1.100:6666，一个绑定在 192.168.1.200:6666，因为 IP 不相同，所以执行 bind() 的时候，能正常绑定。\n\n所以， 如果多个客户端同时绑定的 IP 地址和端口都是相同的，那么执行 bind() 时候就会出错，错误是“Address already in use”。\n\n一般而言，客户端不建议使用 bind 函数，应该交由 connect 函数来选择端口会比较好，因为客户端的端口通常都没什么意义。\n\n\u003e 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？\n\n针对这个问题要看，客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。\n\n如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。\n\n但是，**因为只要客户端连接的服务器不同，端口资源可以重复使用的**。\n\n所以，如果客户端都是与不同的服务器建立连接，即使客户端端口资源只有几万个， 客户端发起百万级连接也是没问题的（当然这个过程还会受限于其他资源，比如文件描述符、内存、CPU 等）。\n\n\u003e 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？\n\n前面我们提到，如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。\n\n针对这个问题，也是有解决办法的，那就是打开 `net.ipv4.tcp_tw_reuse` 这个内核参数。\n\n**因为开启了这个内核参数后，客户端调用 connect  函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于  TIME_WAIT 状态，如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。**\n\n举个例子，假设客户端已经与服务器建立了一个 TCP 连接，并且这个状态处于  TIME_WAIT 状态：\n\n```bash\n客户端地址:端口           服务端地址:端口         TCP 连接状态\n192.168.1.100:2222      172.19.11.21:8888     TIME_WAIT\n```\n\n然后客户端又与该服务器（172.19.11.21:8888）发起了连接，**在调用 connect 函数时，内核刚好选择了 2222 端口，接着发现已经被相同四元组的连接占用了：**\n\n- 如果**没有开启** net.ipv4.tcp_tw_reuse  内核参数，那么内核就会选择下一个端口，然后继续判断，直到找到一个没有被相同四元组的连接使用的端口， 如果端口资源耗尽还是没找到，那么 connect 函数就会返回错误。\n- 如果**开启**了 net.ipv4.tcp_tw_reuse  内核参数，就会判断该四元组的连接状态是否处于 TIME_WAIT 状态，**如果连接处于 TIME_WAIT 状态并且该状态持续的时间超过了 1 秒，那么就会重用该连接**，于是就可以使用 2222 端口了，这时 connect 就会返回成功。\n\n再次提醒一次，开启了 net.ipv4.tcp_tw_reuse  内核参数，是客户端（连接发起方） 在调用 connect() 函数时才起作用，所以在服务端开启这个参数是没有效果的。\n\n\u003e 客户端端口选择的流程总结\n\n至此，我们已经把客户端在执行 connect 函数时，内核选择端口的情况大致说了一遍，为了让大家更明白客户端端口的选择过程，我画了一流程图。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/端口选择.jpg)\n\n## **总结**\n\n\u003e TCP 和 UDP 可以同时绑定相同的端口吗？\n\n可以的。\n\nTCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。\n\n当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。\n\n因此， TCP/UDP 各自的端口号也相互独立，互不影响。\n\n\u003e 多个 TCP 服务进程可以同时绑定同一个端口吗？\n\n如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。\n\n如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行 bind() 不会出错。\n\n\u003e 如何解决服务端重启时，报错“Address already in use”的问题？\n\n当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。\n\n当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。\n\n要解决这个问题，我们可以对 socket 设置 SO_REUSEADDR 属性。\n\n这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。\n\n\u003e 客户端的端口可以重复使用吗？\n\n在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。\n\nTCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。\n\n所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。\n\n\u003e 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？\n\n要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。\n\n如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。\n\n\u003e 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？\n\n打开 net.ipv4.tcp_tw_reuse  这个内核参数。\n\n因为开启了这个内核参数后，客户端调用 connect  函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于  TIME_WAIT 状态。\n\n如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/quic":{"title":"quic","content":"# 4.17 如何基于 UDP 协议实现可靠传输？\n\n大家好，我是小林。\n\n我记得之前在群里看到，有位读者字节一面的时候被问到：「**如何基于 UDP 协议实现可靠传输？**」\n\n很多同学第一反应就会说把 TCP 可靠传输的特性（序列号、确认应答、超时重传、流量控制、拥塞控制）在应用层实现一遍。\n\n实现的思路确实这样没错，但是有没有想过，**既然 TCP 天然支持可靠传输，为什么还需要基于 UDP 实现可靠传输呢？这不是重复造轮子吗？**\n\n所以，我们要先弄清楚 TCP 协议有哪些痛点？而这些痛点是否可以在基于 UDP 协议实现的可靠传输协议中得到改进？\n\n在之前这篇文章：[TCP 就没什么缺陷吗？](https://mp.weixin.qq.com/s/9kHoRk6QIYOFUR_PCmHY6g)，我已经说了 TCP 协议四个方面的缺陷：\n\n- 升级 TCP 的工作很困难；\n- TCP 建立连接的延迟；\n- TCP 存在队头阻塞问题；\n- 网络迁移需要重新建立 TCP 连接；\n\n现在市面上已经有基于 UDP 协议实现的可靠传输协议的成熟方案了，那就是 QUIC 协议，已经应用在了 HTTP/3。\n\n这次，**聊聊 QUIC 是如何实现可靠传输的？又是如何解决上面 TCP 协议四个方面的缺陷**？\n\n![](https://img-blog.csdnimg.cn/605d1026df934f20a5ee12f3c55aa6a7.png)\n\n## QUIC 是如何实现可靠传输的？\n\n要基于 UDP 实现的可靠传输协议，那么就要在应用层下功夫，也就是要设计好协议的头部字段。\n\n拿 HTTP/3 举例子，在 UDP 报文头部与 HTTP 消息之间，共有 3 层头部：\n\n![](https://static001.geekbang.org/resource/image/ab/7c/ab3283383013b707d1420b6b4cb8517c.png)\n\n整体看的视角是这样的：\n\n![](https://docs.citrix.com/en-us/citrix-adc/media/http3-over-quic-protocol-works.png)\n\n接下来，分别对每一个 Header 做个介绍。\n\n### Packet Header\n\nPacket Header 首次建立连接时和日常传输数据时使用的 Header 是不同的。如下图（*注意我没有把 Header 所有字段都画出来，只是画出了重要的字段*）：\n\n![Packet Header](https://img-blog.csdnimg.cn/bcf3ccb6a15c4cdebe1cd0527fdd9a5e.png)\n\nPacket Header 细分这两种：\n\n- Long Packet Header 用于首次建立连接。\n- Short Packet Header 用于日常传输数据。\n\nQUIC 也是需要三次握手来建立连接的，主要目的是为了协商连接 ID。协商出连接 ID 后，后续传输时，双方只需要固定住连接 ID，从而实现连接迁移功能。所以，你可以看到日常传输数据的 Short Packet Header 不需要在传输 Source Connection ID 字段了，只需要传输 Destination Connection ID。\n\nShort Packet Header 中的 `Packet Number` 是每个报文独一无二的编号，它是**严格递增**的，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。\n\n![](https://img-blog.csdnimg.cn/635813465fbb449882da2e2bee39f24e.png)\n\n\u003e  为什么要这么设计呢？\n\n我们先来看看  TCP 的问题，TCP 在重传报文时的序列号和原始报文的序列号是一样的，也正是由于这个特性，引入了 TCP 重传的歧义问题。\n\n![TCP 重传的歧义问题](https://img-blog.csdnimg.cn/7e4e778413c1452bb6d58ec3d5452316.png)\n\n比如上图，当 TCP 发生超时重传后，客户端发起重传，然后接收到了服务端确认 ACK 。由于客户端原始报文和重传报文序列号都是一样的，那么服务端针对这两个报文回复的都是相同的 ACK。\n\n这样的话，客户端就无法判断出是「原始报文的响应」还是「重传报文的响应」，这样在计算 RTT（往返时间） 时应该选择从发送原始报文开始计算，还是重传原始报文开始计算呢？\n\n- 如果算成原始报文的响应，但实际上是重传报文的响应（上图左），会导致采样 RTT 变大；\n- 如果算成重传报文的响应，但实际上是原始报文的响应（上图右），又很容易导致采样 RTT 过小；\n\nRTO （超时时间）是基于 RTT 来计算的，那么如果 RTT 计算不精准，那么 RTO （超时时间）也会不精确，这样可能导致重传的概率事件增大。\n\nQUIC 报文中的 Pakcet Number 是严格递增的， 即使是重传报文，它的 Pakcet Number 也是递增的，这样就能更加精确计算出报文的 RTT。\n\n![](https://img-blog.csdnimg.cn/ca91985c9a94487a8a29db1249109717.png)\n\n如果 ACK 的 Packet Number 是 N+M，就根据重传报文计算采样 RTT。如果 ACK 的 Pakcet Number 是 N，就根据原始报文的时间计算采样 RTT，没有歧义性的问题。\n\n另外，还有一个好处，**QUIC 使用的 Packet Number 单调递增的设计，可以让数据包不再像 TCP 那样必须有序确认，QUIC 支持乱序确认，当数据包Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会继续向右滑动**（后面讲流量控制的时候，会举例子）。\n\n待发送端获知数据包Packet N 丢失后，会将需要重传的数据包放到待发送队列，重新编号比如数据包Packet N+M 后重新发送给接收端，对重传数据包的处理跟发送新的数据包类似，这样就不会因为丢包重传将当前窗口阻塞在原地，从而解决了队头阻塞问题。\n\n所以，Packet Number 单调递增的两个好处：\n\n- 可以更加精确计算 RTT，没有 TCP 重传的歧义性问题；\n- 可以支持乱序确认，因为丢包重传将当前窗口阻塞在原地，而 TCP 必须是顺序确认的，丢包时会导致窗口不滑动；\n\n### QUIC Frame Header \n\n一个 Packet 报文中可以存放多个 QUIC Frame。\n\n![](https://img-blog.csdnimg.cn/6a94d41ef3d14cb6b7846e73da6c3104.png)\n\n每一个 Frame 都有明确的类型，针对类型的不同，功能也不同，自然格式也不同。\n\n我这里只举例  Stream 类型的 Frame 格式，Stream 可以认为就是一条 HTTP 请求，它长这样：\n\n![](https://img-blog.csdnimg.cn/536298d2c54a43b699026bffe0f85010.png)\n\n- Stream ID 作用：多个并发传输的 HTTP 消息，通过不同的 Stream ID 加以区别，类似于 HTTP2 的 Stream ID；\n- Offset 作用：类似于 TCP 协议中的 Seq 序号，**保证数据的顺序性和可靠性**；\n- Length 作用：指明了 Frame 数据的长度。\n\n在前面介绍 Packet Header 时，说到 Packet Number 是严格递增，即使重传报文的 Packet Number 也是递增的，既然重传数据包的 Packet N+M 与丢失数据包的 Packet N 编号并不一致，我们怎么确定这两个数据包的内容一样呢？\n\n所以引入 Frame Header 这一层，**通过 Stream ID + Offset 字段信息实现数据的有序性**，通过比较两个数据包的 Stream ID 与 Stream Offset ，如果都是一致，就说明这两个数据包的内容一致。\n\n举个例子，下图中，数据包 Packet N 丢失了，后面重传该数据包的编号为 Packet N+2，**丢失的数据包和重传的数据包 Stream ID 与 Offset 都一致，说明这两个数据包的内容一致**。这些数据包传输到接收端后，接收端能根据 Stream ID 与 Offset 字段信息将  Stream x 和 Stream x+y 按照顺序组织起来，然后交给应用程序处理。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/Packet丢失.jpeg)\n\n总的来说，**QUIC 通过单向递增的 Packet Number，配合 Stream ID 与 Offset 字段信息，可以支持乱序确认而不影响数据包的正确组装**，摆脱了TCP 必须按顺序确认应答 ACK 的限制，解决了 TCP 因某个数据包重传而阻塞后续所有待发送数据包的问题。\n\n## QUIC 是如何解决 TCP 队头阻塞问题的？\n\n### 什么是 TCP 队头阻塞问题？\n\nTCP 队头阻塞的问题要从两个角度看，一个是**发送窗口的队头阻塞**，另外一个是**接收窗口的队头阻塞**。\n\n*1、发送窗口的队头阻塞。*\n\nTCP 发送出去的数据，都是需要按序确认的，只有在数据都被按顺序确认完后，发送窗口才会往前滑动。\n\n举个例子，比如下图的发送方把发送窗口内的数据全部都发出去了，可用窗口的大小就为 0 了，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据了。\n\n![可用窗口耗尽](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/17.jpg?)\n\n接着，当发送方收到对第 `32~36` 字节的 ACK 确认应答后，则**滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认**，接下来第 `52~56` 字节又变成了可用窗口，那么后续也就可以发送 `52~56` 这 5 个字节的数据了。\n\n![32 ~ 36 字节已确认](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/18.jpg)\n\n**但是如果某个数据报文丢失或者其对应的 ACK 报文在网络中丢失，会导致发送方无法移动发送窗口，这时就无法再发送新的数据**，只能超时重传这个数据报文，直到收到这个重传报文的 ACK，发送窗口才会移动，继续后面的发送行为。\n\n举个例子，比如下图，客户端是发送方，服务器是接收方。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/ack丢失.jpeg)\n\n客户端发送了第 5～9 字节的数据，但是第 5 字节的 ACK 确认报文在网络中丢失了，那么即使客户端收到第 6～9 字节的 ACK 确认报文，发送窗口也不会往前移动。\n\n**此时的第 5 字节相当于“队头”，因为没有收到“队头”的 ACK 确认报文，导致发送窗口无法往前移动，此时发送方就无法继续发送后面的数据，相当于按下了发送行为的暂停键，这就是发送窗口的队头阻塞问题**。\n\n*2、接收窗口的队头阻塞。*\n\n接收方收到的数据范围必须在接收窗口范围内，如果收到超过接收窗口范围的数据，就会丢弃该数据，比如下图接收窗口的范围是 32 ～ 51 字节，如果收到第 52 字节以上数据都会被丢弃。\n\n![接收窗口](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/20.jpg)\n\n接收窗口什么时候才能滑动？当接收窗口收到有序数据时，接收窗口才能往前滑动，然后那些已经接收并且被确认的「有序」数据就可以被应用层读取。\n\n但是，**当接收窗口收到的数据不是有序的，比如收到第 33～40 字节的数据，由于第 32 字节数据没有收到， 接收窗口无法向前滑动，那么即使先收到第 33～40 字节的数据，这些数据也无法被应用层读取的**。只有当发送方重传了第 32 字节数据并且被接收方收到后，接收窗口才会往前滑动，然后应用层才能从内核读取第 32～40 字节的数据。\n\n好了，至此发送窗口和接收窗口的队头阻塞问题都说完了，这两个问题的原因都是因为 TCP 必须按序处理数据，也就是 TCP 层为了保证数据的有序性，只有在处理完有序的数据后，滑动窗口才能往前滑动，否则就停留。\n\n- 停留「发送窗口」会使得发送方无法继续发送数据。\n\n- 停留「接收窗口」会使得应用层无法读取新的数据。\n\n其实也不能怪 TCP 协议，它本来设计目的就是为了保证数据的有序性。\n\n### HTTP/2  的队头阻塞\n\nHTTP/2 通过抽象出 Stream 的概念，实现了 HTTP 并发传输，一个 Stream 就代表 HTTP/1.1 里的请求和响应。\n\n![HTTP/2](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http2/stream2.png)\n\n在 HTTP/2 连接上，不同 Stream 的帧是可以乱序发送的（因此可以并发不同的 Stream ），因为每个帧的头部会携带 Stream ID 信息，所以接收端可以通过 Stream ID 有序组装成 HTTP 消息，而同一 Stream 内部的帧必须是严格有序的。\n\n**但是 HTTP/2 多个 Stream 请求都是在一条 TCP 连接上传输，这意味着多个 Stream 共用同一个 TCP 滑动窗口，那么当发生数据丢失，滑动窗口是无法往前移动的，此时就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞**。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/http2阻塞.jpeg)\n\n### 没有队头阻塞的 QUIC\n\nQUIC 也借鉴 HTTP/2 里的 Stream 的概念，在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (Stream)。\n\n但是 **QUIC 给每一个 Stream 都分配了一个独立的滑动窗口，这样使得一个连接上的多个 Stream 之间没有依赖关系，都是相互独立的，各自控制的滑动窗口**。\n\n假如 Stream2 丢了一个 UDP 包，也只会影响 Stream2 的处理，不会影响其他 Stream，与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/quic无阻塞.jpeg)\n\n## QUIC 是如何做流量控制的？\n\nTCP 流量控制是通过让「接收方」告诉「发送方」，它（接收方）的接收窗口有多大，从而让「发送方」根据「接收方」的实际接收能力控制发送的数据量。\n\nQUIC 实现流量控制的方式：\n\n- 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。\n- 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。\n\n在前面说到，TCP 的接收窗口在收到有序的数据后，接收窗口才能往前滑动，否则停止滑动；TCP 的发送窗口在收到对已发送数据的顺序确认 ACK后，发送窗口才能往前滑动，否则停止滑动。\n\nQUIC 是基于 UDP 传输的，而 UDP 没有流量控制，因此 QUIC 实现了自己的流量控制机制，QUIC 的滑动窗口滑动的条件跟 TCP 有一点差别，但是同一个 Stream 的数据也是要保证顺序的，不然无法实现可靠传输，因此同一个 Stream 的数据包丢失了，也会造成窗口无法滑动。\n\n**QUIC 的 每个 Stream 都有各自的滑动窗口，不同 Stream 互相独立，队头的 Stream A 被阻塞后，不妨碍 StreamB、C的读取**。而对于 HTTP/2 而言，所有的 Stream 都跑在一条 TCP 连接上，而这些 Stream 共享一个滑动窗口，因此同一个Connection内，Stream A 被阻塞后，StreamB、C 必须等待。\n\nQUIC 实现了两种级别的流量控制，分别为 Stream 和 Connection 两种级别：\n\n- **Stream 级别的流量控制**：Stream 可以认为就是一条 HTTP 请求，每个 Stream 都有独立的滑动窗口，所以每个 Stream 都可以做流量控制，防止单个 Stream 消耗连接（Connection）的全部接收缓冲。\n- **Connection 流量控制**：限制连接中所有 Stream 相加起来的总字节数，防止发送方超过连接的缓冲容量。\n\n### Stream 级别的流量控制\n\n最开始，接收方的接收窗口初始状态如下（网上的讲 QUIC 流量控制的资料太少了，下面的例子我是参考 google 文档的：[Flow control in QUIC](https://docs.google.com/document/d/1F2YfdDXKpy20WVKJueEf4abn_LVZHhMUMS5gX6Pgjl4/mobilebasic)）：\n\n![](https://img-blog.csdnimg.cn/f1070a6eccd24559904815297b07f789.png)\n\n接着，接收方收到了发送方发送过来的数据，有的数据被上层读取了，有的数据丢包了，此时的接收窗口状况如下：\n\n![](https://img-blog.csdnimg.cn/77e9a7cf70da4a1b981f61e78db2ad56.png)\n\n可以看到，**接收窗口的左边界取决于接收到的最大偏移字节数**，此时的`接收窗口  = 最大窗口数 - 接收到的最大偏移数`。\n\n这里就可以看出 QUIC 的流量控制和 TCP 有点区别了：\n\n- TCP 的接收窗口只有在前面所有的 Segment 都接收的情况下才会移动左边界，当在前面还有字节未接收但收到后面字节的情况下，窗口也不会移动。\n- QUIC 的接收窗口的左边界滑动条件取决于接收到的最大偏移字节数。\n\n*PS：但是你要问我这么设计有什么好处？我也暂时没想到，因为资料太少了，至今没找到一个合理的说明，如果你知道，欢迎告诉我啊！*\n\n那接收窗口右边界触发的滑动条件是什么呢？看下图：\n\n![接收窗口触发的滑动](https://img-blog.csdnimg.cn/bbde0c66088f439b919a6d18b389aadb.png)\n\n当图中的绿色部分数据超过最大接收窗口的一半后，最大接收窗口向右移动，接收窗口的右边界也向右扩展，同时给对端发送「窗口更新帧」，当发送方收到接收方的窗口更新帧后，发送窗口的右边界也会往右扩展，以此达到窗口滑动的效果。\n\n绿色部分的数据是已收到的顺序的数据，**如果中途丢失了数据包，导致绿色部分的数据没有超过最大接收窗口的一半，那接收窗口就无法滑动了**，这个只影响同一个 Stream，其他 Stream 是不会影响的，因为每个 Stream 都有各自的滑动窗口。\n\n在前面我们说过 QUIC 支持乱序确认，具体是怎么做到的呢？\n\n接下来，举个例子（下面的例子来源于：[QUIC——快速UDP网络连接协议](https://juejin.cn/post/7066993430102016037)）：\n\n如图所示，当前发送方的缓冲区大小为8，发送方 QUIC 按序（offset顺序）发送 29-36 的数据包：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/乱序确认1.png)\n\n31、32、34数据包先到达，基于 offset 被优先乱序确认，但 30 数据包没有确认，所以当前已提交的字节偏移量不变，发送方的缓存区不变。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/乱序确认2.png)\n\n30 到达并确认，发送方的缓存区收缩到阈值，接收方发送 MAX_STREAM_DATA Frame（协商缓存大小的特定帧）给发送方，请求增长最大绝对字节偏移量。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/乱序确认3.png)\n\n协商完毕后最大绝对字节偏移量右移，发送方的缓存区变大，同时发送方发现数据包33超时\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/乱序确认4.png)\n\n发送方将超时数据包重新编号为 42 继续发送\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/乱序确认5.png)\n\n以上就是最基本的数据包发送-接收过程，控制数据发送的唯一限制就是最大绝对字节偏移量，该值是接收方基于当前已经提交的偏移量（连续已确认并向上层应用提交的数据包offset）和发送方协商得出。\n\n### Connection 流量控制\n\n而对于 Connection 级别的流量窗口，其接收窗口大小就是各个 Stream 接收窗口大小之和。\n\n![Connection 流量控制](https://img-blog.csdnimg.cn/839501cffa7146cbb8d992264594e61d.png)\n\n上图所示的例子，所有 Streams 的最大窗口数为 120，其中：\n\n- Stream 1 的最大接收偏移为 100，可用窗口 = 120 - 100 = 20\n- Stream 2 的最大接收偏移为 90，可用窗口 = 120 - 90 = 30\n- Stream 3 的最大接收偏移为 110，可用窗口 = 120 - 110 = 10\n\n那么整个 Connection 的可用窗口 = 20 + 30 + 10 = 60\n\n```text\n可用窗口 = Stream 1 可用窗口 + Stream 2 可用窗口 + Stream 3 可用窗口\n```\n\n## QUIC 对拥塞控制改进\n\nQUIC 协议当前默认使用了 TCP 的 Cubic 拥塞控制算法（我们熟知的慢开始、拥塞避免、快重传、快恢复策略），同时也支持 CubicBytes、Reno、RenoBytes、BBR、PCC 等拥塞控制算法，相当于将 TCP 的拥塞控制算法照搬过来了。\n\nQUIC 是如何改进 TCP 的拥塞控制算法的呢？\n\nQUIC 是处于应用层的，应用程序层面就能实现不同的拥塞控制算法，不需要操作系统，不需要内核支持。这是一个飞跃，因为传统的 TCP 拥塞控制，必须要端到端的网络协议栈支持，才能实现控制效果。而内核和操作系统的部署成本非常高，升级周期很长，所以 TCP 拥塞控制算法迭代速度是很慢的。而 **QUIC 可以随浏览器更新，QUIC 的拥塞控制算法就可以有较快的迭代速度**。\n\nTCP 更改拥塞控制算法是对系统中所有应用都生效，无法根据不同应用设定不同的拥塞控制策略。但是因为 QUIC 处于应用层，所以就**可以针对不同的应用设置不同的拥塞控制算法**，这样灵活性就很高了。\n\n## QUIC 更快的连接建立\n\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手（1RTT），再 TLS 握手（2RTT），所以需要 3RTT 的延迟才能传输数据，就算 Session 会话服用，也需要至少 2 个 RTT。\n\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是**QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果**。\n\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT（下图的右下角）：\n\n![](https://img-blog.csdnimg.cn/4cad213f5125432693e0e2a512c2d1a1.png)\n\n## QUIC 是如何迁移连接的？\n\n基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n\n![TCP 四元组](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEwLmpwZw?x-oss-process=image/format,png)\n\n那么**当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接**。\n\n而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n\nQUIC 协议没有用四元组的方式来“绑定”连接，而是通过**连接 ID**来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了**连接迁移**的功能。\n\n---\n\n参考资料：\n\n- https://www.taohui.tech/2021/02/04/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90HTTP3%E5%8D%8F%E8%AE%AE/\n- https://zhuanlan.zhihu.com/p/32553477\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/syn_drop":{"title":"syn_drop","content":"# 4.8 SYN 报文什么时候情况下会被丢弃？\n\n大家好，我是小林。\n\n之前有个读者在秋招面试的时候，被问了这么一个问题：SYN 报文什么时候情况下会被丢弃？\n\n![](https://img-blog.csdnimg.cn/img_convert/d4df0c85e08f66f6a2aa2038af73adcc.png)\n\n好家伙，现在面试都问那么细节了吗？\n\n不过话说回来，这个问题跟工作上也是有关系的，因为我就在工作中碰到这么奇怪的时候，客户端向服务端发起了连接，但是连接并没有建立起来，通过抓包分析发现，服务端是收到 SYN 报文了，但是并没有回复 SYN+ACK（TCP 第二次握手），说明 SYN 报文被服务端忽略了，然后客户端就一直在超时重传 SYN 报文，直到达到最大的重传次数。\n\n接下来，我就给出我遇到过 SYN 报文被丢弃的两种场景：\n\n- 开启 tcp_tw_recycle 参数，并且在 NAT 环境下，造成 SYN 报文被丢弃\n\n- TCP 两个队列满了（半连接队列和全连接队列），造成 SYN 报文被丢弃\n\n## 坑爹的 tcp_tw_recycle\n\nTCP 四次挥手过程中，主动断开连接方会有一个 TIME_WAIT 的状态，这个状态会持续 2 MSL 后才会转变为 CLOSED 状态。\n\n![](https://img-blog.csdnimg.cn/img_convert/bee0c8e8d84047e7434803fb340f9e5d.png)\n\n在 Linux  操作系统下，TIME_WAIT 状态的持续时间是 60 秒，这意味着这 60 秒内，客户端一直会占用着这个端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768~61000 ，也可以通过如下参数设置指定范围：\n\n```\n net.ipv4.ip_local_port_range\n```\n\n**如果客户端（发起连接方）的 TIME_WAIT 状态过多**，占满了所有端口资源，那么就无法对「目的 IP+ 目的 PORT」都一样的服务器发起连接了，但是被使用的端口，还是可以继续对另外一个服务器发起连接的。具体可以看我这篇文章：[客户端的端口可以重复使用吗？](https://xiaolincoding.com/network/3_tcp/port.html#%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%AF%E4%BB%A5%E9%87%8D%E5%A4%8D%E4%BD%BF%E7%94%A8%E5%90%97)\n\n因此，客户端（发起连接方）都是和「目的 IP+ 目的 PORT 」都一样的服务器建立连接的话，当客户端的 TIME_WAIT 状态连接过多的话，就会受端口资源限制，如果占满了所有端口资源，那么就无法再跟「目的 IP+ 目的 PORT」都一样的服务器建立连接了。\n\n不过，即使是在这种场景下，只要连接的是不同的服务器，端口是可以重复使用的，所以客户端还是可以向其他服务器发起连接的，这是因为内核在定位一个连接的时候，是通过四元组（源IP、源端口、目的IP、目的端口）信息来定位的，并不会因为客户端的端口一样，而导致连接冲突。\n\n但是 TIME_WAIT 状态也不是摆设作用，它的作用有两个：\n\n- 防止具有相同四元组的旧数据包被收到，也就是防止历史连接中的数据，被后面的连接接受，否则就会导致后面的连接收到一个无效的数据，\n- 保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭;\n\n不过，Linux 操作系统提供了两个可以系统参数来快速回收处于 TIME_WAIT 状态的连接，这两个参数都是默认关闭的：\n\n- net.ipv4.tcp_tw_reuse，如果开启该选项的话，客户端（连接发起方） 在调用 connect() 函数时，**如果内核选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态，如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。**所以该选项只适用于连接发起方。\n- net.ipv4.tcp_tw_recycle，如果开启该选项的话，允许处于 TIME_WAIT 状态的连接被快速回收；\n\n要使得这两个选项生效，有一个前提条件，就是要打开 TCP 时间戳，即 net.ipv4.tcp_timestamps=1（默认即为 1)）。\n\n**tcp_tw_recycle 在使用了 NAT 的网络下是不安全的！**\n\n对于服务器来说，如果同时开启了recycle 和 timestamps 选项，则会开启一种称之为「 per-host 的 PAWS 机制」。\n\n\u003e 首先给大家说说什么是  PAWS 机制？\n\ntcp_timestamps 选项开启之后， PAWS 机制会自动开启，它的作用是防止 TCP 包中的序列号发生绕回。\n\n正常来说每个 TCP 包都会有自己唯一的 SEQ，出现 TCP 数据包重传的时候会复用 SEQ 号，这样接收方能通过 SEQ 号来判断数据包的唯一性，也能在重复收到某个数据包的时候判断数据是不是重传的。**但是 TCP 这个 SEQ 号是有限的，一共 32 bit，SEQ 开始是递增，溢出之后从 0 开始再次依次递增**。\n\n所以当 SEQ 号出现溢出后单纯通过 SEQ 号无法标识数据包的唯一性，某个数据包延迟或因重发而延迟时可能导致连接传递的数据被破坏，比如：\n\n![](https://img-blog.csdnimg.cn/img_convert/f5fbe947240026cc2f076267cb698496.png)\n\n上图 A 数据包出现了重传，并在 SEQ 号耗尽再次从 A 递增时，第一次发的 A 数据包延迟到达了 Server，这种情况下如果没有别的机制来保证，Server 会认为延迟到达的 A 数据包是正确的而接收，反而是将正常的第三次发的 SEQ 为 A 的数据包丢弃，造成数据传输错误。\n\nPAWS 就是为了避免这个问题而产生的，在开启 tcp_timestamps 选项情况下，一台机器发的所有 TCP 包都会带上发送时的时间戳，PAWS 要求连接双方维护最近一次收到的数据包的时间戳（Recent TSval），每收到一个新数据包都会读取数据包中的时间戳值跟 Recent TSval 值做比较，**如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包**。\n\n对于上面图中的例子有了 PAWS 机制就能做到在收到 Delay 到达的 A 号数据包时，识别出它是个过期的数据包而将其丢掉。\n\n\u003e 那什么是 per-host 的 PAWS 机制呢？\n\n前面我提到，开启了 recycle 和 timestamps 选项，就会开启一种叫 per-host 的 PAWS 机制。**per-host 是对「对端 IP 做 PAWS 检查」**，而非对「IP + 端口」四元组做 PAWS 检查。\n\n但是如果客户端网络环境是用了 NAT 网关，那么客户端环境的每一台机器通过 NAT 网关后，都会是相同的 IP 地址，在服务端看来，就好像只是在跟一个客户端打交道一样，无法区分出来。\n\nPer-host PAWS 机制利用TCP option里的 timestamp 字段的增长来判断串扰数据，而 timestamp 是根据客户端各自的 CPU tick 得出的值。\n\n当客户端 A 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，**客户端 B 也通过 NAT 网关和服务器建立 TCP 连接，注意客户端 A  和 客户端 B 因为经过相同的 NAT 网关，所以是用相同的 IP 地址与服务端建立 TCP 连接，如果客户端 B 的 timestamp 比 客户端 A 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 B 发来的 SYN 包**。\n\n因此，tcp_tw_recycle 在使用了 NAT 的网络下是存在问题的，如果它是对 TCP 四元组做 PAWS 检查，而不是对「相同的 IP 做 PAWS 检查」，那么就不会存在这个问题了。\n\n网上很多博客都说开启 tcp_tw_recycle 参数来优化 TCP，我信你个鬼，糟老头坏的很！\n\ntcp_tw_recycle 在 Linux 4.12 版本后，直接取消了这一参数。\n\n## accpet 队列满了\n\n在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：\n\n- 半连接队列，也称 SYN 队列；\n- 全连接队列，也称 accepet 队列；\n\n服务端收到客户端发起的 SYN 请求后，**内核会把该连接存储到半连接队列**，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，**内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。**\n\n![](https://img-blog.csdnimg.cn/img_convert/c9959166180b0e239bb48234ff7c2f5b.png)\n\n\n\n### 半连接队列满了\n\n当服务器造成syn攻击，就有可能导致 **TCP 半连接队列满了，这时后面来的 syn 包都会被丢弃**。\n\n但是，**如果开启了syncookies 功能，即使半连接队列满了，也不会丢弃syn 包**。\n\nsyncookies 是这么做的：服务器根据当前状态计算出一个值，放在己方发出的 SYN+ACK 报文中发出，当客户端返回 ACK 报文时，取出该值验证，如果合法，就认为连接建立成功，如下图所示。\n\n![](https://img-blog.csdnimg.cn/img_convert/58e01036d1febd0103dd0ec4d5acff05.png)\n\nsyncookies 参数主要有以下三个值：\n\n- 0 值，表示关闭该功能；\n- 1 值，表示仅当 SYN 半连接队列放不下时，再启用它；\n- 2 值，表示无条件开启功能；\n\n那么在应对 SYN 攻击时，只需要设置为 1 即可：\n\n\n![](https://img-blog.csdnimg.cn/img_convert/e795b4ff5be76c85814ee190b4921f25.png)\n\n这里给出几种防御 SYN 攻击的方法：\n\n- 增大半连接队列；\n- 开启 tcp_syncookies 功能\n- 减少 SYN+ACK 重传次数\n\n*方式一：增大半连接队列*\n\n**要想增大半连接队列，我们得知不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大全连接队列**。否则，只单纯增大 tcp_max_syn_backlog 是无效的。\n\n增大 tcp_max_syn_backlog 和 somaxconn 的方法是修改 Linux 内核参数：\n\n![](https://img-blog.csdnimg.cn/img_convert/29f1fd2894162e15cbac938a2373b543.png)\n\n增大 backlog 的方式，每个 Web 服务都不同，比如 Nginx 增大 backlog 的方法如下：\n\n![](https://img-blog.csdnimg.cn/img_convert/a6b11fbd1fcb742cdcc87447fc23b73f.png)\n\n最后，改变了如上这些参数后，要重启 Nginx 服务，因为半连接队列和全连接队列都是在 listen() 初始化的。\n\n*方式二：开启 tcp_syncookies 功能*\n\n开启 tcp_syncookies 功能的方式也很简单，修改 Linux 内核参数：\n\n![](https://img-blog.csdnimg.cn/img_convert/54b7411607978cb9ff36d88cf47eb5c4.png)\n\n*方式三：减少 SYN+ACK 重传次数*\n\n当服务端受到 SYN 攻击时，就会有大量处于 SYN_RECV 状态的 TCP 连接，处于这个状态的 TCP 会重传 SYN+ACK ，当重传超过次数达到上限后，就会断开连接。\n\n那么针对 SYN 攻击的场景，我们可以减少 SYN+ACK 的重传次数，以加快处于 SYN_RECV 状态的 TCP 连接断开。\n\n![](https://img-blog.csdnimg.cn/img_convert/19443a03430368b72c201113150471c5.png)\n\n### 全连接队列满了\n\n**在服务端并发处理大量请求时，如果 TCP accpet 队列过小，或者应用程序调用 accept() 不及时，就会造成 accpet 队列满了 ，这时后续的连接就会被丢弃，这样就会出现服务端请求数量上不去的现象。**\n\n![](https://img-blog.csdnimg.cn/img_convert/d1538f8d3b50da26039bc6b171a13ad1.png)\n\n我们可以通过 ss 命令来看 accpet 队列大小，在「LISTEN 状态」时，`Recv-Q/Send-Q` 表示的含义如下：\n\n![](https://img-blog.csdnimg.cn/img_convert/d7e8fcbb4afa583687b76064b7f1afac.png)\n\n\n- Recv-Q：当前 accpet 队列的大小，也就是当前已完成三次握手并等待服务端 `accept()` 的 TCP 连接个数；\n- Send-Q：当前 accpet 最大队列长度，上面的输出结果说明监听 8088 端口的 TCP 服务进程，accpet 队列的最大长度为 128；\n\n如果 Recv-Q 的大小超过 Send-Q，就说明发生了 accpet 队列满的情况。\n\n要解决这个问题，我们可以：\n\n- 调大 accpet 队列的最大长度，调大的方式是通过**调大 backlog 以及 somaxconn 参数。**\n- 检查系统或者代码为什么调用 accept()  不及时；\n\n关于 SYN 队列和 accpet 队列，我之前写过一篇很详细的文章：[TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？](https://mp.weixin.qq.com/s/2qN0ulyBtO2I67NB_RnJbg)\n\n---\n\n好了，今天就分享到这里啦。\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_down_and_crash":{"title":"tcp_down_and_crash","content":"# 4.12 TCP 连接，一端断电和进程崩溃有什么区别？\n\n有位读者找我说，他在面试腾讯的时候，遇到了这么个问题：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021061513401120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n这个属于 **TCP 异常断开连接**的场景，这部分内容在我的「图解网络」还没有详细介绍过，这次就乘着这次机会补一补。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210615134020994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n这个问题有几个关键词：\n\n- 没有开启 keepalive；\n- 一直没有数据交互；\n- 进程崩溃；\n- 主机崩溃；\n\n\n我们先来认识认识什么是 TCP keepalive 呢？\n\n这东西其实就是 **TCP 的保活机制**，它的工作原理我之前的文章写过，这里就直接贴下以前的内容。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210615134028909.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n\n如果两端的 TCP 连接一直没有数据交互，达到了触发 TCP 保活机制的条件，那么内核里的 TCP 协议栈就会发送探测报文。\n- 如果对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。\n- 如果对端主机崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。\n\n\n所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210615134036676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。\n\n## 主机崩溃\n\n知道了 TCP keepalive 作用，我们再回过头看题目中的「主机崩溃」这种情况。\n\n\u003e 在没有开启 TCP keepalive，且双方一直没有数据交互的情况下，如果客户端的「主机崩溃」了，会发生什么。\n\n\n客户端主机崩溃了，服务端是**无法感知到的**，在加上服务端没有开启 TCP keepalive，又没有数据交互的情况下，**服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态**，直到服务端重启进程。\n\n所以，我们可以得知一个点，在没有使用 TCP 保活机制且双方不传输数据的情况下，一方的 TCP 连接处在 ESTABLISHED 状态，并不代表另一方的连接还一定正常。\n\n## 进程崩溃\n\n\n\u003e 那题目中的「进程崩溃」的情况呢？\n\nTCP 的连接信息是由内核维护的，所以当服务端的进程崩溃后，内核需要回收该进程的所有 TCP 连接资源，于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与，所以即使服务端的进程退出了，还是能与客户端完成 TCP四次挥手的过程。\n\n我自己做了实验，使用 kill -9 来模拟进程崩溃的情况，发现**在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手**。\n\n\n所以，即使没有开启 TCP keepalive，且双方也没有数据交互的情况下，如果其中一方的进程发生了崩溃，这个过程操作系统是可以感知的到的，于是就会发送 FIN 报文给对方，然后与对方进行 TCP 四次挥手。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021061513405211.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n---\n\n## 有数据传输的场景\n\n以上就是对这个面试题的回答，接下来我们看看在「**有数据传输**」的场景下的一些异常情况：\n\n- 第一种，客户端主机宕机，又迅速重启，会发生什么？\n- 第二种，客户端主机宕机，一直没有重启，会发生什么？\n\n### 客户端主机宕机，又迅速重启\n\n在客户端主机宕机后，服务端向客户端发送的报文会得不到任何的响应，在一定时长后，服务端就会触发**超时重传**机制，重传未得到响应的报文。\n\n服务端重传报文的过程中，客户端主机重启完成后，客户端的内核就会接收重传的报文，然后根据报文的信息传递给对应的进程：\n- 如果客户端主机上**没有**进程绑定该 TCP 报文的目标端口号，那么客户端内核就会**回复 RST 报文，重置该 TCP 连接**；\n- 如果客户端主机上**有**进程绑定该 TCP 报文的目标端口号，由于客户端主机重启后，之前的 TCP 连接的数据结构已经丢失了，客户端内核里协议栈会发现找不到该 TCP 连接的 socket 结构体，于是就会**回复 RST 报文，重置该 TCP 连接**。\n\n所以，**只要有一方重启完成后，收到之前 TCP 连接的报文，都会回复 RST 报文，以断开连接**。\n\n\n### 客户端主机宕机，一直没有重启\n\n这种情况，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，于是服务端的 TCP 连接就会断开。\n\n![](https://img-blog.csdnimg.cn/20210615134110763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\u003e 那 TCP 的数据报文具体重传几次呢？\n\n在 Linux 系统中，提供一个叫 tcp_retries2 配置项，默认值是 15，如下图：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210615134059647.png)\n\n\n这个内核参数是控制，在 TCP 连接建立的情况下，超时重传的最大次数。\n\n不过 tcp_retries2 设置了 15 次，并不代表 TCP 超时重传了 15 次才会通知应用程序终止该 TCP 连接，**内核会根据 tcp_retries2 设置的值，计算出一个 timeout**（*如果 tcp_retries2 =15，那么计算得到的 timeout = 924600 ms*），**如果重传间隔超过这个 timeout，则认为超过了阈值，就会停止重传，然后就会断开 TCP 连接**。\n\n在发生超时重传的过程中，每一轮的超时时间（RTO）都是**倍数增长**的，比如如果第一轮 RTO 是 200 毫秒，那么第二轮 RTO 是 400 毫秒，第三轮 RTO 是 800 毫秒，以此类推。\n\n而 RTO 是基于 RTT（一个包的往返时间） 来计算的，如果 RTT 较大，那么计算出来的 RTO 就越大，那么经过几轮重传后，很快就达到了上面的 timeout 值了。\n\n举个例子，如果 tcp_retries2 =15，那么计算得到的 timeout = 924600 ms，如果重传总间隔时长达到了 timeout 就会停止重传，然后就会断开 TCP 连接：\n\n- 如果 RTT 比较小，那么 RTO 初始值就约等于下限 200ms，也就是第一轮的超时时间是 200 毫秒，由于 timeout 总时长是 924600 ms，表现出来的现象刚好就是重传了 15 次，超过了 timeout 值，从而断开 TCP 连接\n- 如果 RTT 比较大，假设 RTO 初始值计算得到的是 1000 ms，也就是第一轮的超时时间是 1 秒，那么根本不需要重传 15 次，重传总间隔就会超过 924600 ms。\n\n最小 RTO 和最大 RTO 是在 Linux 内核中定义好了：\n\n```c\n#define TCP_RTO_MAX ((unsigned)(120*HZ))\n#define TCP_RTO_MIN ((unsigned)(HZ/5))\n```\n\nLinux 2.6+ 使用 1000 毫秒的 HZ，因此`TCP_RTO_MIN`约为 200 毫秒，`TCP_RTO_MAX`约为 120 秒。\n\n如果`tcp_retries`设置为`15`，且  RTT 比较小，那么 RTO 初始值就约等于下限 200ms，这意味着**它需要 924.6 秒**才能将断开的 TCP 连接通知给上层（即应用程序），每一轮的 RTO 增长关系如下表格：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/2021061513410645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n---\n\n## 总结\n\n如果「**客户端进程崩溃**」，客户端的进程在发生崩溃的时候，内核会发送 FIN 报文，与服务端进行四次挥手。\n\n但是，「**客户端主机宕机**」，那么是不会发生四次挥手的，具体后续会发生什么？还要看服务端会不会发送数据？\n\n- 如果服务端会发送数据，由于客户端已经不存在，收不到数据报文的响应报文，服务端的数据报文会超时重传，当重传总间隔时长达到一定阈值（内核会根据 tcp_retries2 设置的值计算出一个阈值）后，会断开 TCP 连接；\n- 如果服务端一直不会发送数据，再看服务端有没有开启 TCP keepalive 机制？\n  - 如果有开启，服务端在一段时间没有进行数据交互时，会触发 TCP keepalive 机制，探测对方是否存在，如果探测到对方已经消亡，则会断开自身的 TCP 连接；\n  - 如果没有开启，服务端的 TCP 连接会一直存在，并且一直保持在 ESTABLISHED 状态。\n\n最后说句，TCP 牛逼，啥异常都考虑到了。\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_drop":{"title":"tcp_drop","content":"# 4.21 用了 TCP 协议，数据一定不会丢吗？\n\n\u003e来源：公众号@小白debug\n\u003e\n\u003e原文地址：[用了 TCP 协议，数据一定不会丢吗？](https://mp.weixin.qq.com/s/XNJoaVnYT1SxHsdNWeAaUw)\n\n大家后，我是小林。\n\n问大家一句：TCP 是一个可靠的传输协议，那它一定能保证数据不丢失吗？\n\n这次，就跟大家探讨这个问题。\n\n## 数据包的发送流程\n\n首先，我们两个手机的绿皮聊天软件客户端，要通信，中间会通过它们家服务器。大概长这样。\n\n![聊天软件三端通信](https://img-blog.csdnimg.cn/img_convert/1d0a1d60ca4f720423911cf8f25c4ac3.png)\n\n但为了**简化模型**，我们把中间的服务器给省略掉，假设这是个端到端的通信。且为了保证消息的可靠性，我们盲猜它们之间用的是**TCP协议**进行通信。\n\n![聊天软件两端通信](https://img-blog.csdnimg.cn/img_convert/7e8bae365b8d27560aac1cd28f501156.png)\n\n为了发送数据包，两端首先会通过**三次握手**，建立TCP连接。\n\n一个数据包，从聊天框里发出，消息会从**聊天软件**所在的**用户空间**拷贝到**内核空间**的**发送缓冲区（send buffer）**，数据包就这样顺着**传输层、网络层，进入到数据链路层，在这里数据包会经过流控（qdisc），再通过RingBuffer发到物理层的网卡**。数据就这样顺着**网卡**发到了**纷繁复杂**的网络世界里。这里头数据会经过n多个**路由器和交换机**之间的跳转，最后到达**目的机器的网卡**处。\n\n此时目的机器的网卡会通知**DMA**将数据包信息放到`RingBuffer`中，再触发一个**硬中断**给`CPU`，`CPU`触发**软中断**让`ksoftirqd`去`RingBuffer`收包，于是一个数据包就这样顺着**物理层，数据链路层，网络层，传输层**，最后从内核空间拷贝到用户空间里的**聊天软件**里。\n\n![网络发包收包全景图](https://img-blog.csdnimg.cn/img_convert/28e4d6b004530fbf75fe346d181baa81.png)\n\n\u003e 画了那么大一张图，只水了200字做解释，我多少是有些心痛的。\n\n到这里，抛开一些细节，大家大概知道了一个数据包从**发送到接收**的宏观过程。\n\n可以看到，这上面全是密密麻麻的**名词**。\n\n整条链路下来，有不少地方可能会发生丢包。\n\n但为了不让大家**保持蹲姿太久**影响身体健康，我这边只重点讲下几个**常见容易发生丢包的场景**。\n\n## 建立连接时丢包\n\nTCP协议会通过**三次握手**建立连接。大概长下面这样。\n\n![TCP三次握手](https://img-blog.csdnimg.cn/img_convert/923f5005edb536c0d07b096bbf2ca282.png)\n\n在服务端，第一次握手之后，会先建立个**半连接**，然后再发出第二次握手。这时候需要有个地方可以**暂存**这些半连接。这个地方就叫**半连接队列**。\n\n如果之后第三次握手来了，半连接就会升级为全连接，然后暂存到另外一个叫**全连接队列**的地方，坐等程序执行`accept()`方法将其取走使用。\n\n![半连接队列和全连接队列](https://img-blog.csdnimg.cn/img_convert/02a78bb83fe167324f26e8c910d7a7a2.png)\n\n是队列就有长度，有长度就有可能会满，如果它们**满了**，那新来的包就会被**丢弃**。\n\n可以通过下面的方式查看是否存在这种丢包行为。\n\n```shell\n# 全连接队列溢出次数\n# netstat -s | grep overflowed\n    4343 times the listen queue of a socket overflowed\n\n# 半连接队列溢出次数\n# netstat -s | grep -i \"SYNs to LISTEN sockets dropped\"\n    109 times the listen queue of a socket overflowed \n```\n\n从现象来看就是连接建立失败。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/591d630098b4fc5316a5005f1e94b844.png)\n\n## 流量控制丢包\n\n应用层能发网络数据包的软件有那么多，如果所有数据不加控制一股脑冲入到网卡，网卡会吃不消，那怎么办？让数据按一定的规则排个队依次处理，也就是所谓的**qdisc**(**Q**ueueing **Disc**iplines，排队规则)，这也是我们常说的**流量控制**机制。\n\n排队，得先有个队列，而队列有个**长度**。\n\n我们可以通过下面的`ifconfig`命令查看到，里面涉及到的`txqueuelen`后面的数字`1000`，其实就是流控队列的长度。\n\n当发送数据过快，流控队列长度`txqueuelen`又不够大时，就容易出现**丢包**现象。\n\n![qdisc丢包](https://img-blog.csdnimg.cn/img_convert/6f2821018be08a2f27561155e8085de4.png)\n\n可以通过下面的`ifconfig`命令，查看TX下的dropped字段，当它大于0时，则**有可能**是发生了流控丢包。\n\n```shell\n# ifconfig eth0\neth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e  mtu 1500\n        inet 172.21.66.69  netmask 255.255.240.0  broadcast 172.21.79.255\n        inet6 fe80::216:3eff:fe25:269f  prefixlen 64  scopeid 0x20\u003clink\u003e\n        ether 00:16:3e:25:26:9f  txqueuelen 1000  (Ethernet)\n        RX packets 6962682  bytes 1119047079 (1.0 GiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 9688919  bytes 2072511384 (1.9 GiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n```\n\n当遇到这种情况时，我们可以尝试修改下流控队列的长度。比如像下面这样将eth0网卡的流控队列长度从1000提升为1500.\n\n```shell\n# ifconfig eth0 txqueuelen 1500\n```\n\n## 网卡丢包\n\n网卡和它的驱动导致丢包的场景也比较常见，原因很多，比如**网线质量差，接触不良**。除此之外，我们来聊几个常见的场景。\n\n### RingBuffer过小导致丢包\n\n上面提到，在接收数据时，会将数据暂存到`RingBuffer`接收缓冲区中，然后等着内核触发软中断慢慢收走。如果这个**缓冲区过小**，而这时候发送的数据又过快，就有可能发生溢出，此时也会产生**丢包**。\n\n![RingBuffer满了导致丢包](https://img-blog.csdnimg.cn/img_convert/8f3ed2d6c4e2e154849f1e661528fe89.png)\n\n我们可以通过下面的命令去查看是否发生过这样的事情。\n\n```shell\n# ifconfig\neth0:  RX errors 0  dropped 0  overruns 0  frame 0\n```\n\n查看上面的`overruns`指标，它记录了由于`RingBuffer`长度不足导致的溢出次数。\n\n当然，用`ethtool`命令也能查看。\n\n```shell\n# ethtool -S eth0|grep rx_queue_0_drops\n```\n\n但这里需要注意的是，因为一个网卡里是可以有**多个RingBuffer**的，所以上面的`rx_queue_0_drops`里的0代表的是**第0个RingBuffer**的丢包数，对于多队列的网卡，这个0还可以改成其他数字。但我的家庭条件不允许我看其他队列的丢包数，所以上面的命令对我来说是够用了。。。\n\n当发现有这类型丢包的时候，可以通过下面的命令查看当前网卡的配置。\n\n```shell\n#ethtool -g eth0\nRing parameters for eth0:\nPre-set maximums:\nRX:        4096\nRX Mini:    0\nRX Jumbo:    0\nTX:        4096\nCurrent hardware settings:\nRX:        1024\nRX Mini:    0\nRX Jumbo:    0\nTX:        1024\n```\n\n上面的输出内容，含义是**RingBuffer最大支持4096的长度，但现在实际只用了1024。**\n\n想要修改这个长度可以执行`ethtool -G eth1 rx 4096 tx 4096`将发送和接收RingBuffer的长度都改为4096。\n\n**RingBuffer**增大之后，可以减少因为容量小而导致的丢包情况。\n\n### 网卡性能不足\n\n网卡作为硬件，**传输速度是有上限的**。当网络传输速度过大，达到网卡上限时，就会发生丢包。这种情况一般常见于压测场景。\n\n我们可以通过`ethtool`加网卡名，获得当前网卡支持的最大速度。\n\n```shell\n# ethtool eth0\nSettings for eth0:\n    Speed: 10000Mb/s\n```\n\n可以看到，我这边用的网卡能支持的最大传输速度**speed=1000Mb/s**。\n\n也就是俗称的千兆网卡，但注意这里的单位是**Mb**，这里的**b是指bit，而不是Byte。1Byte=8bit**。所以10000Mb/s还要除以8，也就是理论上网卡最大传输速度是`1000/8 = 125MB/s`。\n\n我们可以通过`sar命令`从网络接口层面来分析数据包的收发情况。\n\n```shell\n# sar -n DEV 1\nLinux 3.10.0-1127.19.1.el7.x86_64      2022年07月27日     _x86_64_    (1 CPU)\n\n08时35分39秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s    rxcmp/s   txcmp/s  rxmcst/s\n08时35分40秒      eth0      6.06      4.04      0.35    121682.33   0.00    0.00     0.00\n```\n\n其中 **txkB/s是指当前每秒发送的字节（byte）总数，rxkB/s是指每秒接收的字节（byte）总数**。\n\n当两者加起来的值约等于`12~13w字节`的时候，也就对应大概`125MB/s`的传输速度。此时达到网卡性能极限，就会开始丢包。\n\n遇到这个问题，优先看下你的服务是不是真有这么大的**真实流量**，如果是的话可以考虑下拆分服务，或者就忍痛充钱升级下配置吧。\n\n## 接收缓冲区丢包\n\n我们一般使用`TCP socket`进行网络编程的时候，内核都会分配一个**发送缓冲区**和一个**接收缓冲区**。\n\n当我们想要发一个数据包，会在代码里执行`send(msg)`，这时候数据包并不是一把梭直接就走网卡飞出去的。而是将数据拷贝到内核**发送缓冲区**就完事**返回**了，至于**什么时候发数据，发多少数据**，这个后续由内核自己做决定。\n\n![tcp_sendmsg逻辑](https://img-blog.csdnimg.cn/img_convert/9cd22437777205662048c73cc5855add.png)\n\n而**接收缓冲区**作用也类似，从外部网络收到的数据包就暂存在这个地方，然后坐等用户空间的应用程序将数据包取走。\n\n这两个缓冲区是有大小限制的，可以通过下面的命令去查看。\n\n```shell\n# 查看接收缓冲区\n# sysctl net.ipv4.tcp_rmem\nnet.ipv4.tcp_rmem = 4096    87380   6291456\n\n# 查看发送缓冲区\n# sysctl net.ipv4.tcp_wmem\nnet.ipv4.tcp_wmem = 4096    16384   4194304\n```\n\n不管是接收缓冲区还是发送缓冲区，都能看到三个数值，分别对应缓冲区的**最小值，默认值和最大值 （min、default、max）。缓冲区会在min和max之间动态调整。**\n\n**那么问题来了，如果缓冲区设置过小会怎么样？**\n\n对于**发送缓冲区**，执行send的时候，如果是**阻塞**调用，那就会等，等到缓冲区有空位可以发数据。\n\n![send阻塞](https://img-blog.csdnimg.cn/img_convert/7312e536393463dcf0d57aeb07f28ed5.gif)\n\n如果是**非阻塞**调用，就会**立刻返回**一个 `EAGAIN` 错误信息，意思是  `Try again`。让应用程序下次再重试。这种情况下一般不会发生丢包。\n\n![send非阻塞](https://img-blog.csdnimg.cn/img_convert/f378a299ca60c490ee5437e1143916c8.gif)\n\n当接受缓冲区满了，事情就不一样了，它的TCP接收窗口会变为0，也就是所谓的**零窗口**，并且会通过数据包里的`win=0`，告诉发送端，\"球球了，顶不住了，别发了\"。一般这种情况下，发送端就该停止发消息了，但如果这时候确实还有数据发来，就会发生**丢包**。\n\n![recv_buffer丢包](https://img-blog.csdnimg.cn/img_convert/2df66c2e1d9f1245813e8d1de7482e0c.png)\n\n我们可以通过下面的命令里的`TCPRcvQDrop`查看到有没有发生过这种丢包现象。\n\n```shell\ncat /proc/net/netstat\nTcpExt: SyncookiesSent TCPRcvQDrop SyncookiesFailed\nTcpExt: 0              157              60116\n```\n\n但是说个伤心的事情，我们一般也看不到这个`TCPRcvQDrop`，因为这个是`5.9版本`里引入的打点，而我们的服务器用的一般是`2.x~3.x`左右版本。你可以通过下面的命令查看下你用的是什么版本的linux内核。\n\n```shell\n# cat /proc/version\nLinux version 3.10.0-1127.19.1.el7.x86_64\n```\n\n## 两端之间的网络丢包\n\n前面提到的是两端机器内部的网络丢包，除此之外，两端之间那么长的一条链路都属于外部网络，这中间有各种路由器和交换机还有光缆啥的，丢包也是很经常发生的。\n\n这些丢包行为发生在中间链路的某些个机器上，我们当然是没权限去登录这些机器。但我们可以通过一些命令观察整个链路的连通情况。\n\n### **ping命令查看丢包**\n\n比如我们知道目的地的域名是 `baidu.com`。想知道你的机器到baidu服务器之间，有没有产生丢包行为。可以使用ping命令。\n\n![ping查看丢包](https://img-blog.csdnimg.cn/img_convert/56bdca9995c0c2a343b2b73b67933b78.png)\n\n倒数第二行里有个`100% packet loss`，意思是丢包率100%。\n\n但这样其实你只能知道**你的机器和目的机器之间有没有丢包。**\n\n**那如果你想知道你和目的机器之间的这条链路，哪个节点丢包了，有没有办法呢?**\n\n有。\n\n### **mtr命令**\n\nmtr命令可以查看到你的机器和目的机器之间的每个节点的丢包情况。\n\n像下面这样执行命令。\n\n![mtr_icmp](https://img-blog.csdnimg.cn/img_convert/4a2d8dbfb648bcced864fb653af9f036.png)\n\n其中 -r 是指report，以报告的形式打印结果。\n\n可以看到`Host`那一列，出现的都是链路中间每一跳的机器，`Loss`的那一列就是指这一跳对应的丢包率。\n\n需要注意的是，中间有一些是host是`???`，那个是因为**mtr默认用的是ICMP包**，有些节点限制了**ICMP包**，导致不能正常展示。\n\n我们可以在mtr命令里加个`-u`，也就是使用**udp包**，就能看到部分???对应的IP。\n\n![mtr-udp](https://img-blog.csdnimg.cn/img_convert/0650adc524ab7d82028dc83cfc9961e1.png)\n\n把**ICMP包和UDP包的结果**拼在一起看，就是**比较完整**的链路图了。\n\n还有个小细节，`Loss`那一列，我们在icmp的场景下，关注**最后一行**，如果是0%，那不管前面loss是100%还是80%都无所谓，那些都是**节点限制**导致的**虚报**。\n\n但如果**最后一行是20%，再往前几行都是20%左右**，那说明丢包就是从最接近的那一行开始产生的，长时间是这样，那很可能这一跳出了点问题。如果是公司内网的话，你可以带着这条线索去找对应的网络同事。如果是外网的话，那耐心点等等吧，别人家的开发会比你更着急。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/7142a4e285024dc6aadea4255984c485.png)\n\n## 发生丢包了怎么办\n\n说了这么多。只是想告诉大家，**丢包是很常见的，几乎不可避免的一件事情**。\n\n但问题来了，发生丢包了怎么办？\n\n这个好办，用**TCP协议**去做传输。\n\n![TCP是什么](https://img-blog.csdnimg.cn/img_convert/b2225e071fec7cfb240aa295ed4037bf.png)\n\n建立了TCP连接的两端，发送端在发出数据后会等待接收端回复`ack包`，`ack包`的目的是为了告诉对方自己确实收到了数据，但如果中间链路发生了丢包，那发送端会迟迟收不到确认ack，于是就会进行**重传**。以此来保证每个数据包都确确实实到达了接收端。\n\n假设现在网断了，我们还用聊天软件发消息，聊天软件会使用TCP不断尝试重传数据，**如果重传期间网络恢复了**，那数据就能正常发过去。但如果多次重试直到超时都还是失败，这时候你将收获一个**红色感叹号**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c1460d52efe7c5e4d80c2f7160d5b126.png)\n\n这时候问题又来了。\n\n假设**某绿皮聊天软件用的就是TCP协议。**\n\n在聊天的时候， 发生丢包了，丢包了会**重试**，重试失败了还会出现**红色感叹号。**\n\n于是乎，问题就变成了，**用了 TCP 协议，就一定不会丢包吗？**\n\n## 用了TCP协议就一定不会丢包吗\n\n我们知道TCP位于**传输层**，在它的上面还有各种**应用层协议**，比如常见的HTTP或者各类RPC协议。\n\n![四层网络协议](https://img-blog.csdnimg.cn/img_convert/c6794dd51c8780f12e4022fc964ebb0a.png)\n\nTCP保证的可靠性，是**传输层的可靠性**。也就是说，**TCP只保证数据从A机器的传输层可靠地发到B机器的传输层。**\n\n至于数据到了接收端的传输层之后，能不能保证到应用层，TCP并不管。\n\n假设现在，我们输入一条消息，从聊天框发出，走到**传输层TCP协议的发送缓冲区**，不管中间有没有丢包，最后通过重传都保证发到了对方的**传输层TCP接收缓冲区**，此时接收端回复了一个`ack`，发送端收到这个`ack`后就会将自己**发送缓冲区**里的消息给扔掉。到这里TCP的任务就结束了。\n\nTCP任务是结束了，但聊天软件的任务没结束。\n\n**聊天软件还需要将数据从TCP的接收缓冲区里读出来，如果在读出来这一刻，手机由于内存不足或其他各种原因，导致软件崩溃闪退了。**\n\n发送端以为自己发的消息已经发给对方了，但接收端却并没有收到这条消息。\n\n于是乎，**消息就丢了。**\n\n![使用TCP协议却发生丢包](https://img-blog.csdnimg.cn/img_convert/9286ab84bcaa74576bc11c8e9322fee9.png)\n\n**虽然概率很小，但它就是发生了**。\n\n合情合理，逻辑自洽。\n\n## 这类丢包问题怎么解决？\n\n故事到这里也到尾声了，感动之余，我们来**聊点掏心窝子的话**。\n\n**其实前面说的都对，没有一句是假话**。\n\n但某绿皮聊天软件这么成熟，怎么可能没考虑过这一点呢。\n\n大家应该还记得我们文章开头提到过，**为了简单**，就将服务器那一方给省略了，从三端通信变成了两端通信，所以才有了这个丢包问题。\n\n**现在我们重新将服务器加回来。**\n\n![聊天软件三端通信](https://img-blog.csdnimg.cn/img_convert/d53659df39d64db4780d2816bd8314d1.png)\n\n大家有没有发现，有时候我们在手机里聊了一大堆内容，然后登录电脑版，它能将最近的聊天记录都同步到电脑版上。也就是说服务器**可能**记录了我们最近发过什么数据，假设**每条消息都有个id**，服务器和聊天软件每次都拿**最新消息的id**进行对比，就能知道两端消息是否一致，就像**对账**一样。\n\n对于**发送方**，只要定时跟服务端的内容对账一下，就知道哪条消息没发送成功，直接重发就好了。\n\n如果**接收方**的聊天软件崩溃了，重启后跟服务器稍微通信一下就知道少了哪条数据，同步上来就是了，所以也不存在上面提到的丢包情况。\n\n可以看出，**TCP只保证传输层的消息可靠性，并不保证应用层的消息可靠性。如果我们还想保证应用层的消息可靠性，就需要应用层自己去实现逻辑做保证。**\n\n那么问题叒来了，**两端通信的时候也能对账，为什么还要引入第三端服务器？**\n\n主要有三个原因。\n\n- 第一，如果是两端通信，你聊天软件里有`1000个`好友，你就得建立`1000个`连接。但如果引入服务端，你只需要跟服务器建立`1个`连接就够了，**聊天软件消耗的资源越少，手机就越省电**。\n- 第二，就是**安全问题**，如果还是两端通信，随便一个人找你对账一下，你就把聊天记录给同步过去了，这并不合适吧。如果对方别有用心，信息就泄露了。引入第三方服务端就可以很方便的做各种**鉴权**校验。\n- 第三，是**软件版本问题**。软件装到用户手机之后，软件更不更新就是由用户说了算了。如果还是两端通信，且两端的**软件版本跨度太大**，很容易产生各种兼容性问题，但引入第三端服务器，就可以强制部分过低版本升级，否则不能使用软件。但对于大部分兼容性问题，给服务端加兼容逻辑就好了，不需要强制用户更新软件。\n\n所以看到这里大家应该明白了，我把服务端去掉，并不单纯是**为了简单**。\n\n## 总结\n\n- 数据从发送端到接收端，链路很长，任何一个地方都可能发生丢包，几乎可以说丢包不可避免。\n- 平时没事也不用关注丢包，大部分时候TCP的重传机制保证了消息可靠性。\n- 当你发现服务异常的时候，比如接口延时很高，总是失败的时候，可以用ping或者mtr命令看下是不是中间链路发生了丢包。\n- TCP只保证传输层的消息可靠性，并不保证应用层的消息可靠性。如果我们还想保证应用层的消息可靠性，就需要应用层自己去实现逻辑做保证。\n\n----\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」***\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_feature":{"title":"tcp_feature","content":"# 4.2 TCP 重传、滑动窗口、流量控制、拥塞控制\n\nTCP **巨复杂**，它为了保证可靠性，用了巨多的机制来保证，真是个「伟大」的协议，写着写着发现这水太深了。。。\n\n本文的全部图片都是小林绘画的，非常的辛苦且累，不废话了，直接进入正文，Go！\n\n相信大家都知道 TCP 是一个可靠传输的协议，那它是如何保证可靠的呢？\n\n为了实现可靠性传输，需要考虑很多事情，例如数据的破坏、丢包、重复以及分片顺序混乱等问题。如不能解决这些问题，也就无从谈起可靠传输。\n\n那么，TCP 是通过序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输的。\n\n今天，将重点介绍 TCP 的**重传机制、滑动窗口、流量控制、拥塞控制。**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/3.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n---\n\n## 重传机制\n\nTCP 实现可靠传输的方式之一，是通过序列号与确认应答。\n\n在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。\n\n![正常的数据传输](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/4.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？\n\n所以 TCP 针对数据包丢失的情况，会用**重传机制**解决。\n\n接下来说说常见的重传机制：\n\n- 超时重传\n- 快速重传\n- SACK\n- D-SACK\n\n### 超时重传\n\n重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 `ACK` 确认应答报文，就会重发该数据，也就是我们常说的**超时重传**。\n\nTCP 会在以下两种情况发生超时重传：\n\n- 数据包丢失\n- 确认应答丢失\n\n![超时重传的两种情况](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/5.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n\u003e 超时时间应该设置为多少呢？\n\n我们先来了解一下什么是 `RTT`（Round-Trip Time 往返时延），从下图我们就可以知道：\n\n![RTT](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/6.jpg?)\n\n`RTT` 指的是**数据发送时刻到接收到确认的时刻的差值**，也就是包的往返时间。\n\n超时重传时间是以 `RTO` （Retransmission Timeout 超时重传时间）表示。\n\n假设在重传的情况下，超时时间 `RTO` 「较长或较短」时，会发生什么事情呢？\n\n![超时时间较长与较短](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/7.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n上图中有两种超时时间不同的情况：\n\n- 当超时时间 **RTO 较大**时，重发就慢，丢了老半天才重发，没有效率，性能差；\n- 当超时时间 **RTO 较小**时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。\n\n精确的测量超时时间 `RTO` 的值是非常重要的，这可让我们的重传机制更高效。\n\n根据上述的两种情况，我们可以得知，**超时重传时间 RTO 的值应该略大于报文往返  RTT 的值**。\n\n![RTO 应略大于 RTT](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/8.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n至此，可能大家觉得超时重传时间 `RTO` 的值计算，也不是很复杂嘛。\n\n好像就是在发送端发包时记下 `t0` ，然后接收端再把这个 `ack` 回来时再记一个 `t1`，于是 `RTT = t1 – t0`。没那么简单，**这只是一个采样，不能代表普遍情况**。\n\n实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个**动态变化的值**。\n\n我们来看看 Linux 是如何计算 `RTO` 的呢？\n\n估计往返时间，通常需要采样以下两个：\n\n- 需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个平滑 RTT 的值，而且这个值还是要不断变化的，因为网络状况不断地变化。\n- 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况。\n\nRFC6289 建议使用以下的公式计算 RTO：\n\n![RFC6289 建议的 RTO 计算 ](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/9.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n其中 `SRTT` 是计算平滑的RTT ，`DevRTR` 是计算平滑的RTT 与 最新 RTT 的差距。\n\n在 Linux 下，**α = 0.125，β = 0.25， μ = 1，∂ = 4**。别问怎么来的，问就是大量实验中调出来的。\n\n如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是**超时间隔加倍。**\n\n也就是**每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。**\n\n超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？\n\n于是就可以用「快速重传」机制来解决超时重发的时间等待。\n\n### 快速重传\n\nTCP 还有另外一种**快速重传（Fast Retransmit）机制**，它**不以时间为驱动，而是以数据驱动重传**。\n\n快速重传机制，是如何工作的呢？其实很简单，一图胜千言。\n\n![快速重传机制](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/10.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n在上图，发送方发出了 1，2，3，4，5 份数据：\n\n- 第一份 Seq1 先送到了，于是就 Ack 回 2；\n- 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；\n- 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；\n- **发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。**\n- 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。\n\n所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。\n\n快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是**重传的时候，是重传一个，还是重传所有的问题。**\n\n举个例子，假设发送方发了 6 个数据，编号的顺序是 Seq1 ~ Seq6 ，但是 Seq2、Seq3 都丢失了，那么接收方在收到 Seq4、Seq5、Seq6 时，都是回复 ACK2 给发送方，但是发送方并不清楚这连续的 ACK2 是接收方收到哪个报文而回复的， 那是选择重传 Seq2 一个报文，还是重传  Seq2 之后已发送的所有报文呢（Seq2、Seq3、 Seq4、Seq5、 Seq6） 呢？\n\n- 如果只选择重传 Seq2 一个报文，那么重传的效率很低。因为对于丢失的 Seq3 报文，还得在后续收到三个重复的 ACK3 才能触发重传。\n\n- 如果选择重传 Seq2 之后已发送的所有报文，虽然能同时重传已丢失的 Seq2 和 Seq3 报文，但是  Seq4、Seq5、Seq6 的报文是已经被接收过了，对于重传 Seq4 ～Seq6 折部分数据相当于做了一次无用功，浪费资源。\n\n可以看到，不管是重传一个报文，还是重传已发送的报文，都存在问题。\n\n为了解决不知道该重传哪些 TCP 报文，于是就有 `SACK` 方法。\n\n### SACK 方法\n\n还有一种实现重传机制的方式叫：`SACK`（ Selective Acknowledgment）， **选择性确认**。\n\n这种方式需要在 TCP 头部「选项」字段里加一个 `SACK` 的东西，它**可以将已收到的数据的信息发送给「发送方」**，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以**只重传丢失的数据**。\n\n如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 `SACK` 信息发现只有 `200~299` 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。\n\n![选择性确认](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/11.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n如果要支持 `SACK`，必须双方都要支持。在 Linux 下，可以通过 `net.ipv4.tcp_sack` 参数打开这个功能（Linux 2.4 后默认打开）。\n\n### Duplicate SACK\n\nDuplicate SACK 又称 `D-SACK`，其主要**使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。**\n\n下面举例两个栗子，来说明 `D-SACK` 的作用。\n\n*栗子一号：ACK 丢包*\n\n![ACK 丢包](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/12.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n- 「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499）\n- **于是「接收方」发现数据是重复收到的，于是回了一个 SACK = 3000~3500**，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 `D-SACK`。\n- 这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。\n\n*栗子二号：网络延时*\n\n![网络延时](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/13.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n- 数据包（1000~1499） 被网络延迟了，导致「发送方」没有收到 Ack 1500 的确认报文。\n- 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」；\n- **所以「接收方」回了一个 SACK=1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。**\n- 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。\n\n\n可见，`D-SACK` 有这么几个好处：\n\n1. 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了;\n2. 可以知道是不是「发送方」的数据包被网络延迟了;\n3. 可以知道网络中是不是把「发送方」的数据包给复制了;\n\n在 Linux 下可以通过 `net.ipv4.tcp_dsack` 参数开启/关闭这个功能（Linux 2.4 后默认打开）。\n\n---\n\n## 滑动窗口\n\n\u003e 引入窗口概念的原因\n\n我们都知道 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。\n\n这个模式就有点像我和你面对面聊天，你一句我一句。但这种方式的缺点是效率比较低的。\n\n如果你说完一句话，我在处理其他事情，没有及时回复你，那你不是要干等着我做完其他事情后，我回复你，你才能说下一句话，很显然这不现实。\n\n![按数据包进行确认应答](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/14.jpg?)\n\n所以，这样的传输方式有一个缺点：数据包的**往返时间越长，通信的效率就越低**。\n\n为解决这个问题，TCP 引入了**窗口**这个概念。即使在往返时间较长的情况下，它也不会降低网络通信的效率。\n\n那么有了窗口，就可以指定窗口大小，窗口大小就是指**无需等待确认应答，而可以继续发送数据的最大值**。\n\n窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。\n\n假设窗口大小为 `3` 个 TCP 段，那么发送方就可以「连续发送」 `3` 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下一个确认应答进行确认」。如下图：\n\n![用滑动窗口方式并行处理](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/15.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通过下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据「接收方」都收到了。这个模式就叫**累计确认**或者**累计应答**。\n\n\u003e 窗口大小由哪一方决定？\n\nTCP 头里有一个字段叫 `Window`，也就是窗口大小。\n\n**这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。**\n\n所以，通常窗口的大小是由接收方的窗口大小来决定的。\n\n发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。\n\n\n\u003e 发送方的滑动窗口\n\n我们先来看看发送方的窗口，下图就是发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/16.jpg?)\n\n- #1 是已发送并收到 ACK确认的数据：1~31 字节\n- #2 是已发送但未收到 ACK确认的数据：32~45 字节\n- #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节\n- #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后\n\n在下图，当发送方把数据「全部」都一下发送出去后，可用窗口的大小就为 0 了，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据了。\n\n![可用窗口耗尽](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/17.jpg?)\n\n在下图，当收到之前发送的数据 `32~36` 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则**滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认**，接下来 `52~56` 字节又变成了可用窗口，那么后续也就可以发送 `52~56` 这 5 个字节的数据了。\n\n![32 ~ 36 字节已确认](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/18.jpg)\n\n\u003e 程序是如何表示发送方的四个部分的呢？\n\nTCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移）。\n\n![SND.WND、SND.UN、SND.NXT](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/19.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n- `SND.WND`：表示发送窗口的大小（大小是由接收方指定的）；\n- `SND.UNA`（*Send Unacknoleged*）：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节。\n- `SND.NXT`：也是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的第一个字节。\n\n- 指向 #4 的第一个字节是个相对指针，它需要 `SND.UNA` 指针加上 `SND.WND` 大小的偏移量，就可以指向 #4 的第一个字节了。\n\n那么可用窗口大小的计算就可以是：\n\n**可用窗口大小 = SND.WND -（SND.NXT - SND.UNA）**\n\n\u003e 接收方的滑动窗口\n\n接下来我们看看接收方的窗口，接收窗口相对简单一些，根据处理的情况划分成三个部分：\n\n- #1 + #2 是已成功接收并确认的数据（等待应用进程读取）；\n- #3 是未收到数据但可以接收的数据；\n- #4 未收到数据并不可以接收的数据；\n\n![接收窗口](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/20.jpg)\n\n其中三个接收部分，使用两个指针进行划分:\n\n- `RCV.WND`：表示接收窗口的大小，它会通告给发送方。\n- `RCV.NXT`：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节。\n- 指向 #4 的第一个字节是个相对指针，它需要 `RCV.NXT` 指针加上 `RCV.WND` 大小的偏移量，就可以指向 #4 的第一个字节了。\n\n\u003e 接收窗口和发送窗口的大小是相等的吗？\n\n并不是完全相等，接收窗口的大小是**约等于**发送窗口的大小的。\n\n因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。\n\n---\n\n## 流量控制\n\n发送方不能无脑的发数据给接收方，要考虑接收方处理能力。\n\n如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。\n\n为了解决这种现象发生，**TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。**\n\n下面举个栗子，为了简单起见，假设以下场景：\n\n- 客户端是接收方，服务端是发送方\n- 假设接收窗口和发送窗口相同，都为 `200` \n- 假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响\n\n\n![流量控制](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/21.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n根据上图的流量控制，说明下每个过程：\n\n1. 客户端向服务端发送请求数据报文。这里要说明下，本次例子是把服务端作为发送方，所以没有画出服务端的接收窗口。\n2. 服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 `Usable` 减少为 120 字节，同时 `SND.NXT` 指针也向右偏移 80 字节后，指向 321，**这意味着下次发送数据的时候，序列号是 321。**\n3. 客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节，`RCV.NXT` 也就指向 321，**这意味着客户端期望的下一个报文的序列号是 321**，接着发送确认报文给服务端。\n4. 服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法再继续发送数据。\n5. 客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节，`RCV.NXT` 也就指向 441，接着发送确认报文给服务端。\n6. 服务端收到对 80 字节数据的确认报文后，`SND.UNA` 指针往右偏移后指向 321，于是可用窗口 `Usable` 增大到 80。\n7. 服务端收到对 120 字节数据的确认报文后，`SND.UNA` 指针往右偏移后指向 441，于是可用窗口 `Usable` 增大到 200。\n8. 服务端可以继续发送了，于是发送了 160 字节的数据后，`SND.NXT` 指向 601，于是可用窗口  `Usable` 减少到 40。\n9. 客户端收到 160 字节后，接收窗口往右移动了 160 字节，`RCV.NXT` 也就是指向了 601，接着发送确认报文给服务端。\n10. 服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 `SND.UNA` 指针偏移了 160 后指向 601，可用窗口 `Usable` 也就增大至了 200。\n\n### 操作系统缓冲区与滑动窗口的关系\n\n前面的流量控制例子，我们假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会**被操作系统调整**。\n\n当应用进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。\n\n\u003e 那操作系统的缓冲区，是如何影响发送窗口和接收窗口的呢？\n\n*我们先来看看第一个例子。*\n\n当应用程序没有及时读取缓存时，发送窗口和接收窗口的变化。\n\n考虑以下场景：\n\n- 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 `360`；\n- 服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/22.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n根据上图的流量控制，说明下每个过程：\n\n1. 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140）。\n2. 服务端收到 140 字节数据，**但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100）**，最后发送确认信息时，将窗口大小通告给客户端。\n3. 客户端收到确认和窗口通告报文后，发送窗口减少为 260。\n4. 客户端发送 180 字节数据，此时可用窗口减少到 80。\n5. 服务端收到 180 字节数据，**但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区，于是接收窗口收缩到了 80 （260 - 180）**，并在发送确认信息时，通过窗口大小给客户端。\n6. 客户端收到确认和窗口通告报文后，发送窗口减少为 80。\n7. 客户端发送 80 字节数据后，可用窗口耗尽。\n8. 服务端收到 80 字节数据，**但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是接收窗口收缩到了 0**，并在发送确认信息时，通过窗口大小给客户端。\n9. 客户端收到确认和窗口通告报文后，发送窗口减少为 0。\n\n可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变，这个内容后面会说，这里先简单提一下。\n\n*我们先来看看第二个例子。*\n\n当服务端系统资源非常紧张的时候，操作系统可能会直接减少了接收缓冲区大小，这时应用程序又无法及时读取缓存数据，那么这时候就有严重的事情发生了，会出现数据包丢失的现象。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/23.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n说明下每个过程：\n\n1. 客户端发送 140 字节的数据，于是可用窗口减少到了 220。\n2. **服务端因为现在非常的繁忙，操作系统于是就把接收缓存减少了 120 字节，当收到 140 字节数据后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大小从 360 收缩成了 100**，最后发送确认信息时，通告窗口大小给对方。\n3. 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40。\n4. 服务端收到了 180 字节数据时，**发现数据大小超过了接收窗口的大小，于是就把数据包丢失了。**\n5. 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。\n\n\n所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。\n\n**为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况。**\n\n### 窗口关闭\n\n在前面我们都看到了，TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控制。\n\n**如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。**\n\n\u003e 窗口关闭潜在的危险\n\n接收方向发送方通告窗口大小时，是通过 `ACK` 报文来通告的。\n\n那么，当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那麻烦就大了。\n\n![窗口关闭潜在的危险](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/24.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，如不采取措施，这种相互等待的过程，会造成了死锁的现象。\n\n\u003e TCP 是如何解决窗口关闭时，潜在的死锁现象呢？\n\n为了解决这个问题，TCP 为每个连接设有一个持续定时器，**只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。**\n\n如果持续计时器超时，就会发送**窗口探测 ( Window\nprobe ) 报文**，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。\n\n![窗口探测](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/25.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n- 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；\n- 如果接收窗口不是 0，那么死锁的局面就可以被打破了。\n\n窗口探测的次数一般为 3 次，每次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 `RST` 报文来中断连接。\n\n### 糊涂窗口综合症\n\n如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。\n\n到最后，**如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症**。\n\n要知道，我们的 `TCP + IP` 头有 `40` 个字节，为了传输那几个字节的数据，要搭上这么大的开销，这太不经济了。\n\n就好像一个可以承载 50 人的大巴车，每次来了一两个人，就直接发车。除非家里有矿的大巴司机，才敢这样玩，不然迟早破产。要解决这个问题也不难，大巴司机等乘客数量超过了 25 个，才认定可以发车。\n\n现举个糊涂窗口综合症的栗子，考虑以下场景：\n\n接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下：\n\n- 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据；\n- 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节；\n\n![糊涂窗口综合症](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/26.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n每个过程的窗口大小的变化，在图中都描述的很清楚了，可以发现窗口不断减少了，并且发送的数据都是比较小的了。\n\n\n所以，糊涂窗口综合症的现象是可以发生在发送方和接收方：\n\n- 接收方可以通告一个小的窗口\n- 而发送方可以发送小数据\n\n于是，要解决糊涂窗口综合症，就要同时解决上面两个问题就可以了：\n\n- 让接收方不通告小窗口给发送方\n- 让发送方避免发送小数据\n\n\u003e 怎么让接收方不通告小窗口呢？\n\n接收方通常的策略如下:\n\n当「窗口大小」小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会向发送方通告窗口为 `0`，也就阻止了发送方再发数据过来。\n\n等到接收方处理了一些数据后，窗口大小 \u003e= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。\n\n\u003e 怎么让发送方避免发送小数据呢？\n\n发送方通常的策略如下:\n\n使用 Nagle 算法，该算法的思路是延时处理，只有满足下面两个条件中的任意一个条件，才可以发送数据：\n\n- 条件一：要等到窗口大小 \u003e= `MSS` 并且 数据大小 \u003e= `MSS`；\n- 条件二：收到之前发送数据的 `ack` 回包；\n\n只要上面两个条件都不满足，发送方一直在囤积数据，直到满足上面的发送条件。\n\nNagle 伪代码如下：\n\n```c\nif 有数据要发送 {\n    if 可用窗口大小 \u003e= MSS and 可发送的数据 \u003e= MSS {\n    \t立刻发送MSS大小的数据\n    } else {\n        if 有未确认的数据 {\n            将数据放入缓存等待接收ACK\n        } else {\n            立刻发送数据\n        }\n    }\n}\n```\n\n注意，如果接收方不能满足「不通告小窗口给发送方」，那么即使开了 Nagle 算法，也无法避免糊涂窗口综合症，因为如果对端 ACK 回复很快的话（达到 Nagle 算法的条件二），Nagle 算法就不会拼接太多的数据包，这种情况下依然会有小数据包的传输，网络总体的利用率依然很低。\n\n所以，**接收方得满足「不通告小窗口给发送方」+ 发送方开启 Nagle 算法，才能避免糊涂窗口综合症**。\n\n另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。\n\n可以在 Socket 设置 `TCP_NODELAY` 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）\n\n```c\nsetsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)\u0026value, sizeof(int));\n```\n\n---\n\n## 拥塞控制\n\n\u003e 为什么要有拥塞控制呀，不是有流量控制了吗？\n\n前面的流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么。\n\n一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。\n\n**在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大....**\n\n所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发生拥塞时，TCP 会自我牺牲，降低发送的数据量。\n\n于是，就有了**拥塞控制**，控制的目的就是**避免「发送方」的数据填满整个网络。**\n\n为了在「发送方」调节所要发送数据的量，定义了一个叫做「**拥塞窗口**」的概念。\n\n\u003e 什么是拥塞窗口？和发送窗口有什么关系呢？\n\n**拥塞窗口 cwnd**是发送方维护的一个的状态变量，它会根据**网络的拥塞程度动态变化的**。\n\n我们在前面提到过发送窗口 `swnd` 和接收窗口 `rwnd` 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。\n\n拥塞窗口 `cwnd` 变化的规则：\n\n- 只要网络中没有出现拥塞，`cwnd` 就会增大；\n- 但网络中出现了拥塞，`cwnd` 就减少；\n\n\u003e 那么怎么知道当前网络是否出现了拥塞呢？\n\n其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是**发生了超时重传，就会认为网络出现了拥塞。**\n\n\n\u003e 拥塞控制有哪些控制算法？\n\n拥塞控制主要是四个算法：\n\n- 慢启动\n- 拥塞避免\n- 拥塞发生\n- 快速恢复\n\n### 慢启动\n\nTCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？\n\n慢启动的算法记住一个规则就行：**当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。**\n\n这里假定拥塞窗口 `cwnd` 和发送窗口 `swnd` 相等，下面举个栗子：\n\n- 连接建立完成后，一开始初始化 `cwnd = 1`，表示可以传一个 `MSS` 大小的数据。\n- 当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个 \n- 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个\n- 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。\n\n慢启动算法的变化过程如下图：\n\n![慢启动算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/27.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n\n可以看出慢启动算法，发包的个数是**指数性的增长**。\n\n\u003e 那慢启动涨到什么时候是个头呢？\n\n有一个叫慢启动门限  `ssthresh` （slow start threshold）状态变量。\n\n- 当 `cwnd` \u003c  `ssthresh` 时，使用慢启动算法。\n- 当 `cwnd` \u003e= `ssthresh` 时，就会使用「拥塞避免算法」。\n\n### 拥塞避免算法\n\n前面说道，当拥塞窗口 `cwnd` 「超过」慢启动门限 `ssthresh` 就会进入拥塞避免算法。\n\n一般来说 `ssthresh` 的大小是 `65535` 字节。\n\n那么进入拥塞避免算法后，它的规则是：**每当收到一个 ACK 时，cwnd 增加 1/cwnd。**\n\n接上前面的慢启动的栗子，现假定 `ssthresh` 为 `8`：\n\n- 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 `MSS` 大小的数据，变成了**线性增长。**\n\n拥塞避免算法的变化过程如下图：\n\n![拥塞避免](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/28.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。\n\n就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。\n\n当触发了重传机制，也就进入了「拥塞发生算法」。\n\n### 拥塞发生\n\n当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种：\n\n- 超时重传\n- 快速重传\n\n这两种使用的拥塞发送算法是不同的，接下来分别来说说。\n\n\u003e 发生超时重传的拥塞发生算法\n\n当发生了「超时重传」，则就会使用拥塞发生算法。\n\n这个时候，ssthresh 和 cwnd 的值会发生变化：\n\n- `ssthresh` 设为 `cwnd/2`，\n- `cwnd` 重置为 `1` （是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1）\n\n\u003e 怎么查看系统的  cwnd  初始化值？\n\nLinux 针对每一个 TCP 连接的 cwnd 初始化值是 10，也就是 10 个 MSS，我们可以用 ss -nli 命令查看每一个 TCP 连接的 cwnd 初始化值，如下图\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/cwnd.png)\n\n拥塞发生算法的变化如下图：\n\n![拥塞发送 —— 超时重传](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/29.jpg?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。\n\n就好像本来在秋名山高速漂移着，突然来个紧急刹车，轮胎受得了吗。。。\n\n\u003e 发生快速重传的拥塞发生算法\n\n还有更好的方式，前面我们讲过「快速重传算法」。当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。\n\nTCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，则 `ssthresh` 和 `cwnd` 变化如下：\n\n- `cwnd = cwnd/2` ，也就是设置为原来的一半;\n- `ssthresh = cwnd`;\n- 进入快速恢复算法\n\n### 快速恢复\n\n快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 `RTO` 超时那么强烈。\n\n正如前面所说，进入快速恢复之前，`cwnd` 和 `ssthresh` 已被更新了：\n\n- `cwnd = cwnd/2` ，也就是设置为原来的一半;\n- `ssthresh = cwnd`;\n\n然后，进入快速恢复算法如下：\n\n- 拥塞窗口 `cwnd = ssthresh + 3` （ 3 的意思是确认有 3 个数据包被收到了）；\n- 重传丢失的数据包；\n- 如果再收到重复的 ACK，那么 cwnd 增加 1；\n- 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated  ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态；\n\n快速恢复算法的变化过程如下图：\n\n![快速重传和快速恢复](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/拥塞发生-快速重传.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n也就是没有像「超时重传」一夜回到解放前，而是还在比较高的值，后续呈线性增长。\n\n::: tip\n\n很多人问题，快速恢复算法过程中，为什么收到新的数据后，cwnd 设置回了 ssthresh ？\n\n我在评论区看到@[muum641651](https://github.com/muum641651)回答的不错，这里贴出来给大家。\n\n我的理解是：\n\n1. 在快速恢复的过程中，首先 ssthresh = cwnd/2，然后 cwnd = ssthresh + 3，表示网络可能出现了阻塞，所以需要减小 cwnd 以避免，加 3 代表快速重传时已经确认接收到了 3 个重复的数据包；\n2. 随后继续重传丢失的数据包，如果再收到重复的 ACK，那么 cwnd 增加 1。加 1 代表每个收到的重复的 ACK 包，都已经离开了网络。这个过程的目的是尽快将丢失的数据包发给目标。\n3. 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，恢复过程结束。\n\n**首先，快速恢复是拥塞发生后慢启动的优化，其首要目的仍然是降低 cwnd 来减缓拥塞，所以必然会出现 cwnd 从大到小的改变。**\n\n**其次，过程2（cwnd逐渐加1）的存在是为了尽快将丢失的数据包发给目标，从而解决拥塞的根本问题（三次相同的 ACK 导致的快速重传），所以这一过程中 cwnd 反而是逐渐增大的。**\n\n:::\n\n---\n\n参考资料：\n\n[1] 趣谈网络协议专栏.刘超.极客时间\n\n[2] Web协议详解与抓包实战专栏.陶辉.极客时间\n\n[3] TCP/IP详解 卷1：协议.范建华 译.机械工业出版社\n\n[4] 图解TCP/IP.竹下隆史.人民邮电出版社\n\n[5] The TCP/IP Guide.Charles M. Kozierok.\n\n[6] TCP那些事（上）.陈皓.酷壳博客.\nhttps://coolshell.cn/articles/11564.html\n\n[7] TCP那些事（下）.陈皓.酷壳博客.https://coolshell.cn/articles/11609.html\n\n---\n\n## 读者问答\n\n\u003e 读者问：“整个看完收获很大，下面是我的一些疑问（稍后\n\u003e 会去确认）：\n\u003e 1.拥塞避免这一段，蓝色字体：每当收到一个\n\u003e ACK时，cwnd增加1/cwnd。是否应该是\n\u003e 1/ssthresh?否则不符合线性增长。\n\u003e 2.快速重传的拥塞发生算法，步骤一和步骤2是\n\u003e 否写反了？否则快速恢复算法中最后一步【如果\n\u003e 收到新数据的ACK后，设置cwnd为\n\u003e ssthresh,接看就进入了拥塞避免算法】没什么\n\u003e 意义。\n\u003e 3.对ssthresh的变化介绍的比较含糊。”\n\n1. 是 1/cwnd，你可以在 RFC2581 第 3 页找到答案  \n2. 没有写反，同样你可以在 RFC2581 第 5 页找到答案  \n3. ssthresh 就是慢启动门限，我觉得 ssthresh 我已经说的很清楚了，当然你可以找其他资料补充你的疑惑\n\n---\n\n是吧？ TCP 巨复杂吧？看完很累吧？ \n\n但这还只是 TCP 冰山一脚，它的更深处就由你们自己去探索啦。\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)\n\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_http_keepalive":{"title":"tcp_http_keepalive","content":"# 4.15 TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？\n\n大家好，我是小林。\n\n之前有读者问了我这么个问题：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20210715090027883.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODI3Njc0,size_16,color_FFFFFF,t_70)\n\n\n大致问题是，**TCP 的 Keepalive 和 HTTP 的 Keep-Alive 是一个东西吗？**\n\n这是个好问题，应该有不少人都会搞混，因为这两个东西看上去太像了，很容易误以为是同一个东西。\n\n事实上，**这两个完全是两样不同东西**，实现的层面也不同：\n- HTTP 的 Keep-Alive，是由**应用层（用户态）** 实现的，称为 HTTP 长连接；\n- TCP 的 Keepalive，是由 **TCP 层（内核态）** 实现的，称为 TCP 保活机制；\n\n接下来，分别说说它们。\n\n## HTTP 的 Keep-Alive\n\nHTTP 协议采用的是「请求-应答」的模式，也就是客户端发起了请求，服务端才会返回响应，一来一回这样子。\n\n![请求-应答](https://img-blog.csdnimg.cn/img_convert/6c062074058f40ae65ed722e2d082a90.png)\n\n\n由于 HTTP 是基于 TCP 传输协议实现的，客户端与服务端要进行 HTTP 通信前，需要先建立 TCP 连接，然后客户端发送 HTTP  请求，服务端收到后就返回响应，至此「请求-应答」的模式就完成了，随后就会释放 TCP 连接。\n\n![一个 HTTP 请求](https://img-blog.csdnimg.cn/img_convert/9acbaebbbe07cc870858a350052d9c87.png)\n\n\n如果每次请求都要经历这样的过程：建立 TCP -\u003e 请求资源 -\u003e 响应资源 -\u003e 释放连接，那么此方式就是 **HTTP 短连接**，如下图：\n\n\n![HTTP 短连接](https://img-blog.csdnimg.cn/img_convert/d6f6757c02e3afbf113d1048c937f8ee.png)\n\n\n这样实在太累人了，一次连接只能请求一次资源。\n\n能不能在第一个 HTTP 请求完后，先不断开 TCP 连接，让后续的 HTTP 请求继续使用此连接？\n\n当然可以，HTTP 的 Keep-Alive 就是实现了这个功能，可以使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，这个方法称为 **HTTP 长连接**。\n\n![HTTP 长连接](https://img-blog.csdnimg.cn/img_convert/d2b20d1cc03936332adb2a68512eb167.png)\n\nHTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n\n怎么才能使用 HTTP 的 Keep-Alive 功能？\n\n在 HTTP 1.0 中默认是关闭的，如果浏览器要开启 Keep-Alive，它必须在请求的包头中添加：\n\n\n```\nConnection: Keep-Alive\n```\n\n然后当服务器收到请求，作出回应的时候，它也添加一个头在响应中：\n\n```\nConnection: Keep-Alive\n```\n\n这样做，连接就不会中断，而是保持连接。当客户端发送另一个请求时，它会使用同一个连接。这一直继续到客户端或服务器端提出断开连接。\n\n**从 HTTP 1.1 开始， 就默认是开启了 Keep-Alive**，如果要关闭 Keep-Alive，需要在 HTTP 请求的包头里添加：\n\n```\nConnection:close\n```\n\n现在大多数浏览器都默认是使用 HTTP/1.1，所以 Keep-Alive 都是默认打开的。一旦客户端和服务端达成协议，那么长连接就建立好了。\n\nHTTP 长连接不仅仅减少了 TCP 连接资源的开销，而且这给 **HTTP 流水线**技术提供了可实现的基础。\n\n所谓的 HTTP 流水线，是**客户端可以先一次性发送多个请求，而在发送过程中不需先等待服务器的回应**，可以减少整体的响应时间。\n\n举例来说，客户端需要请求两个资源。以前的做法是，在同一个 TCP 连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。HTTP 流水线机制则允许客户端同时发出 A 请求和 B 请求。\n\n![右边为 HTTP 流水线机制](https://img-blog.csdnimg.cn/img_convert/b3fa409edd8aa1dea830af2a69fc8a31.png)\n\n但是**服务器还是按照顺序响应**，先回应 A 请求，完成后再回应 B 请求。\n\n而且要等服务器响应完客户端第一批发送的请求后，客户端才能发出下一批的请求，也就说如果服务器响应的过程发生了阻塞，那么客户端就无法发出下一批的请求，此时就造成了「队头阻塞」的问题。\n\n可能有的同学会问，如果使用了 HTTP 长连接，如果客户端完成一个 HTTP 请求后，就不再发起新的请求，此时这个 TCP 连接一直占用着不是挺浪费资源的吗？\n\n对没错，所以为了避免资源浪费的情况，web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。\n\n比如设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**\n\n![HTTP 长连接超时](https://img-blog.csdnimg.cn/img_convert/7e995ecb2e42941342f97256707496c9.png)\n\n## TCP 的 Keepalive\n\nTCP 的 Keepalive 这东西其实就是 **TCP 的保活机制**，它的工作原理我之前的文章写过，这里就直接贴下以前的内容。\n\n\n如果两端的 TCP 连接一直没有数据交互，达到了触发 TCP 保活机制的条件，那么内核里的 TCP 协议栈就会发送探测报文。\n- 如果对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。\n- 如果对端主机宕机（*注意不是进程崩溃，进程崩溃后操作系统在回收进程资源的时候，会发送 FIN 报文，而主机宕机则是无法感知的，所以需要 TCP 保活机制来探测对方是不是发生了主机宕机*），或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。\n\n\n所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活，这个工作是在内核完成的。\n\n![TCP 保活机制](https://img-blog.csdnimg.cn/img_convert/87e138ae9f2438c8f4e2c9c46ec40b95.png)\n\n\n注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。\n\n\n## 总结\n\nHTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。\n\n\nTCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。\n\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_interview":{"title":"tcp_interview","content":"# 4.1 TCP 三次握手与四次挥手面试题\n\n大家好，我是小林。\n\n**任 TCP 虐我千百遍，我仍待 TCP 如初恋。**\n\n巨巨巨巨长的提纲，发车！发车！\n\n![](https://img-blog.csdnimg.cn/1310bf5ed78e4c8186481c47719e0793.png)\n\n\n\n\u003e PS：本次文章不涉及 TCP 流量控制、拥塞控制、可靠性传输等方面知识，这些知识在这篇：[你还在为 TCP 重传、滑动窗口、流量控制、拥塞控制发愁吗？看完图解就不愁了](https://mp.weixin.qq.com/s/Tc09ovdNacOtnMOMeRc_uA)\n\n---\n\n## TCP 基本认识\n\n### TCP 头格式有哪些？\n\n我们先来看看 TCP 头的格式，标注颜色的表示与本文关联比较大的字段，其他字段不做详细阐述。\n\n![TCP 头格式](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzYuanBn?x-oss-process=image/format,png)\n\n**序列号**：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。**用来解决网络包乱序问题。**\n\n**确认应答号**：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。**用来解决丢包的问题。**\n\n**控制位：**\n\n- *ACK*：该位为 `1` 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 `SYN` 包之外该位必须设置为 `1` 。\n- *RST*：该位为 `1` 时，表示 TCP 连接中出现异常必须强制断开连接。\n- *SYN*：该位为 `1` 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。\n- *FIN*：该位为 `1` 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 `FIN` 位为 1 的 TCP 段。\n\n### 为什么需要 TCP 协议？ TCP 工作在哪一层？\n\n`IP` 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。\n\n![OSI 参考模型与 TCP/IP 的关系](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzcuanBn?x-oss-process=image/format,png)\n\n如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 `TCP` 协议来负责。\n\n因为 TCP 是一个工作在**传输层**的**可靠**数据传输的服务，它能确保接收端接收的网络包是**无损坏、无间隔、非冗余和按序的。**\n\n### 什么是 TCP ？\n\nTCP 是**面向连接的、可靠的、基于字节流**的传输层通信协议。\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzguanBn?x-oss-process=image/format,png)\n\n- **面向连接**：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的；\n\n- **可靠的**：无论网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端；\n\n- **字节流**：用户消息通过 TCP 协议传输时，消息可能会被操作系统「分组」成多个的 TCP 报文，如果接收方的程序如果不知道「消息的边界」，是无法读出一个有效的用户消息的。并且 TCP 报文是「有序的」，当「前一个」TCP 报文没有收到的时候，即使它先收到了后面的 TCP 报文，那么也不能扔给应用层去处理，同时对「重复」的 TCP 报文会自动丢弃。\n\n### 什么是 TCP 连接？\n\n我们来看看 RFC 793 是如何定义「连接」的：\n\n*Connections:\nThe reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream.  The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection.*\n\n简单来说就是，**用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接。**\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzkuanBn?x-oss-process=image/format,png)\n\n所以我们可以知道，建立一个 TCP 连接是需要客户端与服务端达成上述三个信息的共识。\n\n- **Socket**：由 IP 地址和端口号组成\n- **序列号**：用来解决乱序问题等\n- **窗口大小**：用来做流量控制\n\n### 如何唯一确定一个 TCP 连接呢？\n\nTCP 四元组可以唯一的确定一个连接，四元组包括如下：\n\n- 源地址\n- 源端口\n- 目的地址\n- 目的端口\n\n![TCP 四元组](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEwLmpwZw?x-oss-process=image/format,png)\n\n源地址和目的地址的字段（32 位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。\n\n源端口和目的端口的字段（16 位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。\n\n\u003e 有一个 IP 的服务端监听了一个端口，它的 TCP 的最大连接数是多少？\n\n服务端通常固定在某个本地端口上监听，等待客户端的连接请求。\n\n因此，客户端 IP 和端口是可变的，其理论值计算公式如下:\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzExLmpwZw?x-oss-process=image/format,png)\n\n对 IPv4，客户端的 IP 数最多为 `2` 的 `32` 次方，客户端的端口数最多为 `2` 的 `16` 次方，也就是服务端单机最大 TCP 连接数，约为 `2` 的 `48` 次方。\n\n当然，服务端最大并发 TCP 连接数远不能达到理论上限，会受以下因素影响：\n\n- **文件描述符限制**，每个 TCP 连接都是一个文件，如果文件描述符被占满了，会发生 Too many open files。Linux 对可打开的文件描述符的数量分别作了三个方面的限制：\n  - **系统级**：当前系统可打开的最大数量，通过 `cat /proc/sys/fs/file-max` 查看；\n  - **用户级**：指定用户可打开的最大数量，通过 `cat /etc/security/limits.conf` 查看；\n  - **进程级**：单个进程可打开的最大数量，通过 `cat /proc/sys/fs/nr_open` 查看；\n- **内存限制**，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的，如果内存资源被占满后，会发生 OOM。\n\n### UDP 和 TCP 有什么区别呢？分别的应用场景是？\n\nUDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。\n\nUDP 协议真的非常简，头部只有 `8` 个字节（64 位），UDP 的头部格式如下：\n\n![UDP 头部格式](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEyLmpwZw?x-oss-process=image/format,png)\n\n- 目标和源端口：主要是告诉 UDP 协议应该把报文发给哪个进程。\n- 包长度：该字段保存了 UDP 首部的长度跟数据的长度之和。\n- 校验和：校验和是为了提供可靠的 UDP 首部和数据而设计，防止收到在网络传输中受损的 UDP 包。\n\n**TCP 和 UDP 区别：**\n\n*1. 连接*\n\n- TCP 是面向连接的传输层协议，传输数据前先要建立连接。\n- UDP 是不需要连接，即刻传输数据。\n\n*2. 服务对象*\n\n- TCP 是一对一的两点服务，即一条连接只有两个端点。\n- UDP 支持一对一、一对多、多对多的交互通信\n\n*3. 可靠性*\n\n- TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达。\n- UDP 是尽最大努力交付，不保证可靠交付数据。但是我们可以基于 UDP 传输协议实现一个可靠的传输协议，比如 QUIC 协议，具体可以参见这篇文章：[如何基于 UDP 协议实现可靠传输？](https://xiaolincoding.com/network/3_tcp/quic.html)\n\n*4. 拥塞控制、流量控制*\n\n- TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。\n- UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。\n\n*5. 首部开销*\n\n- TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 `20` 个字节，如果使用了「选项」字段则会变长的。\n- UDP 首部只有 8 个字节，并且是固定不变的，开销较小。\n\n*6. 传输方式*\n\n- TCP 是流式传输，没有边界，但保证顺序和可靠。\n- UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。\n\n*7. 分片不同*\n\n- TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。\n- UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。\n\n**TCP 和 UDP 应用场景：**\n\n由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：\n\n- `FTP` 文件传输；\n- HTTP / HTTPS；\n\n由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于：\n\n- 包总量较少的通信，如 `DNS` 、`SNMP` 等；\n- 视频、音频等多媒体通信；\n- 广播通信；\n\n\u003e 为什么 UDP 头部没有「首部长度」字段，而 TCP 头部有「首部长度」字段呢？\n\n原因是 TCP 有**可变长**的「选项」字段，而 UDP 头部长度则是**不会变化**的，无需多一个字段去记录 UDP 的首部长度。\n\n\u003e 为什么 UDP 头部有「包长度」字段，而 TCP 头部则没有「包长度」字段呢？\n\n先说说 TCP 是如何计算负载数据长度：\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEzLmpwZw?x-oss-process=image/format,png)\n\n其中 IP 总长度 和 IP 首部长度，在 IP 首部格式是已知的。TCP 首部长度，则是在 TCP 首部格式已知的，所以就可以求得 TCP 数据的长度。\n\n大家这时就奇怪了问：“UDP 也是基于 IP 层的呀，那 UDP 的数据长度也可以通过这个公式计算呀？ 为何还要有「包长度」呢？”\n\n这么一问，确实感觉 UDP 的「包长度」是冗余的。\n\n我查阅了很多资料，我觉得有两个比较靠谱的说法：\n- 第一种说法：因为为了网络设备硬件设计和处理方便，首部长度需要是 `4 ` 字节的整数倍。如果去掉 UDP 的「包长度」字段，那 UDP 首部长度就不是 `4` 字节的整数倍了，所以我觉得这可能是为了补全 UDP 首部长度是  `4` 字节的整数倍，才补充了「包长度」字段。\n- 第二种说法：如今的 UDP 协议是基于 IP 协议发展的，而当年可能并非如此，依赖的可能是别的不提供自身报文长度或首部长度的网络层协议，因此 UDP 报文首部需要有长度字段以供计算。\n\n### TCP 和 UDP 可以使用同一个端口吗？\n\n答案：**可以的**。\n\n在数据链路层中，通过 MAC 地址来寻找局域网中的主机。在网际层中，通过 IP 地址来寻找网络中互连的主机或路由器。在传输层中，需要通过端口进行寻址，来识别同一计算机中同时通信的不同应用程序。\n\n所以，传输层的「端口号」的作用，是为了区分同一个主机上不同应用程序的数据包。\n\n传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。\n\n当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp%E5%92%8Cudp%E6%A8%A1%E5%9D%97.jpeg)\n\n因此，TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。\n\n关于端口的知识点，还是挺多可以讲的，比如还可以牵扯到这几个问题：\n\n- 多个 TCP 服务进程可以同时绑定同一个端口吗？\n- 重启 TCP 服务进程时，为什么会出现“Address in use”的报错信息？又该怎么避免？\n- 客户端的端口可以重复使用吗？\n- 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？\n\n上面这些问题，可以看这篇文章：[TCP 和 UDP 可以使用同一个端口吗？](https://xiaolincoding.com/network/3_tcp/port.html)\n\n## TCP 连接建立\n\n### TCP 三次握手过程是怎样的？\n\nTCP 是面向连接的协议，所以使用 TCP 前必须先建立连接，而**建立连接是通过三次握手来进行的**。三次握手的过程如下图：\n\n![TCP 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/%E7%BD%91%E7%BB%9C/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png)\n\n- 一开始，客户端和服务端都处于 `CLOSE` 状态。先是服务端主动监听某个端口，处于 `LISTEN` 状态\n\n![第一个报文 —— SYN 报文](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzE1LmpwZw?x-oss-process=image/format,png)\n\n- 客户端会随机初始化序号（`client_isn`），将此序号置于 TCP 首部的「序号」字段中，同时把 `SYN` 标志位置为 `1`，表示 `SYN` 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 `SYN-SENT` 状态。 \n\n![第二个报文 —— SYN + ACK 报文](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzE2LmpwZw?x-oss-process=image/format,png)\n\n- 服务端收到客户端的 `SYN` 报文后，首先服务端也随机初始化自己的序号（`server_isn`），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 `client_isn + 1`, 接着把 `SYN` 和 `ACK` 标志位置为 `1`。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 `SYN-RCVD` 状态。\n\n![第三个报文 —— ACK 报文](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzE3LmpwZw?x-oss-process=image/format,png)\n\n- 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 `ACK` 标志位置为 `1` ，其次「确认应答号」字段填入 `server_isn + 1` ，最后把报文发送给服务端，这次报文可以携带客户到服务端的数据，之后客户端处于 `ESTABLISHED` 状态。\n\n- 服务端收到客户端的应答报文后，也进入 `ESTABLISHED` 状态。\n\n从上面的过程可以发现**第三次握手是可以携带数据的，前两次握手是不可以携带数据的**，这也是面试常问的题。\n\n一旦完成三次握手，双方都处于 `ESTABLISHED` 状态，此时连接就已建立完成，客户端和服务端就可以相互发送数据了。\n\n### 如何在 Linux 系统中查看 TCP 状态？\n\nTCP 的连接状态查看，在 Linux 可以通过 `netstat -napt` 命令查看。\n\n![TCP 连接状态查看](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzE4LmpwZw?x-oss-process=image/format,png)\n\n### 为什么是三次握手？不是两次、四次？\n\n相信大家比较常回答的是：“因为三次握手才能保证双方具有接收和发送的能力。”\n\n这回答是没问题，但这回答是片面的，并没有说出主要的原因。\n\n在前面我们知道了什么是 **TCP 连接**：\n\n- 用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 **Socket、序列号和窗口大小**称为连接。\n\n所以，重要的是**为什么三次握手才可以初始化 Socket、序列号和窗口大小并建立 TCP 连接。**\n\n接下来，以三个方面分析三次握手的原因：\n\n- 三次握手才可以阻止重复历史连接的初始化（主要原因）\n- 三次握手才可以同步双方的初始序列号\n- 三次握手才可以避免资源浪费\n\n*原因一：避免历史连接*\n\n我们来看看 RFC 793 指出的 TCP 连接使用三次握手的**首要原因**：\n\n*The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion.*\n\n简单来说，三次握手的**首要原因是为了防止旧的重复连接初始化造成混乱。**\n\n我们考虑一个场景，客户端先发送了 SYN（seq = 90）报文，然后客户端宕机了，而且这个 SYN 报文还被网络阻塞了，服务端并没有收到，接着客户端重启后，又重新向服务端建立连接，发送了 SYN（seq = 100）报文（*注意！不是重传 SYN，重传的 SYN 的序列号是一样的*）。\n\n看看三次握手是如何阻止历史连接的：\n\n![三次握手避免历史连接](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzE5LmpwZw?x-oss-process=image/format,png)\n\n客户端连续发送多次 SYN（都是同一个四元组）建立连接的报文，在**网络拥堵**情况下：\n\n- 一个「旧 SYN 报文」比「最新的 SYN」 报文早到达了服务端，那么此时服务端就会回一个 `SYN + ACK` 报文给客户端，此报文中的确认号是 91（90+1）。\n- 客户端收到后，发现自己期望收到的确认号应该是 100 + 1，而不是 90 + 1，于是就会回 RST 报文。\n- 服务端收到 RST 报文后，就会释放连接。\n- 后续最新的 SYN 抵达了服务端后，客户端与服务端就可以正常的完成三次握手了。\n\n上述中的「旧 SYN 报文」称为历史连接，TCP 使用三次握手建立连接的**最主要原因就是防止「历史连接」初始化了连接**。\n\n::: tip\n\n有很多人问，如果服务端在收到 RST 报文之前，先收到了「新 SYN 报文」，也就是服务端收到客户端报文的顺序是：「旧 SYN 报文」-\u003e「新 SYN 报文」，此时会发生什么?\n\n当服务端第一次收到 SYN 报文，也就是收到 「旧 SYN 报文」时，就会回复 `SYN + ACK` 报文给客户端，此报文中的确认号是 91（90+1）。\n\n然后这时再收到「新 SYN 报文」时，就会回 [Challenge Ack](https://xiaolincoding.com/network/3_tcp/challenge_ack.html) 报文给客户端，**这个 ack 报文并不是确认收到「新 SYN 报文」的，而是上一次的 ack 确认号**，也就是91（90+1）。所以客户端收到此 ACK 报文时，发现自己期望收到的确认号应该是 101，而不是 91，于是就会回 RST 报文。\n\n:::\n\n**如果是两次握手连接，就无法阻止历史连接**，那为什么 TCP 两次握手为什么无法阻止历史连接呢？\n\n我先直接说结论，主要是因为**在两次握手的情况下，服务端没有中间状态给客户端来阻止历史连接，导致服务端可能建立一个历史连接，造成资源浪费**。\n\n你想想，在两次握手的情况下，服务端在收到 SYN 报文后，就进入 ESTABLISHED 状态，意味着这时可以给对方发送数据，但是客户端此时还没有进入 ESTABLISHED 状态，假设这次是历史连接，客户端判断到此次连接为历史连接，那么就会回 RST 报文来断开连接，而服务端在第一次握手的时候就进入 ESTABLISHED 状态，所以它可以发送数据的，但是它并不知道这个是历史连接，它只有在收到 RST 报文后，才会断开连接。\n\n![两次握手无法阻止历史连接](https://img-blog.csdnimg.cn/img_convert/fe898053d2e93abac950b1637645943f.png)\n\n可以看到，如果采用两次握手建立 TCP 连接的场景下，服务端在向客户端发送数据前，并没有阻止掉历史连接，导致服务端建立了一个历史连接，又白白发送了数据，妥妥地浪费了服务端的资源。\n\n因此，**要解决这种现象，最好就是在服务端发送数据前，也就是建立连接之前，要阻止掉历史连接，这样就不会造成资源浪费，而要实现这个功能，就需要三次握手**。\n\n所以，**TCP 使用三次握手建立连接的最主要原因是防止「历史连接」初始化了连接。**\n\n::: tip\n\n有人问：客户端发送三次握手（ack 报文）后就可以发送数据了，而被动方此时还是 syn_received 状态，如果 ack 丢了，那客户端发的数据是不是也白白浪费了？\n\n不是的，即使服务端还是在 syn_received 状态，收到了客户端发送的数据，还是可以建立连接的，并且还可以正常收到这个数据包。这是因为数据报文中是有 ack 标识位，也有确认号，这个确认号就是确认收到了第二次握手。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/相同ack.png)\n\n所以，服务端收到这个数据报文，是可以正常建立连接的，然后就可以正常接收这个数据包了。\n\n:::\n\n*原因二：同步双方初始序列号*\n\nTCP 协议的通信双方， 都必须维护一个「序列号」， 序列号是可靠传输的一个关键因素，它的作用：  \n\n- 接收方可以去除重复的数据；\n- 接收方可以根据数据包的序列号按序接收；\n- 可以标识发送出去的数据包中， 哪些是已经被对方收到的（通过 ACK 报文中的序列号知道）；\n\n可见，序列号在 TCP 连接中占据着非常重要的作用，所以当客户端发送携带「初始序列号」的 `SYN` 报文的时候，需要服务端回一个 `ACK` 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，**这样一来一回，才能确保双方的初始序列号能被可靠的同步。**\n\n![四次握手与三次握手](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzIwLmpwZw?x-oss-process=image/format,png)\n\n四次握手其实也能够可靠的同步双方的初始化序号，但由于**第二步和第三步可以优化成一步**，所以就成了「三次握手」。\n\n而两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收。\n\n*原因三：避免资源浪费*\n\n如果只有「两次握手」，当客户端发出的 `SYN` 报文在网络中阻塞，客户端没有接收到 `ACK` 报文，就会重新发送 `SYN`报文。**由于没有第三次握手，服务端不清楚客户端是否收到了自己回复的 `ACK` 报文，所以服务端每收到一个 `SYN` 就只能先主动建立一个连接**，这会造成什么情况呢？\n\n如果客户端发送的 `SYN` 报文在网络中阻塞了，重复发送多次 `SYN` 报文，那么服务端在收到请求后就会**建立多个冗余的无效连接，造成不必要的资源浪费。**\n\n![两次握手会造成资源浪费](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzIyLmpwZw?x-oss-process=image/format,png)\n\n即两次握手会造成消息滞留情况下，服务端重复接受无用的连接请求 `SYN` 报文，而造成重复分配资源。\n\n::: tip\n\n很多人问，两次握手不是也可以根据上下文信息丢弃`SYN`历史报文吗？\n\n我这里两次握手是假设「由于没有第三次握手，服务端不清楚客户端是否收到了自己发送的建立连接的 `ACK` 确认报文，所以每收到一个 `SYN` 就只能先主动建立一个连接」这个场景。\n\n当然你要实现成类似三次握手那样，根据上下文丢弃 syn 历史报文也是可以的，两次握手没有具体的实现，怎么假设都行。\n\n:::\n\n*小结*\n\nTCP 建立连接时，通过三次握手**能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同步初始化序列号**。序列号能够保证数据包不重复、不丢弃和按序传输。\n\n不使用「两次握手」和「四次握手」的原因：\n\n- 「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；\n- 「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。\n\n### 为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？\n\n主要原因有两个方面：\n\n- 为了防止历史报文被下一个相同四元组的连接接收（主要方面）；\n- 为了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收；\n\n接下来，详细说说第一点。\n\n假设每次建立连接，客户端和服务端的初始化序列号都是从 0 开始：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/isn相同.png)\n\n过程如下：\n\n- 客户端和服务端建立一个 TCP 连接，在客户端发送数据包被网络阻塞了，然后超时重传了这个数据包，而此时服务端设备断电重启了，之前与客户端建立的连接就消失了，于是在收到客户端的数据包的时候就会发送 RST 报文。\n- 紧接着，客户端又与服务端建立了与上一个连接相同四元组的连接；\n- 在新连接建立完成后，上一个连接中被网络阻塞的数据包正好抵达了服务端，刚好该数据包的序列号正好是在服务端的接收窗口内，所以该数据包会被服务端正常接收，就会造成数据错乱。\n\n可以看到，**如果每次建立连接，客户端和服务端的初始化序列号都是一样的话，很容易出现历史报文被下一个相同四元组的连接接收的问题**。\n\n如果每次建立连接客户端和服务端的初始化序列号都「不一样」，就有大概率因为历史报文的序列号「不在」对方接收窗口，从而很大程度上避免了历史报文，比如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/isn不相同.png)\n\n相反，如果每次建立连接客户端和服务端的初始化序列号都「一样」，就有大概率遇到历史报文的序列号刚「好在」对方的接收窗口内，从而导致历史报文被新连接成功接收。\n\n所以，每次初始化序列号不一样很大程度上能够避免历史报文被下一个相同四元组的连接接收，注意是很大程度上，并不是完全避免了（因为序列号会有回绕的问题，所以需要用时间戳的机制来判断历史报文，详细看篇：[TCP 是如何避免历史报文的？](https://xiaolincoding.com/network/3_tcp/isn_deff.html)）。\n\n### 初始序列号 ISN 是如何随机产生的？\n\n起始 `ISN` 是基于时钟的，每 4 微秒 + 1，转一圈要 4.55 个小时。\n\nRFC793 提到初始化序列号 ISN 随机生成算法：ISN = M + F(localhost, localport, remotehost, remoteport)。\n\n- `M` 是一个计时器，这个计时器每隔 4 微秒加 1。\n- `F` 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口生成一个随机数值。要保证 Hash 算法不能被外部轻易推算得出，用 MD5 算法是一个比较好的选择。\n\n可以看到，随机数是会基于时钟计时器递增的，基本不可能会随机成一样的初始化序列号。\n\n### 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？\n\n我们先来认识下 MTU 和 MSS\n\n![MTU 与 MSS](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzIzLmpwZw?x-oss-process=image/format,png)\n\n- `MTU`：一个网络包的最大长度，以太网中一般为 `1500` 字节；\n- `MSS`：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度；\n\n如果在 TCP 的整个报文（头部 + 数据）交给 IP 层进行分片，会有什么异常呢？\n\n当 IP 层有一个超过 `MTU` 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，把数据分片成若干片，保证每一个分片都小于 MTU。把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，再交给上一层 TCP 传输层。\n\n这看起来井然有序，但这存在隐患的，**那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传**。\n\n因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。\n\n当某一个 IP 分片丢失后，接收方的 IP 层就无法组装成一个完整的 TCP 报文（头部 + 数据），也就无法将数据报文送到 TCP 层，所以接收方不会响应 ACK 给发送方，因为发送方迟迟收不到 ACK 确认报文，所以会触发超时重传，就会重发「整个 TCP 报文（头部 + 数据）」。\n\n因此，可以得知由 IP 层进行分片传输，是非常没有效率的。\n\n所以，为了达到最佳的传输效能 TCP 协议在**建立连接的时候通常要协商双方的 MSS 值**，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。\n\n![握手阶段协商 MSS](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzI0LmpwZw?x-oss-process=image/format,png)\n\n经过 TCP 层分片后，如果一个 TCP 分片丢失后，**进行重发时也是以 MSS 为单位**，而不用重传所有的分片，大大增加了重传的效率。\n\n### 第一次握手丢失了，会发生什么？\n\n当客户端想和服务端建立 TCP 连接的时候，首先第一个发的就是 SYN 报文，然后进入到 `SYN_SENT` 状态。\n\n在这之后，如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手），就会触发「超时重传」机制，重传 SYN 报文，而且**重传的 SYN 报文的序列号都是一样的**。\n\n不同版本的操作系统可能超时时间不同，有的 1 秒的，也有 3 秒的，这个超时时间是写死在内核里的，如果想要更改则需要重新编译内核，比较麻烦。\n\n当客户端在 1 秒后没收到服务端的 SYN-ACK 报文后，客户端就会重发 SYN 报文，那到底重发几次呢？\n\n在 Linux 里，客户端的 SYN 报文最大重传次数由 `tcp_syn_retries`内核参数控制，这个参数是可以自定义的，默认值一般是 5。\n\n```shell\n# cat /proc/sys/net/ipv4/tcp_syn_retries\n5\n```\n\n通常，第一次超时重传是在 1 秒后，第二次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，**每次超时的时间是上一次的 2 倍**。\n\n当第五次超时重传后，会继续等待 32 秒，如果服务端仍然没有回应 ACK，客户端就不再发送 SYN 包，然后断开 TCP 连接。\n\n所以，总耗时是 1+2+4+8+16+32=63 秒，大约 1 分钟左右。\n\n举个例子，假设 tcp_syn_retries 参数值为 3，那么当客户端的 SYN 报文一直在网络中丢失时，会发生下图的过程：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第1次握手丢失.png)\n\n具体过程：\n\n- 当客户端超时重传 3 次 SYN 报文后，由于  tcp_syn_retries 为 3，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到服务端的第二次握手（SYN-ACK 报文），那么客户端就会断开连接。\n\n### 第二次握手丢失了，会发生什么？\n\n当服务端收到客户端的第一次握手后，就会回 SYN-ACK 报文给客户端，这个就是第二次握手，此时服务端会进入 `SYN_RCVD` 状态。\n\n第二次握手的 `SYN-ACK` 报文其实有两个目的 ：\n\n- 第二次握手里的 ACK， 是对第一次握手的确认报文；\n- 第二次握手里的 SYN，是服务端发起建立 TCP 连接的报文；\n\n所以，如果第二次握手丢了，就会发生比较有意思的事情，具体会怎么样呢？\n\n因为第二次握手报文里是包含对客户端的第一次握手的 ACK 确认报文，所以，如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是**客户端就会触发超时重传机制，重传 SYN 报文**。\n\n然后，因为第二次握手中包含服务端的 SYN 报文，所以当客户端收到后，需要给服务端发送 ACK 确认报文（第三次握手），服务端才会认为该 SYN 报文被客户端收到了。\n\n那么，如果第二次握手丢失了，服务端就收不到第三次握手，于是**服务端这边会触发超时重传机制，重传 SYN-ACK 报文**。\n\n在 Linux 下，SYN-ACK 报文的最大重传次数由 `tcp_synack_retries`内核参数决定，默认值是 5。\n\n```shell\n# cat /proc/sys/net/ipv4/tcp_synack_retries\n5\n```\n\n因此，当第二次握手丢失了，客户端和服务端都会重传：\n\n- 客户端会重传 SYN 报文，也就是第一次握手，最大重传次数由 `tcp_syn_retries`内核参数决定；\n- 服务端会重传 SYN-ACK 报文，也就是第二次握手，最大重传次数由 `tcp_synack_retries` 内核参数决定。\n\n举个例子，假设 tcp_syn_retries  参数值为 1，tcp_synack_retries 参数值为 2，那么当第二次握手一直丢失时，发生的过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第2次握手丢失.png)\n\n具体过程：\n\n- 当客户端超时重传 1 次 SYN 报文后，由于  tcp_syn_retries 为 1，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到服务端的第二次握手（SYN-ACK 报文），那么客户端就会断开连接。\n- 当服务端超时重传 2 次 SYN-ACK 报文后，由于 tcp_synack_retries 为 2，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到客户端的第三次握手（ACK 报文），那么服务端就会断开连接。\n\n### 第三次握手丢失了，会发生什么？\n\n客户端收到服务端的 SYN-ACK 报文后，就会给服务端回一个 ACK 报文，也就是第三次握手，此时客户端状态进入到 `ESTABLISH` 状态。\n\n因为这个第三次握手的 ACK 是对第二次握手的 SYN 的确认报文，所以当第三次握手丢失了，如果服务端那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数。\n\n注意，**ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文**。\n\n举个例子，假设 tcp_synack_retries 参数值为 2，那么当第三次握手一直丢失时，发生的过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第三次握手丢失.drawio.png)\n\n具体过程：\n\n- 当服务端超时重传 2 次 SYN-ACK 报文后，由于 tcp_synack_retries 为 2，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到客户端的第三次握手（ACK 报文），那么服务端就会断开连接。\n\n### 什么是 SYN 攻击？如何避免 SYN 攻击？\n\n我们都知道 TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 `SYN` 报文，服务端每接收到一个 `SYN` 报文，就进入`SYN_RCVD` 状态，但服务端发送出去的 `ACK + SYN` 报文，无法得到未知 IP 主机的 `ACK` 应答，久而久之就会**占满服务端的半连接队列**，使得服务端不能为正常用户服务。\n\n![SYN 攻击](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzI1LmpwZw?x-oss-process=image/format,png)\n\n先跟大家说一下，什么是 TCP 半连接和全连接队列。\n\n在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：\n\n- 半连接队列，也称 SYN 队列；\n- 全连接队列，也称 accept 队列；\n\n我们先来看下 Linux 内核的 `SYN` 队列（半连接队列）与 `Accpet` 队列（全连接队列）是如何工作的？\n\n![正常流程](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzI2LmpwZw?x-oss-process=image/format,png)\n\n正常流程：\n\n- 当服务端接收到客户端的 SYN 报文时，会创建一个半连接的对象，然后将其加入到内核的「 SYN 队列」；\n- 接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文；\n- 服务端接收到 ACK 报文后，从「 SYN 队列」取出一个半连接对象，然后创建一个新的连接对象放入到「 Accept 队列」；\n- 应用通过调用 `accpet()` socket 接口，从「 Accept 队列」取出连接对象。\n\n不管是半连接队列还是全连接队列，都有最大长度限制，超过限制时，默认情况都会丢弃报文。\n\nSYN 攻击方式最直接的表现就会把 TCP 半连接队列打满，这样**当 TCP 半连接队列满了，后续再在收到 SYN 报文就会丢弃**，导致客户端无法和服务端建立连接。\n\n避免 SYN 攻击方式，可以有以下四种方法：\n\n- 调大 netdev_max_backlog；\n- 增大 TCP 半连接队列；\n- 开启 tcp_syncookies；\n- 减少 SYN+ACK 重传次数\n\n\u003e 方式一：调大 netdev_max_backlog\n\n当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包。控制该队列的最大值如下参数，默认值是 1000，我们要适当调大该参数的值，比如设置为 10000：\n\n```bash\nnet.core.netdev_max_backlog = 10000\n```\n\n\u003e 方式二：增大 TCP 半连接队列\n\n增大 TCP 半连接队列，要同时增大下面这三个参数：\n\n- 增大 net.ipv4.tcp_max_syn_backlog\n- 增大  listen() 函数中的 backlog\n- 增大 net.core.somaxconn\n\n具体为什么是三个参数决定  TCP 半连接队列的大小，可以看这篇：可以看这篇：[TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？](https://xiaolincoding.com/network/3_tcp/tcp_queue.html)\n\n\u003e 方式三：开启 net.ipv4.tcp_syncookies \n\n开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接，相当于绕过了 SYN 半连接来建立连接。\n\n![tcp_syncookies 应对 SYN 攻击](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzI5LmpwZw?x-oss-process=image/format,png)\n\n具体过程：\n\n- 当 「 SYN 队列」满之后，后续服务端收到 SYN 包，不会丢弃，而是根据算法，计算出一个 `cookie` 值；\n- 将 cookie 值放到第二次握手报文的「序列号」里，然后服务端回第二次握手给客户端；\n- 服务端接收到客户端的应答报文时，服务端会检查这个 ACK 包的合法性。如果合法，将该连接对象放入到「 Accept 队列」。 \n- 最后应用程序通过调用 `accpet()` 接口，从「 Accept 队列」取出的连接。\n\n可以看到，当开启了 tcp_syncookies 了，即使受到 SYN 攻击而导致 SYN 队列满时，也能保证正常的连接成功建立。\n\nnet.ipv4.tcp_syncookies 参数主要有以下三个值：\n\n- 0 值，表示关闭该功能；\n- 1 值，表示仅当 SYN 半连接队列放不下时，再启用它；\n- 2 值，表示无条件开启功能；\n\n那么在应对 SYN 攻击时，只需要设置为 1 即可。\n\n```bash\n$ echo 1 \u003e /proc/sys/net/ipv4/tcp_syncookies\n```\n\n\u003e 方式四：减少 SYN+ACK 重传次数\n\n当服务端受到 SYN 攻击时，就会有大量处于 SYN_REVC 状态的 TCP 连接，处于这个状态的 TCP 会重传 SYN+ACK ，当重传超过次数达到上限后，就会断开连接。\n\n那么针对 SYN 攻击的场景，我们可以减少 SYN-ACK 的重传次数，以加快处于 SYN_REVC 状态的 TCP 连接断开。\n\nSYN-ACK 报文的最大重传次数由 `tcp_synack_retries`内核参数决定（默认值是 5 次），比如将 tcp_synack_retries 减少到 2 次：\n\n```shell\n$ echo 2 \u003e /proc/sys/net/ipv4/tcp_synack_retries\n```\n\n## TCP 连接断开\n\n### TCP 四次挥手过程是怎样的？\n\n天下没有不散的宴席，对于 TCP 连接也是这样， TCP 断开连接是通过**四次挥手**方式。\n\n双方都可以主动断开连接，断开连接后主机中的「资源」将被释放，四次挥手的过程如下图：\n\n![客户端主动关闭连接 —— TCP 四次挥手](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzMwLmpwZw?x-oss-process=image/format,png)\n\n\n- 客户端打算关闭连接，此时会发送一个 TCP 首部 `FIN` 标志位被置为 `1` 的报文，也即 `FIN` 报文，之后客户端进入 `FIN_WAIT_1` 状态。\n- 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，接着服务端进入 `CLOSE_WAIT` 状态。\n- 客户端收到服务端的 `ACK` 应答报文后，之后进入 `FIN_WAIT_2` 状态。\n- 等待服务端处理完数据后，也向客户端发送 `FIN` 报文，之后服务端进入 `LAST_ACK` 状态。\n- 客户端收到服务端的 `FIN` 报文后，回一个 `ACK` 应答报文，之后进入 `TIME_WAIT` 状态\n- 服务端收到了 `ACK` 应答报文后，就进入了 `CLOSE` 状态，至此服务端已经完成连接的关闭。\n- 客户端在经过 `2MSL` 一段时间后，自动进入 `CLOSE` 状态，至此客户端也完成连接的关闭。\n\n你可以看到，每个方向都需要**一个 FIN 和一个 ACK**，因此通常被称为**四次挥手**。\n\n这里一点需要注意是：**主动关闭连接的，才有 TIME_WAIT 状态。**\n\n### 为什么挥手需要四次？\n\n再来回顾下四次挥手双方发 `FIN` 包的过程，就能理解为什么需要四次了。\n\n- 关闭连接时，客户端向服务端发送 `FIN` 时，仅仅表示客户端不再发送数据了但是还能接收数据。\n- 服务端收到客户端的 `FIN` 报文时，先回一个 `ACK` 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 `FIN` 报文给客户端来表示同意现在关闭连接。\n\n从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 `ACK` 和 `FIN` 一般都会分开发送，因此是需要四次挥手。\n\n但是**在特定情况下，四次挥手是可以变成三次挥手的**，具体情况可以看这篇：[TCP 四次挥手，可以变成三次吗？](https://xiaolincoding.com/network/3_tcp/tcp_three_fin.html)\n\n### 第一次挥手丢失了，会发生什么？\n\n当客户端（主动关闭方）调用 close 函数后，就会向服务端发送 FIN 报文，试图与服务端断开连接，此时客户端的连接进入到 `FIN_WAIT_1` 状态。\n\n正常情况下，如果能及时收到服务端（被动关闭方）的 ACK，则会很快变为 `FIN_WAIT2`状态。\n\n如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN 报文，重发次数由 `tcp_orphan_retries` 参数控制。\n\n当客户端重传 FIN 报文的次数超过 `tcp_orphan_retries` 后，就不再发送 FIN 报文，则会在等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到第二次挥手，那么直接进入到 `close` 状态。\n\n举个例子，假设 tcp_orphan_retries 参数值为 3，当第一次挥手一直丢失时，发生的过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第一次挥手丢失.png)\n\n具体过程：\n\n- 当客户端超时重传 3 次 FIN 报文后，由于 tcp_orphan_retries 为 3，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到服务端的第二次挥手（ACK报文），那么客户端就会断开连接。\n\n### 第二次挥手丢失了，会发生什么？\n\n当服务端收到客户端的第一次挥手后，就会先回一个 ACK 确认报文，此时服务端的连接进入到 `CLOSE_WAIT` 状态。\n\n在前面我们也提了，ACK 报文是不会重传的，所以如果服务端的第二次挥手丢失了，客户端就会触发超时重传机制，重传 FIN 报文，直到收到服务端的第二次挥手，或者达到最大的重传次数。\n\n举个例子，假设 tcp_orphan_retries 参数值为 2，当第二次挥手一直丢失时，发生的过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第二次挥手丢失.png)\n\n具体过程：\n\n- 当客户端超时重传 2 次 FIN 报文后，由于 tcp_orphan_retries 为 2，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到服务端的第二次挥手（ACK 报文），那么客户端就会断开连接。\n\n这里提一下，当客户端收到第二次挥手，也就是收到服务端发送的 ACK 报文后，客户端就会处于 `FIN_WAIT2` 状态，在这个状态需要等服务端发送第三次挥手，也就是服务端的 FIN 报文。\n\n对于 close 函数关闭的连接，由于无法再发送和接收数据，所以`FIN_WAIT2` 状态不可以持续太久，而 `tcp_fin_timeout` 控制了这个状态下连接的持续时长，默认值是 60 秒。\n\n这意味着对于调用 close 关闭的连接，如果在 60 秒后还没有收到 FIN 报文，客户端（主动关闭方）的连接就会直接关闭，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/fin_wait_2.drawio.png)\n\n但是注意，如果主动关闭方使用 shutdown 函数关闭连接，指定了只关闭发送方向，而接收方向并没有关闭，那么意味着主动关闭方还是可以接收数据的。\n\n此时，如果主动关闭方一直没收到第三次挥手，那么主动关闭方的连接将会一直处于 `FIN_WAIT2` 状态（`tcp_fin_timeout` 无法控制 shutdown 关闭的连接）。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/fin_wait_2死等.drawio.png)\n\n### 第三次挥手丢失了，会发生什么？\n\n当服务端（被动关闭方）收到客户端（主动关闭方）的 FIN 报文后，内核会自动回复 ACK，同时连接处于 `CLOSE_WAIT` 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。\n\n此时，内核是没有权利替代进程关闭连接，必须由进程主动调用 close 函数来触发服务端发送 FIN 报文。\n\n服务端处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文，同时连接进入 LAST_ACK 状态，等待客户端返回 ACK 来确认连接关闭。\n\n如果迟迟收不到这个 ACK，服务端就会重发 FIN 报文，重发次数仍然由 `tcp_orphan_retries` 参数控制，这与客户端重发 FIN 报文的重传次数控制方式是一样的。\n\n举个例子，假设 `tcp_orphan_retries` = 3，当第三次挥手一直丢失时，发生的过程如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第三次挥手丢失.drawio.png)\n\n具体过程：\n\n- 当服务端重传第三次挥手报文的次数达到了 3 次后，由于 tcp_orphan_retries 为 3，达到了重传最大次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到客户端的第四次挥手（ACK报文），那么服务端就会断开连接。\n- 客户端因为是通过 close 函数关闭连接的，处于 FIN_WAIT_2 状态是有时长限制的，如果 tcp_fin_timeout 时间内还是没能收到服务端的第三次挥手（FIN 报文），那么客户端就会断开连接。\n\n### 第四次挥手丢失了，会发生什么？\n\n当客户端收到服务端的第三次挥手的 FIN 报文后，就会回 ACK 报文，也就是第四次挥手，此时客户端连接进入 `TIME_WAIT` 状态。\n\n在 Linux 系统，TIME_WAIT 状态会持续 2MSL 后才会进入关闭状态。\n\n然后，服务端（被动关闭方）没有收到 ACK 报文前，还是处于 LAST_ACK 状态。\n\n如果第四次挥手的 ACK 报文没有到达服务端，服务端就会重发 FIN 报文，重发次数仍然由前面介绍过的 `tcp_orphan_retries` 参数控制。\n\n举个例子，假设 tcp_orphan_retries 为 2，当第四次挥手一直丢失时，发生的过程如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/第四次挥手丢失drawio.drawio.png)\n\n具体过程：\n\n- 当服务端重传第三次挥手报文达到 2 时，由于 tcp_orphan_retries 为 2， 达到了最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到客户端的第四次挥手（ACK 报文），那么服务端就会断开连接。\n- 客户端在收到第三次挥手后，就会进入 TIME_WAIT 状态，开启时长为 2MSL 的定时器，如果途中再次收到第三次挥手（FIN 报文）后，就会重置定时器，当等待 2MSL 时长后，客户端就会断开连接。\n\n### 为什么 TIME_WAIT 等待的时间是 2MSL？\n\n`MSL` 是 Maximum Segment Lifetime，**报文最大生存时间**，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 `TTL` 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。\n\nMSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 **MSL 应该要大于等于 TTL 消耗为 0 的时间**，以确保报文已被自然消亡。\n\n**TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了**。\n\nTIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以**一来一回需要等待 2 倍的时间**。\n\n比如，如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 `FIN` 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。\n\n可以看到 **2MSL时长** 这其实是相当于**至少允许报文丢失一次**。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。\n\n为什么不是 4 或者 8 MSL 的时长呢？你可以想象一个丢包率达到百分之一的糟糕网络，连续两次丢包的概率只有万分之一，这个概率实在是太小了，忽略它比解决它更具性价比。\n\n`2MSL` 的时间是从**客户端接收到 FIN 后发送 ACK 开始计时的**。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 **2MSL 时间将重新计时**。\n\n在 Linux 系统里 `2MSL` 默认是 `60` 秒，那么一个 `MSL` 也就是 `30` 秒。**Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒**。\n\n其定义在 Linux 内核代码里的名称为 TCP_TIMEWAIT_LEN：\n\n```c\n#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT \n                                    state, about 60 seconds  */\n```\n\n如果要修改 TIME_WAIT 的时间长度，只能修改 Linux 内核代码里 TCP_TIMEWAIT_LEN 的值，并重新编译 Linux 内核。\n\n### 为什么需要 TIME_WAIT 状态？ \n\n主动发起关闭连接的一方，才会有 `TIME-WAIT` 状态。\n\n需要 TIME-WAIT 状态，主要是两个原因：\n\n- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；\n- 保证「被动关闭连接」的一方，能被正确的关闭；\n\n*原因一：防止历史连接中的数据，被后面相同四元组的连接错误的接收*\n\n为了能更好的理解这个原因，我们先来了解序列号（SEQ）和初始序列号（ISN）。\n\n- **序列号**，是 TCP 一个头部字段，标识了 TCP 发送端到 TCP 接收端的数据流的一个字节，因为 TCP 是面向字节流的可靠协议，为了保证消息的顺序性和可靠性，TCP 为每个传输方向上的每个字节都赋予了一个编号，以便于传输成功后确认、丢失后重传以及在接收端保证不会乱序。**序列号是一个 32 位的无符号数，因此在到达 4G 之后再循环回到 0**。\n- **初始序列号**，在 TCP 建立连接的时候，客户端和服务端都会各自生成一个初始序列号，它是基于时钟生成的一个随机数，来保证每个连接都拥有不同的初始序列号。**初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时**。\n\n给大家抓了一个包，下图中的 Seq 就是序列号，其中红色框住的分别是客户端和服务端各自生成的初始序列号。\n\n![TCP 抓包图](https://img-blog.csdnimg.cn/img_convert/c9ea9b844e87bcd4acd3e320403ecab3.png)\n\n通过前面我们知道，**序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据**。\n\n假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？\n\n![TIME-WAIT 时间过短，收到旧连接的数据报文](https://img-blog.csdnimg.cn/img_convert/6385cc99500b01ba2ef288c27523c1e7.png)\n\n如上图：\n\n- 服务端在关闭连接之前发送的 `SEQ = 301` 报文，被网络延迟了。\n- 接着，服务端以相同的四元组重新打开了新连接，前面被延迟的 `SEQ = 301` 这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。\n\n为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 `2MSL` 时长，这个时间**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。**\n\n*原因二：保证「被动关闭连接」的一方，能被正确的关闭*\n\n在 RFC 793 指出 TIME-WAIT 另一个重要的作用是：\n\n*TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.*\n\n也就是说，TIME-WAIT 作用是**等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。**\n\n如果客户端（主动关闭方）最后一次 ACK 报文（第四次挥手）在网络中丢失了，那么按照 TCP 可靠性原则，服务端（被动关闭方）会重发 FIN 报文。\n\n假设客户端没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSE 状态，如果该  ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文。\n\n![TIME-WAIT 时间过短，没有确保连接正常关闭](https://img-blog.csdnimg.cn/img_convert/3a81c23ce57c27cf63fc2b77e34de0ab.png)\n\n服务端收到这个 RST 并将其解释为一个错误（Connection reset by peer），这对于一个可靠的协议来说不是一个优雅的终止方式。\n\n为了防止这种情况出现，客户端必须等待足够长的时间，确保服务端能够收到 ACK，如果服务端没有收到 ACK，那么就会触发 TCP 重传机制，服务端会重新发送一个 FIN，这样一去一来刚好两个 MSL 的时间。\n\n![TIME-WAIT 时间正常，确保了连接正常关闭](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/TIME-WAIT连接正常关闭.drawio.png)\n\n客户端在收到服务端重传的 FIN 报文时，TIME_WAIT 状态的等待时间，会重置回 2MSL。\n\n### TIME_WAIT 过多有什么危害？\n\n过多的 TIME-WAIT 状态主要的危害有两种：\n\n- 第一是占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等；\n- 第二是占用端口资源，端口资源也是有限的，一般可以开启的端口为 `32768～61000`，也可以通过 `net.ipv4.ip_local_port_range `参数指定范围。\n\n客户端和服务端 TIME_WAIT 过多，造成的影响是不同的。\n\n**如果客户端（主动发起关闭连接方）的 TIME_WAIT 状态过多**，占满了所有端口资源，那么就无法对「目的 IP+ 目的 PORT」都一样的服务端发起连接了，但是被使用的端口，还是可以继续对另外一个服务端发起连接的。具体可以看我这篇文章：[客户端的端口可以重复使用吗？](https://xiaolincoding.com/network/3_tcp/port.html#%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%AF%E4%BB%A5%E9%87%8D%E5%A4%8D%E4%BD%BF%E7%94%A8%E5%90%97)\n\n因此，客户端（发起连接方）都是和「目的 IP+ 目的 PORT 」都一样的服务端建立连接的话，当客户端的 TIME_WAIT 状态连接过多的话，就会受端口资源限制，如果占满了所有端口资源，那么就无法再跟「目的 IP+ 目的 PORT」都一样的服务端建立连接了。\n\n不过，即使是在这种场景下，只要连接的是不同的服务端，端口是可以重复使用的，所以客户端还是可以向其他服务端发起连接的，这是因为内核在定位一个连接的时候，是通过四元组（源IP、源端口、目的IP、目的端口）信息来定位的，并不会因为客户端的端口一样，而导致连接冲突。\n\n**如果服务端（主动发起关闭连接方）的 TIME_WAIT 状态过多**，并不会导致端口资源受限，因为服务端只监听一个端口，而且由于一个四元组唯一确定一个 TCP 连接，因此理论上服务端可以建立很多连接，但是 TCP 连接过多，会占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等。\n\n### 如何优化 TIME_WAIT？\n\n这里给出优化 TIME-WAIT 的几个方式，都是有利有弊：\n\n- 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；\n- net.ipv4.tcp_max_tw_buckets\n- 程序中使用 SO_LINGER ，应用强制使用 RST 关闭。\n\n*方式一：net.ipv4.tcp_tw_reuse 和 tcp_timestamps*\n\n如下的 Linux 内核参数开启后，则可以**复用处于 TIME_WAIT 的 socket 为新的连接所用**。\n\n有一点需要注意的是，**tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。**\n\n```shell\nnet.ipv4.tcp_tw_reuse = 1\n```\n\n使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即\n\n```\nnet.ipv4.tcp_timestamps=1（默认即为 1）\n```\n\n这个时间戳的字段是在 TCP 头部的「选项」里，它由一共 8 个字节表示时间戳，其中第一个 4 字节字段用来保存发送该数据包的时间，第二个 4 字节字段用来保存最近一次接收对方发送到达数据的时间。\n\n由于引入了时间戳，我们在前面提到的 `2MSL` 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。\n\n*方式二：net.ipv4.tcp_max_tw_buckets*\n\n这个值默认为 18000，**当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置**，这个方法比较暴力。\n\n*方式三：程序中使用 SO_LINGER*\n\n我们可以通过设置 socket 选项，来设置调用 close 关闭连接行为。\n\n```c\nstruct linger so_linger;\nso_linger.l_onoff = 1;\nso_linger.l_linger = 0;\nsetsockopt(s, SOL_SOCKET, SO_LINGER, \u0026so_linger,sizeof(so_linger));\n```\n\n如果`l_onoff`为非 0， 且`l_linger`值为 0，那么调用`close`后，会立该发送一个`RST`标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了`TIME_WAIT`状态，直接关闭。\n\n但这为跨越`TIME_WAIT`状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。\n\n前面介绍的方法都是试图越过 `TIME_WAIT`状态的，这样其实不太好。虽然 TIME_WAIT 状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。\n\n《UNIX网络编程》一书中却说道：**TIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它**。\n\n**如果服务端要避免过多的 TIME_WAIT 状态的连接，就永远不要主动断开连接，让客户端去断开，由分布在各处的客户端去承受 TIME_WAIT**。\n\n### 服务器出现大量 TIME_WAIT 状态的原因有哪些？\n\n首先要知道 TIME_WAIT 状态是主动关闭连接方才会出现的状态，所以如果服务器出现大量的 TIME_WAIT 状态的 TCP 连接，就是说明服务器主动断开了很多 TCP 连接。\n\n问题来了，**什么场景下服务端会主动断开连接呢？**\n\n- 第一个场景：HTTP 没有使用长连接\n- 第二个场景：HTTP 长连接超时\n- 第三个场景：HTTP 长连接的请求数量达到上限\n\n接下来，分别介绍下。\n\n*第一个场景：HTTP 没有使用长连接*\n\n我们先来看看 HTTP 长连接（Keep-Alive）机制是怎么开启的。\n\n在 HTTP/1.0 中默认是关闭的，如果浏览器要开启 Keep-Alive，它必须在请求的 header 中添加：\n\n```text\nConnection: Keep-Alive\n```\n\n然后当服务器收到请求，作出回应的时候，它也被添加到响应中 header 里：\n\n```text\nConnection: Keep-Alive\n```\n\n这样做，TCP 连接就不会中断，而是保持连接。当客户端发送另一个请求时，它会使用同一个 TCP 连接。这一直继续到客户端或服务器端提出断开连接。\n\n**从 HTTP/1.1 开始， 就默认是开启了 Keep-Alive**，现在大多数浏览器都默认是使用 HTTP/1.1，所以 Keep-Alive 都是默认打开的。一旦客户端和服务端达成协议，那么长连接就建立好了。\n\n如果要关闭 HTTP Keep-Alive，需要在 HTTP 请求或者响应的 header 里添加 `Connection:close` 信息，也就是说，**只要客户端和服务端任意一方的 HTTP header 中有 `Connection:close` 信息，那么就无法使用 HTTP 长连接的机制**。\n\n关闭 HTTP 长连接机制后，每次请求都要经历这样的过程：建立 TCP -\u003e 请求资源 -\u003e 响应资源 -\u003e 释放连接，那么此方式就是 **HTTP 短连接**，如下图：\n\n![HTTP 短连接](https://img-blog.csdnimg.cn/img_convert/d6f6757c02e3afbf113d1048c937f8ee.png)\n\n在前面我们知道，只要任意一方的 HTTP header 中有 `Connection:close` 信息，就无法使用 HTTP 长连接机制，这样在完成一次 HTTP 请求/处理后，就会关闭连接。\n\n问题来了，**这时候是客户端还是服务端主动关闭连接呢？**\n\n在 RFC 文档中，并没有明确由谁来关闭连接，**请求和响应的双方都可以主动关闭 TCP 连接。**\n\n不过，**根据大多数 Web 服务的实现，不管哪一方禁用了 HTTP Keep-Alive，都是由服务端主动关闭连接**，那么此时服务端上就会出现 TIME_WAIT 状态的连接。\n\n\u003e 客户端禁用了 HTTP Keep-Alive，服务端开启 HTTP Keep-Alive，谁是主动关闭方？ \n\n当客户端禁用了 HTTP Keep-Alive，这时候 HTTP 请求的 header 就会有 `Connection:close` 信息，这时服务端在发完 HTTP 响应后，就会主动关闭连接。\n\n为什么要这么设计呢？HTTP 是请求-响应模型，发起方一直是客户端，HTTP Keep-Alive 的初衷是**为客户端后续的请求重用连接**，如果我们**在某次 HTTP 请求-响应模型中，请求的 header 定义了 `connection：close` 信息，那不再重用这个连接的时机就只有在服务端了**，所以我们在 HTTP 请求-响应这个周期的「末端」关闭连接是合理的。\n\n\u003e 客户端开启了 HTTP Keep-Alive，服务端禁用了 HTTP Keep-Alive，谁是主动关闭方？ \n\n当客户端开启了 HTTP Keep-Alive，而服务端禁用了 HTTP Keep-Alive，这时服务端在发完 HTTP 响应后，服务端也会主动关闭连接。\n\n为什么要这么设计呢？在服务端主动关闭连接的情况下，只要调用一次 close() 就可以释放连接，剩下的工作由内核 TCP 栈直接进行了处理，整个过程只有一次 syscall；如果是要求 客户端关闭，则服务端在写完最后一个 response 之后需要把这个 socket 放入 readable 队列，调用 select / epoll 去等待事件；然后调用一次 read() 才能知道连接已经被关闭，这其中是两次 syscall，多一次用户态程序被激活执行，而且 socket 保持时间也会更长。\n\n因此，**当服务端出现大量的 TIME_WAIT 状态连接的时候，可以排查下是否客户端和服务端都开启了 HTTP Keep-Alive**，因为任意一方没有开启  HTTP Keep-Alive，都会导致服务端在处理完一个 HTTP 请求后，就主动关闭连接，此时服务端上就会出现大量的 TIME_WAIT 状态的连接。\n\n针对这个场景下，解决的方式也很简单，让客户端和服务端都开启 HTTP Keep-Alive 机制。\n\n*第二个场景：HTTP 长连接超时*\n\nHTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n\nHTTP 长连接可以在同一个 TCP 连接上接收和发送多个 HTTP 请求/应答，避免了连接建立和释放的开销。\n\n![](https://img-blog.csdnimg.cn/img_convert/d2b20d1cc03936332adb2a68512eb167.png)\n\n可能有的同学会问，如果使用了 HTTP 长连接，如果客户端完成一个 HTTP 请求后，就不再发起新的请求，此时这个 TCP 连接一直占用着不是挺浪费资源的吗？\n\n对没错，所以为了避免资源浪费的情况，web 服务软件一般都会提供一个参数，用来指定 HTTP 长连接的超时时间，比如 nginx 提供的 keepalive_timeout 参数。\n\n假设设置了 HTTP 长连接的超时时间是 60 秒，nginx 就会启动一个「定时器」，**如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，nginx 就会触发回调函数来关闭该连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接**。\n\n![HTTP 长连接超时](https://img-blog.csdnimg.cn/img_convert/7e995ecb2e42941342f97256707496c9.png)\n\n当服务端出现大量 TIME_WAIT 状态的连接时，如果现象是有大量的客户端建立完 TCP 连接后，很长一段时间没有发送数据，那么大概率就是因为 HTTP 长连接超时，导致服务端主动关闭连接，产生大量处于 TIME_WAIT 状态的连接。\n\n可以往网络问题的方向排查，比如是否是因为网络问题，导致客户端发送的数据一直没有被服务端接收到，以至于 HTTP 长连接超时。\n\n*第三个场景：HTTP 长连接的请求数量达到上限*\n\nWeb 服务端通常会有个参数，来定义一条 HTTP 长连接上最大能处理的请求数量，当超过最大限制时，就会主动关闭连接。\n\n比如 nginx 的 keepalive_requests 这个参数，这个参数是指一个 HTTP 长连接建立之后，nginx 就会为这个连接设置一个计数器，记录这个 HTTP 长连接上已经接收并处理的客户端请求的数量。**如果达到这个参数设置的最大值时，则 nginx 会主动关闭这个长连接**，那么此时服务端上就会出现 TIME_WAIT 状态的连接。\n\nkeepalive_requests 参数的默认值是 100 ，意味着每个 HTTP 长连接最多只能跑 100  次请求，这个参数往往被大多数人忽略，因为当 QPS (每秒请求数) 不是很高时，默认值 100 凑合够用。\n\n但是，**对于一些 QPS 比较高的场景，比如超过 10000 QPS，甚至达到 30000 , 50000 甚至更高，如果 keepalive_requests 参数值是 100，这时候就 nginx 就会很频繁地关闭连接，那么此时服务端上就会出大量的 TIME_WAIT 状态**。\n\n针对这个场景下，解决的方式也很简单，调大 nginx 的 keepalive_requests 参数就行。\n\n### 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？\n\nCLOSE_WAIT 状态是「被动关闭方」才会有的状态，而且如果「被动关闭方」没有调用 close 函数关闭连接，那么就无法发出 FIN 报文，从而无法使得 CLOSE_WAIT 状态的连接转变为 LAST_ACK 状态。\n\n所以，**当服务端出现大量 CLOSE_WAIT 状态的连接的时候，说明服务端的程序没有调用 close 函数关闭连接**。\n\n那什么情况会导致服务端的程序没有调用 close 函数关闭连接？这时候通常需要排查代码。\n\n我们先来分析一个普通的 TCP 服务端的流程：\n\n1. 创建服务端 socket，bind 绑定端口、listen 监听端口\n2. 将服务端 socket 注册到 epoll\n3. epoll_wait 等待连接到来，连接到来时，调用 accpet 获取已连接的 socket\n4. 将已连接的 socket 注册到 epoll\n5. epoll_wait 等待事件发生\n6. 对方连接关闭时，我方调用 close\n\n可能导致服务端没有调用 close 函数的原因，如下。\n\n**第一个原因**：第 2 步没有做，没有将服务端 socket 注册到 epoll，这样有新连接到来时，服务端没办法感知这个事件，也就无法获取到已连接的 socket，那服务端自然就没机会对 socket 调用 close 函数了。\n\n不过这种原因发生的概率比较小，这种属于明显的代码逻辑 bug，在前期 read view 阶段就能发现的了。\n\n**第二个原因**： 第 3 步没有做，有新连接到来时没有调用 accpet 获取该连接的 socket，导致当有大量的客户端主动断开了连接，而服务端没机会对这些 socket 调用 close 函数，从而导致服务端出现大量 CLOSE_WAIT 状态的连接。\n\n发生这种情况可能是因为服务端在执行 accpet  函数之前，代码卡在某一个逻辑或者提前抛出了异常。\n\n**第三个原因**：第 4 步没有做，通过 accpet 获取已连接的 socket 后，没有将其注册到 epoll，导致后续收到 FIN 报文的时候，服务端没办法感知这个事件，那服务端就没机会调用 close 函数了。\n\n发生这种情况可能是因为服务端在将已连接的 socket 注册到 epoll 之前，代码卡在某一个逻辑或者提前抛出了异常。之前看到过别人解决 close_wait 问题的实践文章，感兴趣的可以看看：[一次 Netty 代码不健壮导致的大量 CLOSE_WAIT 连接原因分析](https://mp.weixin.qq.com/s?__biz=MzU3Njk0MTc3Ng==\u0026mid=2247486020\u0026idx=1\u0026sn=f7cf41aec28e2e10a46228a64b1c0a5c\u0026scene=21#wechat_redirect)\n\n**第四个原因**：第 6 步没有做，当发现客户端关闭连接后，服务端没有执行 close 函数，可能是因为代码漏处理，或者是在执行 close 函数之前，代码卡在某一个逻辑，比如发生死锁等等。\n\n可以发现，**当服务端出现大量 CLOSE_WAIT 状态的连接的时候，通常都是代码的问题，这时候我们需要针对具体的代码一步一步的进行排查和定位，主要分析的方向就是服务端为什么没有调用 close**。\n\n### 如果已经建立了连接，但是客户端突然出现故障了怎么办？\n\n客户端出现故障指的是客户端的主机发生了宕机，或者断电的场景。发生这种情况的时候，如果服务端一直不会发送数据给客户端，那么服务端是永远无法感知到客户端宕机这个事件的，也就是服务端的 TCP 连接将一直处于   `ESTABLISH` 状态，占用着系统资源。\n\n为了避免这种情况，TCP 搞了个**保活机制**。这个机制的原理是这样的：\n\n定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。\n\n在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：\n\n```shell\nnet.ipv4.tcp_keepalive_time=7200\nnet.ipv4.tcp_keepalive_intvl=75  \nnet.ipv4.tcp_keepalive_probes=9\n```\n\n- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制\n- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；\n- tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。\n\n也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzMzLmpwZw?x-oss-process=image/format,png)\n\n注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。\n\n如果开启了 TCP 保活，需要考虑以下几种情况：\n\n- 第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。\n\n- 第二种，对端主机宕机并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，**会产生一个 RST 报文**，这样很快就会发现 TCP 连接已经被重置。\n\n- 第三种，是对端主机宕机（*注意不是进程崩溃，进程崩溃后操作系统在回收进程资源的时候，会发送 FIN 报文，而主机宕机则是无法感知的，所以需要 TCP 保活机制来探测对方是不是发生了主机宕机*），或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。\n\nTCP 保活的这个机制检测的时间是有点长，我们可以自己在应用层实现一个心跳机制。\n\n比如，web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完成一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**\n\n![web 服务的 心跳机制](https://img-blog.csdnimg.cn/img_convert/2d872f947dedd24800a1867dc4f8b9ce.png)\n\n### 如果已经建立了连接，但是服务端的进程崩溃会发生什么？\n\nTCP 的连接信息是由内核维护的，所以当服务端的进程崩溃后，内核需要回收该进程的所有 TCP 连接资源，于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与，所以即使服务端的进程退出了，还是能与客户端完成 TCP 四次挥手的过程。\n\n我自己做了个实验，使用 kill -9 来模拟进程崩溃的情况，发现**在 kill 掉进程后，服务端会发送 FIN 报文，与客户端进行四次挥手**。\n\n::: tip\n\n关于进程崩溃和主机宕机的区别，可以参考这篇：[TCP 连接，一端断电和进程崩溃有什么区别？](https://xiaolincoding.com/network/3_tcp/tcp_down_and_crash.html)\n\n还有一个类似的问题：「拔掉网线后， 原本的 TCP 连接还存在吗？」，具体可以看这篇：[拔掉网线后， 原本的 TCP 连接还存在吗？](https://xiaolincoding.com/network/3_tcp/tcp_unplug_the_network_cable.html)\n\n:::\n\n---\n\n## Socket 编程\n\n### 针对 TCP 应该如何 Socket 编程？\n\n![基于 TCP 协议的客户端和服务端工作](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM0LmpwZw?x-oss-process=image/format,png)\n\n\n- 服务端和客户端初始化 `socket`，得到文件描述符；\n- 服务端调用 `bind`，将 socket 绑定在指定的 IP 地址和端口;\n- 服务端调用 `listen`，进行监听；\n- 服务端调用 `accept`，等待客户端连接；\n- 客户端调用 `connect`，向服务端的地址和端口发起连接请求；\n- 服务端 `accept` 返回用于传输的 `socket` 的文件描述符；\n- 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；\n- 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。\n\n这里需要注意的是，服务端调用 `accept` 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。\n\n所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。\n\n成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。\n\n### listen 时候参数 backlog 的意义？\n\nLinux内核中会维护两个队列：\n\n- 半连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态；\n- 全连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态；\n\n![ SYN 队列 与 Accpet 队列 ](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM1LmpwZw?x-oss-process=image/format,png)\n\n```c\nint listen (int socketfd, int backlog)\n```\n\n- 参数一 socketfd 为 socketfd 文件描述符\n- 参数二 backlog，这参数在历史版本有一定的变化\n\n在早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。\n\n在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，**所以现在通常认为 backlog 是 accept 队列。**\n\n**但是上限值是内核参数 somaxconn 的大小，也就说 accpet 队列长度 = min(backlog, somaxconn)。**\n\n想详细了解 TCP 半连接队列和全连接队列，可以看这篇：[TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？](https://xiaolincoding.com/network/3_tcp/tcp_queue.html)\n\n### accept 发生在三次握手的哪一步？\n\n我们先看看客户端连接服务端时，发送了什么？\n\n![socket 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/socket三次握手.drawio.png)\n\n- 客户端的协议栈向服务端发送了 SYN 包，并告诉服务端当前发送序列号 client_isn，客户端进入 SYN_SENT 状态；\n- 服务端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 client_isn+1，表示对 SYN 包 client_isn 的确认，同时服务端也发送一个 SYN 包，告诉客户端当前我的发送序列号为 server_isn，服务端进入 SYN_RCVD 状态；\n- 客户端协议栈收到 ACK 之后，使得应用程序从 `connect` 调用返回，表示客户端到服务端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务端的 SYN 包进行应答，应答数据为 server_isn+1；\n- ACK 应答包到达服务端后，服务端的 TCP 连接进入 ESTABLISHED 状态，同时服务端协议栈使得 `accept` 阻塞调用返回，这个时候服务端到客户端的单向连接也建立成功。至此，客户端与服务端两个方向的连接都建立成功。\n\n从上面的描述过程，我们可以得知**客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。**\n\n### 客户端调用 close 了，连接是断开的流程是什么？\n\n我们看看客户端主动调用了 `close`，会发生什么？\n\n![客户端调用 close 过程](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM3LmpwZw?x-oss-process=image/format,png)\n\n- 客户端调用 `close`，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报文，进入 FIN_WAIT_1 状态；\n- 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 `EOF` 到接收缓冲区中，应用程序可以通过 `read` 调用来感知这个 FIN 包。这个 `EOF` 会被**放在已排队等候的其他已接收的数据之后**，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态；\n- 接着，当处理完数据后，自然就会读到 `EOF`，于是也调用 `close` 关闭它的套接字，这会使得服务端发出一个 FIN 包，之后处于 LAST_ACK 状态；\n- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；\n- 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；\n- 客户端经过 `2MSL` 时间之后，也进入 CLOSE 状态；\n\n### 没有 accept，能建立 TCP 连接吗？\n\n答案：**可以的**。\n\naccpet 系统调用并不参与 TCP 三次握手过程，它只是负责从 TCP 全连接队列取出一个已经建立连接的 socket，用户层通过 accpet 系统调用拿到了已经建立连接的 socket，就可以对该 socket 进行读写操作了。\n\n![半连接队列与全连接队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/3.jpg)\n\n更想了解这个问题，可以参考这篇文章：[没有 accept，能建立 TCP 连接吗？](https://xiaolincoding.com/network/3_tcp/tcp_no_accpet.html)\n\n### 没有 listen，能建立 TCP 连接吗？\n\n答案：**可以的**。\n\n客户端是可以自己连自己的形成连接（**TCP自连接**），也可以两个客户端同时向对方发出请求建立连接（**TCP同时打开**），这两个情况都有个共同点，就是**没有服务端参与，也就是没有 listen，就能 TCP 建立连接。**\n\n更想了解这个问题，可以参考这篇文章：[服务端没有 listen，客户端发起连接建立，会发生什么？](https://xiaolincoding.com/network/3_tcp/tcp_no_listen.html)\n\n---\n\n## 唠叨\n\n希望这篇能破除大家对 TCP 的大多数疑惑，有任何问题都可以在留言区和我交流。\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_no_accpet":{"title":"tcp_no_accpet","content":"# 4.20 没有 accept，能建立 TCP 连接吗？\n\n\u003e 来源：公众号@小白debug\n\u003e 原文地址：[阿里二面：没有 accept，能建立 TCP 连接吗？](https://mp.weixin.qq.com/s/oPX_JoZUaLn6sW54yppfvA)\n\n大家好，我是小林。\n\n这次，我们来讨论一下，**没有 accept，能建立 TCP 连接吗？**\n\n下面这个动图，是我们平时客户端和服务端建立连接时的代码流程。\n\n![握手建立连接流程](https://img-blog.csdnimg.cn/img_convert/e0d405a55626eb8e4a52553a54680618.gif)\n\n对应的是下面一段简化过的服务端伪代码。\n\n```c\nint main()\n{\n    /*Step 1: 创建服务器端监听 socket 描述符 listen_fd*/    \n    listen_fd = socket(AF_INET, SOCK_STREAM, 0);\n\n    /*Step 2: bind 绑定服务器端的 IP 和端口，所有客户端都向这个 IP 和端口发送和请求数据*/    \n    bind(listen_fd, xxx);\n\n    /*Step 3: 服务端开启监听*/    \n    listen(listen_fd, 128);\n\n    /*Step 4: 服务器等待客户端的链接，返回值 cfd 为客户端的 socket 描述符*/    \n    cfd = accept(listen_fd, xxx);\n\n    /*Step 5: 读取客户端发来的数据*/\n    n = read(cfd, buf, sizeof(buf));\n}\n```\n\n估计大家也是老熟悉这段伪代码了。\n\n需要注意的是，在执行 `listen()` 方法之后还会执行一个 `accept()` 方法。\n\n**一般情况**下，如果启动服务器，会发现最后程序会**阻塞在** `accept()` 里。\n\n此时服务端就算 ok 了，就等客户端了。\n\n那么，再看下简化过的客户端伪代码。\n\n```c\nint main()\n{\n    /*Step 1: 创建客户端端 socket 描述符 cfd*/    \n    cfd = socket(AF_INET, SOCK_STREAM, 0);\n\n    /*Step 2: connect 方法,对服务器端的 IP 和端口号发起连接*/    \n    ret = connect(cfd, xxxx);\n\n    /*Step 3: 向服务器端写数据*/\n    write(cfd, buf, strlen(buf));\n}\n```\n\n客户端比较简单，创建好 `socket` 之后，直接就发起 `connect` 方法。\n\n此时回到服务端，会发现**之前一直阻塞的 accept 方法，返回结果了**。\n\n这就算两端成功建立好了一条连接。之后就可以愉快的进行读写操作了。\n\n那么，我们今天的问题是，**如果没有这个 accept 方法，TCP 连接还能建立起来吗？**\n\n其实只要在执行 `accept()` 之前执行一个 `sleep(20)`，然后立刻执行客户端相关的方法，同时抓个包，就能得出结论。\n\n![不执行 accept 时抓包结果](https://img-blog.csdnimg.cn/img_convert/2cfc1d028f3e37f10c2f81375ddb998a.png)\n\n从抓包结果看来，**就算不执行 `accept()` 方法，三次握手照常进行，并顺利建立连接。**\n\n更骚气的是，**在服务端执行 `accept()` 前，如果客户端发送消息给服务端，服务端是能够正常回复 ack 确认包的。**\n\n并且，`sleep(20)` 结束后，服务端正常执行 `accept()`，客户端前面发送的消息，还是能正常收到的。\n\n通过这个现象，我们可以多想想为什么。顺便好好了解下三次握手的细节。\n\n## 三次握手的细节分析\n\n我们先看面试八股文的老股，三次握手。\n\n![TCP 三次握手](https://img-blog.csdnimg.cn/img_convert/8d55a06f2efa946921ff61a008c76b00.png)\n\n服务端代码，对 socket 执行 bind 方法可以绑定监听端口，然后执行 `listen` 方法后，就会进入监听（`LISTEN`）状态。内核会为每一个处于 `LISTEN` 状态的 `socket` 分配两个队列，分别叫**半连接队列和全连接队列**。\n\n![每个 listen Socket 都有一个全连接和半连接队列](https://img-blog.csdnimg.cn/img_convert/d7e2d60b28b0f9b460aafbf1bd6e7892.png)\n\n### 半连接队列、全连接队列是什么\n\n![半连接队列和全连接队列](https://img-blog.csdnimg.cn/img_convert/36242c85809865fcd2da48594de15ebb.png)\n\n- **半连接队列（`SYN` 队列）**，服务端收到**第一次握手**后，会将 `sock` 加入到这个队列中，队列内的 `sock` 都处于 `SYN_RECV` 状态。\n- **全连接队列（`ACCEPT` 队列）**，在服务端收到**第三次握手**后，会将半连接队列的 `sock` 取出，放到全连接队列中。队列里的 `sock` 都处于 `ESTABLISHED` 状态。这里面的连接，就**等着服务端执行 `accept()` 后被取出了。**\n\n看到这里，文章开头的问题就有了答案，建立连接的过程中根本不需要 `accept()` 参与， **执行 `accept()` 只是为了从全连接队列里取出一条连接。**\n\n我们把话题再重新回到这两个队列上。\n\n虽然都叫**队列**，但其实**全连接队列（icsk_accept_queue）是个链表**，而**半连接队列（syn_table）是个哈希表**。\n\n![半连接全连接队列的内部结构](https://img-blog.csdnimg.cn/img_convert/6f964fb09d6971dab1762a45dfa30b3b.png)\n\n### 为什么半连接队列要设计成哈希表\n\n先对比下**全连接里队列**，他本质是个链表，因为也是线性结构，说它是个队列也没毛病。它里面放的都是已经建立完成的连接，这些连接正等待被取走。而服务端取走连接的过程中，并不关心具体是哪个连接，只要是个连接就行，所以直接从队列头取就行了。这个过程算法复杂度为 `O(1)`。\n\n而**半连接队列**却不太一样，因为队列里的都是不完整的连接，嗷嗷等待着第三次握手的到来。那么现在有一个第三次握手来了，则需要从队列里把相应 IP 端口的连接取出，**如果半连接队列还是个链表，那我们就需要依次遍历，才能拿到我们想要的那个连接，算法复杂度就是 `O(n)`。**\n\n而如果将半连接队列设计成哈希表，那么查找半连接的算法复杂度就回到 `O(1)` 了。\n\n因此出于效率考虑，全连接队列被设计成链表，而半连接队列被设计为哈希表。\n\n### 怎么观察两个队列的大小\n\n#### 查看全连接队列\n\n```shell\n# ss -lnt\nState      Recv-Q Send-Q     Local Address:Port           Peer Address:Port\nLISTEN     0      128        127.0.0.1:46269              *:*              \n```\n\n通过 `ss -lnt` 命令，可以看到全连接队列的大小，其中 `Send-Q` 是指全连接队列的最大值，可以看到我这上面的最大值是 `128`；`Recv-Q` 是指当前的全连接队列的使用值，我这边用了 `0` 个，也就是全连接队列里为空，连接都被取出来了。\n\n当上面 `Send-Q` 和 `Recv-Q` 数值很接近的时候，那么全连接队列可能已经满了。可以通过下面的命令查看是否发生过队列**溢出**。\n\n```shell\n# netstat -s | grep overflowed\n    4343 times the listen queue of a socket overflowed\n```\n\n上面说明发生过 `4343` 次全连接队列溢出的情况。这个查看到的是**历史发生过的次数**。\n\n如果配合使用 `watch -d` 命令，可以自动每 `2s` 间隔执行相同命令，还能高亮显示变化的数字部分，如果溢出的数字不断变多，说明**正在发生**溢出的行为。\n\n```shell\n# watch -d 'netstat -s | grep overflowed'\nEvery 2.0s: netstat -s | grep overflowed                                \nFri Sep 17 09:00:45 2021\n\n    4343 times the listen queue of a socket overflowed\n```\n\n#### 查看半连接队列\n\n半连接队列没有命令可以直接查看到，但因为半连接队列里，放的都是 `SYN_RECV` 状态的连接，那可以通过统计处于这个状态的连接的数量，间接获得半连接队列的长度。\n\n```shell\n# netstat -nt | grep -i '127.0.0.1:8080' | grep -i 'SYN_RECV' | wc -l\n0\n```\n\n注意半连接队列和全连接队列都是挂在某个 `Listen socket` 上的，我这里用的是 `127.0.0.1:8080`，大家可以替换成自己想要查看的 **IP 端口**。\n\n可以看到我的机器上的半连接队列长度为 `0`，这个很正常，**正经连接谁会没事老待在半连接队列里。**\n\n当队列里的半连接不断增多，最终也是会发生溢出，可以通过下面的命令查看。\n\n```shell\n# netstat -s | grep -i \"SYNs to LISTEN sockets dropped\" \n    26395 SYNs to LISTEN sockets dropped\n```\n\n可以看到，我的机器上一共发生了 `26395` 次半连接队列溢出。同样建议配合 `watch -d` 命令使用。\n\n```shell\n# watch -d 'netstat -s | grep -i \"SYNs to LISTEN sockets dropped\"'\nEvery 2.0s: netstat -s | grep -i \"SYNs to LISTEN sockets dropped\"       \nFri Sep 17 08:36:38 2021\n\n    26395 SYNs to LISTEN sockets dropped\n```\n\n### 全连接队列满了会怎么样？\n\n如果队列满了，服务端还收到客户端的第三次握手 ACK，默认当然会丢弃这个 ACK。\n\n但除了丢弃之外，还有一些附带行为，这会受 `tcp_abort_on_overflow` 参数的影响。\n\n```shell\n# cat /proc/sys/net/ipv4/tcp_abort_on_overflow\n0\n```\n\n- `tcp_abort_on_overflow` 设置为 `0`，全连接队列满了之后，会丢弃这个第三次握手 ACK 包，并且开启定时器，重传第二次握手的 SYN+ACK，如果重传超过一定限制次数，还会把对应的**半连接队列里的连接**给删掉。\n\n![tcp_abort_on_overflow 为 0](https://img-blog.csdnimg.cn/img_convert/874f2fb7108020fd4dcfa021f377ec66.png)\n\n- `tcp_abort_on_overflow`设置为 `1`，全连接队列满了之后，就直接发 RST 给客户端，效果上看就是连接断了。\n\n这个现象是不是很熟悉，服务端**端口未监听**时，客户端尝试去连接，服务端也会回一个RST。这两个情况长一样，所以客户端这时候收到RST之后，其实无法区分到底是**端口未监听**，还是**全连接队列满了**。\n\n![tcp_abort_on_overflow 为 1](https://img-blog.csdnimg.cn/img_convert/6a01c5df74748870a69921da89825d9c.png)\n\n### 半连接队列要是满了会怎么样\n\n**一般是丢弃**，但这个行为可以通过 `tcp_syncookies` 参数去控制。但比起这个，更重要的是先了解下半连接队列为什么会被打满。\n\n首先我们需要明白，一般情况下，半连接的\"生存\"时间其实很短，只有在第一次和第三次握手间，如果半连接都满了，说明服务端疯狂收到第一次握手请求，如果是线上游戏应用，能有这么多请求进来，那说明你可能要富了。但现实往往比较骨感，你可能遇到了 **SYN Flood 攻击**。\n\n所谓 **SYN Flood 攻击**，可以简单理解为，攻击方模拟客户端疯狂发第一次握手请求过来，在服务端憨憨地回复第二次握手过去之后，客户端死活不发第三次握手过来，这样做，可以把服务端半连接队列打满，从而导致正常连接不能正常进来。\n\n![syn攻击](https://img-blog.csdnimg.cn/img_convert/d894de5374a12bd5d75d86d4a718d186.png)\n\n那这种情况怎么处理？有没有一种方法可以**绕过半连接队列**？\n\n有，上面提到的 `tcp_syncookies` 派上用场了。\n\n```shell\n# cat /proc/sys/net/ipv4/tcp_syncookies\n1\n```\n\n当它被设置为 `1` 的时候，客户端发来**第一次握手** SYN 时，服务端**不会将其放入半连接队列中**，而是直接生成一个 `cookies`，这个 `cookies` 会跟着**第二次握手**，发回客户端。客户端在发**第三次握手**的时候带上这个 `cookies`，服务端验证到它就是当初发出去的那个，就会建立连接并放入到全连接队列中。可以看出整个过程不再需要半连接队列的参与。\n\n![tcp_syncookies=1](https://img-blog.csdnimg.cn/img_convert/d696b8b345526533bde8fa990e205c32.png)\n\n#### 会有一个 cookies 队列吗\n\n生成是 `cookies`，保存在哪呢？**是不是会有一个队列保存这些 cookies？**\n\n我们可以反过来想一下，如果有 `cookies` 队列，那它会跟半连接队列一样，到头来，还是会被 **SYN Flood 攻击**打满。\n\n实际上 `cookies` 并不会有一个专门的队列保存，它是通过**通信双方的 IP 地址端口、时间戳、MSS**等信息进行**实时计算**的，保存在 **TCP 报头**的 `seq` 里。\n\n![tcp 报头_seq 的位置](https://img-blog.csdnimg.cn/img_convert/6d280b0946a73ea6185653cbcfcc489f.png)\n\n当服务端收到客户端发来的第三次握手包时，会通过 seq 还原出**通信双方的 IP 地址端口、时间戳、MSS**，验证通过则建立连接。\n\n#### cookies 方案为什么不直接取代半连接队列？\n\n目前看下来 `syn cookies` 方案省下了半连接队列所需要的队列内存，还能解决 **SYN Flood 攻击**，那为什么不直接取代半连接队列？\n\n凡事皆有利弊，`cookies` 方案虽然能防 **SYN Flood 攻击**，但是也有一些问题。因为服务端并不会保存连接信息，所以如果传输过程中数据包丢了，也不会重发第二次握手的信息。\n\n另外，编码解码 `cookies`，都是比较**耗 CPU** 的，利用这一点，如果此时攻击者构造大量的**第三次握手包（ACK 包）**，同时带上各种瞎编的 `cookies` 信息，服务端收到 `ACK 包`后**以为是正经 cookies**，憨憨地跑去解码（**耗 CPU**），最后发现不是正经数据包后才丢弃。\n\n这种通过构造大量 `ACK 包`去消耗服务端资源的攻击，叫 **ACK 攻击**，受到攻击的服务器可能会因为 **CPU 资源耗尽**导致没能响应正经请求。\n\n![ack 攻击](https://img-blog.csdnimg.cn/img_convert/15a0a5f7fe15ee2bc5e07492eda5a8ea.gif)\n\n### 没有 listen，为什么还能建立连接\n\n那既然没有 `accept` 方法能建立连接，那是不是没有 `listen` 方法，也能建立连接？是的，之前写的一篇文章提到过客户端是可以自己连自己的形成连接（**TCP 自连接**），也可以两个客户端同时向对方发出请求建立连接（**TCP 同时打开**），这两个情况都有个共同点，就是**没有服务端参与，也就是没有 listen，就能建立连接。**\n\n当时文章最后也留了个疑问，**没有 listen，为什么还能建立连接？**\n\n我们知道执行 `listen` 方法时，会创建半连接队列和全连接队列。\n\n三次握手的过程中会在这两个队列中暂存连接信息。\n\n所以形成连接，前提是你得**有个地方存放着**，方便握手的时候能根据 IP 端口等信息找到 socket 信息。\n\n**那么客户端会有半连接队列吗？**\n\n**显然没有**，因为客户端没有执行 `listen`，因为半连接队列和全连接队列都是在执行 `listen` 方法时，内核自动创建的。\n\n但内核还有个**全局 `hash` 表**，可以用于存放 `sock` 连接的信息。这个全局 `hash` 表其实还细分为 `ehash，bhash和listen_hash` 等，但因为过于细节，大家理解成有一个**全局 hash** 就够了。\n\n在 TCP 自连接的情况中，客户端在 `connect` 方法时，最后会将自己的连接信息放入到这个**全局 hash 表**中，然后将信息发出，消息在经过回环地址重新回到 TCP 传输层的时候，就会根据 IP 端口信息，再一次从这个**全局 hash** 中取出信息。于是握手包一来一回，最后成功建立连接。\n\nTCP 同时打开的情况也类似，只不过从一个客户端变成了两个客户端而已。\n\n## 总结\n\n- **每一个** `socket` 执行 `listen` 时，内核都会自动创建一个半连接队列和全连接队列。\n- 第三次握手前，TCP 连接会放在半连接队列中，直到第三次握手到来，才会被放到全连接队列中。\n- `accept` 方法只是为了从全连接队列中拿出一条连接，本身跟三次握手几乎**毫无关系**。\n- 出于效率考虑，虽然都叫队列，但半连接队列其实被设计成了**哈希表**，而全连接队列本质是链表。\n- 全连接队列满了，再来第三次握手也会丢弃，此时如果 `tcp_abort_on_overflow=1`，还会直接发 `RST` 给客户端。\n- 半连接队列满了，可能是因为受到了 `SYN Flood` 攻击，可以设置 `tcp_syncookies`，绕开半连接队列。\n- 客户端没有半连接队列和全连接队列，但有一个**全局 hash**，可以通过它实现自连接或 TCP 同时打开。\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_no_listen":{"title":"tcp_no_listen","content":"# 4.19 服务端没有 listen，客户端发起连接建立，会发生什么？\n\n大家好，我是小林。\n\n早上看到一个读者说面字节三面的时候，问了这个问题：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/5f5b9c96c86580e3f14978d5c10c7721.jpeg)\n\n这位读者的角度是以为服务端没有调用 listen，客户端会 ping 不通服务器，很明显，搞错了。\n\nping 使用的协议是 ICMP，属于网络层的事情，而面试官问的是传输层的问题。\n\n针对这个问题，服务端如果只 bind 了 IP 地址和端口，而没有调用 listen 的话，然后客户端对服务端发起了 TCP 连接建立，此时那么会发生什么呢？\n\n## 做个实验\n\n这个问题，自己做个实验就知道了。\n\n我用下面这个程序作为例子，绑定了 IP 地址 + 端口，而没有调用 listen。\n\n```c\n/*******服务器程序  TCPServer.c ************/\n#include \u003cstdlib.h\u003e\n#include \u003cstdio.h\u003e\n#include \u003cerrno.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cnetdb.h\u003e\n#include \u003csys/types.h\u003e\n#include \u003cnetinet/in.h\u003e\n#include \u003csys/socket.h\u003e\n\nint main(int argc, char *argv[])\n{\n    int sockfd, ret;\n    struct sockaddr_in server_addr;\n\n    /* 服务器端创建 tcp socket 描述符 */\n    sockfd = socket(AF_INET, SOCK_STREAM, 0);\n    if(sockfd \u003c 0)\n    {\n        fprintf(stderr, \"Socket error:%s\\n\\a\", strerror(errno));\n        exit(1);\n    }\n\n    /* 服务器端填充 sockaddr 结构 */\n    bzero(\u0026server_addr, sizeof(struct sockaddr_in));\n    server_addr.sin_family = AF_INET;\n    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);\n    server_addr.sin_port = htons(8888);\n  \n  /* 绑定 ip + 端口 */\n    ret = bind(sockfd, (struct sockaddr *)(\u0026server_addr), sizeof(struct sockaddr));\n    if(ret \u003c 0)\n    {\n        fprintf(stderr, \"Bind error:%s\\n\\a\", strerror(errno));\n        exit(1);\n    }\n  \n  //没有调用 listen\n    \n    sleep(1000);\n    close(sockfd);\n    return 0;\n}\n```\n\n然后，我用浏览器访问这个地址：http://121.43.173.240:8888/\n\n![图片](https://img-blog.csdnimg.cn/img_convert/5bdb5443db5b97ff724ab94e014af6a5.png)\n\n报错连接服务器失败。\n\n同时，我也用抓包工具，抓了这个过程。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/a77921ffafbbff86d07983ca0db3e6e0.png)\n\n可以看到，客户端对服务端发起 SYN 报文后，服务端回了 RST 报文。\n\n所以，这个问题就有了答案，**服务端如果只 bind 了 IP 地址和端口，而没有调用 listen 的话，然后客户端对服务端发起了连接建立，服务端会回 RST 报文。**\n\n## 源码分析\n\n接下来，带大家源码分析一下。\n\nLinux 内核处理收到 TCP 报文的入口函数是  tcp_v4_rcv，在收到 TCP 报文后，会调用 __inet_lookup_skb 函数找到 TCP 报文所属 socket 。\n\n```\nint tcp_v4_rcv(struct sk_buff *skb)\n{\n ...\n  \n sk = __inet_lookup_skb(\u0026tcp_hashinfo, skb, th-\u003esource, th-\u003edest);\n if (!sk)\n  goto no_tcp_socket;\n ...\n}\n```\n\n__inet_lookup_skb 函数首先查找连接建立状态的socket（__inet_lookup_established），在没有命中的情况下，才会查找监听套接口（__inet_lookup_listener）。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/88416aa95d255495e07fb3a002b2167b.png)\n\n查找监听套接口（__inet_lookup_listener）这个函数的实现是，根据目的地址和目的端口算出一个哈希值，然后在哈希表找到对应监听该端口的 socket。\n\n本次的案例中，服务端是没有调用 listen 函数的，所以自然也是找不到监听该端口的 socket。\n\n所以，__inet_lookup_skb 函数最终找不到对应的 socket，于是跳转到no_tcp_socket。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/54ee363e149ee3dfba30efb1a542ef5c.png)\n\n在这个错误处理中，只要收到的报文（skb）的「校验和」没问题的话，内核就会调用 tcp_v4_send_reset 发送 RST 中止这个连接。\n\n至此，整个源码流程就解析完。\n\n其实很多网络的问题，大家都可以自己做实验来找到答案的。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/8d04584bf7fa40f02229d611a569f370.jpeg)\n\n## 没有 listen，能建立 TCP 连接吗？\n\n标题的问题在前面已经解答，**现在我们看另外一个相似的问题**。\n\n之前看群消息，看到有读者面试腾讯的时候，被问到这么一个问题。\n\n\u003e 不使用 listen ，可以建立 TCP 连接吗？\n\n答案，**是可以的，客户端是可以自己连自己的形成连接（TCP自连接），也可以两个客户端同时向对方发出请求建立连接（TCP同时打开），这两个情况都有个共同点，就是没有服务端参与，也就是没有listen，就能建立连接**。\n\n\u003e 那没有listen，为什么还能建立连接？\n\n我们知道执行 listen 方法时，会创建半连接队列和全连接队列。\n\n三次握手的过程中会在这两个队列中暂存连接信息。\n\n所以形成连接，前提是你得有个地方存放着，方便握手的时候能根据 IP + 端口等信息找到对应的 socket。\n\n\u003e 那么客户端会有半连接队列吗？\n\n显然没有，因为客户端没有执行listen，因为半连接队列和全连接队列都是在执行 listen 方法时，内核自动创建的。\n\n但内核还有个全局 hash 表，可以用于存放 sock 连接的信息。\n\n这个全局 hash 表其实还细分为 ehash，bhash和listen_hash等，但因为过于细节，大家理解成有一个全局 hash 就够了，\n\n**在 TCP 自连接的情况中，客户端在 connect 方法时，最后会将自己的连接信息放入到这个全局 hash 表中，然后将信息发出，消息在经过回环地址重新回到 TCP 传输层的时候，就会根据 IP + 端口信息，再一次从这个全局 hash 中取出信息。于是握手包一来一回，最后成功建立连接**。\n\nTCP 同时打开的情况也类似，只不过从一个客户端变成了两个客户端而已。\n\n\u003e 做个实验\n\n客户端自连接的代码，TCP socket 可以 connect 它本身 bind 的地址和端口：\n\n\n```c\n#include \u003csys/types.h\u003e \n#include \u003csys/socket.h\u003e\n#include \u003cnetinet/in.h\u003e\n#include \u003cunistd.h\u003e\n#include \u003cstdlib.h\u003e\n#include \u003cstdio.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cerrno.h\u003e\n\n#define LOCAL_IP_ADDR\t\t(0x7F000001) // IP 127.0.0.1\n#define LOCAL_TCP_PORT\t\t(34567) // 端口\n\nint main(void)\n{\n\tstruct sockaddr_in local, peer;\n\tint ret;\n\tchar buf[128];\n\tint sock = socket(AF_INET, SOCK_STREAM, 0);\n\n\tmemset(\u0026local, 0, sizeof(local));\n\tmemset(\u0026peer, 0, sizeof(peer));\n\n\tlocal.sin_family = AF_INET;\n\tlocal.sin_port = htons(LOCAL_TCP_PORT);\n\tlocal.sin_addr.s_addr = htonl(LOCAL_IP_ADDR);\n\n\tpeer = local;\t\n\n    int flag = 1;\n    ret = setsockopt(sock, SOL_SOCKET, SO_REUSEADDR, \u0026flag, sizeof(flag));\n    if (ret == -1) {\n        printf(\"Fail to setsocket SO_REUSEADDR: %s\\n\", strerror(errno));\n        exit(1);\n    }\n\n\tret = bind(sock, (const struct sockaddr *)\u0026local, sizeof(local));\n\tif (ret) {\n\t\tprintf(\"Fail to bind: %s\\n\", strerror(errno));\n\t\texit(1);\n\t}\n\t\n\tret = connect(sock, (const struct sockaddr *)\u0026peer, sizeof(peer));\n\tif (ret) {\n\t\tprintf(\"Fail to connect myself: %s\\n\", strerror(errno));\n\t\texit(1);\n\t}\n\t\n\tprintf(\"Connect to myself successfully\\n\");\n\n    //发送数据\n\tstrcpy(buf, \"Hello, myself~\");\n\tsend(sock, buf, strlen(buf), 0);\n\n\tmemset(buf, 0, sizeof(buf));\n\t\n\t//接收数据\n\trecv(sock, buf, sizeof(buf), 0);\n\tprintf(\"Recv the msg: %s\\n\", buf);\n\n    sleep(1000);\n\tclose(sock);\n\treturn 0;\n}\n```\n\n编译运行：\n\n![](https://img-blog.csdnimg.cn/9db974179b9e4a279f7edb0649752c27.png)\n\n\n通过 netstat 命令命令客户端自连接的 TCP 连接：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/e2b116e843c14e468eadf9d30e1b877c.png)\n\n从截图中，可以看到 TCP socket 成功的“连接”了自己，并发送和接收了数据包，netstat 的输出更证明了 TCP 的两端地址和端口是完全相同的。\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_optimize":{"title":"tcp_optimize","content":"# 4.5 如何优化 TCP?\n\nTCP 性能的提升不仅考察 TCP 的理论知识，还考察了对于操作系统提供的内核参数的理解与应用。\n\nTCP 协议是由操作系统实现，所以操作系统提供了不少调节 TCP 的参数。\n\n![Linux TCP 参数](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/2.jpg)\n\n\n如何正确有效的使用这些参数，来提高 TCP 性能是一个不那么简单事情。我们需要针对 TCP 每个阶段的问题来对症下药，而不是病急乱投医。\n\n接下来，将以三个角度来阐述提升 TCP 的策略，分别是：\n\n- TCP 三次握手的性能提升；\n- TCP 四次挥手的性能提升；\n- TCP 数据传输的性能提升；\n\n![本节提纲](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/3.jpg)\n\n---\n\n## TCP 三次握手的性能提升\n\nTCP 是面向连接的、可靠的、双向传输的传输层通信协议，所以在传输数据之前需要经过三次握手才能建立连接。\n\n![三次握手与数据传输](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/4.jpg)\n\n那么，三次握手的过程在一个 HTTP 请求的平均时间占比 10% 以上，在网络状态不佳、高并发或者遭遇 SYN 攻击等场景中，如果不能有效正确的调节三次握手中的参数，就会对性能产生很多的影响。\n\n如何正确有效的使用这些参数，来提高 TCP 三次握手的性能，这就需要理解「三次握手的状态变迁」，这样当出现问题时，先用 `netstat` 命令查看是哪个握手阶段出现了问题，再来对症下药，而不是病急乱投医。\n\n![TCP 三次握手的状态变迁](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/5.jpg)\n\n客户端和服务端都可以针对三次握手优化性能。主动发起连接的客户端优化相对简单些，而服务端需要监听端口，属于被动连接方，其间保持许多的中间状态，优化方法相对复杂一些。\n\n所以，客户端（主动发起连接方）和服务端（被动连接方）优化的方式是不同的，接下来分别针对客户端和服务端优化。\n\n### 客户端优化\n\n三次握手建立连接的首要目的是「同步序列号」。\n\n只有同步了序列号才有可靠传输，TCP 许多特性都依赖于序列号实现，比如流量控制、丢包重传等，这也是三次握手中的报文称为 SYN 的原因，SYN 的全称就叫 *Synchronize Sequence Numbers*（同步序列号）。\n\n![TCP 头部](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/6.jpg)\n\n\u003e SYN_SENT 状态的优化\n\n客户端作为主动发起连接方，首先它将发送 SYN 包，于是客户端的连接就会处于 `SYN_SENT` 状态。\n\n客户端在等待服务端回复的 ACK 报文，正常情况下，服务器会在几毫秒内返回 SYN+ACK ，但如果客户端长时间没有收到 SYN+ACK 报文，则会重发 SYN 包，**重发的次数由 tcp_syn_retries 参数控制**，默认是 5 次：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/7.jpg)\n\n通常，第一次超时重传是在 1 秒后，第二次超时重传是在 2 秒，第三次超时重传是在 4 秒后，第四次超时重传是在 8 秒后，第五次是在超时重传 16 秒后。没错，**每次超时的时间是上一次的 2 倍**。\n\n当第五次超时重传后，会继续等待 32 秒，如果服务端仍然没有回应 ACK，客户端就会终止三次握手。\n\n所以，总耗时是 1+2+4+8+16+32=63 秒，大约 1 分钟左右。\n\n![SYN 超时重传](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/8.jpg)\n\n你可以根据网络的稳定性和目标服务器的繁忙程度修改 SYN 的重传次数，调整客户端的三次握手时间上限。比如内网中通讯时，就可以适当调低重试次数，尽快把错误暴露给应用程序。\n\n### 服务端优化\n\n\n当服务端收到 SYN 包后，服务端会立马回复 SYN+ACK 包，表明确认收到了客户端的序列号，同时也把自己的序列号发给对方。\n\n此时，服务端出现了新连接，状态是 `SYN_RCV`。在这个状态下，Linux 内核就会建立一个「半连接队列」来维护「未完成」的握手信息，当半连接队列溢出后，服务端就无法再建立新的连接。\n\n![半连接队列与全连接队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/9.jpg)\n\nSYN 攻击，攻击的是就是这个半连接队列。\n\n\u003e 如何查看由于 SYN 半连接队列已满，而被丢弃连接的情况？\n\n我们可以通过该 `netstat -s` 命令给出的统计结果中，  可以得到由于半连接队列已满，引发的失败次数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/10.jpg)\n\n上面输出的数值是**累计值**，表示共有多少个 TCP 连接因为半连接队列溢出而被丢弃。**隔几秒执行几次，如果有上升的趋势，说明当前存在半连接队列溢出的现象**。\n\n\u003e 如何调整 SYN 半连接队列大小？\n\n要想增大半连接队列，**不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大 accept 队列。否则，只单纯增大 tcp_max_syn_backlog 是无效的。**\n\n增大 tcp_max_syn_backlog 和 somaxconn 的方法是修改 Linux 内核参数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/11.jpg)\n\n增大 backlog 的方式，每个 Web 服务都不同，比如 Nginx 增大 backlog 的方法如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/12.jpg)\n\n最后，改变了如上这些参数后，要重启 Nginx 服务，因为 SYN 半连接队列和 accept 队列都是在 `listen()` 初始化的。\n\n\u003e 如果 SYN 半连接队列已满，只能丢弃连接吗？\n\n并不是这样，**开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接**。\n\nsyncookies 的工作原理：服务器根据当前状态计算出一个值，放在己方发出的 SYN+ACK 报文中发出，当客户端返回 ACK 报文时，取出该值验证，如果合法，就认为连接建立成功，如下图所示。\n\n![开启 syncookies 功能](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/13.jpg)\n\nsyncookies 参数主要有以下三个值：\n\n- 0 值，表示关闭该功能；\n- 1 值，表示仅当 SYN 半连接队列放不下时，再启用它；\n- 2 值，表示无条件开启功能；\n\n那么在应对 SYN 攻击时，只需要设置为 1 即可：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/14.jpg)\n\n\u003e SYN_RCV 状态的优化\n\n当客户端接收到服务器发来的 SYN+ACK 报文后，就会回复 ACK 给服务器，同时客户端连接状态从 SYN_SENT 转换为 ESTABLISHED，表示连接建立成功。\n\n服务器端连接成功建立的时间还要再往后，等到服务端收到客户端的 ACK 后，服务端的连接状态才变为 ESTABLISHED。\n\n如果服务器没有收到 ACK，就会重发 SYN+ACK 报文，同时一直处于 SYN_RCV 状态。\n\n当网络繁忙、不稳定时，报文丢失就会变严重，此时应该调大重发次数。反之则可以调小重发次数。**修改重发次数的方法是，调整 tcp_synack_retries 参数**：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/15.jpg)\n\ntcp_synack_retries 的默认重试次数是 5 次，与客户端重传 SYN 类似，它的重传会经历 1、2、4、8、16 秒，最后一次重传后会继续等待 32 秒，如果服务端仍然没有收到 ACK，才会关闭连接，故共需要等待 63 秒。\n\n服务器收到 ACK 后连接建立成功，此时，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。\n\n如果进程不能及时地调用 accept 函数，就会造成 accept 队列（也称全连接队列）溢出，最终导致建立好的 TCP 连接被丢弃。\n\n![ accept 队列溢出](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/16.jpg)\n\n\u003e accept 队列已满，只能丢弃连接吗？\n\n丢弃连接只是 Linux 的默认行为，我们还可以选择向客户端发送 RST 复位报文，告诉客户端连接已经建立失败。打开这一功能需要将 tcp_abort_on_overflow 参数设置为 1。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/17.jpg)\n\ntcp_abort_on_overflow 共有两个值分别是 0 和 1，其分别表示：\n\n- 0 ：如果 accept 队列满了，那么 server 扔掉 client  发过来的 ack ；\n- 1 ：如果 accept 队列满了，server 发送一个 `RST` 包给 client，表示废掉这个握手过程和这个连接；\n\n如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把 tcp_abort_on_overflow 设置为 1，这时如果在客户端异常中可以看到很多 `connection reset by peer` 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。\n\n通常情况下，应当把 tcp_abort_on_overflow 设置为 0，因为这样更有利于应对突发流量。\n\n举个例子，当 accept 队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED，客户端进程就在建立好的连接上发送请求。只要服务器没有为请求回复 ACK，客户端的请求就会被多次「重发」。**如果服务器上的进程只是短暂的繁忙造成 accept 队列满，那么当 accept 队列有空位时，再次接收到的请求报文由于含有 ACK，仍然会触发服务器端成功建立连接。**\n\n![tcp_abort_on_overflow 为 0 可以应对突发流量](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/18.jpg)\n\n\n所以，tcp_abort_on_overflow 设为 0 可以提高连接建立的成功率，只有你非常肯定 TCP 全连接队列会长期溢出时，才能设置为 1 以尽快通知客户端。\n\n\n\u003e 如何调整 accept 队列的长度呢？\n\naccept 队列的长度取决于 somaxconn 和 backlog 之间的最小值，也就是 min(somaxconn, backlog)，其中：\n\n- somaxconn 是 Linux 内核的参数，默认值是 128，可以通过 `net.core.somaxconn` 来设置其值；\n- backlog 是 `listen(int sockfd, int backlog)` 函数中的 backlog 大小；\n\nTomcat、Nginx、Apache 常见的 Web 服务的 backlog 默认值都是 511。\n\n\u003e 如何查看服务端进程 accept 队列的长度？\n\n可以通过 `ss -ltn` 命令查看：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/19.jpg)\n\n- Recv-Q：当前 accept 队列的大小，也就是当前已完成三次握手并等待服务端 `accept()` 的 TCP 连接；\n- Send-Q：accept 队列最大长度，上面的输出结果说明监听 8088 端口的 TCP 服务，accept 队列的最大长度为 128；\n\n\u003e 如何查看由于 accept 连接队列已满，而被丢弃的连接？\n\n当超过了 accept 连接队列，服务端则会丢掉后续进来的 TCP 连接，丢掉的 TCP 连接的个数会被统计起来，我们可以使用 netstat -s 命令来查看：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/20.jpg)\n\n上面看到的 41150 times ，表示 accept 队列溢出的次数，注意这个是累计值。可以隔几秒钟执行下，如果这个数字一直在增加的话，说明 accept 连接队列偶尔满了。\n\n如果持续不断地有连接因为 accept 队列溢出被丢弃，就应该调大 backlog 以及 somaxconn 参数。\n\n### 如何绕过三次握手？\n\n以上我们只是在对三次握手的过程进行优化，接下来我们看看如何绕过三次握手发送数据。\n\n三次握手建立连接造成的后果就是，HTTP 请求必须在一个 RTT（从客户端到服务器一个往返的时间）后才能发送。\n\n![常规 HTTP 请求](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/21.jpg)\n\n在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。\n\n\u003e 接下来说说，TCP Fast Open 功能的工作方式。\n\n![开启 TCP Fast Open 功能](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/22.jpg)\n\n在客户端首次建立连接时的过程：\n\n1. 客户端发送 SYN 报文，该报文包含 Fast Open 选项，且该选项的 Cookie 为空，这表明客户端请求 Fast Open Cookie；\n2. 支持 TCP Fast Open 的服务器生成 Cookie，并将其置于 SYN-ACK 数据包中的 Fast Open 选项以发回客户端；\n3. 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。\n\n所以，第一次发起 HTTP GET 请求的时候，还是需要正常的三次握手流程。\n\n之后，如果客户端再次向服务器建立连接时的过程：\n\n1. 客户端发送 SYN 报文，该报文包含「数据」（对于非 TFO 的普通 TCP 握手过程，SYN 报文中不包含「数据」）以及此前记录的 Cookie；\n2. 支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 报文中对 SYN 和「数据」进行确认，服务器随后将「数据」递送至相应的应用程序；如果 Cookie 无效，服务器将丢弃 SYN 报文中包含的「数据」，且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；\n3. 如果服务器接受了 SYN 报文中的「数据」，服务器可在握手完成之前发送「数据」，**这就减少了握手带来的 1 个 RTT 的时间消耗**；\n4. 客户端将发送 ACK 确认服务器发回的 SYN 以及「数据」，但如果客户端在初始的 SYN 报文中发送的「数据」没有被确认，则客户端将重新发送「数据」；\n5. 此后的 TCP 连接的数据传输过程和非 TFO 的正常情况一致。\n\n所以，之后发起 HTTP GET 请求的时候，可以绕过三次握手，这就减少了握手带来的 1 个 RTT 的时间消耗。\n\n开启了 TFO 功能，cookie 的值是存放到 TCP option 字段里的：\n\n![TCP option 字段 - TFO](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%82%E6%95%B0/TCP%20option%E5%AD%97%E6%AE%B5%20-%20TFO.png)\n\n注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断重复 TCP Fast Open 直至服务器认为 Cookie 无效（通常为过期）。\n\n\u003e Linux 下怎么打开 TCP Fast Open 功能呢？\n\n在 Linux 系统中，可以通过**设置 tcp_fastopn 内核参数，来打开 Fast Open 功能**：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/23.jpg)\n\ntcp_fastopn 各个值的意义: \n\n- 0 关闭\n- 1 作为客户端使用 Fast Open 功能\n- 2 作为服务端使用 Fast Open 功能\n- 3 无论作为客户端还是服务器，都可以使用 Fast Open 功能\n\n**TCP Fast Open 功能需要客户端和服务端同时支持，才有效果。**\n\n### 小结\n\n本小结主要介绍了关于优化 TCP 三次握手的几个 TCP 参数。\n\n![三次握手优化策略](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/24.jpg)\n\n\u003e 客户端的优化\n\n当客户端发起 SYN 包时，可以通过 `tcp_syn_retries` 控制其重传的次数。\n\n\u003e 服务端的优化\n\n当服务端 SYN 半连接队列溢出后，会导致后续连接被丢弃，可以通过 `netstat -s` 观察半连接队列溢出的情况，如果 SYN 半连接队列溢出情况比较严重，可以通过 `tcp_max_syn_backlog、somaxconn、backlog` 参数来调整 SYN 半连接队列的大小。\n\n服务端回复 SYN+ACK 的重传次数由 `tcp_synack_retries` 参数控制。如果遭受 SYN 攻击，应把 `tcp_syncookies` 参数设置为 1，表示仅在 SYN 队列满后开启 syncookie 功能，可以保证正常的连接成功建立。\n\n服务端收到客户端返回的 ACK，会把连接移入 accpet 队列，等待进行调用 accpet() 函数取出连接。\n\n可以通过 `ss -lnt` 查看服务端进程的 accept 队列长度，如果 accept 队列溢出，系统默认丢弃 ACK，如果可以把 `tcp_abort_on_overflow` 设置为 1 ，表示用 RST 通知客户端连接建立失败。\n\n如果 accpet 队列溢出严重，可以通过 listen 函数的 `backlog` 参数和 `somaxconn` 系统参数提高队列大小，accept 队列长度取决于 min(backlog, somaxconn)。\n\n\u003e 绕过三次握手\n\nTCP Fast Open 功能可以绕过三次握手，使得 HTTP 请求减少了 1 个 RTT 的时间，Linux 下可以通过 `tcp_fastopen` 开启该功能，同时必须保证服务端和客户端同时支持。\n\n---\n\n## TCP 四次挥手的性能提升\n\n接下来，我们一起看看针对 TCP 四次挥手关闭连接时，如何优化性能。\n\n在开始之前，我们得先了解四次挥手状态变迁的过程。\n\n客户端和服务端双方都可以主动断开连接，**通常先关闭连接的一方称为主动方，后关闭连接的一方称为被动方。**\n\n![客户端主动关闭](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/25.jpg)\n\n可以看到，**四次挥手过程只涉及了两种报文，分别是 FIN 和 ACK**：\n\n- FIN 就是结束连接的意思，谁发出 FIN 报文，就表示它将不会再发送任何数据，关闭这一方向上的传输通道；\n- ACK 就是确认的意思，用来通知对方：你方的发送通道已经关闭；\n\n四次挥手的过程:\n\n- 当主动方关闭连接时，会发送 FIN 报文，此时发送方的 TCP 连接将从 ESTABLISHED 变成 FIN_WAIT1。\n- 当被动方收到 FIN 报文后，内核会自动回复 ACK 报文，连接状态将从 ESTABLISHED 变成 CLOSE_WAIT，表示被动方在等待进程调用 close 函数关闭连接。\n- 当主动方收到这个 ACK 后，连接状态由 FIN_WAIT1 变为 FIN_WAIT2，也就是表示**主动方的发送通道就关闭了**。\n- 当被动方进入 CLOSE_WAIT 时，被动方还会继续处理数据，等到进程的 read 函数返回 0 后，应用程序就会调用 close 函数，进而触发内核发送 FIN 报文，此时被动方的连接状态变为 LAST_ACK。\n- 当主动方收到这个 FIN 报文后，内核会回复 ACK 报文给被动方，同时主动方的连接状态由 FIN_WAIT2 变为 TIME_WAIT，**在 Linux 系统下大约等待 1 分钟后，TIME_WAIT 状态的连接才会彻底关闭**。\n- 当被动方收到最后的 ACK 报文后，**被动方的连接就会关闭**。\n\n你可以看到，每个方向都需要**一个 FIN 和一个 ACK**，因此通常被称为**四次挥手**。\n\n这里一点需要注意是：**主动关闭连接的，才有 TIME_WAIT 状态。**\n\n主动关闭方和被动关闭方优化的思路也不同，接下来分别说说如何优化他们。\n\n### 主动方的优化\n\n关闭连接的方式通常有两种，分别是 RST 报文关闭和 FIN 报文关闭。\n\n如果进程收到 RST 报文，就直接关闭连接了，不需要走四次挥手流程，是一个暴力关闭连接的方式。\n\n安全关闭连接的方式必须通过四次挥手，它由进程调用 `close` 和 `shutdown` 函数发起 FIN 报文（shutdown 参数须传入 SHUT_WR 或者 SHUT_RDWR 才会发送 FIN）。\n\n\u003e 调用 close 函数和 shutdown 函数有什么区别？\n\n调用了 close 函数意味着完全断开连接，**完全断开不仅指无法传输数据，而且也不能发送数据。 此时，调用了 close 函数的一方的连接叫做「孤儿连接」，如果你用 netstat -p 命令，会发现连接对应的进程名为空。**\n\n使用 close 函数关闭连接是不优雅的。于是，就出现了一种优雅关闭连接的 `shutdown` 函数，**它可以控制只关闭一个方向的连接**：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/26.jpg)\n\n第二个参数决定断开连接的方式，主要有以下三种方式：\n\n- SHUT_RD(0)：**关闭连接的「读」这个方向**，如果接收缓冲区有已接收的数据，则将会被丢弃，并且后续再收到新的数据，会对数据进行 ACK，然后悄悄地丢弃。也就是说，对端还是会接收到 ACK，在这种情况下根本不知道数据已经被丢弃了。\n- SHUT_WR(1)：**关闭连接的「写」这个方向**，这就是常被称为「半关闭」的连接。如果发送缓冲区还有未发送的数据，将被立即发送出去，并发送一个 FIN 报文给对端。\n- SHUT_RDWR(2)：相当于 SHUT_RD 和 SHUT_WR 操作各一次，**关闭套接字的读和写两个方向**。\n\nclose 和 shutdown 函数都可以关闭连接，但这两种方式关闭的连接，不只功能上有差异，控制它们的 Linux 参数也不相同。\n\n\u003e FIN_WAIT1 状态的优化\n\n主动方发送 FIN 报文后，连接就处于 FIN_WAIT1 状态，正常情况下，如果能及时收到被动方的 ACK，则会很快变为 FIN_WAIT2 状态。\n\n但是当迟迟收不到对方返回的 ACK 时，连接就会一直处于 FIN_WAIT1 状态。此时，**内核会定时重发 FIN 报文，其中重发次数由 tcp_orphan_retries 参数控制**（注意，orphan 虽然是孤儿的意思，该参数却不只对孤儿连接有效，事实上，它对所有 FIN_WAIT1 状态下的连接都有效），默认值是 0。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/27.jpg)\n\n你可能会好奇，这 0 表示几次？**实际上当为 0 时，特指 8 次**，从下面的内核源码可知：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/28.jpg)\n\n如果 FIN_WAIT1 状态连接很多，我们就需要考虑降低 tcp_orphan_retries 的值，当重传次数超过 tcp_orphan_retries 时，连接就会直接关闭掉。\n\n对于普遍正常情况时，调低 tcp_orphan_retries 就已经可以了。如果遇到恶意攻击，FIN 报文根本无法发送出去，这由 TCP 两个特性导致的：\n\n- 首先，TCP 必须保证报文是有序发送的，FIN 报文也不例外，当发送缓冲区还有数据没有发送时，FIN 报文也不能提前发送。\n- 其次，TCP 有流量控制功能，当接收方接收窗口为 0 时，发送方就不能再发送数据。所以，当攻击者下载大文件时，就可以通过接收窗口设为 0 ，这就会使得 FIN 报文都无法发送出去，那么连接会一直处于 FIN_WAIT1 状态。\n\n解决这种问题的方法，是**调整 tcp_max_orphans 参数，它定义了「孤儿连接」的最大数量**：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/29.jpg)\n\n当进程调用了 `close` 函数关闭连接，此时连接就会是「孤儿连接」，因为它无法再发送和接收数据。Linux 系统为了防止孤儿连接过多，导致系统资源长时间被占用，就提供了 `tcp_max_orphans` 参数。如果孤儿连接数量大于它，新增的孤儿连接将不再走四次挥手，而是直接发送 RST 复位报文强制关闭。\n\n\u003e FIN_WAIT2 状态的优化\n\n当主动方收到 ACK 报文后，会处于 FIN_WAIT2 状态，就表示主动方的发送通道已经关闭，接下来将等待对方发送 FIN 报文，关闭对方的发送通道。\n\n这时，**如果连接是用 shutdown 函数关闭的，连接可以一直处于 FIN_WAIT2 状态，因为它可能还可以发送或接收数据。但对于 close 函数关闭的孤儿连接，由于无法再发送和接收数据，所以这个状态不可以持续太久，而 tcp_fin_timeout 控制了这个状态下连接的持续时长**，默认值是 60 秒：\n\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/30.jpg)\n\n它意味着对于孤儿连接（调用 close 关闭的连接），如果在 60 秒后还没有收到 FIN 报文，连接就会直接关闭。\n\n这个 60 秒不是随便决定的，它与 TIME_WAIT 状态持续的时间是相同的，后面我们再来说说为什么是 60 秒。\n\n\u003e TIME_WAIT 状态的优化\n\nTIME_WAIT 是主动方四次挥手的最后一个状态，也是最常遇见的状态。\n\n当收到被动方发来的 FIN 报文后，主动方会立刻回复 ACK，表示确认对方的发送通道已经关闭，接着就处于 TIME_WAIT 状态。在 Linux 系统，TIME_WAIT 状态会持续 60 秒后才会进入关闭状态。\n\nTIME_WAIT 状态的连接，在主动方看来确实快已经关闭了。然后，被动方没有收到 ACK 报文前，还是处于 LAST_ACK 状态。如果这个 ACK 报文没有到达被动方，被动方就会重发 FIN 报文。重发次数仍然由前面介绍过的 tcp_orphan_retries 参数控制。\n\nTIME-WAIT 的状态尤其重要，主要是两个原因：\n\n- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；\n- 保证「被动关闭连接」的一方，能被正确的关闭；\n\n*原因一：防止历史连接中的数据，被后面相同四元组的连接错误的接收*\n\nTIME-WAIT 的一个作用是**防止收到历史数据，从而导致数据错乱的问题。**\n\n假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？\n\n![TIME-WAIT 时间过短，收到旧连接的数据报文](https://img-blog.csdnimg.cn/img_convert/6385cc99500b01ba2ef288c27523c1e7.png)\n\n\n- 如上图：\n\n  - 服务端在关闭连接之前发送的 `SEQ = 301` 报文，被网络延迟了。\n  - 接着，服务端以相同的四元组重新打开了新连接，前面被延迟的 `SEQ = 301` 这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。\n\n为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 `2MSL` 时长，这个时间**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。**\n\n*原因二：保证「被动关闭连接」的一方，能被正确的关闭*\n\n在 RFC 793 指出 TIME-WAIT 另一个重要的作用是：\n\n*TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.*\n\n也就是说，TIME-WAIT 作用是**等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。**\n\n如果客户端（主动关闭方）最后一次 ACK 报文（第四次挥手）在网络中丢失了，那么按照 TCP 可靠性原则，服务端（被动关闭方）会重发 FIN 报文。\n\n假设客户端没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSED 状态，如果该  ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文。\n\n![TIME-WAIT 时间过短，没有确保连接正常关闭](https://img-blog.csdnimg.cn/img_convert/3a81c23ce57c27cf63fc2b77e34de0ab.png)\n\n服务端收到这个 RST 并将其解释为一个错误（Connection reset by peer），这对于一个可靠的协议来说不是一个优雅的终止方式。\n\n为了防止这种情况出现，客户端必须等待足够长的时间，确保服务端能够收到 ACK，如果服务端没有收到 ACK，那么就会触发 TCP 重传机制，服务端会重新发送一个 FIN，这样一去一来刚好两个 MSL 的时间。\n\n![TIME-WAIT 时间正常，确保了连接正常关闭](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/TIME-WAIT连接正常关闭.drawio.png)\n\n客户端在收到服务端重传的 FIN 报文时，TIME_WAIT 状态的等待时间，会重置回 2MSL。\n\n我们再回过头来看看，为什么 TIME_WAIT 状态要保持 60 秒呢？\n\n这与孤儿连接 FIN_WAIT2 状态默认保留 60 秒的原理是一样的，**因为这两个状态都需要保持 2MSL 时长。MSL 全称是 Maximum Segment Lifetime，它定义了一个报文在网络中的最长生存时间**（报文每经过一次路由器的转发，IP 头部的 TTL 字段就会减 1，减到 0 时报文就被丢弃，这就限制了报文的最长存活时间）。\n\n为什么是 2 MSL 的时长呢？这其实是相当于**至少允许报文丢失一次**。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。\n\n为什么不是 4 或者 8 MSL 的时长呢？你可以想象一个丢包率达到百分之一的糟糕网络，连续两次丢包的概率只有万分之一，这个概率实在是太小了，忽略它比解决它更具性价比。\n\n**因此，TIME_WAIT 和 FIN_WAIT2 状态的最大时长都是 2 MSL，由于在 Linux 系统中，MSL 的值固定为 30 秒，所以它们都是 60 秒。**\n\n\u003e TIME_WAIT 状态优化方式一\n\n**Linux 提供了 tcp_max_tw_buckets 参数，当 TIME_WAIT 的连接数量超过该参数时，新关闭的连接就不再经历 TIME_WAIT 而直接关闭：**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/33.jpg)\n\n当服务器的并发连接增多时，相应地，同时处于 TIME_WAIT 状态的连接数量也会变多，此时就应当调大 `tcp_max_tw_buckets` 参数，减少不同连接间数据错乱的概率。tcp_max_tw_buckets 也不是越大越好，毕竟系统资源是有限的。\n\n\u003e TIME_WAIT 状态优化方式二\n\n**有一种方式可以在建立新连接时，复用处于 TIME_WAIT 状态的连接，那就是打开 tcp_tw_reuse 参数。但是需要注意，该参数是只用于客户端（建立连接的发起方），因为是在调用 connect() 时起作用的，而对于服务端（被动连接方）是没有用的。**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/34.jpg)\n\n网上很多博客都说在服务端开启 tcp_tw_reuse 参数来优化 TCP，我信你个鬼，糟老头坏的很！**tcp_tw_reuse 只作用在 connect 函数，也就是客户端，跟服务端一毛关系的没有**。\n\ntcp_tw_reuse 从协议角度理解是安全可控的，可以复用处于 TIME_WAIT 的端口为新的连接所用。\n\n什么是协议角度理解的安全可控呢？主要有两点：\n\n- 只适用于连接发起方，也就是 C/S 模型中的客户端；\n- 对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。\n\n使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持（对方也要打开 ）：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/35.jpg)\n\n由于引入了时间戳，它能带来了些好处：\n\n- 我们在前面提到的 2MSL（TIME_WAIT状态的持续时间） 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃；\n- 同时，它还可以防止序列号绕回，也是因为重复的数据包会由于时间戳过期被自然丢弃；\n\n时间戳是在 TCP 的选项字段里定义的，开启了时间戳功能，在 TCP 报文传输的时候会带上发送报文的时间戳。\n\n![TCP option 字段 - 时间戳](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%82%E6%95%B0/TCP%20option%E5%AD%97%E6%AE%B5-%E6%97%B6%E9%97%B4%E6%88%B3.png)\n\n另外，老版本的 Linux 还提供了 `tcp_tw_recycle` 参数，但是当开启了它，允许处于 TIME_WAIT 状态的连接被快速回收，但是有个**大坑**。\n\n开启了 recycle 和 timestamps 选项，就会开启一种叫 per-host 的 PAWS（判断TCP 报文中时间戳是否是历史报文） 机制，**per-host 是对「对端 IP 做 PAWS 检查」**，而非对「IP + 端口」四元组做 PAWS 检查。\n\n如果客户端网络环境是用了 NAT 网关，那么客户端环境的每一台机器通过 NAT 网关后，都会是相同的 IP 地址，在服务端看来，就好像只是在跟一个客户端打交道一样，无法区分出来。\n\nPer-host PAWS 机制利用 TCP option 里的 timestamp 字段的增长来判断串扰数据，而 timestamp 是根据客户端各自的 CPU tick 得出的值。\n\n当客户端 A 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME-WAIT 状态的连接后，**客户端 B 也通过 NAT 网关和服务器建立 TCP 连接，注意客户端 A 和 客户端 B 因为经过相同的 NAT 网关，所以是用相同的 IP 地址与服务端建立 TCP 连接，如果客户端 B 的 timestamp 比 客户端 A 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 B 发来的 SYN 包**。\n\n因此，tcp_tw_recycle 在使用了 NAT 的网络下是存在问题的，如果它是对 TCP 四元组做 PAWS 检查，而不是对「相同的 IP 做 PAWS 检查」，那么就不会存在这个问题了。\n\n网上很多博客都说开启 tcp_tw_recycle 参数来优化 TCP，我信你个鬼，糟老头坏的很！\n\n所以，不建议设置为 1 ，在 Linux 4.12 版本后，Linux 内核直接取消了这一参数，建议关闭它：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/36.jpg)\n\n\u003e TIME_WAIT 状态优化方式三\n\n我们可以在程序中设置 socket 选项，来设置调用 close 关闭连接行为。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/37.jpg)\n\n如果 `l_onoff` 为非 0， 且 `l_linger` 值为 0，**那么调用 close 后，会立该发送一个 RST 标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了 TIME_WAIT 状态，直接关闭。**\n\n这种方式只推荐在客户端使用，服务端千万不要使用。因为服务端一调用 close，就发送 RST 报文的话，客户端就总是看到 TCP 连接错误 “connnection reset by peer”。\n\n### 被动方的优化\n\n当被动方收到 FIN 报文时，内核会自动回复 ACK，同时连接处于 CLOSE_WAIT 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。\n\n内核没有权利替代进程去关闭连接，因为如果主动方是通过 shutdown 关闭连接，那么它就是想在半关闭连接上接收数据或发送数据。因此，Linux 并没有限制 CLOSE_WAIT 状态的持续时间。\n\n当然，大多数应用程序并不使用 shutdown 函数关闭连接。所以，**当你用 netstat 命令发现大量 CLOSE_WAIT 状态。就需要排查你的应用程序，因为可能因为应用程序出现了 Bug，read 函数返回 0 时，没有调用 close 函数。**\n\n处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文关闭发送通道，同时连接进入 LAST_ACK 状态，等待主动方返回 ACK 来确认连接关闭。\n\n如果迟迟收不到这个 ACK，内核就会重发 FIN 报文，重发次数仍然由 tcp_orphan_retries 参数控制，这与主动方重发 FIN 报文的优化策略一致。\n\n还有一点我们需要注意的，**如果被动方迅速调用 close 函数，那么被动方的 ACK 和 FIN 有可能在一个报文中发送，这样看起来，四次挥手会变成三次挥手，这只是一种特殊情况，不用在意。**\n\n\u003e 如果连接双方同时关闭连接，会怎么样？\n\n由于 TCP 是双全工的协议，所以是会出现两方同时关闭连接的现象，也就是同时发送了 FIN 报文。\n\n此时，上面介绍的优化策略仍然适用。两方发送 FIN 报文时，都认为自己是主动方，所以都进入了 FIN_WAIT1 状态，FIN 报文的重发次数仍由 tcp_orphan_retries 参数控制。\n\n![同时关闭](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/38.jpg)\n\n接下来，**双方在等待 ACK 报文的过程中，都等来了 FIN 报文。这是一种新情况，所以连接会进入一种叫做 CLOSING 的新状态，它替代了 FIN_WAIT2 状态**。接着，双方内核回复 ACK 确认对方发送通道的关闭后，进入 TIME_WAIT 状态，等待 2MSL 的时间后，连接自动关闭。\n\n### 小结\n\n针对 TCP 四次挥手的优化，我们需要根据主动方和被动方四次挥手状态变化来调整系统 TCP 内核参数。\n\n![四次挥手的优化策略](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/39.jpg)\n\n\u003e 主动方的优化\n\n主动发起 FIN 报文断开连接的一方，如果迟迟没收到对方的 ACK 回复，则会重传 FIN 报文，重传的次数由 `tcp_orphan_retries` 参数决定。\n\n当主动方收到 ACK 报文后，连接就进入 FIN_WAIT2 状态，根据关闭的方式不同，优化的方式也不同：\n\n- 如果这是 close 函数关闭的连接，那么它就是孤儿连接。如果 `tcp_fin_timeout` 秒内没有收到对方的 FIN 报文，连接就直接关闭。同时，为了应对孤儿连接占用太多的资源，`tcp_max_orphans` 定义了最大孤儿连接的数量，超过时连接就会直接释放。\n- 反之是 shutdown 函数关闭的连接，则不受此参数限制；\n\n当主动方接收到 FIN 报文，并返回 ACK 后，主动方的连接进入 TIME_WAIT 状态。这一状态会持续 1 分钟，为了防止 TIME_WAIT 状态占用太多的资源，`tcp_max_tw_buckets` 定义了最大数量，超过时连接也会直接释放。\n\n当 TIME_WAIT 状态过多时，还可以通过设置 `tcp_tw_reuse` 和 `tcp_timestamps` 为 1 ，将 TIME_WAIT 状态的端口复用于作为客户端的新连接，注意该参数只适用于客户端。\n\n\u003e 被动方的优化\n\n被动关闭的连接方应对非常简单，它在回复 ACK 后就进入了 CLOSE_WAIT 状态，等待进程调用 close 函数关闭连接。因此，出现大量 CLOSE_WAIT 状态的连接时，应当从应用程序中找问题。\n\n当被动方发送 FIN 报文后，连接就进入 LAST_ACK 状态，在未等到 ACK 时，会在 `tcp_orphan_retries` 参数的控制下重发 FIN 报文。\n\n---\n\n## TCP 传输数据的性能提升\n\n在前面介绍的是三次握手和四次挥手的优化策略，接下来主要介绍的是 TCP 传输数据时的优化策略。\n\nTCP 连接是由内核维护的，内核会为每个连接建立内存缓冲区：\n\n- 如果连接的内存配置过小，就无法充分使用网络带宽，TCP 传输效率就会降低；\n- 如果连接的内存配置过大，很容易把服务器资源耗尽，这样就会导致新连接无法建立；\n\n因此，我们必须理解 Linux 下 TCP 内存的用途，才能正确地配置内存大小。\n\n### 滑动窗口是如何影响传输速度的？\n\nTCP 会保证每一个报文都能够抵达对方，它的机制是这样：报文发出去后，必须接收到对方返回的确认报文 ACK，如果迟迟未收到，就会超时重发该报文，直到收到对方的 ACK 为止。\n\n**所以，TCP 报文发出去后，并不会立马从内存中删除，因为重传时还需要用到它。**\n\n由于 TCP 是内核维护的，所以报文存放在内核缓冲区。如果连接非常多，我们可以通过 free 命令观察到 `buff/cache` 内存是会增大。\n\n\n如果 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。这个模式就有点像我和你面对面聊天，你一句我一句，但这种方式的缺点是效率比较低的。\n\n![按数据包进行确认应答](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/40.jpg)\n\n所以，这样的传输方式有一个缺点：数据包的**往返时间越长，通信的效率就越低**。\n\n**要解决这一问题不难，并行批量发送报文，再批量确认报文即可。**\n\n![并行处理](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/41.jpg)\n\n然而，这引出了另一个问题，发送方可以随心所欲的发送报文吗？**当然这不现实，我们还得考虑接收方的处理能力。**\n\n当接收方硬件不如发送方，或者系统繁忙、资源紧张时，是无法瞬间处理这么多报文的。于是，这些报文只能被丢掉，使得网络效率非常低。\n\n**为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是滑动窗口的由来。**\n\n接收方根据它的缓冲区，可以计算出后续能够接收多少字节的报文，这个数字叫做接收窗口。当内核接收到报文时，必须用缓冲区存放它们，这样剩余缓冲区空间变小，接收窗口也就变小了；当进程调用 read 函数后，数据被读入了用户空间，内核缓冲区就被清空，这意味着主机可以接收更多的报文，接收窗口就会变大。\n\n因此，接收窗口并不是恒定不变的，接收方会把当前可接收的大小放在 TCP 报文头部中的**窗口字段**，这样就可以起到窗口大小通知的作用。\n\n发送方的窗口等价于接收方的窗口吗？如果不考虑拥塞控制，发送方的窗口大小「约等于」接收方的窗口大小，因为窗口通知报文在网络传输是存在时延的，所以是约等于的关系。\n\n![TCP 头部](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/42.jpg)\n\n从上图中可以看到，窗口字段只有 2 个字节，因此它最多能表达 65535 字节大小的窗口，也就是 64KB 大小。\n\n这个窗口大小最大值，在当今高速网络下，很明显是不够用的。所以后续有了扩充窗口的方法：**在 TCP 选项字段定义了窗口扩大因子，用于扩大 TCP 通告窗口，其值大小是 2^14，这样就使 TCP 的窗口大小从 16 位扩大为 30 位（2^16 * 2^ 14 = 2^30），所以此时窗口的最大值可以达到 1GB。**\n\n![TCP option 选项 - 窗口扩展](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%82%E6%95%B0/TCP%20option%E5%AD%97%E6%AE%B5-%E7%AA%97%E5%8F%A3.png)\n\nLinux 中打开这一功能，需要把 tcp_window_scaling 配置设为 1（默认打开）：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/43.jpg)\n\n要使用窗口扩大选项，通讯双方必须在各自的 SYN 报文中发送这个选项：\n\n- 主动建立连接的一方在 SYN 报文中发送这个选项；\n- 而被动建立连接的一方只有在收到带窗口扩大选项的 SYN 报文之后才能发送这个选项。\n\n\n这样看来，只要进程能及时地调用 read 函数读取数据，并且接收缓冲区配置得足够大，那么接收窗口就可以无限地放大，发送方也就无限地提升发送速度。\n\n**这是不可能的，因为网络的传输能力是有限的，当发送方依据发送窗口，发送超过网络处理能力的报文时，路由器会直接丢弃这些报文。因此，缓冲区的内存并不是越大越好。**\n\n\n### 如何确定最大传输速度？\n\n在前面我们知道了 TCP 的传输速度，受制于发送窗口与接收窗口，以及网络设备传输能力。其中，窗口大小由内核缓冲区大小决定。如果缓冲区与网络传输能力匹配，那么缓冲区的利用率就达到了最大化。\n\n问题来了，如何计算网络的传输能力呢？\n\n相信大家都知道网络是有「带宽」限制的，带宽描述的是网络传输能力，它与内核缓冲区的计量单位不同:\n\n- 带宽是单位时间内的流量，表达是「速度」，比如常见的带宽 100 MB/s；\n- 缓冲区单位是字节，当网络速度乘以时间才能得到字节数；\n\n这里需要说一个概念，就是带宽时延积，它决定网络中飞行报文的大小，它的计算方式：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/44.jpg)\n\n比如最大带宽是 100 MB/s，网络时延（RTT）是 10ms 时，意味着客户端到服务端的网络一共可以存放 100MB/s * 0.01s = 1MB 的字节。\n\n这个 1MB 是带宽和时延的乘积，所以它就叫「带宽时延积」（缩写为 BDP，Bandwidth Delay Product）。同时，这 1MB 也表示「飞行中」的 TCP 报文大小，它们就在网络线路、路由器等网络设备上。如果飞行报文超过了 1 MB，就会导致网络过载，容易丢包。\n\n**由于发送缓冲区大小决定了发送窗口的上限，而发送窗口又决定了「已发送未确认」的飞行报文的上限。因此，发送缓冲区不能超过「带宽时延积」。**\n\n发送缓冲区与带宽时延积的关系：\n\n- 如果发送缓冲区「超过」带宽时延积，超出的部分就没办法有效的网络传输，同时导致网络过载，容易丢包；\n- 如果发送缓冲区「小于」带宽时延积，就不能很好的发挥出网络的传输效率。\n\n所以，发送缓冲区的大小最好是往带宽时延积靠近。\n\n\n### 怎样调整缓冲区大小？\n\n在 Linux 中发送缓冲区和接收缓冲都是可以用参数调节的。设置完后，Linux 会根据你设置的缓冲区进行**动态调节**。\n\n\u003e 调节发送缓冲区范围\n\n先来看看发送缓冲区，它的范围通过 tcp_wmem 参数配置；\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/45.jpg)\n\n上面三个数字单位都是字节，它们分别表示：\n\n- 第一个数值是动态范围的最小值，4096 byte = 4K；\n- 第二个数值是初始默认值，16384 byte ≈ 16K；\n- 第三个数值是动态范围的最大值，4194304 byte = 4096K（4M）；\n\n**发送缓冲区是自行调节的**，当发送方发送的数据被确认后，并且没有新的数据要发送，就会把发送缓冲区的内存释放掉。\n\n\u003e 调节接收缓冲区范围\n\n而接收缓冲区的调整就比较复杂一些，先来看看设置接收缓冲区范围的 tcp_rmem 参数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/46.jpg)\n\n上面三个数字单位都是字节，它们分别表示：\n\n- 第一个数值是动态范围的最小值，表示即使在内存压力下也可以保证的最小接收缓冲区大小，4096 byte = 4K；\n- 第二个数值是初始默认值，87380 byte ≈ 86K；\n- 第三个数值是动态范围的最大值，6291456 byte = 6144K（6M）；\n\n**接收缓冲区可以根据系统空闲内存的大小来调节接收窗口：**\n\n- 如果系统的空闲内存很多，就可以自动把缓冲区增大一些，这样传给对方的接收窗口也会变大，因而提升发送方发送的传输数据数量；\n- 反之，如果系统的内存很紧张，就会减少缓冲区，这虽然会降低传输效率，可以保证更多的并发连接正常工作；\n\n发送缓冲区的调节功能是自动开启的，**而接收缓冲区则需要配置 tcp_moderate_rcvbuf 为 1 来开启调节功能**：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/47.jpg)\n\n\u003e 调节 TCP 内存范围\n\n接收缓冲区调节时，怎么知道当前内存是否紧张或充分呢？这是通过 tcp_mem 配置完成的：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/48.jpg)\n\n上面三个数字单位不是字节，而是「页面大小」，1 页表示 4KB，它们分别表示：\n\n- 当 TCP 内存小于第 1 个值时，不需要进行自动调节；\n- 在第 1 和第 2 个值之间时，内核开始调节接收缓冲区的大小；\n- 大于第 3 个值时，内核不再为 TCP 分配新内存，此时新连接是无法建立的；\n\n一般情况下这些值是在系统启动时根据系统内存数量计算得到的。根据当前 tcp_mem 最大内存页面数是 177120，当内存为 (177120 * 4) / 1024K ≈ 692M 时，系统将无法为新的 TCP 连接分配内存，即 TCP 连接将被拒绝。\n\n\u003e 根据实际场景调节的策略\n\n在高并发服务器中，为了兼顾网速与大量的并发连接，**我们应当保证缓冲区的动态调整的最大值达到带宽时延积，而最小值保持默认的 4K 不变即可。而对于内存紧张的服务而言，调低默认值是提高并发的有效手段。**\n\n同时，如果这是网络 IO 型服务器，那么，**调大 tcp_mem 的上限可以让 TCP 连接使用更多的系统内存，这有利于提升并发能力**。需要注意的是，tcp_wmem 和 tcp_rmem 的单位是字节，而 tcp_mem 的单位是页面大小。而且，**千万不要在 socket 上直接设置 SO_SNDBUF 或者 SO_RCVBUF，这样会关闭缓冲区的动态调整功能。**\n\n### 小结\n\n本节针对 TCP 优化数据传输的方式，做了一些介绍。\n\n![数据传输的优化策略](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-参数/49.jpg)\n\nTCP 可靠性是通过 ACK 确认报文实现的，又依赖滑动窗口提升了发送速度也兼顾了接收方的处理能力。\n\n可是，默认的滑动窗口最大值只有 64 KB，不满足当今的高速网络的要求，要想提升发送速度必须提升滑动窗口的上限，在 Linux 下是通过设置 `tcp_window_scaling` 为 1 做到的，此时最大值可高达 1GB。\n\n滑动窗口定义了网络中飞行报文的最大字节数，当它超过带宽时延积时，网络过载，就会发生丢包。而当它小于带宽时延积时，就无法充分利用网络带宽。因此，滑动窗口的设置，必须参考带宽时延积。\n\n内核缓冲区决定了滑动窗口的上限，缓冲区可分为：发送缓冲区 tcp_wmem 和接收缓冲区 tcp_rmem。\n\nLinux 会对缓冲区动态调节，我们应该把缓冲区的上限设置为带宽时延积。发送缓冲区的调节功能是自动打开的，而接收缓冲区需要把 tcp_moderate_rcvbuf 设置为 1 来开启。其中，调节的依据是 TCP 内存范围 tcp_mem。\n\n但需要注意的是，如果程序中的 socket 设置 SO_SNDBUF 和 SO_RCVBUF，则会关闭缓冲区的动态整功能，所以不建议在程序设置它俩，而是交给内核自动调整比较好。\n\n有效配置这些参数后，既能够最大程度地保持并发性，也能让资源充裕时连接传输速度达到最大值。\n\n---\n\n参考资料：\n\n\n[1] 系统性能调优必知必会.陶辉.极客时间.\n\n[2] 网络编程实战专栏.盛延敏.极客时间.\n\n[3] http://www.blogjava.net/yongboy/archive/2013/04/11/397677.html\n\n[4] http://blog.itpub.net/31559359/viewspace-2284113/\n\n[5] https://blog.51cto.com/professor/1909022\n\n[6] https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux\n\n---\n\n## 读者问答\n\n\u003e 读者问：“小林，请教个问题，somaxconn和backlog是不是都是指的是accept队列？然后somaxconn是内核参数，backlog是通过系统调用间隔地修改somaxconn，比如Linux中listen()函数？”\n\n两者取最小值才是 accpet 队列。\n\n\u003e 读者问：“小林，还有个问题要请教下，“如果 accept 队列满了，那么 server 扔掉 client  发过来的 ack”，也就是说该TCP连接还是位于半连接队列中，没有丢弃吗？”\n\n1. 当 accept 队列满了，后续新进来的syn包都会被丢失\n2. 我文章的突发流量例子是，那个连接进来的时候 accept 队列还没满，但是在第三次握手的时候，accept 队列突然满了，就会导致 ack 被丢弃，就一直处于半连接队列。\n\n----\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_problem":{"title":"tcp_problem","content":"# 4.16 TCP 协议有什么缺陷？\n\n大家好，我是小林。\n\n写的多了后，忽然思考一个问题，TCP 通过序列号、确认应答、超时重传、流量控制、拥塞控制等方式实现了可靠传输，看起来它很完美，事实真的是这样吗？TCP 就没什么缺陷吗？\n\n所以，今天就跟大家聊聊，TCP 协议有哪些缺陷？主要有四个方面：\n\n- 升级 TCP 的工作很困难；\n- TCP 建立连接的延迟；\n- TCP 存在队头阻塞问题；\n- 网络迁移需要重新建立 TCP 连接；\n\n接下来，针对这四个方面详细说一下。\n\n## 升级 TCP 的工作很困难\n\nTCP 协议是诞生在 1973 年，至今 TCP 协议依然还在实现更多的新特性。\n\n但是 TCP 协议是在内核中实现的，应用程序只能使用不能修改，如果要想升级 TCP 协议，那么只能升级内核。\n\n而升级内核这个工作是很麻烦的事情，麻烦的事情不是说升级内核这个操作很麻烦，而是由于内核升级涉及到底层软件和运行库的更新，我们的服务程序就需要回归测试是否兼容新的内核版本，所以服务器的内核升级也比较保守和缓慢。\n\n很多 TCP 协议的新特性，都是需要客户端和服务端同时支持才能生效的，比如  TCP Fast Open 这个特性，虽然在2013 年就被提出了，但是 Windows 很多系统版本依然不支持它，这是因为 PC 端的系统升级滞后很严重，W\tindows Xp 现在还有大量用户在使用，尽管它已经存在快 20 年。\n\n所以，即使 TCP 有比较好的特性更新，也很难快速推广，用户往往要几年或者十年才能体验到。\n\n## TCP 建立连接的延迟\n\n基于 TCP 实现的应用协议，都是需要先建立三次握手才能进行数据传输，比如 HTTP 1.0/1.1、HTTP/2、HTTPS。\n\n现在大多数网站都是使用 HTTPS 的，这意味着在 TCP 三次握手之后，还需要经过 TLS 四次握手后，才能进行 HTTP 数据的传输，这在一定程序上增加了数据传输的延迟。\n\nTCP 三次握手和 TLS 握手延迟，如图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/TCP%2BTLS.gif)\n\nTCP 三次握手的延迟被 TCP Fast Open （快速打开）这个特性解决了，这个特性可以在「第二次建立连接」时减少 TCP 连接建立的时延。\n\n![常规 HTTP 请求 与 Fast  Open HTTP 请求](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/45.jpg)\n\n过程如下：\n\n- 在第一次建立连接的时候，服务端在第二次握手产生一个 `Cookie` （已加密）并通过 SYN、ACK 包一起发给客户端，于是客户端就会缓存这个 `Cookie`，所以第一次发起 HTTP Get 请求的时候，还是需要 2 个 RTT 的时延；\n- 在下次请求的时候，客户端在 SYN 包带上 `Cookie` 发给服务端，就提前可以跳过三次握手的过程，因为 `Cookie` 中维护了一些信息，服务端可以从 `Cookie` 获取 TCP 相关的信息，这时发起的 HTTP GET 请求就只需要 1 个 RTT 的时延；\n\nTCP Fast Open 这个特性是不错，但是它需要服务端和客户端的操作系统同时支持才能体验到，而 TCP Fast Open 是在 2013 年提出的，所以市面上依然有很多老式的操作系统不支持，而升级操作系统是很麻烦的事情，因此  TCP Fast Open 很难被普及开来。\n\n还有一点，针对 HTTPS 来说，TLS 是在应用层实现的握手，而 TCP 是在内核实现的握手，这两个握手过程是无法结合在一起的，总是得先完成 TCP 握手，才能进行 TLS 握手。\n\n也正是 TCP 是在内核实现的，所以 TLS 是无法对 TCP 头部加密的，这意味着 TCP 的序列号都是明文传输，所以就存安全的问题。\n\n一个典型的例子就是攻击者伪造一个的 RST 报文强制关闭一条 TCP 连接，而攻击成功的关键则是 TCP 字段里的序列号位于接收方的滑动窗口内，该报文就是合法的。\n\n为此 TCP 也不得不进行三次握手来同步各自的序列号，而且初始化序列号时是采用随机的方式（不完全随机，而是随着时间流逝而线性增长，到了 2^32 尽头再回滚）来提升攻击者猜测序列号的难度，以增加安全性。\n\n但是这种方式只能避免攻击者预测出合法的 RST 报文，而无法避免攻击者截获客户端的报文，然后中途伪造出合法 RST 报文的攻击的方式。\n\n![](https://gw.alipayobjects.com/mdn/rms_1c90e8/afts/img/A*po6LQIBU7zIAAAAAAAAAAAAAARQnAQ)\n\n大胆想一下，如果 TCP 的序列号也能被加密，或许真的不需要三次握手了，客户端和服务端的初始序列号都从 0 开始，也就不用做同步序列号的工作了，但是要实现这个要改造整个协议栈，太过于麻烦，即使实现出来了，很多老的网络设备未必能兼容。\n\n## TCP 存在队头阻塞问题\n\nTCP 是字节流协议，**TCP 层必须保证收到的字节数据是完整且有序的**，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/http3/tcp%E9%98%9F%E5%A4%B4%E9%98%BB%E5%A1%9E.gif)\n\n图中发送方发送了很多个 packet，每个 packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 `packet #3` 在网络中丢失了，即使 `packet #4-6` 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 `packet #3` 重传后，接收方的应用层才可以从内核中读取到数据。\n\n这就是 TCP 队头阻塞问题，但这也不能怪 TCP ，因为只有这样做才能保证数据的有序性。\n\nHTTP/2 多个请求是跑在一个 TCP 连接中的，那么当 TCP 丢包时，整个 TCP 都要等待重传，那么就会阻塞该 TCP 连接中的所有请求，所以 HTTP/2 队头阻塞问题就是因为 TCP 协议导致的。\n\n![](https://pic2.zhimg.com/80/v2-2dd2a9fb8693489b9a0b24771c8a40a1_1440w.jpg)\n\n\n\n## 网络迁移需要重新建立 TCP 连接\n\n基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n\n![TCP 四元组](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEwLmpwZw?x-oss-process=image/format,png)\n\n那么**当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接**。\n\n而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n\n## 结尾\n\n我记得之前在群里看到，有位读者字节一面的时候被问到：「**如何基于 UDP 协议实现可靠传输？**」\n\n很多同学第一反应就会说把 TCP 可靠传输的特性（序列号、确认应答、超时重传、流量控制、拥塞控制）在应用层实现一遍。\n\n实现的思路确实这样没错，但是有没有想过，**既然 TCP 天然支持可靠传输，为什么还需要基于 UDP 实现可靠传输呢？这不是重复造轮子吗？**\n\n所以，我们要先弄清楚 TCP 协议有哪些痛点？而这些痛点是否可以在基于 UDP 协议实现的可靠传输协议中得到改进？\n\n现在市面上已经有基于 UDP 协议实现的可靠传输协议的成熟方案了，那就是 QUIC 协议，**QUIC 协议把我本文说的 TCP 的缺点都给解决了**，而且已经应用在了 HTTP/3。\n\n![](https://miro.medium.com/max/1400/1*uk5OZPL7gtUwqRLwaoGyFw.png)\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_queue":{"title":"tcp_queue","content":"# 4.4 TCP 半连接队列和全连接队列\n\n网上许多博客针对增大 TCP 半连接队列和全连接队列的方式如下：\n\n- 增大 TCP 半连接队列的方式是增大 /proc/sys/net/ipv4/tcp_max_syn_backlog；\n- 增大 TCP 全连接队列的方式是增大 listen() 函数中的 backlog；\n\n这里先跟大家说下，**上面的方式都是不准确的。**\n\n\u003e “你怎么知道不准确？”\n\n很简单呀，因为我做了实验和看了 TCP 协议栈的内核源码，发现要增大这两个队列长度，不是简简单单增大某一个参数就可以的。\n\n接下来，就会以**实战 + 源码分析，带大家解密 TCP 半连接队列和全连接队列。**\n\n\u003e “源码分析，那不是劝退吗？我们搞 Java 的看不懂呀”\n\n放心，本文的源码分析不会涉及很深的知识，因为都被我删减了，你只需要会条件判断语句 if、左移右移操作符、加减法等基本语法，就可以看懂。\n\n另外，不仅有源码分析，还会介绍 Linux 排查半连接队列和全连接队列的命令。\n\n\u003e “哦？似乎很有看头，那我姑且看一下吧！”\n\n行，没有被劝退的小伙伴，值得鼓励，下面这图是本文的提纲：\n\n![本文提纲](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/2.jpg)\n    \n\n---\n\n\n## 什么是 TCP 半连接队列和全连接队列？\n\n在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：\n\n- 半连接队列，也称 SYN 队列；\n- 全连接队列，也称 accept 队列；\n\n服务端收到客户端发起的 SYN 请求后，**内核会把该连接存储到半连接队列**，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，**内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列，等待进程调用 accept 函数时把连接取出来。**\n\n![半连接队列与全连接队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/3.jpg)\n\n\n不管是半连接队列还是全连接队列，都有最大长度限制，超过限制时，内核会直接丢弃，或返回 RST 包。\n\n---\n\n## 实战 - TCP 全连接队列溢出\n\n\u003e 如何知道应用程序的 TCP 全连接队列大小？\n\n在服务端可以使用 `ss` 命令，来查看 TCP 全连接队列的情况：\n\n但需要注意的是 `ss` 命令获取的 `Recv-Q/Send-Q` 在「LISTEN 状态」和「非 LISTEN 状态」所表达的含义是不同的。从下面的内核代码可以看出区别：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/4.jpg)\n\n\n在「LISTEN 状态」时，`Recv-Q/Send-Q` 表示的含义如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/5.jpg)\n\n- Recv-Q：当前全连接队列的大小，也就是当前已完成三次握手并等待服务端 `accept()` 的 TCP 连接；\n- Send-Q：当前全连接最大队列长度，上面的输出结果说明监听 8088 端口的 TCP 服务，最大全连接长度为 128；\n\n\n在「非 LISTEN 状态」时，`Recv-Q/Send-Q` 表示的含义如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/6.jpg)\n\n- Recv-Q：已收到但未被应用进程读取的字节数；\n- Send-Q：已发送但未收到确认的字节数；\n\n\n\u003e 如何模拟 TCP 全连接队列溢出的场景？\n\n\n![测试环境](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/7.jpg)\n\n实验环境：\n\n- 客户端和服务端都是 CentOs 6.5 ，Linux 内核版本 2.6.32\n- 服务端 IP 192.168.3.200，客户端 IP 192.168.3.100 \n- 服务端是 Nginx 服务，端口为 8088\n\n这里先介绍下 `wrk` 工具，它是一款简单的 HTTP 压测工具，它能够在单机多核 CPU 的条件下，使用系统自带的高性能 I/O 机制，通过多线程和事件模式，对目标机器产生大量的负载。\n\n本次模拟实验就使用 `wrk` 工具来压力测试服务端，发起大量的请求，一起看看服务端 TCP 全连接队列满了会发生什么？有什么观察指标？\n\n客户端执行 `wrk` 命令对服务端发起压力测试，并发 3 万个连接：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/8.jpg)\n\n\n在服务端可以使用 `ss` 命令，来查看当前 TCP 全连接队列的情况：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/9.jpg)\n\n其间共执行了两次 ss 命令，从上面的输出结果，可以发现当前 TCP 全连接队列上升到了 129 大小，超过了最大 TCP 全连接队列。\n\n**当超过了 TCP 最大全连接队列，服务端则会丢掉后续进来的 TCP 连接**，丢掉的 TCP 连接的个数会被统计起来，我们可以使用 netstat -s 命令来查看：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/10.jpg)\n\n上面看到的 41150 times ，表示全连接队列溢出的次数，注意这个是累计值。可以隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了。\n\n从上面的模拟结果，可以得知，**当服务端并发处理大量请求时，如果 TCP 全连接队列过小，就容易溢出。发生 TCP 全连接队溢出的时候，后续的请求就会被丢弃，这样就会出现服务端请求数量上不去的现象。**\n\n![全连接队列溢出](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/11.jpg)\n\n\u003e Linux 有个参数可以指定当 TCP 全连接队列满了会使用什么策略来回应客户端。\n\n实际上，丢弃连接只是 Linux 的默认行为，我们还可以选择向客户端发送 RST 复位报文，告诉客户端连接已经建立失败。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/12.jpg)\n\ntcp_abort_on_overflow 共有两个值分别是 0 和 1，其分别表示：\n\n- 0 ：如果全连接队列满了，那么 server 扔掉 client  发过来的 ack ；\n- 1 ：如果全连接队列满了，server 发送一个 `reset` 包给 client，表示废掉这个握手过程和这个连接；\n\n如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把 tcp_abort_on_overflow 设置为 1，这时如果在客户端异常中可以看到很多 `connection reset by peer` 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。\n\n通常情况下，应当把 tcp_abort_on_overflow 设置为 0，因为这样更有利于应对突发流量。\n\n举个例子，当 TCP 全连接队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED，进程就在建立好的连接上发送请求。只要服务器没有为请求回复 ACK，请求就会被多次**重发**。如果服务器上的进程只是**短暂的繁忙造成 accept 队列满，那么当 TCP 全连接队列有空位时，再次接收到的请求报文由于含有 ACK，仍然会触发服务器端成功建立连接。**\n\n所以，tcp_abort_on_overflow 设为 0 可以提高连接建立的成功率，只有你非常肯定 TCP 全连接队列会长期溢出时，才能设置为 1 以尽快通知客户端。\n\n\n\u003e 如何增大 TCP 全连接队列呢？\n\n是的，当发现 TCP 全连接队列发生溢出的时候，我们就需要增大该队列的大小，以便可以应对客户端大量的请求。\n\n**TCP 全连接队列的最大值取决于 somaxconn 和 backlog 之间的最小值，也就是 min(somaxconn, backlog)**。从下面的 Linux 内核代码可以得知：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/13.jpg)\n\n\n- `somaxconn` 是 Linux 内核的参数，默认值是 128，可以通过 ` /proc/sys/net/core/somaxconn` 来设置其值；\n- `backlog` 是 `listen(int sockfd, int backlog)` 函数中的 backlog 大小，Nginx 默认值是 511，可以通过修改配置文件设置其长度；\n\n前面模拟测试中，我的测试环境：\n\n- somaxconn 是默认值 128；\n- Nginx 的 backlog 是默认值 511\n\n所以测试环境的 TCP 全连接队列最大值为 min(128, 511)，也就是 `128`，可以执行 `ss` 命令查看：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/14.jpg)\n\n\n现在我们重新压测，把 TCP 全连接队列**搞大**，把 `somaxconn` 设置成 5000：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/15.jpg)\n\n接着把 Nginx 的 backlog 也同样设置成 5000：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/16.jpg)\n\n最后要重启 Nginx 服务，因为只有重新调用 `listen()` 函数 TCP 全连接队列才会重新初始化。\n\n重启完后 Nginx 服务后，服务端执行 ss 命令，查看 TCP 全连接队列大小：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/17.jpg)\n\n从执行结果，可以发现 TCP 全连接最大值为 5000。\n\n\u003e 增大 TCP 全连接队列后，继续压测\n\n客户端同样以 3 万个连接并发发送请求给服务端：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/18.jpg)\n\n服务端执行 `ss` 命令，查看 TCP 全连接队列使用情况：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/19.jpg)\n\n从上面的执行结果，可以发现全连接队列使用增长的很快，但是一直都没有超过最大值，所以就不会溢出，那么 `netstat -s` 就不会有 TCP 全连接队列溢出个数的显示：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/20.jpg)\n\n说明 TCP 全连接队列最大值从 128 增大到 5000 后，服务端抗住了 3 万连接并发请求，也没有发生全连接队列溢出的现象了。\n\n**如果持续不断地有连接因为 TCP 全连接队列溢出被丢弃，就应该调大 backlog 以及 somaxconn 参数。**\n\n---\n\n## 实战 - TCP 半连接队列溢出\n\n\u003e 如何查看 TCP 半连接队列长度？\n\n很遗憾，TCP 半连接队列长度的长度，没有像全连接队列那样可以用 ss 命令查看。\n\n但是我们可以抓住 TCP 半连接的特点，就是服务端处于 `SYN_RECV` 状态的 TCP 连接，就是 TCP 半连接队列。\n\n于是，我们可以使用如下命令计算当前 TCP 半连接队列长度：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/21.jpg)\n\n\u003e 如何模拟 TCP 半连接队列溢出场景？\n\n模拟 TCP 半连接溢出场景不难，实际上就是对服务端一直发送 TCP SYN 包，但是不回第三次握手 ACK，这样就会使得服务端有大量的处于 `SYN_RECV` 状态的 TCP 连接。\n\n这其实也就是所谓的 SYN 洪泛、SYN 攻击、DDos 攻击。\n\n![测试环境](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/22.jpg)\n\n实验环境：\n\n- 客户端和服务端都是 CentOs 6.5 ，Linux 内核版本 2.6.32\n- 服务端 IP 192.168.3.200，客户端 IP 192.168.3.100 \n- 服务端是 Nginx 服务，端口为 8088\n\n注意：本次模拟实验是没有开启 tcp_syncookies，关于 tcp_syncookies 的作用，后续会说明。\n\n本次实验使用 `hping3` 工具模拟 SYN 攻击：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/23.jpg)\n\n当服务端受到 SYN 攻击后，连接服务端 ssh 就会断开了，无法再连上。只能在服务端主机上执行查看当前 TCP 半连接队列大小：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/24.jpg)\n\n同时，还可以通过 netstat -s 观察半连接队列溢出的情况：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/25.jpg)\n\n上面输出的数值是**累计值**，表示共有多少个 TCP 连接因为半连接队列溢出而被丢弃。**隔几秒执行几次，如果有上升的趋势，说明当前存在半连接队列溢出的现象**。\n\n\u003e 大部分人都说 tcp_max_syn_backlog 是指定半连接队列的大小，是真的吗？\n\n很遗憾，半连接队列的大小并不单单只跟 `tcp_max_syn_backlog` 有关系。\n\n上面模拟 SYN 攻击场景时，服务端的 tcp_max_syn_backlog 的默认值如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/26.jpg)\n\n但是在测试的时候发现，服务端最多只有 256 个半连接队列，而不是 512，所以**半连接队列的最大长度不一定由 tcp_max_syn_backlog 值决定的**。\n\n\u003e 接下来，走进 Linux 内核的源码，来分析 TCP 半连接队列的最大值是如何决定的。\n\nTCP 第一次握手（收到 SYN 包）的 Linux 内核代码如下，其中缩减了大量的代码，只需要重点关注 TCP 半连接队列溢出的处理逻辑：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/27.jpg)\n\n从源码中，我可以得出共有三个条件因队列长度的关系而被丢弃的：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/28.jpg)\n\n1. **如果半连接队列满了，并且没有开启 tcp_syncookies，则会丢弃；**\n2. **若全连接队列满了，且没有重传 SYN+ACK 包的连接请求多于 1 个，则会丢弃；**\n3. **如果没有开启 tcp_syncookies，并且 max_syn_backlog 减去 当前半连接队列长度小于 (max_syn_backlog \u003e\u003e 2)，则会丢弃；**\n\n关于 tcp_syncookies 的设置，后面在详细说明，可以先给大家说一下，开启 tcp_syncookies 是缓解 SYN 攻击其中一个手段。\n\n接下来，我们继续跟一下检测半连接队列是否满的函数 inet_csk_reqsk_queue_is_full 和 检测全连接队列是否满的函数 sk_acceptq_is_full ：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/29.jpg)\n\n从上面源码，可以得知：\n\n- **全**连接队列的最大值是 `sk_max_ack_backlog` 变量，sk_max_ack_backlog 实际上是在 listen() 源码里指定的，也就是 **min(somaxconn, backlog)**；\n- **半**连接队列的最大值是 `max_qlen_log` 变量，max_qlen_log 是在哪指定的呢？现在暂时还不知道，我们继续跟进；\n\n我们继续跟进代码，看一下是哪里初始化了半连接队列的最大值 max_qlen_log：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/30.jpg)\n\n从上面的代码中，我们可以算出 max_qlen_log 是 8，于是代入到 检测半连接队列是否满的函数 reqsk_queue_is_full ：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/31.jpg)\n\n也就是 `qlen \u003e\u003e 8` 什么时候为 1 就代表半连接队列满了。这计算这不难，很明显是当 qlen 为 256 时，`256 \u003e\u003e 8 = 1`。\n\n至此，总算知道为什么上面模拟测试 SYN 攻击的时候，服务端处于 `SYN_RECV` 连接最大只有 256 个。\n\n可见，**半连接队列最大值不是单单由 max_syn_backlog 决定，还跟 somaxconn 和 backlog 有关系。**\n\n在 Linux 2.6.32 内核版本，它们之间的关系，总体可以概况为：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/32.jpg)\n\n- 当 max_syn_backlog \u003e min(somaxconn, backlog) 时， 半连接队列最大值 max_qlen_log = min(somaxconn, backlog) * 2;\n- 当 max_syn_backlog \u003c min(somaxconn, backlog) 时， 半连接队列最大值 max_qlen_log = max_syn_backlog * 2;\n\n\n\u003e 半连接队列最大值 max_qlen_log 就表示服务端处于 SYN_RECV 状态的最大个数吗？\n\n依然很遗憾，并不是。\n\nmax_qlen_log 是**理论**半连接队列最大值，并不一定代表服务端处于 SYN_RECV 状态的最大个数。\n\n在前面我们在分析 TCP 第一次握手（收到 SYN 包）时会被丢弃的三种条件：\n\n1. 如果半连接队列满了，并且没有开启 tcp_syncookies，则会丢弃；\n2. 若全连接队列满了，且没有重传 SYN+ACK 包的连接请求多于 1 个，则会丢弃；\n3. **如果没有开启 tcp_syncookies，并且 max_syn_backlog 减去 当前半连接队列长度小于 (max_syn_backlog \u003e\u003e 2)，则会丢弃；**\n\n假设条件 1 当前半连接队列的长度 「没有超过」理论的半连接队列最大值  max_qlen_log，那么如果条件 3 成立，则依然会丢弃 SYN 包，也就会使得服务端处于 SYN_RECV 状态的最大个数不会是理论值 max_qlen_log。\n\n似乎很难理解，我们继续接着做实验，实验见真知。\n\n服务端环境如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/33.jpg)\n\n配置完后，服务端要重启 Nginx，因为全连接队列最大值和半连接队列最大值是在 listen() 函数初始化。\n\n根据前面的源码分析，我们可以计算出半连接队列 max_qlen_log 的最大值为 256：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/34.jpg)\n\n客户端执行 hping3 发起 SYN 攻击：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/35.jpg)\n\n服务端执行如下命令，查看处于 SYN_RECV 状态的最大个数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/36.jpg)\n\n可以发现，服务端处于 SYN_RECV 状态的最大个数并不是 max_qlen_log 变量的值。\n\n这就是前面所说的原因：**如果当前半连接队列的长度 「没有超过」理论半连接队列最大值  max_qlen_log，那么如果条件 3 成立，则依然会丢弃 SYN 包，也就会使得服务端处于 SYN_RECV 状态的最大个数不会是理论值 max_qlen_log。**\n\n我们来分析一波条件 3 :\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/37.jpg)\n\n从上面的分析，可以得知如果触发「当前半连接队列长度 \u003e 192」条件，TCP 第一次握手的 SYN 包是会被丢弃的。\n\n在前面我们测试的结果，服务端处于 SYN_RECV 状态的最大个数是 193，正好是触发了条件 3，所以处于 SYN_RECV 状态的个数还没到「理论半连接队列最大值 256」，就已经把 SYN 包丢弃了。\n\n所以，服务端处于 SYN_RECV 状态的最大个数分为如下两种情况：\n\n- 如果「当前半连接队列」**没超过**「理论半连接队列最大值」，但是**超过** max_syn_backlog  - (max_syn_backlog \u003e\u003e 2)，那么处于 SYN_RECV 状态的最大个数就是 max_syn_backlog  - (max_syn_backlog \u003e\u003e 2)；\n- 如果「当前半连接队列」**超过**「理论半连接队列最大值」，那么处于 SYN_RECV 状态的最大个数就是「理论半连接队列最大值」；\n\n\n\u003e 每个 Linux 内核版本「理论」半连接最大值计算方式会不同。\n\n在上面我们是针对 Linux 2.6.32 版本分析的「理论」半连接最大值的算法，可能每个版本有些不同。\n\n比如在 Linux 5.0.0 的时候，「理论」半连接最大值就是全连接队列最大值，但依然还是有队列溢出的三个条件：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/38.jpg)\n\n\u003e 如果 SYN 半连接队列已满，只能丢弃连接吗？\n\n并不是这样，**开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接**，在前面我们源码分析也可以看到这点，当开启了  syncookies 功能就不会丢弃连接。\n\nsyncookies 是这么做的：服务器根据当前状态计算出一个值，放在己方发出的 SYN+ACK 报文中发出，当客户端返回 ACK 报文时，取出该值验证，如果合法，就认为连接建立成功，如下图所示。\n\n![开启 syncookies 功能](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/39.jpg)\n\nsyncookies 参数主要有以下三个值：\n\n- 0 值，表示关闭该功能；\n- 1 值，表示仅当 SYN 半连接队列放不下时，再启用它；\n- 2 值，表示无条件开启功能；\n\n那么在应对 SYN 攻击时，只需要设置为 1 即可：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/40.jpg)\n\n\u003e 如何防御 SYN 攻击？\n\n这里给出几种防御 SYN 攻击的方法：\n\n- 增大半连接队列；\n- 开启 tcp_syncookies 功能\n- 减少 SYN+ACK 重传次数\n\n*方式一：增大半连接队列*\n\n在前面源码和实验中，得知**要想增大半连接队列，我们得知不能只单纯增大 tcp_max_syn_backlog 的值，还需一同增大 somaxconn 和 backlog，也就是增大全连接队列**。否则，只单纯增大 tcp_max_syn_backlog 是无效的。\n\n增大 tcp_max_syn_backlog 和 somaxconn 的方法是修改 Linux 内核参数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/41.jpg)\n\n\n增大 backlog 的方式，每个 Web 服务都不同，比如 Nginx 增大 backlog 的方法如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/42.jpg)\n\n\n最后，改变了如上这些参数后，要重启 Nginx 服务，因为半连接队列和全连接队列都是在 listen() 初始化的。\n\n*方式二：开启 tcp_syncookies 功能*\n\n开启 tcp_syncookies 功能的方式也很简单，修改 Linux 内核参数：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/43.jpg)\n\n\n*方式三：减少 SYN+ACK 重传次数*\n\n当服务端受到 SYN 攻击时，就会有大量处于 SYN_RECV 状态的 TCP 连接，处于这个状态的 TCP 会重传 SYN+ACK ，当重传超过次数达到上限后，就会断开连接。\n\n那么针对 SYN 攻击的场景，我们可以减少 SYN+ACK 的重传次数，以加快处于 SYN_RECV 状态的 TCP 连接断开。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/44.jpg)\n\n\n---\n\n参考资料：\n\n[1] 系统性能调优必知必会.陶辉.极客时间.\n\n[2] https://www.cnblogs.com/zengkefu/p/5606696.html\n\n[3] https://blog.cloudflare.com/syn-packet-handling-in-the-wild/\n\n---\n\n## 读者问答\n\n\u003e 读者问：“咦 我比较好奇博主都是从哪里学到这些知识的呀？书籍？视频？还是多种参考资料”\n\n你可以看我的参考文献呀，知识点我主要是在极客专栏学的，实战模拟实验和源码解析是自己瞎折腾出来的。\n\n\u003e 读者问：“syncookies 启用后就不需要半链接了？那请求的数据会存在哪里？”\n\nsyncookies = 1 时，半连接队列满后，后续的请求就不会存放到半连接队列了，而是在第二次握手的时候，服务端会计算一个 cookie 值，放入到 SYN +ACK 包中的序列号发给客户端，客户端收到后并回 ack ，服务端就会校验连接是否合法，合法就直接把连接放入到全连接队列。\n\n----\n\n## 最后\n\n本文是以 Linux 2.6.32 版本的内核用实验 + 源码的方式，给大家说明了 TCP 半连接队列和全连接队列，我们可以看到 TCP 半连接队列「并不是」如网上说的那样 tcp_max_syn_backlog 表示半连接队列。\n\nTCP 半连接队列的大小对于不同的 Linux 内核版本会有不同的计算方式，所以并不要求大家要死记住本文计算 TCP 半连接队列的大小。\n\n重要的是要学会自我源码分析，这样不管碰到什么版本的 Linux 内核，都不再怕了。\n\n\n网上搜索出来的信息，并不一定针对你的系统，通过自我分析一波，你会更了解你当前使用的 Linux 内核版本！\n\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_stream":{"title":"tcp_stream","content":"# 4.6 如何理解是 TCP 面向字节流协议？\n\n有个读者问我，这么个问题：\n\n\u003e TCP 是面向字节流的协议，UDP 是面向报文的协议？这里的「面向字节流」和「面向报文」该如何理解。\n\n\n------\n\n## 如何理解字节流？\n\n之所以会说 TCP 是面向字节流的协议，UDP 是面向报文的协议，是因为操作系统对 TCP 和 UDP 协议的**发送方的机制不同**，也就是问题原因在发送方。\n\n\u003e 先来说说为什么 UDP 是面向报文的协议？\n\n当用户消息通过 UDP 协议传输时，**操作系统不会对消息进行拆分**，在组装好 UDP 头部后就交给网络层来处理，所以发出去的 UDP 报文中的数据部分就是完整的用户消息，也就是**每个 UDP 报文就是一个用户消息的边界**，这样接收方在接收到 UDP 报文后，读一个 UDP 报文就能读取到完整的用户消息。\n\n你可能会问，如果收到了两个 UDP 报文，操作系统是怎么区分开的？\n\n操作系统在收到 UDP 报文后，会将其插入到队列里，**队列里的每一个元素就是一个 UDP 报文**，这样当用户调用 recvfrom() 系统调用读数据的时候，就会从队列里取出一个数据，然后从内核里拷贝给用户缓冲区。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/a9116c5b375d356048df033dcb53582e.png)\n\n\n\n\u003e 再来说说为什么 TCP 是面向字节流的协议？\n\n当用户消息通过 TCP 协议传输时，**消息可能会被操作系统分组成多个的 TCP 报文**，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。\n\n这时，接收方的程序如果不知道发送方发送的消息的长度，也就是不知道消息的边界时，是无法读出一个有效的用户消息的，因为用户消息被拆分成多个 TCP 报文后，并不能像 UDP 那样，一个 UDP 报文就能代表一个完整的用户消息。\n\n举个实际的例子来说明。\n\n发送方准备发送 「Hi.」和「I am Xiaolin」这两个消息。\n\n在发送端，当我们调用 send 函数完成数据“发送”以后，数据并没有被真正从网络上发送出去，只是从应用程序拷贝到了操作系统内核协议栈中。\n\n至于什么时候真正被发送，**取决于发送窗口、拥塞窗口以及当前发送缓冲区的大小等条件**。也就是说，我们不能认为每次 send 调用发送的数据，都会作为一个整体完整地消息被发送出去。\n\n如果我们考虑实际网络传输过程中的各种影响，假设发送端陆续调用 send 函数先后发送 「Hi.」和「I am Xiaolin」 报文，那么实际的发送很有可能是这几种情况。\n\n第一种情况，这两个消息被分到同一个 TCP 报文，像这样：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/02dce678f870c8c70482b6e37dbb5574.png)\n\n第二种情况，「I am Xiaolin」的部分随 「Hi」 在一个 TCP 报文中发送出去，像这样：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f58b70cde860188b8f95a433e2f5293b.png)\n\n第三种情况，「Hi.」 的一部分随 TCP 报文被发送出去，另一部分和 「I am Xiaolin」 一起随另一个 TCP 报文发送出去，像这样。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/68080e783d7acc842fa254e4f9ec5630.png)\n\n类似的情况还能举例很多种，这里主要是想说明，我们不知道 「Hi.」和 「I am Xiaolin」 这两个用户消息是如何进行 TCP 分组传输的。\n\n因此，**我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议**。\n\n当两个消息的某个部分内容被分到同一个 TCP 报文时，就是我们常说的 TCP 粘包问题，这时接收方不知道消息的边界的话，是无法读出有效的消息。\n\n要解决这个问题，要交给**应用程序**。\n\n## 如何解决粘包？\n\n粘包的问题出现是因为不知道一个用户消息的边界在哪，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。\n\n一般有三种方式分包的方式：\n\n- 固定长度的消息；\n- 特殊字符作为边界；\n- 自定义消息结构。\n\n#### 固定长度的消息\n\n这种是最简单方法，即每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。\n\n但是这种方式灵活性不高，实际中很少用。\n\n### 特殊字符作为边界\n\n我们可以在两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。\n\nHTTP 是一个非常好的例子。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/a49a6bb8cd38ae1738d9c00aec68b444.png)\n\nHTTP 通过设置回车符、换行符作为 HTTP 报文协议的边界。\n\n有一点要注意，这个作为边界点的特殊字符，如果刚好消息内容里有这个特殊字符，我们要对这个字符转义，避免被接收方当作消息的边界点而解析到无效的数据。\n\n### 自定义消息结构\n\n我们可以自定义一个消息结构，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明紧随其后的数据有多大。\n\n比如这个消息结构体，首先 4 个字节大小的变量来表示数据长度，真正的数据则在后面。\n\n```c\nstruct { \n    u_int32_t message_length; \n    char message_data[]; \n} message;\n```\n\n当接收方接收到包头的大小（比如 4 个字节）后，就解析包头的内容，于是就可以知道数据的长度，然后接下来就继续读取数据，直到读满数据的长度，就可以组装成一个完整到用户消息来处理了。","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_tcpdump":{"title":"tcp_tcpdump","content":"# 4.3 TCP 实战抓包分析\n\n为了让大家更容易「看得见」 TCP，我搭建不少测试环境，并且数据包抓很多次，花费了不少时间，才抓到比较容易分析的数据包。\n\n接下来丢包、乱序、超时重传、快速重传、选择性确认、流量控制等等 TCP 的特性，都能「一览无余」。\n\n没错，我把 TCP 的\"衣服扒光\"了，就为了给大家看的清楚，嘻嘻。\n\n![提纲](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/2.jpg)\n\n---\n\n## 显形“不可见”的网络包\n\n网络世界中的数据包交互我们肉眼是看不见的，它们就好像隐形了一样，我们对着课本学习计算机网络的时候就会觉得非常的抽象，加大了学习的难度。\n\n还别说，我自己在大学的时候，也是如此。\n\n直到工作后，认识了两大分析网络的利器：**tcpdump 和 Wireshark**，这两大利器把我们“看不见”的数据包，呈现在我们眼前，一目了然。\n\n唉，当初大学学习计网的时候，要是能知道这两个工具，就不会学的一脸懵逼。\n\n\u003e tcpdump 和 Wireshark 有什么区别？\n\ntcpdump 和 Wireshark 就是最常用的网络抓包和分析工具，更是分析网络性能必不可少的利器。\n\n- tcpdump 仅支持命令行格式使用，常用在 Linux 服务器中抓取和分析网络包。\n- Wireshark 除了可以抓包外，还提供了可视化分析网络包的图形页面。\n\n所以，这两者实际上是搭配使用的，先用 tcpdump 命令在 Linux 服务器上抓包，接着把抓包的文件拖出到 Windows 电脑后，用 Wireshark 可视化分析。\n\n当然，如果你是在 Windows 上抓包，只需要用 Wireshark 工具就可以。\n\n\u003e tcpdump 在 Linux 下如何抓包？\n\ntcpdump 提供了大量的选项以及各式各样的过滤表达式，来帮助你抓取指定的数据包，不过不要担心，只需要掌握一些常用选项和过滤表达式，就可以满足大部分场景的需要了。\n\n假设我们要抓取下面的 ping 的数据包：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/3.jpg)\n\n要抓取上面的 ping 命令数据包，首先我们要知道 ping 的数据包是 `icmp` 协议，接着在使用 tcpdump 抓包的时候，就可以指定只抓 icmp 协议的数据包：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/4.jpg)\n\n那么当 tcpdump 抓取到 icmp 数据包后， 输出格式如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/5.jpg)\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/6.jpg)\n\n从 tcpdump 抓取的 icmp 数据包，我们很清楚的看到 `icmp echo` 的交互过程了，首先发送方发起了 `ICMP echo request` 请求报文，接收方收到后回了一个 `ICMP echo reply` 响应报文，之后 `seq` 是递增的。\n\n我在这里也帮你整理了一些最常见的用法，并且绘制成了表格，你可以参考使用。\n\n首先，先来看看常用的选项类，在上面的 ping 例子中，我们用过 `-i` 选项指定网口，用过 `-nn` 选项不对 IP 地址和端口名称解析。其他常用的选项，如下表格：\n\n![tcpdump 常用选项类](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/7.jpg)\n\n接下来，我们再来看看常用的过滤表用法，在上面的 ping 例子中，我们用过的是 `icmp and host 183.232.231.174`，表示抓取 icmp 协议的数据包，以及源地址或目标地址为 183.232.231.174 的包。其他常用的过滤选项，我也整理成了下面这个表格。\n\n![tcpdump 常用过滤表达式类](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/8.jpg)\n\n说了这么多，你应该也发现了，tcpdump 虽然功能强大，但是输出的格式并不直观。\n\n所以，在工作中 tcpdump 只是用来抓取数据包，不用来分析数据包，而是把 tcpdump 抓取的数据包保存成 pcap 后缀的文件，接着用 Wireshark 工具进行数据包分析。\n\n\u003e Wireshark 工具如何分析数据包？\n\nWireshark 除了可以抓包外，还提供了可视化分析网络包的图形页面，同时，还内置了一系列的汇总分析工具。\n\n比如，拿上面的 ping 例子来说，我们可以使用下面的命令，把抓取的数据包保存到 ping.pcap 文件\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/9.jpg)\n\n接着把 ping.pcap 文件拖到电脑，再用 Wireshark 打开它。打开后，你就可以看到下面这个界面：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/10.jpg)\n\n\n是吧？在 Wireshark 的页面里，可以更加直观的分析数据包，不仅展示各个网络包的头部信息，还会用不同的颜色来区分不同的协议，由于这次抓包只有 ICMP 协议，所以只有紫色的条目。\n\n接着，在网络包列表中选择某一个网络包后，在其下面的网络包详情中，**可以更清楚的看到，这个网络包在协议栈各层的详细信息**。比如，以编号 1 的网络包为例子：\n\n![ping 网络包](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/11.jpg)\n\n- 可以在数据链路层，看到 MAC 包头信息，如源 MAC 地址和目标 MAC 地址等字段；\n- 可以在 IP 层，看到 IP 包头信息，如源 IP 地址和目标 IP 地址、TTL、IP 包长度、协议等 IP 协议各个字段的数值和含义；\n- 可以在 ICMP 层，看到 ICMP 包头信息，比如 Type、Code 等 ICMP 协议各个字段的数值和含义；\n\nWireshark 用了分层的方式，展示了各个层的包头信息，把“不可见”的数据包，清清楚楚的展示了给我们，还有理由学不好计算机网络吗？是不是**相见恨晚**？\n\n从 ping 的例子中，我们可以看到网络分层就像有序的分工，每一层都有自己的责任范围和信息，上层协议完成工作后就交给下一层，最终形成一个完整的网络包。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/12.jpg)\n\n---\n\n## 解密 TCP 三次握手和四次挥手\n\n既然学会了 tcpdump 和 Wireshark 两大网络分析利器，那我们快马加鞭，接下来用它俩抓取和分析 HTTP 协议网络包，并理解 TCP 三次握手和四次挥手的工作原理。\n\n本次例子，我们将要访问的 http://192.168.3.200 服务端。在终端一用 tcpdump 命令抓取数据包：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/13.jpg)\n\n接着，在终端二执行下面的 curl 命令：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/14.jpg)\n\n最后，回到终端一，按下 Ctrl+C 停止 tcpdump，并把得到的 http.pcap 取出到电脑。\n\n使用 Wireshark 打开 http.pcap 后，你就可以在 Wireshark 中，看到如下的界面：\n\n![HTTP 网络包](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/15.jpg)\n\n我们都知道 HTTP 是基于 TCP 协议进行传输的，那么：\n\n- 最开始的 3 个包就是 TCP 三次握手建立连接的包\n- 中间是 HTTP 请求和响应的包\n- 而最后的 3 个包则是 TCP 断开连接的挥手包\n\n\nWireshark 可以用时序图的方式显示数据包交互的过程，从菜单栏中，点击 统计 (Statistics) -\u003e 流量图 (Flow Graph)，然后，在弹出的界面中的「流量类型」选择 「TCP Flows」，你可以更清晰的看到，整个过程中 TCP 流的执行过程：\n\n\n![TCP 流量图](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/16.jpg)\n\n\u003e 你可能会好奇，为什么三次握手连接过程的 Seq 是 0 ？\n\n实际上是因为 Wireshark 工具帮我们做了优化，它默认显示的是序列号 seq 是相对值，而不是真实值。\n\n如果你想看到实际的序列号的值，可以右键菜单， 然后找到「协议首选项」，接着找到「Relative Seq」后，把它给取消，操作如下：\n\n![取消序列号相对值显示](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/17.jpg)\n\n取消后，Seq 显示的就是真实值了：\n\n![TCP 流量图](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/18.jpg)\n\n可见，客户端和服务端的序列号实际上是不同的，序列号是一个随机值。\n\n这其实跟我们书上看到的 TCP 三次握手和四次挥手很类似，作为对比，你通常看到的 TCP 三次握手和四次挥手的流程，基本是这样的：\n\n![TCP 三次握手和四次挥手的流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/19.jpg)\n\n\n\u003e 为什么抓到的 TCP 挥手是三次，而不是书上说的四次？\n\n当被动关闭方（上图的服务端）在 TCP 挥手过程中，「**没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。**\n\n而通常情况下，服务器端收到客户端的 `FIN` 后，很可能还没发送完数据，所以就会先回复客户端一个 `ACK` 包，稍等一会儿，完成所有数据包的发送后，才会发送 `FIN` 包，这也就是四次挥手了。\n\n---\n\n## TCP 三次握手异常情况实战分析\n\nTCP 三次握手的过程相信大家都背的滚瓜烂熟，那么你有没有想过这三个异常情况：\n\n- **TCP 第一次握手的 SYN 丢包了，会发生了什么？**\n- **TCP 第二次握手的 SYN、ACK 丢包了，会发生什么？**\n- **TCP 第三次握手的 ACK 包丢了，会发生什么？**\n\n有的小伙伴可能说：“很简单呀，包丢了就会重传嘛。”\n\n那我在继续问你：\n\n- 那会重传几次？\n- 超时重传的时间 RTO 会如何变化？\n- 在 Linux 下如何设置重传次数？\n- ....\n\n是不是哑口无言，无法回答？\n\n不知道没关系，接下里我用三个实验案例，带大家一起探究探究这三种异常。\n\n### 实验场景\n\n本次实验用了两台虚拟机，一台作为服务端，一台作为客户端，它们的关系如下：\n\n![实验环境](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/21.jpg)\n\n- 客户端和服务端都是 CentOs 6.5 Linux，Linux 内核版本 2.6.32\n- 服务端 192.168.12.36，apache web 服务\n- 客户端 192.168.12.37\n\n### 实验一：TCP 第一次握手 SYN 丢包\n\n为了模拟 TCP 第一次握手 SYN 丢包的情况，我是在拔掉服务器的网线后，立刻在客户端执行 curl 命令：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/22.jpg)\n\n其间 tcpdump 抓包的命令如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/23.jpg)\n\n过了一会， curl 返回了超时连接的错误：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/24.jpg)\n\n从 `date` 返回的时间，可以发现在超时接近 1 分钟的时间后，curl 返回了错误。\n\n接着，把 tcp_sys_timeout.pcap 文件用 Wireshark 打开分析，显示如下图：\n\n![SYN 超时重传五次](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/25.jpg)\n\n从上图可以发现， 客户端发起了 SYN 包后，一直没有收到服务端的 ACK ，所以一直超时重传了 5 次，并且每次 RTO 超时时间是不同的：\n\n- 第一次是在 1 秒超时重传\n- 第二次是在 3 秒超时重传\n- 第三次是在 7 秒超时重传\n- 第四次是在 15 秒超时重传\n- 第五次是在 31 秒超时重传\n\n\n可以发现，每次超时时间 RTO 是**指数（翻倍）上涨的**，当超过最大重传次数后，客户端不再发送 SYN 包。\n\n在 Linux 中，第一次握手的 `SYN` 超时重传次数，是如下内核参数指定的：\n\n```bash\n$ cat /proc/sys/net/ipv4/tcp_syn_retries\n5\n```\n\n`tcp_syn_retries` 默认值为 5，也就是 SYN 最大重传次数是 5 次。\n\n接下来，我们继续做实验，把 `tcp_syn_retries` 设置为 2 次：\n\n```bash\n$ echo 2 \u003e /proc/sys/net/ipv4/tcp_syn_retries\n```\n\n重传抓包后，用 Wireshark 打开分析，显示如下图：\n\n![SYN 超时重传两次](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/26.jpg)\n\n\u003e 实验一的实验小结\n\n通过实验一的实验结果，我们可以得知，当客户端发起的 TCP 第一次握手 SYN 包，在超时时间内没收到服务端的 ACK，就会在超时重传 SYN 数据包，每次超时重传的 RTO 是翻倍上涨的，直到 SYN 包的重传次数到达 `tcp_syn_retries` 值后，客户端不再发送 SYN 包。\n\n![SYN 超时重传](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/27.jpg)\n\n### 实验二：TCP 第二次握手 SYN、ACK 丢包\n\n为了模拟客户端收不到服务端第二次握手 SYN、ACK 包，我的做法是在客户端加上防火墙限制，直接粗暴的把来自服务端的数据都丢弃，防火墙的配置如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/28.jpg)\n\n接着，在客户端执行 curl 命令：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/29.jpg)\n\n从 `date` 返回的时间前后，可以算出大概 1 分钟后，curl 报错退出了。\n\n客户端在这其间抓取的数据包，用 Wireshark 打开分析，显示的时序图如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/30.jpg)\n\n从图中可以发现：\n\n- 客户端发起 SYN 后，由于防火墙屏蔽了服务端的所有数据包，所以 curl 是无法收到服务端的 SYN、ACK 包，当发生超时后，就会重传 SYN 包\n- 服务端收到客户的 SYN 包后，就会回 SYN、ACK 包，但是客户端一直没有回 ACK，服务端在超时后，重传了 SYN、ACK 包，**接着一会，客户端超时重传的 SYN 包又抵达了服务端，服务端收到后，然后回了 SYN、ACK 包，但是SYN、ACK包的重传定时器并没有重置，还持续在重传，因为第二次握手在没收到第三次握手的 ACK 确认报文时，就会重传到最大次数。**\n- 最后，客户端 SYN 超时重传次数达到了 5 次（tcp_syn_retries 默认值 5 次），就不再继续发送 SYN 包了。\n\n所以，我们可以发现，**当第二次握手的 SYN、ACK 丢包时，客户端会超时重发 SYN 包，服务端也会超时重传 SYN、ACK 包。**\n\n\u003e 咦？客户端设置了防火墙，屏蔽了服务端的网络包，为什么 tcpdump 还能抓到服务端的网络包？\n\n添加 iptables 限制后， tcpdump 是否能抓到包 ，这要看添加的 iptables 限制条件：\n\n- 如果添加的是 `INPUT` 规则，则可以抓得到包\n- 如果添加的是 `OUTPUT` 规则，则抓不到包\n\n网络包进入主机后的顺序如下：\n\n- 进来的顺序 Wire -\u003e NIC -\u003e **tcpdump -\u003e netfilter/iptables**\n- 出去的顺序 **iptables -\u003e tcpdump** -\u003e NIC -\u003e Wire\n\n\u003e tcp_syn_retries 是限制 SYN 重传次数，那第二次握手 SYN、ACK 限制最大重传次数是多少？\n\nTCP 第二次握手 SYN、ACK 包的最大重传次数是通过 `tcp_synack_retries ` 内核参数限制的，其默认值如下：\n\n```bash\n$ cat /proc/sys/net/ipv4/tcp_synack_retries\n5\n```\n\n是的，TCP 第二次握手 SYN、ACK 包的最大重传次数默认值是 `5` 次。\n\n为了验证 SYN、ACK 包最大重传次数是 5 次，我们继续做下实验，我们先把客户端的 `tcp_syn_retries` 设置为 1，表示客户端 SYN 最大超时次数是 1 次，目的是为了防止多次重传 SYN，把服务端 SYN、ACK 超时定时器重置。\n\n接着，还是如上面的步骤：\n\n1. 客户端配置防火墙屏蔽服务端的数据包\n2. 客户端 tcpdump 抓取 curl 执行时的数据包\n\n把抓取的数据包，用 Wireshark 打开分析，显示的时序图如下： \n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/31.jpg)\n\n从上图，我们可以分析出：\n\n- 客户端的 SYN 只超时重传了 1 次，因为 `tcp_syn_retries` 值为 1\n- 服务端应答了客户端超时重传的 SYN 包后，由于一直收不到客户端的 ACK 包，所以服务端一直在超时重传 SYN、ACK 包，每次的 RTO 也是指数上涨的，一共超时重传了 5 次，因为 `tcp_synack_retries` 值为 5\n\n接着，我把 **tcp_synack_retries 设置为 2**，`tcp_syn_retries` 依然设置为 1:\n\n```bash\n$ echo 2 \u003e /proc/sys/net/ipv4/tcp_synack_retries\n$ echo 1 \u003e /proc/sys/net/ipv4/tcp_syn_retries\n```\n\n依然保持一样的实验步骤进行操作，接着把抓取的数据包，用 Wireshark 打开分析，显示的时序图如下： \n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/32.jpg)\n\n可见：\n\n- 客户端的 SYN 包只超时重传了 1 次，符合 tcp_syn_retries 设置的值；\n- 服务端的 SYN、ACK 超时重传了 2 次，符合 tcp_synack_retries 设置的值\n\n\u003e 实验二的实验小结\n\n通过实验二的实验结果，我们可以得知，当 TCP 第二次握手 SYN、ACK 包丢了后，客户端 SYN 包会发生超时重传，服务端 SYN、ACK 也会发生超时重传。\n\n客户端 SYN 包超时重传的最大次数，是由 tcp_syn_retries 决定的，默认值是 5 次；服务端 SYN、ACK 包时重传的最大次数，是由 tcp_synack_retries 决定的，默认值是 5 次。\n\n### 实验三：TCP 第三次握手 ACK 丢包\n\n为了模拟 TCP 第三次握手 ACK 包丢，我的实验方法是**在服务端配置防火墙，屏蔽客户端 TCP 报文中标志位是 ACK 的包**，也就是当服务端收到客户端的 TCP ACK 的报文时就会丢弃。\n\niptables 配置命令如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/33.jpg)\n\n接着，在客户端执行如下 tcpdump 命令：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/34.jpg)\n\n然后，客户端向服务端发起 telnet，因为 telnet 命令是会发起 TCP 连接，所以用此命令做测试：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/35.jpg)\n\n此时，由于服务端收不到第三次握手的 ACK 包，所以一直处于 `SYN_RECV` 状态：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/36.jpg)\n\n而客户端是已完成 TCP 连接建立，处于 `ESTABLISHED` 状态：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/37.jpg)\n\n过了 1 分钟后，观察发现服务端的 TCP 连接不见了：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/38.jpg)\n\n过了 30 分钟，客户端依然还是处于 `ESTABLISHED` 状态：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/39.jpg)\n\n接着，在刚才客户端建立的 telnet 会话，输入 123456 字符，进行发送：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/40.jpg)\n\n持续「好长」一段时间，客户端的 telnet 才断开连接：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/41.jpg)\n\n\n以上就是本次的实现三的现象，这里存在两个疑点：\n\n- 为什么服务端原本处于 `SYN_RECV` 状态的连接，过 1 分钟后就消失了？\n- 为什么客户端 telnet 输入 123456 字符后，过了好长一段时间，telnet 才断开连接？\n\n不着急，我们把刚抓的数据包，用 Wireshark 打开分析，显示的时序图如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/42.jpg)\n\n上图的流程：\n\n- 客户端发送 SYN 包给服务端，服务端收到后，回了个 SYN、ACK 包给客户端，此时服务端的 TCP 连接处于 `SYN_RECV` 状态；\n- 客户端收到服务端的  SYN、ACK 包后，给服务端回了个 ACK 包，此时客户端的 TCP 连接处于 `ESTABLISHED` 状态；\n- 由于服务端配置了防火墙，屏蔽了客户端的 ACK 包，所以服务端一直处于 `SYN_RECV` 状态，没有进入  `ESTABLISHED` 状态，tcpdump 之所以能抓到客户端的 ACK 包，是因为数据包进入系统的顺序是先进入 tcpudmp，后经过 iptables；\n- 接着，服务端超时重传了 SYN、ACK 包，重传了 5 次后，也就是**超过 tcp_synack_retries 的值（默认值是 5），然后就没有继续重传了，此时服务端的 TCP 连接主动中止了，所以刚才处于 SYN_RECV 状态的 TCP 连接断开了**，而客户端依然处于`ESTABLISHED` 状态；\n- 虽然服务端 TCP 断开了，但过了一段时间，发现客户端依然处于`ESTABLISHED` 状态，于是就在客户端的 telnet 会话输入了 123456 字符；\n- 由于服务端的防火墙配置了屏蔽所有携带 ACK 标志位的 TCP 报文，客户端发送的数据报文，服务端并不会接收，而是丢弃（如果服务端没有设置防火墙，由于服务端已经断开连接，此时收到客户的发来的数据报文后，会回 RST 报文）。客户端由于一直收不到数据报文的确认报文，所以触发超时重传，在超时重传过程中，每一次重传，RTO 的值是指数增长的，所以持续了好长一段时间，客户端的 telnet 才报错退出了，此时共重传了 15 次，然后客户端的也断开了连接。\n\n通过这一波分析，刚才的两个疑点已经解除了：\n\n- 服务端在重传 SYN、ACK 包时，超过了最大重传次数 `tcp_synack_retries`，于是服务端的 TCP 连接主动断开了。\n- 客户端向服务端发送数据报文时，如果迟迟没有收到数据包的确认报文，也会触发超时重传，一共重传了 15 次数据报文， 最后 telnet 就断开了连接。\n\n\u003e TCP 第一次握手的 SYN 包超时重传最大次数是由 tcp_syn_retries 指定，TCP 第二次握手的 SYN、ACK 包超时重传最大次数是由 tcp_synack_retries 指定，那 TCP 建立连接后的数据包最大超时重传次数是由什么参数指定呢？\n\nTCP 建立连接后的数据包传输，最大超时重传次数是由 `tcp_retries2` 指定，默认值是 15 次，如下：\n\n```\n$ cat /proc/sys/net/ipv4/tcp_retries2\n15\n```\n\n如果 15 次重传都做完了，TCP 就会告诉应用层说：“搞不定了，包怎么都传不过去！”\n\n\u003e 那如果客户端不发送数据，什么时候才会断开处于 ESTABLISHED 状态的连接？\n\n这里就需要提到 TCP 的 **保活机制**。这个机制的原理是这样的：\n\n定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个「探测报文」，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。\n\n在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：\n\n```\nnet.ipv4.tcp_keepalive_time=7200\nnet.ipv4.tcp_keepalive_intvl=75  \nnet.ipv4.tcp_keepalive_probes=9\n```\n\n- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制\n- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；\n- tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。\n\n也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/43.jpg)\n\n这个时间是有点长的，所以如果我抓包足够久，或许能抓到探测报文。\n\n\u003e 实验三的实验小结\n\n在建立 TCP 连接时，如果第三次握手的 ACK，服务端无法收到，则服务端就会短暂处于 `SYN_RECV` 状态，而客户端会处于 `ESTABLISHED` 状态。\n\n由于服务端一直收不到 TCP 第三次握手的 ACK，则会一直重传 SYN、ACK 包，直到重传次数超过 `tcp_synack_retries` 值（默认值 5 次）后，服务端就会断开 TCP 连接。\n\n而客户端则会有两种情况：\n\n- 如果客户端没发送数据包，一直处于 `ESTABLISHED` 状态，然后经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接，于是客户端连接就会断开连接。\n- 如果客户端发送了数据包，一直没有收到服务端对该数据包的确认报文，则会一直重传该数据包，直到重传次数超过 `tcp_retries2` 值（默认值 15 次）后，客户端就会断开 TCP 连接。\n\n---\n\n## TCP 快速建立连接\n\n客户端在向服务端发起 HTTP GET 请求时，一个完整的交互过程，需要 2.5 个 RTT 的时延。\n\n由于第三次握手是可以携带数据的，这时如果在第三次握手发起 HTTP GET 请求，需要 2 个 RTT 的时延。\n\n但是在下一次（不是同个 TCP 连接的下一次）发起 HTTP GET 请求时，经历的 RTT 也是一样，如下图：\n\n![常规 HTTP 请求](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/44.jpg)\n\n在 Linux 3.7 内核版本中，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。\n\n![常规 HTTP 请求 与 Fast  Open HTTP 请求](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/45.jpg)\n\n- 在第一次建立连接的时候，服务端在第二次握手产生一个 `Cookie` （已加密）并通过 SYN、ACK 包一起发给客户端，于是客户端就会缓存这个 `Cookie`，所以第一次发起 HTTP Get 请求的时候，还是需要 2 个 RTT 的时延；\n- 在下次请求的时候，客户端在 SYN 包带上 `Cookie` 发给服务端，就提前可以跳过三次握手的过程，因为 `Cookie` 中维护了一些信息，服务端可以从 `Cookie` 获取 TCP 相关的信息，这时发起的 HTTP GET 请求就只需要 1 个 RTT 的时延；\n\n\n注：客户端在请求并存储了 Fast Open Cookie 之后，可以不断重复 TCP Fast Open 直至服务器认为 Cookie 无效（通常为过期）\n\n\u003e 在 Linux 上如何打开 Fast Open 功能？\n\n可以通过设置 `net.ipv4.tcp_fastopn` 内核参数，来打开 Fast Open 功能。\n\nnet.ipv4.tcp_fastopn 各个值的意义: \n\n- 0 关闭\n- 1 作为客户端使用 Fast Open 功能\n- 2 作为服务端使用 Fast Open 功能\n- 3 无论作为客户端还是服务器，都可以使用 Fast Open 功能\n\n\u003e TCP Fast Open 抓包分析\n\n在下图，数据包 7 号，客户端发起了第二次 TCP 连接时，SYN 包会携带 Cooike，并且长度为 5 的数据。\n\n服务端收到后，校验 Cooike 合法，于是就回了 SYN、ACK 包，并且确认应答收到了客户端的数据包，ACK = 5 + 1 = 6 \n\n![TCP Fast Open 抓包分析](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/46.jpg)\n\n---\n\n## TCP 重复确认和快速重传\n\n当接收方收到乱序数据包时，会发送重复的 ACK，以便告知发送方要重发该数据包，**当发送方收到 3 个重复 ACK 时，就会触发快速重传，立刻重发丢失数据包。**\n\n![快速重传机制](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/47.jpg)\n\nTCP 重复确认和快速重传的一个案例，用 Wireshark 分析，显示如下：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/48.jpg)\n\n- 数据包 1 期望的下一个数据包 Seq 是 1，但是数据包 2 发送的 Seq 却是 10945，说明收到的是乱序数据包，于是回了数据包 3 ，还是同样的 Seq = 1，Ack = 1，这表明是重复的 ACK；\n- 数据包 4 和 6 依然是乱序的数据包，于是依然回了重复的 ACK；\n- 当对方收到三次重复的 ACK 后，于是就快速重传了 Seq = 1 、Len = 1368 的数据包 8；\n- 当收到重传的数据包后，发现 Seq = 1 是期望的数据包，于是就发送了个确认收到快速重传的 ACK\n\n注意：快速重传和重复 ACK 标记信息是 Wireshark 的功能，非数据包本身的信息。\n\n以上案例在 TCP 三次握手时协商开启了**选择性确认 SACK**，因此一旦数据包丢失并收到重复 ACK ，即使在丢失数据包之后还成功接收了其他数据包，也只需要重传丢失的数据包。如果不启用 SACK，就必须重传丢失包之后的每个数据包。\n\n如果要支持 `SACK`，必须双方都要支持。在 Linux 下，可以通过 `net.ipv4.tcp_sack` 参数打开这个功能（Linux 2.4 后默认打开）。\n\n---\n\n## TCP 流量控制\n\nTCP 为了防止发送方无脑的发送数据，导致接收方缓冲区被填满，所以就有了滑动窗口的机制，它可利用接收方的接收窗口来控制发送方要发送的数据量，也就是流量控制。\n\n接收窗口是由接收方指定的值，存储在 TCP 头部中，它可以告诉发送方自己的 TCP 缓冲空间区大小，这个缓冲区是给应用程序读取数据的空间：\n\n- 如果应用程序读取了缓冲区的数据，那么缓冲空间区就会把被读取的数据移除\n- 如果应用程序没有读取数据，则数据会一直滞留在缓冲区。\n\n接收窗口的大小，是在 TCP 三次握手中协商好的，后续数据传输时，接收方发送确认应答 ACK 报文时，会携带当前的接收窗口的大小，以此来告知发送方。\n\n假设接收方接收到数据后，应用层能很快的从缓冲区里读取数据，那么窗口大小会一直保持不变，过程如下：\n\n![理想状态下的窗口变化](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/49.jpg)\n\n但是现实中服务器会出现繁忙的情况，当应用程序读取速度慢，那么缓存空间会慢慢被占满，于是为了保证发送方发送的数据不会超过缓冲区大小，服务器则会调整窗口大小的值，接着通过 ACK 报文通知给对方，告知现在的接收窗口大小，从而控制发送方发送的数据大小。\n\n![服务端繁忙状态下的窗口变化](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/50.jpg)\n\n### 零窗口通知与窗口探测\n\n假设接收方处理数据的速度跟不上接收数据的速度，缓存就会被占满，从而导致接收窗口为 0，当发送方接收到零窗口通知时，就会停止发送数据。\n\n如下图，可以看到接收方的窗口大小在不断的收缩至 0：\n\n![窗口大小在收缩](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/51.jpg)\n\n\n接着，发送方会**定时发送窗口大小探测报文**，以便及时知道接收方窗口大小的变化。\n\n以下图 Wireshark 分析图作为例子说明：\n\n![零窗口 与 窗口探测](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/52.jpg)\n\n- 发送方发送了数据包 1 给接收方，接收方收到后，由于缓冲区被占满，回了个零窗口通知；\n- 发送方收到零窗口通知后，就不再发送数据了，直到过了 `3.4` 秒后，发送了一个 TCP Keep-Alive 报文，也就是窗口大小探测报文；\n- 当接收方收到窗口探测报文后，就立马回一个窗口通知，但是窗口大小还是 0；\n- 发送方发现窗口还是 0，于是继续等待了 `6.8`（翻倍） 秒后，又发送了窗口探测报文，接收方依然还是回了窗口为 0 的通知；\n- 发送方发现窗口还是 0，于是继续等待了 `13.5`（翻倍） 秒后，又发送了窗口探测报文，接收方依然还是回了窗口为 0 的通知；\n\n可以发现，这些窗口探测报文以 3.4s、6.5s、13.5s 的间隔出现，说明超时时间会**翻倍**递增。\n\n这连接暂停了 25s，想象一下你在打王者的时候，25s 的延迟你还能上王者吗？\n\n### 发送窗口的分析\n\n\u003e 在 Wireshark 看到的 Windows size 也就是 \" win = \"，这个值表示发送窗口吗？\n\n\n这不是发送窗口，而是在向对方声明自己的接收窗口。\n\n你可能会好奇，抓包文件里有「Window size scaling factor」，它其实是算出实际窗口大小的乘法因子，「Window size value」实际上并不是真实的窗口大小，真实窗口大小的计算公式如下：\n\n「Window size value」 * 「Window size scaling factor」 = 「Caculated window size 」\n\n对应的下图案例，也就是 32 * 2048 = 65536。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/53.jpg)\n\n实际上是 Caculated window size 的值是 Wireshark 工具帮我们算好的，Window size scaling factor 和 Window size value 的值是在 TCP 头部中，其中 Window size scaling factor 是在三次握手过程中确定的，如果你抓包的数据没有 TCP 三次握手，那可能就无法算出真实的窗口大小的值，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/54.jpg)\n\n\u003e 如何在包里看出发送窗口的大小？\n\n很遗憾，没有简单的办法，发送窗口虽然是由接收窗口决定，但是它又可以被网络因素影响，也就是拥塞窗口，实际上发送窗口是值是 min(拥塞窗口，接收窗口)。\n\n\u003e 发送窗口和 MSS 有什么关系？\n\n发送窗口决定了一口气能发多少字节，而 MSS 决定了这些字节要分多少包才能发完。\n\n举个例子，如果发送窗口为 16000 字节的情况下，如果 MSS 是 1000 字节，那就需要发送 1600/1000 = 16 个包。\n\n\u003e 发送方在一个窗口发出 n 个包，是不是需要 n 个 ACK 确认报文？\n\n不一定，因为 TCP 有累计确认机制，所以当收到多个数据包时，只需要应答最后一个数据包的 ACK 报文就可以了。\n\n---\n\n## TCP 延迟确认与 Nagle 算法\n\n\n当我们 TCP 报文的承载的数据非常小的时候，例如几个字节，那么整个网络的效率是很低的，因为每个 TCP 报文中都会有 20 个字节的 TCP 头部，也会有 20 个字节的 IP 头部，而数据只有几个字节，所以在整个报文中有效数据占有的比重就会非常低。\n\n这就好像快递员开着大货车送一个小包裹一样浪费。\n\n那么就出现了常见的两种策略，来减少小报文的传输，分别是：\n\n- Nagle 算法\n- 延迟确认\n\n\u003e Nagle 算法是如何避免大量 TCP 小数据报文的传输？\n\nNagle 算法做了一些策略来避免过多的小数据报文发送，这可提高传输效率。\n\nNagle 伪代码如下：\n\n```c\nif 有数据要发送 {\n    if 可用窗口大小 \u003e= MSS and 可发送的数据 \u003e= MSS {\n    \t立刻发送MSS大小的数据\n    } else {\n        if 有未确认的数据 {\n            将数据放入缓存等待接收ACK\n        } else {\n            立刻发送数据\n        }\n    }\n}\n```\n\n使用 Nagle 算法，该算法的思路是延时处理，只有满足下面两个条件中的任意一个条件，才能可以发送数据：\n\n- 条件一：要等到窗口大小 \u003e= `MSS` 并且 数据大小 \u003e= `MSS`；\n- 条件二：收到之前发送数据的 `ack` 回包；\n\n只要上面两个条件都不满足，发送方一直在囤积数据，直到满足上面的发送条件。\n\n![禁用 Nagle 算法 与 启用 Nagle 算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/55.jpg)\n\n上图右侧启用了 Nagle 算法，它的发送数据的过程：\n\n- 一开始由于没有已发送未确认的报文，所以就立刻发了 H 字符；\n- 接着，在还没收到对 H 字符的确认报文时，发送方就一直在囤积数据，直到收到了确认报文后，此时没有已发送未确认的报文，于是就把囤积后的 ELL 字符一起发给了接收方；\n- 待收到对 ELL 字符的确认报文后，于是把最后一个 O 字符发送了出去\n\n可以看出，**Nagle 算法一定会有一个小报文，也就是在最开始的时候。**\n\n另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。\n\n可以在 Socket 设置 `TCP_NODELAY` 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）。\n\n![关闭 Nagle 算法](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/56.jpg)\n\n\u003e 那延迟确认又是什么？\n\n事实上当没有携带数据的 ACK，它的网络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报文。\n\n为了解决 ACK 传输效率低问题，所以就衍生出了 **TCP 延迟确认**。\n\nTCP 延迟确认的策略：\n\n- 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方\n- 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送\n- 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK\n\n![TCP 延迟确认](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/57.jpg)\n\n\n延迟等待的时间是在 Linux 内核中定义的，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/58.jpg)\n\n关键就需要 `HZ` 这个数值大小，HZ 是跟系统的时钟频率有关，每个操作系统都不一样，在我的 Linux 系统中 HZ 大小是 `1000`，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/59.jpg)\n\n知道了 HZ 的大小，那么就可以算出：\n\n* 最大延迟确认时间是 `200` ms （1000/5）\n* 最短延迟确认时间是 `40` ms （1000/25）\n\nTCP 延迟确认可以在 Socket 设置 `TCP_QUICKACK` 选项来关闭这个算法。\n\n![关闭 TCP 延迟确认](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/60.jpg)\n\n\u003e  延迟确认 和 Nagle 算法混合使用时，会产生新的问题\n\n当 TCP 延迟确认 和 Nagle 算法混合使用时，会导致时耗增长，如下图：\n\n![TCP 延迟确认 和 Nagle 算法混合使用](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/TCP-Wireshark/61.jpg)\n\n发送方使用了 Nagle 算法，接收方使用了 TCP 延迟确认会发生如下的过程：\n\n- 发送方先发出一个小报文，接收方收到后，由于延迟确认机制，自己又没有要发送的数据，只能干等着发送方的下一个报文到达；\n- 而发送方由于 Nagle 算法机制，在未收到第一个报文的确认前，是不会发送后续的数据；\n- 所以接收方只能等待最大时间 200 ms 后，才回 ACK 报文，发送方收到第一个报文的确认报文后，也才可以发送后续的数据。\n\n很明显，这两个同时使用会造成额外的时延，这就会使得网络\"很慢\"的感觉。\n\n要解决这个问题，只有两个办法：\n\n- 要不发送方关闭 Nagle 算法\n- 要不接收方关闭 TCP 延迟确认\n\n---\n\n参考资料：\n\n[1] Wireshark网络分析的艺术.林沛满.人民邮电出版社.\n\n[2] Wireshark网络分析就这么简单.林沛满.人民邮电出版社.\n\n[3] Wireshark数据包分析实战.Chris Sanders .人民邮电出版社.读者问答\n\n---\n\n## 读者问答\n\n\u003e 读者问：“两个问题，请教一下作者:\n\u003e tcp_retries1 参数，是什么场景下生效？\n\u003e tcp_retries2是不是只受限于规定的次数，还是受限于次数和时间限制的最小值？”\n\ntcp_retries1和tcp_retries2都是在TCP三次握手之后的场景。\n\n- 当重传次数超过tcp_retries1就会指示 IP 层进行 MTU 探测、刷新路由等过程，并不会断开TCP连接，当重传次数超过 tcp_retries2 才会断开TCP流。\n- tcp_retries1 和 tcp_retries2 两个重传次数都是受一个 timeout 值限制的，timeout 的值是根据它俩的值计算出来的，当重传时间超过 timeout，就不会继续重传了，即使次数还没到达。\n\n\u003e 读者问：“tcp_orphan_retries也是控制tcp连接的关闭。这个跟tcp_retries1 tcp_retries2有什么区别吗？”\n\n主动方发送 FIN 报文后，连接就处于 FIN_WAIT1 状态下，该状态通常应在数十毫秒内转为 FIN_WAIT2。如果迟迟收不到对方返回的 ACK 时，此时，内核会定时重发 FIN 报文，其中重发次数由 tcp_orphan_retries 参数控制。\n\n\u003e 读者问：“请问，为什么连续两个报文的seq会是一样的呢，比如三次握手之后的那个报文？还是说，序号相同的是同一个报文，只是拆开显示了？”\n\n1. 三次握手中的前两次，是 seq+1；\n2. 三次握手中的最后一个 ack，实际上是可以携带数据的，由于我文章的例子是没有发送数据的，你可以看到第三次握手的 len=0 ，在数据传输阶段「下一个 seq=seq+len 」，所以第三次握手的 seq 和下一个数据报的 seq 是一样的，因为 len 为 0；\n\n---\n\n## 最后\n\n文章中 Wireshark 分析的截图，可能有些会看的不清楚，为了方便大家用 Wireshark 分析，**我已把文中所有抓包的源文件，已分享到公众号了，大家在后台回复「抓包」，就可以获取了。**\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/62.png)\n\n**小林是专为大家图解的工具人，Goodbye，我们下次见！**\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_three_fin":{"title":"tcp_three_fin","content":"# 4.22 TCP 四次挥手，可以变成三次吗？\n\n大家好，我是小林。\n\n有位读者面美团时，被问到：**TCP 四次挥手中，能不能把第二次的 ACK 报文， 放到第三次 FIN 报文一起发送？**\n\n![](https://img-blog.csdnimg.cn/6e02477ccea24facbf7eada108158bc2.png)\n\n\n虽然我们在学习 TCP 挥手时，学到的是需要四次来完成 TCP 挥手，但是**在一些情况下， TCP 四次挥手是可以变成 TCP 三次挥手的**。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/52f35dcbe24a4ca7abb23f292837c707.png)\n\n\n而且在用 wireshark 工具抓包的时候，我们也会常看到 TCP 挥手过程是三次，而不是四次，如下图：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/361207c2e5c34bec8708b79990ba7e99.png)\n\n先来回答为什么 RFC 文档里定义 TCP 挥手过程是要四次？\n\n再来回答什么情况下，什么情况会出现三次挥手？\n\n## TCP 四次挥手\n\nTCP 四次挥手的过程如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/18635e15653a4affbdab2c9bf72d599e.png)\n\n\n具体过程：\n\n- 客户端主动调用关闭连接的函数，于是就会发送 FIN 报文，这个  FIN 报文代表客户端不会再发送数据了，进入 FIN_WAIT_1 状态；\n- 服务端收到了 FIN 报文，然后马上回复一个 ACK 确认报文，此时服务端进入 CLOSE_WAIT 状态。在收到 FIN 报文的时候，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，服务端应用程序可以通过 read 调用来感知这个 FIN 包，这个 EOF 会被**放在已排队等候的其他已接收的数据之后**，所以必须要得继续 read 接收缓冲区已接收的数据；\n- 接着，当服务端在 read 数据的时候，最后自然就会读到 EOF，接着 **read() 就会返回 0，这时服务端应用程序如果有数据要发送的话，就发完数据后才调用关闭连接的函数，如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数**，这时服务端就会发一个 FIN 包，这个  FIN 报文代表服务端不会再发送数据了，之后处于 LAST_ACK 状态；\n- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；\n- 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；\n- 客户端经过 2MSL 时间之后，也进入 CLOSE 状态；\n\n你可以看到，每个方向都需要**一个 FIN 和一个 ACK**，因此通常被称为**四次挥手**。\n\n### 为什么 TCP 挥手需要四次呢？\n\n服务器收到客户端的 FIN 报文时，内核会马上回一个 ACK 应答报文，**但是服务端应用程序可能还有数据要发送，所以并不能马上发送 FIN 报文，而是将发送 FIN 报文的控制权交给服务端应用程序**：\n\n   - 如果服务端应用程序有数据要发送的话，就发完数据后，才调用关闭连接的函数；\n   - 如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数，\n\n从上面过程可知，**是否要发送第三次挥手的控制权不在内核，而是在被动关闭方（上图的服务端）的应用程序，因为应用程序可能还有数据要发送，由应用程序决定什么时候调用关闭连接的函数，当调用了关闭连接的函数，内核就会发送 FIN 报文了，** 所以服务端的 ACK 和 FIN 一般都会分开发送。\n\n\u003e FIN 报文一定得调用关闭连接的函数，才会发送吗？\n\n\n不一定。\n\n如果进程退出了，不管是不是正常退出，还是异常退出（如进程崩溃），内核都会发送 FIN 报文，与对方完成四次挥手。\n\n### 粗暴关闭 vs 优雅关闭\n\n前面介绍 TCP 四次挥手的时候，并没有详细介绍关闭连接的函数，其实关闭的连接的函数有两种函数：\n\n- close 函数，同时 socket 关闭发送方向和读取方向，也就是 socket 不再有发送和接收数据的能力。如果有多进程/多线程共享同一个 socket，如果有一个进程调用了 close 关闭只是让 socket 引用计数 -1，并不会导致 socket 不可用，同时也不会发出 FIN 报文，其他进程还是可以正常读写该 socket，直到引用计数变为 0，才会发出 FIN 报文。\n- shutdown 函数，可以指定 socket 只关闭发送方向而不关闭读取方向，也就是 socket 不再有发送数据的能力，但是还是具有接收数据的能力。如果有多进程/多线程共享同一个 socket，shutdown 则不管引用计数，直接使得该 socket 不可用，然后发出 FIN 报文，如果有别的进程企图使用该 socket，将会受到影响。\n\n如果客户端是用 close 函数来关闭连接，那么在 TCP \t四次挥手过程中，如果收到了服务端发送的数据，由于客户端已经不再具有发送和接收数据的能力，所以客户端的内核会回 RST 报文给服务端，然后内核会释放连接，这时就不会经历完成的 TCP 四次挥手，所以我们常说，调用 close 是粗暴的关闭。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/3b5f1897d2d74028aaf4d552fbce1a74.png)\n\n\n当服务端收到 RST 后，内核就会释放连接，当服务端应用程序再次发起读操作或者写操作时，就能感知到连接已经被释放了：\n\n- 如果是读操作，则会返回 RST 的报错，也就是我们常见的Connection reset by peer。\n- 如果是写操作，那么程序会产生 SIGPIPE 信号，应用层代码可以捕获并处理信号，如果不处理，则默认情况下进程会终止，异常退出。\n\n相对的，shutdown 函数因为可以指定只关闭发送方向而不关闭读取方向，所以即使在 TCP 四次挥手过程中，如果收到了服务端发送的数据，客户端也是可以正常读取到该数据的，然后就会经历完整的 TCP 四次挥手，所以我们常说，调用 shutdown 是优雅的关闭。\n\n![优雅关闭.drawio.png](https://img-blog.csdnimg.cn/71f5646ec58849e5921adc08bb6789d4.png)\n\n\n但是注意，shutdown 函数也可以指定「只关闭读取方向，而不关闭发送方向」，但是这时候内核是不会发送 FIN 报文的，因为发送 FIN 报文是意味着我方将不再发送任何数据，而 shutdown 如果指定「不关闭发送方向」，就意味着 socket 还有发送数据的能力，所以内核就不会发送 FIN。\n\n## 什么情况会出现三次挥手？\n\n当被动关闭方（上图的服务端）在 TCP 挥手过程中，「**没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。**\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/d7b349efa4f94453943b433b704a4ca8.png)\n\n\n然后因为 TCP 延迟确认机制是默认开启的，所以导致我们抓包时，看见三次挥手的次数比四次挥手还多。\n\n\u003e 什么是  TCP 延迟确认机制？\n\n当发送没有携带数据的 ACK，它的网络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报文。\n为了解决 ACK 传输效率低问题，所以就衍生出了 **TCP 延迟确认**。\nTCP 延迟确认的策略：\n\n- 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方\n- 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送\n- 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK\n\n![](https://img-blog.csdnimg.cn/33f3d2d54a924b0a80f565038327e0e4.png)\n\n\n延迟等待的时间是在 Linux 内核中定义的，如下图：\n\n![](https://img-blog.csdnimg.cn/ae241915337a4d2c9cb2f7ab91e6661d.png)\n\n关键就需要 HZ 这个数值大小，HZ 是跟系统的时钟频率有关，每个操作系统都不一样，在我的 Linux 系统中 HZ 大小是 1000，如下图：\n\n![](https://img-blog.csdnimg.cn/7a67bd4dc2894335b974e38674ba90b4.png)\n\n\n知道了 HZ 的大小，那么就可以算出：\n\n- 最大延迟确认时间是 200 ms （1000/5）\n- 最短延迟确认时间是 40 ms （1000/25）\n\n\u003e 怎么关闭 TCP 延迟确认机制？\n\n如果要关闭 TCP 延迟确认机制，可以在 Socket 设置里启用 TCP_QUICKACK。\n\n```cpp\n// 1 表示开启 TCP_QUICKACK，即关闭 TCP 延迟确认机制\nint value = 1;\nsetsockopt(socketfd, IPPROTO_TCP, TCP_QUICKACK, (char*)\u0026 value, sizeof(int));\n```\n\n### 实验验证\n\n#### 实验一\n\n接下来，来给大家做个实验，验证这个结论：\n\n\u003e 当被动关闭方（上图的服务端）在 TCP 挥手过程中，「**没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。**\n\n\n服务端的代码如下，做的事情很简单，就读取数据，然后当 read 返回 0 的时候，就马上调用 close 关闭连接。因为 TCP 延迟确认机制是默认开启的，所以不需要特殊设置。\n\n```cpp\n#include \u003cstdlib.h\u003e\n#include \u003cstdio.h\u003e\n#include \u003cerrno.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cnetdb.h\u003e\n#include \u003csys/types.h\u003e\n#include \u003cnetinet/in.h\u003e\n#include \u003csys/socket.h\u003e\n#include \u003cnetinet/tcp.h\u003e\n\n#define MAXLINE 1024\n\nint main(int argc, char *argv[])\n{\n\n    // 1. 创建一个监听 socket\n    int listenfd = socket(AF_INET, SOCK_STREAM, 0);\n    if(listenfd \u003c 0)\n    {\n        fprintf(stderr, \"socket error : %s\\n\", strerror(errno));\n        return -1;\n    }\n\n    // 2. 初始化服务器地址和端口\n    struct sockaddr_in server_addr;\n    bzero(\u0026server_addr, sizeof(struct sockaddr_in));\n    server_addr.sin_family = AF_INET;\n    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);\n    server_addr.sin_port = htons(8888);\n\n    // 3. 绑定地址+端口\n    if(bind(listenfd, (struct sockaddr *)(\u0026server_addr), sizeof(struct sockaddr)) \u003c 0)\n    {\n        fprintf(stderr,\"bind error:%s\\n\", strerror(errno));\n        return -1;\n    }\n\n    printf(\"begin listen....\\n\");\n\n    // 4. 开始监听\n    if(listen(listenfd, 128))\n    {\n        fprintf(stderr, \"listen error:%s\\n\\a\", strerror(errno));\n        exit(1);\n    }\n\n\n    // 5. 获取已连接的socket\n    struct sockaddr_in client_addr;\n    socklen_t client_addrlen = sizeof(client_addr);\n    int clientfd = accept(listenfd, (struct sockaddr *)\u0026client_addr, \u0026client_addrlen);\n    if(clientfd \u003c 0) {\n        fprintf(stderr, \"accept error:%s\\n\\a\", strerror(errno));\n        exit(1);\n    }\n\n    printf(\"accept success\\n\");\n\n    char message[MAXLINE] = {0};\n    \n    while(1) {\n        //6. 读取客户端发送的数据\n        int n = read(clientfd, message, MAXLINE);\n        if(n \u003c 0) { // 读取错误\n            fprintf(stderr, \"read error:%s\\n\\a\", strerror(errno));\n            break;\n        } else if(n == 0) {  // 返回 0 ，代表读到 FIN 报文\n            fprintf(stderr, \"client closed \\n\");\n            close(clientfd); // 没有数据要发送，立马关闭连接\n            break;\n        }\n\n        message[n] = 0; \n        printf(\"received %d bytes: %s\\n\", n, message);\n    }\n\t\n    close(listenfd);\n    return 0;\n}\n```\n\n客户端代码如下，做的事情也很简单，与服务端连接成功后，就发送数据给服务端，然后睡眠一秒后，就调用 close 关闭连接，所以客户端是主动关闭方：\n\n```cpp\n#include \u003cstdlib.h\u003e\n#include \u003cstdio.h\u003e\n#include \u003cerrno.h\u003e\n#include \u003cstring.h\u003e\n#include \u003cnetdb.h\u003e\n#include \u003csys/types.h\u003e\n#include \u003cnetinet/in.h\u003e\n#include \u003csys/socket.h\u003e\n\nint main(int argc, char *argv[])\n{\n\n    // 1. 创建一个监听 socket\n    int connectfd = socket(AF_INET, SOCK_STREAM, 0);\n    if(connectfd \u003c 0)\n    {\n        fprintf(stderr, \"socket error : %s\\n\", strerror(errno));\n        return -1;\n    }\n\n    // 2. 初始化服务器地址和端口\n    struct sockaddr_in server_addr;\n    bzero(\u0026server_addr, sizeof(struct sockaddr_in));\n    server_addr.sin_family = AF_INET;\n    server_addr.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\n    server_addr.sin_port = htons(8888);\n    \n    // 3. 连接服务器\n    if(connect(connectfd, (struct sockaddr *)(\u0026server_addr), sizeof(server_addr)) \u003c 0)\n    {\n        fprintf(stderr,\"connect error:%s\\n\", strerror(errno));\n        return -1;\n    }\n\n    printf(\"connect success\\n\");\n\n\n    char sendline[64] = \"hello, i am xiaolin\";\n\n    //4. 发送数据\n    int ret = send(connectfd, sendline, strlen(sendline), 0);\n    if(ret != strlen(sendline)) {\n        fprintf(stderr,\"send data error:%s\\n\", strerror(errno));\n        return -1;\n    }\n\n    printf(\"already send %d bytes\\n\", ret);\n\n    sleep(1);\n\n    //5. 关闭连接\n    close(connectfd);\n    return 0;\n}\n```\n\n编译服务端和客户端的代码：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/291c6bdf93fa4e04b1606eef57d76836.png)\n\n\n\n\n先启用服务端：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/a975aa542caf41b2a1f303563df697b7.png)\n\n\n然后用 tcpdump 工具开始抓包，命令如下：\n\n```bash\ntcpdump -i lo tcp and port 8888 -s0 -w /home/tcp_close.pcap\n```\n\n然后启用客户端，可以看到，与服务端连接成功后，发完数据就退出了。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/8ea9f527a68a4c0184edc8842aaf55d6.png)\n\n\n此时，服务端的输出：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/ff5f9ae91a3a4576b59e6e9c4716464d.png)\n\n\n接下来，我们来看看抓包的结果。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/b542a2777aca4419b47205484b52cc03.png)\n\n\n可以看到，TCP 挥手次数是 3 次。\n\n所以，下面这个结论是没问题的。\n\n\u003e 结论：当被动关闭方（上图的服务端）在 TCP 挥手过程中，「**没有数据要发送」并且「开启了 TCP 延迟确认机制（默认会开启）」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。**\n\n#### 实验二\n\n我们再做一次实验，来看看**关闭 TCP 延迟确认机制，会出现四次挥手吗？**\n\n客户端代码保持不变，服务端代码需要增加一点东西。\n\n在上面服务端代码中，增加了打开了 TCP_QUICKACK （快速应答）机制的代码，如下：\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/fbbe19e6b1cc4a21b024588950b88eee.png)\n\n\n编译好服务端代码后，就开始运行服务端和客户端的代码，同时用 tcpdump 进行抓包。\n\n抓包的结果如下，可以看到是四次挥手。\n![在这里插入图片描述](https://img-blog.csdnimg.cn/b6327b1057d64f54997c0eb322b28a55.png)\n\n\n所以，当被动关闭方（上图的服务端）在 TCP 挥手过程中，「**没有数据要发送」，同时「关闭了 TCP 延迟确认机制」，那么就会是四次挥手。**\n\n\u003e 设置 TCP_QUICKACK 的代码，为什么要放在 read 返回 0 之后？\n\n\n我也是多次实验才发现，在 bind 之前设置 TCP_QUICKACK 是不生效的，只有在 read 返回 0 的时候，设置 TCP_QUICKACK 才会出现四次挥手。\n\n网上查了下资料说，设置 TCP_QUICKACK 并不是永久的，所以每次读取数据的时候，如果想要立刻回 ACK，那就得在每次读取数据之后，重新设置 TCP_QUICKACK。\n\n而我这里的实验，目的是为了当收到客户端的 FIN 报文（第一次挥手）后，立马回 ACK 报文。所以就在 read 返回 0 的时候，设置 TCP_QUICKACK。当然，实际应用中，没人会在这个位置设置 TCP_QUICKACK，因为操作系统都通过 TCP 延迟确认机制帮我们把四次挥手优化成了三次挥手了。\n\n## 总结\n\n当被动关闭方在 TCP 挥手过程中，如果「没有数据要发送」，同时「没有开启 TCP_QUICKACK（默认情况就是没有开启，没有开启 TCP_QUICKACK，等于就是在使用 TCP 延迟确认机制）」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。\n\n**所以，出现三次挥手现象，是因为 TCP 延迟确认机制导致的。**\n\n----\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」***\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.864106516Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_tls":{"title":"tcp_tls","content":"# 4.14 HTTPS 中 TLS 和 TCP 能同时握手吗？\n\n大家好，我是小林。\n\n有位读者在面试的时候，碰到这么个问题：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/4d07f1ab714bb4b3efefbf5655b2f81e.png)\n\n面试官跟他说 **HTTPS 中的 TLS 握手过程可以同时进行三次握手**，然后读者之前看我的文章是说「*先进行 TCP 三次握手，再进行 TLS 四次握手*」，他跟面试官说了这个，面试官说他不对，他就感到很困惑。\n\n我们先不管面试官说的那句「*HTTPS 中的 TLS 握手过程可以同时进行三次握手*」对不对。\n\n但是面试官说「*HTTPS 建立连接的过程，先进行 TCP 三次握手，再进行 TLS 四次握手*」是错的，**这很明显面试官的水平有问题，这种公司不去也罢！**\n\n如果是我面试遇到这样的面试官，我直接当场给他抓 HTTPS 建立过程的网络包，然后给他看，啪啪啪啪啪的打他脸。\n\n比如，下面这个 TLSv1.2 的 基于 RSA 算法的四次握手过程：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/4e4f0d13effbeaf963992148b022ef3f.png)\n\n难道不是先三次握手，再进行 TLS 四次握手吗？面试官你脸疼吗？\n\n不过 TLS 握手过程的次数还得看版本。\n\nTLSv1.2 握手过程基本都是需要四次，也就是需要经过 2-RTT 才能完成握手，然后才能发送请求，而 TLSv1.3 只需要 1-RTT 就能完成 TLS 握手，如下图。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/0877fe78380bf34ad3b28768e59fb53a.png)\n\n**一般情况下，不管 TLS 握手次数如何，都得先经过 TCP 三次握手后才能进行**，因为 HTTPS 都是基于 TCP 传输协议实现的，得先建立完可靠的 TCP 连接才能做 TLS 握手的事情。\n\n\u003e 那面试官说的这句「HTTPS 中的 TLS 握手过程可以同时进行三次握手」对不对呢？\n\n这个场景是可能发生的，但是需要在特定的条件下才可能发生，**如果没有说任何前提条件，说这句话就是在耍流氓。**\n\n那到底什么条件下，这个场景才能发生呢？需要下面这两个条件同时满足才可以：\n\n- **客户端和服务端都开启了 TCP Fast Open 功能，且 TLS 版本是 1.3；**\n- **客户端和服务端已经完成过一次通信。**\n\n那具体怎么做到的呢？我们先了解些 TCP Fast Open 功能和 TLSv1.3 的特性。\n\n## TCP Fast Open\n\n\u003e 我们先来了解下什么是 TCP Fast Open？\n\n常规的情况下，如果要使用 TCP 传输协议进行通信，则客户端和服务端通信之前，先要经过 TCP 三次握手后，建立完可靠的 TCP 连接后，客户端才能将数据发送给服务端。\n\n其中，TCP 的第一次和第二次握手是不能够携带数据的，而 TCP 的第三次握手是可以携带数据的，因为这时候客户端的 TCP 连接状态已经是 ESTABLISHED，表明客户端这一方已经完成了 TCP 连接建立。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/35bc3541c237686aa36e0a88f80592d4.png)\n\n就算客户端携带数据的第三次握手在网络中丢失了，客户端在一定时间内没有收到服务端对该数据的应答报文，就会触发超时重传机制，然后客户端重传该携带数据的第三次握手的报文，直到重传次数达到系统的阈值，客户端就会销毁该 TCP 连接。\n\n说完常规的 TCP 连接后，我们再来看看 TCP Fast Open。\n\nTCP Fast Open 是为了绕过 TCP 三次握手发送数据，在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。\n\n要使用 TCP Fast Open 功能，客户端和服务端都要同时支持才会生效。\n\n不过，开启了 TCP Fast Open 功能，**想要绕过 TCP 三次握手发送数据，得建立第二次以后的通信过程。**\n\n在客户端首次建立连接时的过程，如下图：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/7cb0bd3cde30493fec9562cbdb549f83.png)\n\n具体介绍：\n\n- 客户端发送 SYN 报文，该报文包含 Fast Open 选项，且该选项的 Cookie 为空，这表明客户端请求 Fast Open Cookie；\n- 支持 TCP Fast Open 的服务器生成 Cookie，并将其置于 SYN-ACK 报文中的 Fast Open 选项以发回客户端；\n- 客户端收到 SYN-ACK 后，本地缓存 Fast Open 选项中的 Cookie。\n\n所以，第一次客户端和服务端通信的时候，还是需要正常的三次握手流程。随后，客户端就有了 Cookie 这个东西，它可以用来向服务器 TCP 证明先前与客户端 IP 地址的三向握手已成功完成。\n\n对于客户端与服务端的后续通信，客户端可以在第一次握手的时候携带应用数据，从而达到绕过三次握手发送数据的效果，整个过程如下图：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/fc452688b9351e0cabf60212dde3f21e.png)\n\n我详细介绍下这个过程：\n\n- 客户端发送 SYN 报文，该报文可以携带「应用数据」以及此前记录的 Cookie；\n- 支持 TCP Fast Open 的服务器会对收到 Cookie 进行校验：如果 Cookie 有效，服务器将在 SYN-ACK 报文中对 SYN 和「数据」进行确认，服务器随后将「应用数据」递送给对应的应用程序；如果 Cookie 无效，服务器将丢弃 SYN 报文中包含的「应用数据」，且其随后发出的 SYN-ACK 报文将只确认 SYN 的对应序列号；\n- **如果服务器接受了 SYN 报文中的「应用数据」，服务器可在握手完成之前发送「响应数据」，这就减少了握手带来的 1 个 RTT 的时间消耗**；\n- 客户端将发送 ACK 确认服务器发回的 SYN 以及「应用数据」，但如果客户端在初始的 SYN 报文中发送的「应用数据」没有被确认，则客户端将重新发送「应用数据」；\n- 此后的 TCP 连接的数据传输过程和非 TCP Fast Open 的正常情况一致。\n\n所以，如果客户端和服务端同时支持 TCP Fast Open 功能，那么在完成首次通信过程后，后续客户端与服务端 的通信则可以绕过三次握手发送数据，这就减少了握手带来的 1 个 RTT 的时间消耗。\n\n## TLSv1.3\n\n\u003e 说完 TCP Fast Open，再来看看 TLSv1.3。\n\n在最开始的时候，我也提到 TLSv1.3 握手过程只需 1-RTT 的时间，它到整个握手过程，如下图：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/1fd5ba4000f82613fdd70cab6da4b9cb.png)\n\nTCP 连接的第三次握手是可以携带数据的，如果客户端在第三次握手发送了 TLSv1.3 第一次握手数据，是不是就表示「*HTTPS 中的 TLS 握手过程可以同时进行三次握手*」？。\n\n不是的，因为服务端只有在收到客户端的 TCP 的第三次握手后，才能和客户端进行后续 TLSv1.3 握手。\n\nTLSv1.3 还有个更厉害到地方在于**会话恢复**机制，在**重连 TLvS1.3 只需要 0-RTT**，用“pre_shared_key”和“early_data”扩展，在 TCP 连接后立即就建立安全连接发送加密消息，过程如下图：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/59539201f006d7dc0a06333617e5ea85.png)\n\n## TCP Fast Open + TLSv1.3 \n\n在前面我们知道，客户端和服务端同时支持 TCP Fast Open 功能的情况下，**在第二次以后到通信过程中，客户端可以绕过三次握手直接发送数据，而且服务端也不需要等收到第三次握手后才发送数据。**\n\n如果 HTTPS 的 TLS 版本是 1.3，那么 TLS 过程只需要 1-RTT。\n\n**因此如果「TCP Fast Open + TLSv1.3」情况下，在第二次以后的通信过程中，TLS 和 TCP 的握手过程是可以同时进行的。**\n\n**如果基于 TCP Fast Open 场景下的 TLSv1.3 0-RTT 会话恢复过程，不仅 TLS 和 TCP 的握手过程是可以同时进行的，而且 HTTP 请求也可以在这期间内一同完成。**\n\n## 总结\n\n最后做个总结。\n\n「HTTPS 是先进行 TCP 三次握手，再进行 TLSv1.2 四次握手」，这句话一点问题都没有，怀疑这句话是错的人，才有问题。\n\n「HTTPS 中的 TLS 握手过程可以同时进行三次握手」，这个场景是可能存在到，但是在没有说任何前提条件，而说这句话就等于耍流氓。需要下面这两个条件同时满足才可以：\n\n- **客户端和服务端都开启了 TCP Fast Open 功能，且 TLS 版本是 1.3；**\n- **客户端和服务端已经完成过一次通信；**\n\n怎么样，那位“面试官”学废了吗？\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_tw_reuse_close":{"title":"tcp_tw_reuse_close","content":"# 4.14 tcp_tw_reuse 为什么默认是关闭的？\n\n大家好，我是小林。\n\n上周有个读者在面试微信的时候，**被问到既然打开 net.ipv4.tcp_tw_reuse 参数可以快速复用处于 TIME_WAIT 状态的 TCP 连接，那为什么 Linux 默认是关闭状态呢？**\n\n![图片](https://img-blog.csdnimg.cn/img_convert/23f1aea82a0b7c37f1031524600626f1.png)\n\n![图片](https://img-blog.csdnimg.cn/img_convert/076e60b984028bf3ad762eb2bd7ed0f3.png)\n\n好家伙，真的问好细节！\n\n当时看到读者这个问题的时候，我也是一脸懵逼的，经过我的一番思考后，终于知道怎么回答这题了。\n\n其实这题在变相问「**如果 TIME_WAIT 状态持续时间过短或者没有，会有什么问题？**」\n\n因为开启 tcp_tw_reuse 参数可以快速复用处于 TIME_WAIT 状态的 TCP 连接时，相当于缩短了 TIME_WAIT 状态的持续时间。\n\n可能有的同学会问说，使用 tcp_tw_reuse 快速复用处于 TIME_WAIT 状态的 TCP 连接时，是需要保证  net.ipv4.tcp_timestamps 参数是开启的（默认是开启的），而 tcp_timestamps 参数可以避免旧连接的延迟报文，这不是解决了没有 TIME_WAIT 状态时的问题了吗？\n\n是解决部分问题，但是不能完全解决，接下来，我跟大家聊聊这个问题。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/d17df1a39a750c33948062ecfc9a8d32.png)\n\n## 什么是 TIME_WAIT 状态？\n\nTCP 四次挥手过程，如下图：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/e973a17cb5b1092085ca1bbcd7083559.png)图片\n\n- 客户端打算关闭连接，此时会发送一个 TCP 首部 `FIN` 标志位被置为 `1`的报文，也即 `FIN` 报文，之后客户端进入 `FIN_WAIT_1` 状态。\n- 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，接着服务端进入 `CLOSED_WAIT` 状态。\n- 客户端收到服务端的 `ACK` 应答报文后，之后进入 `FIN_WAIT_2` 状态。\n- 等待服务端处理完数据后，也向客户端发送 `FIN` 报文，之后服务端进入 `LAST_ACK` 状态。\n- 客户端收到服务端的 `FIN` 报文后，回一个 `ACK` 应答报文，之后进入 `TIME_WAIT` 状态\n- 服务器收到了 `ACK` 应答报文后，就进入了 `CLOSE` 状态，至此服务端已经完成连接的关闭。\n- 客户端在经过 `2MSL` 一段时间后，自动进入 `CLOSE` 状态，至此客户端也完成连接的关闭。\n\n你可以看到，两个方向都需要**一个 FIN 和一个 ACK**，因此通常被称为**四次挥手**。\n\n这里一点需要注意是：**主动关闭连接的，才有 TIME_WAIT 状态。**\n\n可以看到，TIME_WAIT 是「主动关闭方」断开连接时的最后一个状态，该状态会持续 ***2MSL(Maximum Segment Lifetime)\\*** 时长，之后进入CLOSED 状态。\n\nMSL 指的是 TCP 协议中任何报文在网络上最大的生存时间，任何超过这个时间的数据都将被丢弃。虽然 RFC 793 规定 MSL 为 2 分钟，但是在实际实现的时候会有所不同，比如 Linux 默认为 30 秒，那么 2MSL 就是 60 秒。\n\nMSL 是由网络层的 IP 包中的 TTL 来保证的，TTL 是 IP 头部的一个字段，用于设置一个数据报可经过的路由器的数量上限。报文每经过一次路由器的转发，IP 头部的 TTL 字段就会减 1，减到 0 时报文就被丢弃。\n\nMSL 与 TTL 的区别：MSL 的单位是时间，而 TTL 是经过路由跳数。所以 **MSL 应该要大于等于 TTL 消耗为 0 的时间**，以确保报文已被自然消亡。\n\n**TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了**。\n\n## 为什么要设计 TIME_WAIT 状态？\n\n设计 TIME_WAIT 状态，主要有两个原因：\n\n- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；\n- 保证「被动关闭连接」的一方，能被正确的关闭；\n\n#### 原因一：防止历史连接中的数据，被后面相同四元组的连接错误的接收\n\n为了能更好的理解这个原因，我们先来了解序列号（SEQ）和初始序列号（ISN）。\n\n- **序列号**，是 TCP 一个头部字段，标识了 TCP 发送端到 TCP 接收端的数据流的一个字节，因为 TCP 是面向字节流的可靠协议，为了保证消息的顺序性和可靠性，TCP 为每个传输方向上的每个字节都赋予了一个编号，以便于传输成功后确认、丢失后重传以及在接收端保证不会乱序。**序列号是一个 32 位的无符号数，因此在到达 4G 之后再循环回到 0**。\n- **初始序列号**，在 TCP 建立连接的时候，客户端和服务端都会各自生成一个初始序列号，它是基于时钟生成的一个随机数，来保证每个连接都拥有不同的初始序列号。**初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时**。\n\n给大家抓了一个包，下图中的 Seq 就是序列号，其中红色框住的分别是客户端和服务端各自生成的初始序列号。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/b70ee2f17636deeb3930010b6dcdabb7.png)\n\n通过前面我们知道，**序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据**。\n\n假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f1ba45cdb7d772ccd12dc604dee26c91.png)\n\n\n\n- 服务端在关闭连接之前发送的 `SEQ = 301` 报文，被网络延迟了。\n- 接着，服务端以相同的四元组重新打开了新连接，前面被延迟的 `SEQ = 301` 这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，因此客户端会正常接收这个数据报文，但是这个数据报文是上一个连接残留下来的，这样就产生数据错乱等严重的问题。\n\n为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 `2MSL` 时长，这个时间**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。**\n\n#### 原因二：保证「被动关闭连接」的一方，能被正确的关闭\n\n如果客户端（主动关闭方）最后一次 ACK 报文（第四次挥手）在网络中丢失了，那么按照 TCP 可靠性原则，服务端（被动关闭方）会重发 FIN 报文。\n\n假设客户端没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSED 状态，如果该  ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/8016c9f9b875649a5ab8bdd245c34729.png)\n\n服务端收到这个 RST 并将其解释为一个错误（Connection reset by peer），这对于一个可靠的协议来说不是一个优雅的终止方式。\n\n为了防止这种情况出现，客户端必须等待足够长的时间，确保服务端能够收到 ACK，如果服务端没有收到 ACK，那么就会触发 TCP 重传机制，服务端会重新发送一个 FIN，这样一去一来刚好两个 MSL 的时间。\n\n![TIME-WAIT 时间正常，确保了连接正常关闭](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/TIME-WAIT连接正常关闭.drawio.png)\n\n客户端在收到服务端重传的 FIN 报文时，TIME_WAIT 状态的等待时间，会重置回 2MSL。\n\n## tcp_tw_reuse 是什么？\n\n在 Linux  操作系统下，TIME_WAIT 状态的持续时间是 60 秒，这意味着这 60 秒内，客户端一直会占用着这个端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768~61000 ，也可以通过如下参数设置指定范围：\n\n```\n net.ipv4.ip_local_port_range\n```\n\n**如果客户端（主动关闭连接方）的 TIME_WAIT 状态过多**，占满了所有端口资源，那么就无法对「目的 IP+ 目的 PORT」都一样的服务器发起连接了，但是被使用的端口，还是可以继续对另外一个服务器发起连接的。具体可以看我这篇文章：[客户端的端口可以重复使用吗？](https://xiaolincoding.com/network/3_tcp/port.html#%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%AF%E4%BB%A5%E9%87%8D%E5%A4%8D%E4%BD%BF%E7%94%A8%E5%90%97)\n\n因此，客户端（主动关闭连接方）都是和「目的 IP+ 目的 PORT 」都一样的服务器建立连接的话，当客户端的 TIME_WAIT 状态连接过多的话，就会受端口资源限制，如果占满了所有端口资源，那么就无法再跟「目的 IP+ 目的 PORT」都一样的服务器建立连接了。\n\n不过，即使是在这种场景下，只要连接的是不同的服务器，端口是可以重复使用的，所以客户端还是可以向其他服务器发起连接的，这是因为内核在定位一个连接的时候，是通过四元组（源IP、源端口、目的IP、目的端口）信息来定位的，并不会因为客户端的端口一样，而导致连接冲突。\n\n好在，Linux 操作系统提供了两个可以系统参数来快速回收处于 TIME_WAIT 状态的连接，这两个参数都是默认关闭的：\n\n- net.ipv4.tcp_tw_reuse，如果开启该选项的话，客户端（连接发起方） 在调用 connect() 函数时，**如果内核选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态，如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了**。所以该选项只适用于连接发起方。\n- net.ipv4.tcp_tw_recycle，如果开启该选项的话，允许处于 TIME_WAIT 状态的连接被快速回收，该参数在 **NAT 的网络下是不安全的**！详细见这篇文章介绍：[SYN 报文什么时候情况下会被丢弃？](https://xiaolincoding.com/network/3_tcp/syn_drop.html)\n\n要使得上面这两个参数生效，有一个前提条件，就是要打开 TCP 时间戳，即 net.ipv4.tcp_timestamps=1（默认即为 1）。\n\n开启了 tcp_timestamps 参数，TCP 头部就会使用时间戳选项，它有两个好处，**一个是便于精确计算 RTT ，另一个是能防止序列号回绕（PAWS）**，我们先来介绍这个功能。\n\n序列号是一个 32 位的无符号整型，上限值是 4GB，超过 4GB 后就需要将序列号回绕进行重用。这在以前网速慢的年代不会造成什么问题，但在一个速度足够快的网络中传输大量数据时，序列号的回绕时间就会变短。如果序列号回绕的时间极短，我们就会再次面临之前延迟的报文抵达后序列号依然有效的问题。\n\n为了解决这个问题，就需要有 TCP 时间戳。\n\n试看下面的示例，假设 TCP 的发送窗口是 1 GB，并且使用了时间戳选项，发送方会为每个 TCP 报文分配时间戳数值，我们假设每个报文时间加 1，然后使用这个连接传输一个 6GB 大小的数据流。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/bf004909d9e44c3bc740737ced6731a0.png)\n\n32 位的序列号在时刻 D 和 E 之间回绕。假设在时刻B有一个报文丢失并被重传，又假设这个报文段在网络上绕了远路并在时刻 F 重新出现。如果 TCP 无法识别这个绕回的报文，那么数据完整性就会遭到破坏。\n\n使用时间戳选项能够有效的防止上述问题，如果丢失的报文会在时刻 F 重新出现，由于它的时间戳为 2，小于最近的有效时间戳（5 或 6），因此防回绕序列号算法（PAWS）会将其丢弃。\n\n防回绕序列号算法要求连接双方维护最近一次收到的数据包的时间戳（Recent TSval），每收到一个新数据包都会读取数据包中的时间戳值跟 Recent TSval 值做比较，**如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包**。\n\n## 为什么 tcp_tw_reuse  默认是关闭的？\n\n通过前面这么多铺垫，终于可以说这个问题了。\n\n开启 tcp_tw_reuse 会有什么风险呢？我觉得会有 2 个问题。\n\n### 第一个问题\n\n我们知道开启 tcp_tw_reuse 的同时，也需要开启 tcp_timestamps，意味着可以用时间戳的方式有效的判断回绕序列号的历史报文。\n\n但是，在看我看了防回绕序列号函数的源码后，发现对于 **RST 报文的时间戳即使过期了，只要 RST 报文的序列号在对方的接收窗口内，也是能被接受的**。\n\n下面 tcp_validate_incoming 函数就是验证接收到的 TCP 报文是否合格的函数，其中第一步就会进行 PAWS 检查，由 tcp_paws_discard 函数负责。\n\n```c\nstatic bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th, int syn_inerr)\n{\n    struct tcp_sock *tp = tcp_sk(sk);\n\n    /* RFC1323: H1. Apply PAWS check first. */\n    if (tcp_fast_parse_options(sock_net(sk), skb, th, tp) \u0026\u0026\n        tp-\u003erx_opt.saw_tstamp \u0026\u0026\n        tcp_paws_discard(sk, skb)) {\n        if (!th-\u003erst) {\n            ....\n            goto discard;\n        }\n        /* Reset is accepted even if it did not pass PAWS. */\n    }\n```\n\n当 tcp_paws_discard 返回 true，就代表报文是一个历史报文，于是就要丢弃这个报文。但是在丢掉这个报文的时候，会先判断是不是 RST 报文，如果不是 RST 报文，才会将报文丢掉。也就是说，即使 RST 报文是一个历史报文，并不会被丢弃。\n\n假设有这样的场景，如下图：\n\n![](https://img-blog.csdnimg.cn/img_convert/0df2003d41ec0ef23844975a85cfb722.png)\n\n过程如下：\n\n- 客户端向一个还没有被服务端监听的端口发起了 HTTP 请求，接着服务端就会回 RST 报文给对方，很可惜的是 **RST 报文被网络阻塞了**。\n- 由于客户端迟迟没有收到 TCP 第二次握手，于是重发了 SYN 包，与此同时服务端已经开启了服务，监听了对应的端口。于是接下来，客户端和服务端就进行了 TCP 三次握手、数据传输（HTTP应答-响应）、四次挥手。\n- 因为**客户端开启了 tcp_tw_reuse，于是快速复用 TIME_WAIT 状态的端口，又与服务端建立了一个与刚才相同的四元组的连接**。\n- 接着，**前面被网络延迟 RST 报文这时抵达了客户端，而且 RST 报文的序列号在客户端的接收窗口内，由于防回绕序列号算法不会防止过期的 RST，所以 RST 报文会被客户端接受了，于是客户端的连接就断开了**。\n\n上面这个场景就是开启 tcp_tw_reuse 风险，**因为快速复用 TIME_WAIT 状态的端口，导致新连接可能被回绕序列号的 RST 报文断开了，而如果不跳过 TIME_WAIT 状态，而是停留 2MSL 时长，那么这个 RST 报文就不会出现下一个新的连接**。\n\n可能大家会有这样的疑问，为什么 PAWS 检查要放过过期的 RST 报文。我翻了 RFC 1323 ，里面有一句提到：\n\n*It is recommended that RST segments NOT carry timestamps, and that RST segments be acceptable regardless of their timestamp.  Old duplicate RST segments should be exceedingly unlikely, and their cleanup function should take precedence over timestamps.*\n\n大概的意思：*建议 RST 段不携带时间戳，并且无论其时间戳如何，RST 段都是可接受的。老的重复的 RST 段应该是极不可能的，并且它们的清除功能应优先于时间戳。*\n\nRFC 1323 提到说收历史的 RST 报文是极不可能，之所以有这样的想法是因为 TIME_WAIT 状态持续的 2MSL 时间，足以让连接中的报文在网络中自然消失，所以认为按正常操作来说是不会发生的，因此认为清除连接优先于时间戳。\n\n而我前面提到的案例，是因为开启了 tcp_tw_reuse 状态，跳过了 TIME_WAIT 状态，才发生的事情。\n\n有同学会说，都经过一个 HTTP 请求了，延迟的 RST 报文竟然还会存活？\n\n一个 HTTP 请求其实很快的，比如我下面这个抓包，只需要 0.2 秒就完成了，远小于 MSL，所以延迟的 RST 报文存活是有可能的。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/2ac40ca1757888b7154a2baa8bbb9885.png)\n\n### 第二个问题\n\n开启 tcp_tw_reuse 来快速复用 TIME_WAIT 状态的连接，如果第四次挥手的 ACK 报文丢失了，服务端会触发超时重传，重传第三次挥手报文，处于 syn_sent 状态的客户端收到服务端重传第三次挥手报文，则会回 RST 给服务端。如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/tcp_tw_reuse第二个问题.drawio.png)\n\n这时候有同学就问了，如果 TIME_WAIT 状态被快速复用后，刚好第四次挥手的 ACK 报文丢失了，那客户端复用 TIME_WAIT 状态后发送的 SYN 报文被处于 last_ack 状态的服务端收到了会发生什么呢？\n\n处于 last_ack 状态的服务端收到了 SYN 报文后，会回复确认号与服务端上一次发送 ACK 报文一样的 ACK 报文，这个 ACK 报文称为 [Challenge ACK](https://xiaolincoding.com/network/3_tcp/challenge_ack.html)，并不是确认收到 SYN 报文。\n\n处于 syn_sent 状态的客户端收到服务端的  [Challenge ACK](https://xiaolincoding.com/network/3_tcp/challenge_ack.html) 后，发现不是自己期望收到的确认号，于是就会回复 RST 报文，服务端收到后，就会断开连接。\n\n## 总结\n\ntcp_tw_reuse 的作用是让客户端快速复用处于 TIME_WAIT 状态的端口，相当于跳过了 TIME_WAIT 状态，这可能会出现这样的两个问题：\n\n- 历史 RST 报文可能会终止后面相同四元组的连接，因为 PAWS 检查到即使 RST 是过期的，也不会丢弃。\n- 如果第四次挥手的 ACK 报文丢失了，有可能被动关闭连接的一方不能被正常的关闭;\n\n虽然 TIME_WAIT 状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。\n\n《UNIX网络编程》一书中却说道：**TIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它**。\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/tcp_unplug_the_network_cable":{"title":"tcp_unplug_the_network_cable","content":"# 4.13 拔掉网线后， 原本的 TCP 连接还存在吗？\n\n大家好，我是小林。\n\n今天，聊一个有趣的问题：**拔掉网线几秒，再插回去，原本的 TCP 连接还存在吗？**\n\n可能有的同学会说，网线都被拔掉了，那说明物理层被断开了，那在上层的传输层理应也会断开，所以原本的 TCP 连接就不会存在的了。就好像， 我们拨打有线电话的时候，如果某一方的电话线被拔了，那么本次通话就彻底断了。\n\n真的是这样吗？\n\n上面这个逻辑就有问题。问题在于，错误的认为拔掉网线这个动作会影响传输层，事实上并不会影响。\n\n实际上，TCP 连接在 Linux 内核中是一个名为 `struct socket` 的结构体，该结构体的内容包含 TCP 连接的状态等信息。当拔掉网线的时候，操作系统并不会变更该结构体的任何内容，所以 TCP 连接的状态也不会发生改变。\n\n我在我的电脑上做了个小实验，我用 ssh 终端连接了我的云服务器，然后我通过断开 wifi 的方式来模拟拔掉网线的场景，此时查看 TCP 连接的状态没有发生变化，还是处于 ESTABLISHED 状态。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/fff358407ee92aeea1e17386191a5d18.png)\n\n通过上面这个实验结果，我们知道了，拔掉网线这个动作并不会影响 TCP 连接的状态。\n\n接下来，要看拔掉网线后，双方做了什么动作。\n\n所以， 针对这个问题，要分场景来讨论：\n\n- 拔掉网线后，有数据传输；\n- 拔掉网线后，没有数据传输；\n\n## 拔掉网线后，有数据传输\n\n在客户端拔掉网线后，服务端向客户端发送的数据报文会得不到任何的响应，在等待一定时长后，服务端就会触发**超时重传**机制，重传未得到响应的数据报文。\n\n**如果在服务端重传报文的过程中，客户端刚好把网线插回去了**，由于拔掉网线并不会改变客户端的 TCP 连接状态，并且还是处于 ESTABLISHED 状态，所以这时客户端是可以正常接收服务端发来的数据报文的，然后客户端就会回 ACK 响应报文。\n\n此时，客户端和服务端的 TCP 连接依然存在的，就感觉什么事情都没有发生。\n\n但是，**如果如果在服务端重传报文的过程中，客户端一直没有将网线插回去**，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，于是服务端的 TCP 连接就会断开。\n\n而等客户端插回网线后，如果客户端向服务端发送了数据，由于服务端已经没有与客户端相同四元祖的 TCP 连接了，因此服务端内核就会回复 RST 报文，客户端收到后就会释放该 TCP 连接。\n\n此时，客户端和服务端的 TCP 连接都已经断开了。\n\n\u003e 那 TCP 的数据报文具体重传几次呢？\n\n在 Linux 系统中，提供了一个叫 tcp_retries2 配置项，默认值是 15，如下图：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f92c00c7e9cd01e89326e943232e5f04.png)\n\n这个内核参数是控制，在 TCP 连接建立的情况下，超时重传的最大次数。\n\n不过 tcp_retries2 设置了 15 次，并不代表 TCP 超时重传了 15 次才会通知应用程序终止该 TCP 连接，**内核会根据 tcp_retries2 设置的值，计算出一个 timeout**（*如果 tcp_retries2 =15，那么计算得到的 timeout = 924600 ms*），**如果重传间隔超过这个 timeout，则认为超过了阈值，就会停止重传，然后就会断开 TCP 连接**。\n\n在发生超时重传的过程中，每一轮的超时时间（RTO）都是**倍数增长**的，比如如果第一轮 RTO 是 200 毫秒，那么第二轮 RTO 是 400 毫秒，第三轮 RTO 是 800 毫秒，以此类推。\n\n而 RTO 是基于 RTT（一个包的往返时间） 来计算的，如果 RTT 较大，那么计算出来的 RTO 就越大，那么经过几轮重传后，很快就达到了上面的 timeout 值了。\n\n举个例子，如果 tcp_retries2 =15，那么计算得到的 timeout = 924600 ms，如果重传总间隔时长达到了 timeout 就会停止重传，然后就会断开 TCP 连接：\n\n- 如果 RTT 比较小，那么 RTO 初始值就约等于下限 200ms，也就是第一轮的超时时间是 200 毫秒，由于 timeout 总时长是 924600 ms，表现出来的现象刚好就是重传了 15 次，超过了 timeout 值，从而断开 TCP 连接\n- 如果 RTT 比较大，假设 RTO 初始值计算得到的是 1000 ms，也就是第一轮的超时时间是 1 秒，那么根本不需要重传 15 次，重传总间隔就会超过 924600 ms。\n\n最小 RTO 和最大 RTO 是在 Linux 内核中定义好了：\n\n```c\n#define TCP_RTO_MAX ((unsigned)(120*HZ))\n#define TCP_RTO_MIN ((unsigned)(HZ/5))\n```\n\nLinux 2.6+ 使用 1000 毫秒的 HZ，因此`TCP_RTO_MIN`约为 200 毫秒，`TCP_RTO_MAX`约为 120 秒。\n\n如果`tcp_retries`设置为`15`，且  RTT 比较小，那么 RTO 初始值就约等于下限 200ms，这意味着**它需要 924.6 秒**才能将断开的 TCP 连接通知给上层（即应用程序），每一轮的 RTO 增长关系如下表格：\n\n![](https://img-blog.csdnimg.cn/img_convert/10fa6882db83aee68f246c04fcb7d760.png)\n\n## 拔掉网线后，没有数据传输\n\n针对拔掉网线后，没有数据传输的场景，还得看是否开启了 TCP keepalive 机制 （TCP 保活机制）。\n\n如果**没有开启** TCP keepalive 机制，在客户端拔掉网线后，并且双方都没有进行数据传输，那么客户端和服务端的 TCP 连接将会一直保持存在。\n\n而如果**开启**了 TCP keepalive 机制，在客户端拔掉网线后，即使双方都没有进行数据传输，在持续一段时间后，TCP 就会发送探测报文：\n\n- 如果**对端是正常工作**的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。\n- 如果**对端主机宕机**（*注意不是进程崩溃，进程崩溃后操作系统在回收进程资源的时候，会发送 FIN 报文，而主机宕机则是无法感知的，所以需要 TCP 保活机制来探测对方是不是发生了主机宕机*），或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。\n\n所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活。\n\n\u003e TCP keepalive 机制具体是怎么样的？\n\n这个机制的原理是这样的：\n\n定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。\n\n在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：\n\n```\nnet.ipv4.tcp_keepalive_time=7200\nnet.ipv4.tcp_keepalive_intvl=75  \nnet.ipv4.tcp_keepalive_probes=9\n```\n\n- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制\n- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；\n- tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。\n\n也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。\n\n![](https://img-blog.csdnimg.cn/img_convert/46906e588260607680db43a68fe00278.png)\n\n注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。\n\n\u003e TCP keepalive 机制探测的时间也太长了吧？\n\n对的，是有点长。\n\nTCP keepalive  是 **TCP 层（内核态）** 实现的，它是给所有基于 TCP 传输协议的程序一个兜底的方案。\n\n实际上，我们应用层可以自己实现一套探测机制，可以在较短的时间内，探测到对方是否存活。\n\n比如，web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c881f163091a4c6427d68b7144c3a980.png)\n\n## 总结\n\n客户端拔掉网线后，并不会直接影响 TCP 连接状态。所以，拔掉网线后，TCP 连接是否还会存在，关键要看拔掉网线之后，有没有进行数据传输。\n\n有数据传输的情况：\n\n- 在客户端拔掉网线后，如果服务端发送了数据报文，那么在服务端重传次数没有达到最大值之前，客户端就插回了网线，那么双方原本的 TCP 连接还是能正常存在，就好像什么事情都没有发生。\n- 在客户端拔掉网线后，如果服务端发送了数据报文，在客户端插回网线之前，服务端重传次数达到了最大值时，服务端就会断开 TCP 连接。等到客户端插回网线后，向服务端发送了数据，因为服务端已经断开了与客户端相同四元组的 TCP 连接，所以就会回 RST 报文，客户端收到后就会断开 TCP 连接。至此， 双方的 TCP 连接都断开了。\n\n没有数据传输的情况：\n\n- 如果双方都没有开启 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，那么客户端和服务端的 TCP 连接状态将会一直保持存在。\n- 如果双方都开启了 TCP keepalive 机制，那么在客户端拔掉网线后，如果客户端一直不插回网线，TCP keepalive 机制会探测到对方的 TCP 连接没有存活，于是就会断开 TCP 连接。而如果在 TCP 探测期间，客户端插回了网线，那么双方原本的 TCP 连接还是能正常存在。\n\n除了客户端拔掉网线的场景，还有客户端「[主机宕机和进程崩溃](https://xiaolincoding.com/network/3_tcp/tcp_down_and_crash.html)」的两种场景。\n\n第一个场景，客户端宕机这件事跟拔掉网线是一样无法被服务端的感知的，所以如果在没有数据传输，并且没有开启 TCP keepalive 机制时，，**服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态**，直到服务端重启进程。\n\n所以，我们可以得知一个点。在没有使用 TCP 保活机制，且双方不传输数据的情况下，一方的 TCP 连接处在 ESTABLISHED 状态时，并不代表另一方的 TCP 连接还一定是正常的。\n\n第二个场景，客户端的进程崩溃后，客户端的内核就会向服务端发送 FIN 报文，**与服务端进行四次挥手**。\n\n所以，即使没有开启 TCP keepalive，且双方也没有数据交互的情况下，如果其中一方的进程发生了崩溃，这个过程操作系统是可以感知的到的，于是就会发送 FIN 报文给对方，然后与对方进行 TCP 四次挥手。\n\n完！\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/3_tcp/time_wait_recv_syn":{"title":"time_wait_recv_syn","content":"# 4.11 在 TIME_WAIT 状态的 TCP 连接，收到 SYN 后会发生什么？\n\n大家好，我是小林。\n\n周末跟朋友讨论了一些 TCP 的问题，在查阅《Linux 服务器高性能编程》这本书的时候，发现书上写了这么一句话：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/65739ee668999bda02aa9236aad6437f.png)\n\n书上说，处于 TIME_WAIT 状态的连接，在收到相同四元组的 SYN 后，会回 RST 报文，对方收到后就会断开连接。\n\n书中作者只是提了这么一句话，没有给予源码或者抓包图的证据。\n\n起初，我看到也觉得这个逻辑也挺符合常理的，但是当我自己去啃了 TCP 源码后，发现并不是这样的。\n\n所以，今天就来讨论下这个问题，「**在 TCP 正常挥手过程中，处于 TIME_WAIT 状态的连接，收到相同四元组的 SYN 后会发生什么？**」\n\n问题现象如下图，左边是服务端，右边是客户端：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/74b53919396dcda634cfd5b5795cbf16.png)\n\n## 先说结论\n\n在跟大家分析 TCP 源码前，我先跟大家直接说下结论。\n\n针对这个问题，**关键是要看 SYN 的「序列号和时间戳」是否合法**，因为处于 TIME_WAIT 状态的连接收到 SYN 后，会判断 SYN 的「序列号和时间戳」是否合法，然后根据判断结果的不同做不同的处理。\n\n先跟大家说明下， 什么是「合法」的 SYN？\n\n- **合法 SYN**：客户端的  SYN 的「序列号」比服务端「期望下一个收到的序列号」要**大**，**并且** SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要**大**。\n- **非法 SYN**：客户端的  SYN 的「序列号」比服务端「期望下一个收到的序列号」要**小**，**或者** SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要**小**。\n\n上面 SYN 合法判断是基于双方都开启了 TCP 时间戳机制的场景，如果双方都没有开启 TCP 时间戳机制，则 SYN 合法判断如下：\n\n- **合法 SYN**：客户端的  SYN 的「序列号」比服务端「期望下一个收到的序列号」要**大**。\n- **非法 SYN**：客户端的  SYN 的「序列号」比服务端「期望下一个收到的序列号」要**小**。\n\n### 收到合法 SYN\n\n如果处于 TIME_WAIT 状态的连接收到「合法的 SYN 」后，**就会重用此四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程**。\n\n用下图作为例子，双方都启用了 TCP 时间戳机制，TSval 是发送报文时的时间戳：\n\n![图片](https://img-blog.csdnimg.cn/img_convert/39d0d04adf72fe3d37623acff9ae2507.png)\n\n上图中，在收到第三次挥手的 FIN 报文时，会记录该报文的 TSval （21），用 ts_recent 变量保存。然后会计算下一次期望收到的序列号，本次例子下一次期望收到的序列号就是 301，用 rcv_nxt 变量保存。\n\n处于 TIME_WAIT 状态的连接收到 SYN 后，**因为 SYN 的 seq（400） 大于 rcv_nxt（301），并且 SYN 的 TSval（30） 大于 ts_recent（21），所以是一个「合法的 SYN」，于是就会重用此四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程。**\n\n### 收到非法的 SYN\n\n如果处于 TIME_WAIT 状态的连接收到「非法的 SYN 」后，就会**再回复一个第四次挥手的 ACK 报文，客户端收到后，发现并不是自己期望收到确认号（ack num），就回 RST 报文给服务端**。\n\n用下图作为例子，双方都启用了 TCP 时间戳机制，TSval 是发送报文时的时间戳：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/tcp/tw收到不合法.png)\n\n上图中，在收到第三次挥手的 FIN 报文时，会记录该报文的 TSval （21），用 ts_recent 变量保存。然后会计算下一次期望收到的序列号，本次例子下一次期望收到的序列号就是 301，用 rcv_nxt 变量保存。\n\n处于 TIME_WAIT 状态的连接收到 SYN 后，**因为 SYN 的 seq（200） 小于 rcv_nxt（301），所以是一个「非法的 SYN」，就会再回复一个与第四次挥手一样的 ACK 报文，客户端收到后，发现并不是自己期望收到确认号，就回 RST 报文给服务端**。\n\n\u003e PS：这里先埋一个疑问，处于 TIME_WAIT 状态的连接，收到 RST 会断开连接吗？\n\n## 源码分析\n\n下面源码分析是基于 Linux 4.2 版本的内核代码。\n\nLinux 内核在收到 TCP 报文后，会执行 `tcp_v4_rcv` 函数，在该函数和 TIME_WAIT 状态相关的主要代码如下：\n\n```c\nint tcp_v4_rcv(struct sk_buff *skb)\n{\n  struct sock *sk;\n ...\n  //收到报文后，会调用此函数，查找对应的 sock\n sk = __inet_lookup_skb(\u0026tcp_hashinfo, skb, __tcp_hdrlen(th), th-\u003esource,\n          th-\u003edest, sdif, \u0026refcounted);\n if (!sk)\n  goto no_tcp_socket;\n\nprocess:\n  //如果连接的状态为 time_wait，会跳转到 do_time_wait\n if (sk-\u003esk_state == TCP_TIME_WAIT)\n  goto do_time_wait;\n\n...\n\ndo_time_wait:\n  ...\n  //由tcp_timewait_state_process函数处理在 time_wait 状态收到的报文\n switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n    // 如果是TCP_TW_SYN，那么允许此 SYN 重建连接\n    // 即允许TIM_WAIT状态跃迁到SYN_RECV\n    case TCP_TW_SYN: {\n      struct sock *sk2 = inet_lookup_listener(....);\n      if (sk2) {\n          ....\n          goto process;\n      }\n    }\n    // 如果是TCP_TW_ACK，那么，返回记忆中的ACK\n    case TCP_TW_ACK:\n      tcp_v4_timewait_ack(sk, skb);\n      break;\n    // 如果是TCP_TW_RST直接发送RESET包\n    case TCP_TW_RST:\n      tcp_v4_send_reset(sk, skb);\n      inet_twsk_deschedule_put(inet_twsk(sk));\n      goto discard_it;\n     // 如果是TCP_TW_SUCCESS则直接丢弃此包，不做任何响应\n    case TCP_TW_SUCCESS:;\n }\n goto discard_it;\n}\n```\n\n该代码的过程：\n\n1. 接收到报文后，会调用 `__inet_lookup_skb()` 函数查找对应的 sock 结构；\n2. 如果连接的状态是 `TIME_WAIT`，会跳转到 do_time_wait 处理；\n3. 由 `tcp_timewait_state_process()` 函数来处理收到的报文，处理后根据返回值来做相应的处理。\n\n先跟大家说下，如果收到的 SYN 是合法的，`tcp_timewait_state_process()` 函数就会返回 `TCP_TW_SYN`，然后重用此连接。如果收到的 SYN 是非法的，`tcp_timewait_state_process()` 函数就会返回 `TCP_TW_ACK`，然后会回上次发过的 ACK。\n\n接下来，看 `tcp_timewait_state_process()` 函数是如何判断 SYN 包的。\n\n```c\nenum tcp_tw_status\ntcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb,\n      const struct tcphdr *th)\n{\n ...\n  //paws_reject 为 false，表示没有发生时间戳回绕\n  //paws_reject 为 true，表示发生了时间戳回绕\n bool paws_reject = false;\n\n tmp_opt.saw_tstamp = 0;\n  //TCP头中有选项且旧连接开启了时间戳选项\n if (th-\u003edoff \u003e (sizeof(*th) \u003e\u003e 2) \u0026\u0026 tcptw-\u003etw_ts_recent_stamp) { \n  //解析选项\n    tcp_parse_options(twsk_net(tw), skb, \u0026tmp_opt, 0, NULL);\n\n  if (tmp_opt.saw_tstamp) {\n   ...\n      //检查收到的报文的时间戳是否发生了时间戳回绕\n   paws_reject = tcp_paws_reject(\u0026tmp_opt, th-\u003erst);\n  }\n }\n\n....\n\n  //是SYN包、没有RST、没有ACK、时间戳没有回绕，并且序列号也没有回绕，\n if (th-\u003esyn \u0026\u0026 !th-\u003erst \u0026\u0026 !th-\u003eack \u0026\u0026 !paws_reject \u0026\u0026\n     (after(TCP_SKB_CB(skb)-\u003eseq, tcptw-\u003etw_rcv_nxt) ||\n      (tmp_opt.saw_tstamp \u0026\u0026 //新连接开启了时间戳\n       (s32)(tcptw-\u003etw_ts_recent - tmp_opt.rcv_tsval) \u003c 0))) { //时间戳没有回绕\n    // 初始化序列号\n    u32 isn = tcptw-\u003etw_snd_nxt + 65535 + 2; \n    if (isn == 0)\n      isn++;\n    TCP_SKB_CB(skb)-\u003etcp_tw_isn = isn;\n    return TCP_TW_SYN; //允许重用TIME_WAIT四元组重新建立连接\n }\n\n\n if (!th-\u003erst) {\n    // 如果时间戳回绕，或者报文里包含ack，则将 TIMEWAIT 状态的持续时间重新延长\n  if (paws_reject || th-\u003eack)\n    inet_twsk_schedule(tw, \u0026tcp_death_row, TCP_TIMEWAIT_LEN,\n        TCP_TIMEWAIT_LEN);\n\n     // 返回TCP_TW_ACK, 发送上一次的 ACK\n    return TCP_TW_ACK;\n }\n inet_twsk_put(tw);\n return TCP_TW_SUCCESS;\n}\n```\n\n如果双方启用了 TCP 时间戳机制，就会通过 `tcp_paws_reject()` 函数来判断时间戳是否发生了回绕，也就是「当前收到的报文的时间戳」是否大于「上一次收到的报文的时间戳」：\n\n- 如果大于，就说明没有发生时间戳绕回，函数返回 false。\n- 如果小于，就说明发生了时间戳回绕，函数返回 true。\n\n从源码可以看到，当收到 SYN 包后，如果该 SYN 包的时间戳没有发生回绕，也就是时间戳是递增的，并且 SYN 包的序列号也没有发生回绕，也就是 SYN 的序列号「大于」下一次期望收到的序列号。就会初始化一个序列号，然后返回 TCP_TW_SYN，接着就重用该连接，也就跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程。\n\n如果双方都没有启用 TCP 时间戳机制，就只需要判断 SYN 包的序列号有没有发生回绕，如果 SYN 的序列号大于下一次期望收到的序列号，就可以跳过 2MSL，重用该连接。\n\n如果 SYN 包是非法的，就会返回 TCP_TW_ACK，接着就会发送与上一次一样的 ACK 给对方。\n\n## 在 TIME_WAIT 状态，收到 RST 会断开连接吗？\n\n在前面我留了一个疑问，处于 TIME_WAIT 状态的连接，收到 RST 会断开连接吗？\n\n会不会断开，关键看 `net.ipv4.tcp_rfc1337` 这个内核参数（默认情况是为 0）：\n\n- 如果这个参数设置为 0， 收到 RST 报文会提前结束 TIME_WAIT 状态，释放连接。\n- 如果这个参数设置为 1， 就会丢掉 RST 报文。\n\n源码处理如下：\n\n```c\nenum tcp_tw_status\ntcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb,\n      const struct tcphdr *th)\n{\n....\n  //rst报文的时间戳没有发生回绕\n if (!paws_reject \u0026\u0026\n     (TCP_SKB_CB(skb)-\u003eseq == tcptw-\u003etw_rcv_nxt \u0026\u0026\n      (TCP_SKB_CB(skb)-\u003eseq == TCP_SKB_CB(skb)-\u003eend_seq || th-\u003erst))) {\n\n      //处理rst报文\n      if (th-\u003erst) {\n        //不开启这个选项，当收到 RST 时会立即回收tw，但这样做是有风险的\n        if (twsk_net(tw)-\u003eipv4.sysctl_tcp_rfc1337 == 0) {\n          kill:\n          //删除tw定时器，并释放tw\n          inet_twsk_deschedule_put(tw);\n          return TCP_TW_SUCCESS;\n        }\n      } else {\n        //将 TIMEWAIT 状态的持续时间重新延长\n        inet_twsk_reschedule(tw, TCP_TIMEWAIT_LEN);\n      }\n\n      ...\n      return TCP_TW_SUCCESS;\n    }\n}\n```\n\nTIME_WAIT 状态收到 RST 报文而释放连接，这样等于跳过 2MSL 时间，这么做还是有风险。\n\nsysctl_tcp_rfc1337 这个参数是在 rfc 1337 文档提出来的，目的是避免因为 TIME_WAIT 状态收到 RST 报文而跳过  2MSL 的时间，文档里也给出跳过  2MSL 时间会有什么潜在问题。\n\nTIME_WAIT 状态之所以要持续 2MSL 时间，主要有两个目的：\n\n- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；\n- 保证「被动关闭连接」的一方，能被正确的关闭；\n\n详细的为什么要设计 TIME_WAIT 状态，我在这篇有详细说明：[如果 TIME_WAIT 状态持续时间过短或者没有，会有什么问题？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==\u0026mid=2247502380\u0026idx=1\u0026sn=7b82818a5fb6f1127d17f0ded550c4bd\u0026scene=21#wechat_redirect)\n\n虽然 TIME_WAIT 状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。\n\n《UNIX网络编程》一书中却说道：**TIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它**。\n\n所以，我个人觉得将 `net.ipv4.tcp_rfc1337` 设置为 1 会比较安全。\n\n## 总结\n\n在 TCP 正常挥手过程中，处于 TIME_WAIT 状态的连接，收到相同四元组的 SYN 后会发生什么？\n\n如果双方开启了时间戳机制：\n\n- 如果客户端的  SYN 的「序列号」比服务端「期望下一个收到的序列号」要**大**，**并且**SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要**大**。那么就会重用该四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程。\n- 如果客户端的  SYN 的「序列号」比服务端「期望下一个收到的序列号」要**小**，**或者**SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要**小**。那么就会**再回复一个第四次挥手的 ACK 报文，客户端收到后，发现并不是自己期望收到确认号，就回 RST 报文给服务端**。\n\n在 TIME_WAIT 状态，收到 RST 会断开连接吗？\n\n- 如果 `net.ipv4.tcp_rfc1337` 参数为 0，则提前结束 TIME_WAIT 状态，释放连接。\n- 如果 `net.ipv4.tcp_rfc1337` 参数为 1，则会丢掉该 RST 报文。\n\n完！\n\n---\n\n最新的图解文章都在公众号首发，别忘记关注哦！！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/4_ip/ip_base":{"title":"ip_base","content":"#  5.1 IP 基础知识全家桶\n\n前段时间，有读者希望我写一篇关于 IP 分类地址、子网划分等的文章，他反馈常常混淆，摸不着头脑。\n\n那么，说来就来！而且要盘就盘全一点，顺便挑战下小林的图解功力，所以就来个 **IP 基础知识全家桶**。\n\n吃完这个 IP 基础知识全家桶，包你撑着肚子喊出：“**真香！**”\n\n不多说，直接上菜，共分为**三道菜**：\n\n- 首先是前菜 「 IP 基本认识 」\n- 其次是主菜 「IP 地址的基础知识」\n- 最后是点心 「IP 协议相关技术」\n\n![IP 基础知识全家桶](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/1.jpg)\n\n\n\u003e 为啥要比喻成菜？因为小林是**菜狗**（押韵不？）\n\n---\n\n## 前菜 —— IP 基本认识\n\nIP 在 TCP/IP 参考模型中处于第三层，也就是**网络层**。\n\n网络层的主要作用是：**实现主机与主机之间的通信，也叫点对点（end to end）通信。**\n\n![IP 的作用](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/2.jpg)\n\n\u003e 网络层与数据链路层有什么关系呢？\n\n有的小伙伴分不清 IP（网络层） 和 MAC （数据链路层）之间的区别和关系。\n\n其实很容易区分，在上面我们知道 IP 的作用是主机之间通信用的，而 **MAC 的作用则是实现「直连」的两个设备之间通信，而 IP 则负责在「没有直连」的两个网络之间进行通信传输。**\n\n举个生活的栗子，小林要去一个很远的地方旅行，制定了一个行程表，其间需先后乘坐飞机、地铁、公交车才能抵达目的地，为此小林需要买飞机票，地铁票等。\n\n飞机票和地铁票都是去往特定的地点的，每张票只能够在某一限定区间内移动，此处的「区间内」就如同通信网络中数据链路。\n\n在区间内移动相当于数据链路层，充当区间内两个节点传输的功能，区间内的出发点好比源 MAC 地址，目标地点好比目的 MAC 地址。\n\n整个旅游行程表就相当于网络层，充当远程定位的功能，行程的开始好比源 IP，行程的终点好比目的 IP 地址。\n\n![IP 的作用与 MAC 的作用](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/3.jpg)\n\n如果小林只有行程表而没有车票，就无法搭乘交通工具到达目的地。相反，如果除了车票而没有行程表，恐怕也很难到达目的地。因为小林不知道该坐什么车，也不知道该在哪里换乘。\n\n因此，只有两者兼备，既有某个区间的车票又有整个旅行的行程表，才能保证到达目的地。与此类似，**计算机网络中也需要「数据链路层」和「网络层」这个分层才能实现向最终目标地址的通信。**\n\n还有重要一点，旅行途中我们虽然不断变化了交通工具，但是旅行行程的起始地址和目的地址始终都没变。其实，在网络中数据包传输中也是如此，**源IP地址和目标IP地址在传输过程中是不会变化的，只有源 MAC 地址和目标 MAC 一直在变化。**\n\n---\n\n## 主菜 —— IP 地址的基础知识\n\n\n在 TCP/IP 网络通信时，为了保证能正常通信，每个设备都需要配置正确的 IP 地址，否则无法实现正常的通信。\n\nIP 地址（IPv4 地址）由 `32` 位正整数来表示，IP 地址在计算机是以二进制的方式处理的。\n\n而人类为了方便记忆采用了**点分十进制**的标记方式，也就是将 32 位 IP 地址以每 8 位为组，共分为 `4` 组，每组以「`.`」隔开，再将每组转换成十进制。\n\n![点分十进制](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/4.jpg)\n\n那么，IP 地址最大值也就是\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/5.jpg)\n\n也就说，最大允许 43 亿台计算机连接到网络。\n\n实际上，IP 地址并不是根据主机台数来配置的，而是以网卡。像服务器、路由器等设备都是有 2 个以上的网卡，也就是它们会有 2 个以上的 IP 地址。\n\n![每块网卡可以分配一个以上的IP地址](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/6.jpg)\n\n因此，让 43 亿台计算机全部连网其实是不可能的，更何况 IP 地址是由「网络标识」和「主机标识」这两个部分组成的，所以实际能够连接到网络的计算机个数更是少了很多。\n\n\u003e 可能有的小伙伴提出了疑问，现在不仅电脑配了 IP， 手机、IPad 等电子设备都配了 IP 呀，照理来说肯定会超过 43 亿啦，那是怎么能够支持这么多 IP 的呢？\n\n因为会根据一种可以更换 IP 地址的技术 `NAT`，使得可连接计算机数超过 43 亿台。 `NAT` 技术后续会进一步讨论和说明。\n\n### IP 地址的分类\n\n互联网诞生之初，IP 地址显得很充裕，于是计算机科学家们设计了**分类地址**。\n\nIP 地址分类成了 5 种类型，分别是 A 类、B 类、C 类、D 类、E 类。\n\n![IP 地址分类](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/7.jpg)\n\n上图中黄色部分为分类号，用以区分 IP 地址类别。\n\n\u003e 什么是 A、B、C 类地址？\n\n其中对于 A、B、C 类主要分为两个部分，分别是**网络号和主机号**。这很好理解，好比小林是 A 小区 1 栋 101 号，你是 B 小区 1 栋 101 号。\n\n我们可以用下面这个表格， 就能很清楚的知道 A、B、C 分类对应的地址范围、最大主机个数。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/8.jpg)\n\n\u003e A、B、C 分类地址最大主机个数是如何计算的呢？\n\n最大主机个数，就是要看主机号的位数，如 C 类地址的主机号占 8 位，那么 C 类地址的最大主机个数：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/9.jpg)\n\n为什么要减 2 呢？\n\n因为在 IP 地址中，有两个 IP 是特殊的，分别是主机号全为 1 和 全为 0 地址。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/10.jpg)\n\n- 主机号全为 1 指定某个网络下的所有主机，用于广播\n- 主机号全为 0 指定某个网络\n\n因此，在分配过程中，应该去掉这两种情况。\n\n\u003e 广播地址用于什么？\n\n\n广播地址用于在**同一个链路中相互连接的主机之间发送数据包**。\n\n学校班级中就有广播的例子，在准备上课的时候，通常班长会喊：“上课， 全体起立！”，班里的同学听到这句话是不是全部都站起来了？这个句话就有广播的含义。\n\n当主机号全为 1 时，就表示该网络的广播地址。例如把 `172.20.0.0/16` 用二进制表示如下：\n\n10101100.00010100.00000000.00000000\n\n将这个地址的**主机部分全部改为 1**，则形成广播地址：\n\n10101100.00010100.`11111111.11111111`\n\n再将这个地址用十进制表示，则为 `172.20.255.255`。\n\n广播地址可以分为本地广播和直接广播两种。\n\n- **在本网络内广播的叫做本地广播**。例如网络地址为 192.168.0.0/24 的情况下，广播地址是 192.168.0.255 。因为这个广播地址的 IP 包会被路由器屏蔽，所以不会到达 192.168.0.0/24 以外的其他链路上。\n- **在不同网络之间的广播叫做直接广播**。例如网络地址为 192.168.0.0/24 的主机向 192.168.1.255/24 的目标地址发送 IP 包。收到这个包的路由器，将数据转发给 192.168.1.0/24，从而使得所有 192.168.1.1~192.168.1.254 的主机都能收到这个包（由于直接广播有一定的安全问题，多数情况下会在路由器上设置为不转发。） 。\n\n\n![本地广播与直接广播](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/11.jpg)\n\n\n\u003e 什么是 D、E 类地址？\n\n而 D 类和 E 类地址是没有主机号的，所以不可用于主机 IP，D 类常被用于**多播**，E 类是预留的分类，暂时未使用。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/12.jpg)\n\n\u003e 多播地址用于什么？\n\n多播用于**将包发送给特定组内的所有主机。**\n\n还是举班级的栗子，老师说：“最后一排的同学，上来做这道数学题。”，老师指定的是最后一排的同学，也就是多播的含义了。\n\n由于广播无法穿透路由，若想给其他网段发送同样的包，就可以使用可以穿透路由的多播。\n\n![单播、广播、多播通信](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/13.jpg)\n\n多播使用的 D 类地址，其前四位是 `1110` 就表示是多播地址，而剩下的 28 位是多播的组编号。\n\n从 224.0.0.0 ~ 239.255.255.255 都是多播的可用范围，其划分为以下三类：\n\n- 224.0.0.0 ~ 224.0.0.255 为预留的组播地址，只能在局域网中，路由器是不会进行转发的。\n- 224.0.1.0 ~ 238.255.255.255  为用户可用的组播地址，可以用于 Internet 上。\n- 239.0.0.0 ~ 239.255.255.255 为本地管理组播地址，可供内部网在内部使用，仅在特定的本地范围内有效。\n\n\n\u003e IP 分类的优点\n\n不管是路由器还是主机解析到一个 IP 地址时候，我们判断其 IP 地址的首位是否为 0，为 0 则为 A 类地址，那么就能很快的找出网络地址和主机地址。\n\n其余分类判断方式参考如下图：\n\n![IP 分类判断](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/14.jpg)\n\n所以，这种分类地址的优点就是**简单明了、选路（基于网络地址）简单**。\n\n\u003e IP 分类的缺点\n\n*缺点一*\n\n**同一网络下没有地址层次**，比如一个公司里用了 B 类地址，但是可能需要根据生产环境、测试环境、开发环境来划分地址层次，而这种 IP 分类是没有地址层次划分的功能，所以这就**缺少地址的灵活性**。\n\n*缺点二*\n\nA、B、C类有个尴尬处境，就是**不能很好的与现实网络匹配**。\n\n- C 类地址能包含的最大主机数量实在太少了，只有 254 个，估计一个网吧都不够用。\n- 而 B 类地址能包含的最大主机数量又太多了，6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。\n\n\n这两个缺点，都可以在 `CIDR` 无分类地址解决。\n\n### 无分类地址 CIDR\n\n正因为 IP 分类存在许多缺点，所以后面提出了无分类地址的方案，即 `CIDR`。\n\n这种方式不再有分类地址的概念，32 比特的 IP 地址被划分为两部分，前面是**网络号**，后面是**主机号**。\n\n\u003e 怎么划分网络号和主机号的呢？\n\n表示形式 `a.b.c.d/x`，其中 `/x` 表示前 x 位属于**网络号**， x 的范围是 `0 ~ 32`，这就使得 IP 地址更加具有灵活性。\n\n比如 10.100.122.2/24，这种地址表示形式就是 CIDR，/24 表示前 24 位是网络号，剩余的 8 位是主机号。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/15.jpg)\n\n还有另一种划分网络号与主机号形式，那就是**子网掩码**，掩码的意思就是掩盖掉主机号，剩余的就是网络号。\n\n**将子网掩码和 IP 地址按位计算 AND，就可得到网络号。**\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/16.jpg)\n\n\u003e 为什么要分离网络号和主机号？\n\n因为两台计算机要通讯，首先要判断是否处于同一个广播域内，即网络地址是否相同。如果网络地址相同，表明接受方在本网络上，那么可以把数据包直接发送到目标主机。\n\n路由器寻址工作中，也就是通过这样的方式来找到对应的网络号的，进而把数据包转发给对应的网络内。\n\n![IP地址的网络号](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/17.jpg)\n\n\u003e 怎么进行子网划分？\n\n在上面我们知道可以通过子网掩码划分出网络号和主机号，那实际上子网掩码还有一个作用，那就是**划分子网**。\n\n**子网划分实际上是将主机地址分为两个部分：子网网络地址和子网主机地址**。形式如下：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/18.jpg)\n\n- 未做子网划分的 ip 地址：网络地址＋主机地址\n- 做子网划分后的 ip 地址：网络地址＋（子网网络地址＋子网主机地址）\n\n\n假设对 C 类地址进行子网划分，网络地址 192.168.1.0，使用子网掩码 255.255.255.192 对其进行子网划分。\n\nC 类地址中前 24 位是网络号，最后 8 位是主机号，根据子网掩码可知**从 8 位主机号中借用 2 位作为子网号**。\n    \n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/19.jpg)\n\n由于子网网络地址被划分成 2 位，那么子网地址就有 4 个，分别是 00、01、10、11，具体划分如下图：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/20.jpg)\n\n划分后的 4 个子网如下表格：\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/21.jpg)\n\n### 公有 IP 地址与私有 IP 地址\n\n在 A、B、C 分类地址，实际上有分公有 IP 地址和私有 IP 地址。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/22.jpg)\n\n平时我们办公室、家里、学校用的 IP 地址，一般都是私有 IP 地址。因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有 IP 地址和我学校的可以是一样的。\n\n就像每个小区都有自己的楼编号和门牌号，你小区家可以叫  1 栋 101 号，我小区家也可以叫 1 栋 101，没有任何问题。但一旦出了小区，就需要带上中山路 666 号（公网 IP 地址），是国家统一分配的，不能两个小区都叫中山路 666。\n\n所以，公有 IP 地址是有个组织统一分配的，假设你要开一个博客网站，那么你就需要去申请购买一个公有 IP，这样全世界的人才能访问。并且公有 IP 地址基本上要在整个互联网范围内保持唯一。\n\n\n![公有 IP 地址与私有 IP 地址](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/23.jpg)\n\n\u003e 公有 IP 地址由谁管理呢？\n\n私有 IP 地址通常是内部的 IT 人员管理，公有 IP 地址是由 `ICANN` 组织管理，中文叫「互联网名称与数字地址分配机构」。\n\nIANA 是 ICANN 的其中一个机构，它负责分配互联网 IP 地址，是按洲的方式层层分配。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/24.jpg)\n\n- ARIN 北美地区\n- LACNIC 拉丁美洲和一些加勒比群岛\n- RIPE NCC 欧洲、中东和中亚\n- AfriNIC 非洲地区\n- APNIC 亚太地区\n\n其中，在中国是由 CNNIC 的机构进行管理，它是中国国内唯一指定的全局 IP 地址管理的组织。\n\n\n### IP 地址与路由控制\n\nIP地址的**网络地址**这一部分是用于进行路由控制。\n\n路由控制表中记录着网络地址与下一步应该发送至路由器的地址。在主机和路由器上都会有各自的路由器控制表。\n\n在发送 IP 包时，首先要确定 IP 包首部中的目标地址，再从路由控制表中找到与该地址具有**相同网络地址**的记录，根据该记录将 IP 包转发给相应的下一个路由器。如果路由控制表中存在多条相同网络地址的记录，就选择相同位数最多的网络地址，也就是最长匹配。\n\n下面以下图的网络链路作为例子说明：\n\n![IP 地址与路由控制](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/25.jpg)\n\n\n1. 主机 A 要发送一个 IP 包，其源地址是 `10.1.1.30` 和目标地址是 `10.1.2.10`，由于没有在主机 A 的路由表找到与目标地址 `10.1.2.10` 的网络地址，于是包被转发到默认路由（路由器 `1` ）\n2. 路由器 `1` 收到 IP 包后，也在路由器 `1` 的路由表匹配与目标地址相同的网络地址记录，发现匹配到了，于是就把 IP 数据包转发到了 `10.1.0.2` 这台路由器 `2`\n3. 路由器 `2` 收到后，同样对比自身的路由表，发现匹配到了，于是把 IP 包从路由器 `2` 的 `10.1.2.1` 这个接口出去，最终经过交换机把 IP 数据包转发到了目标主机\n\n\u003e 环回地址是不会流向网络\n\n环回地址是在同一台计算机上的程序之间进行网络通信时所使用的一个默认地址。\n\n计算机使用一个特殊的 IP 地址 **127.0.0.1 作为环回地址**。与该地址具有相同意义的是一个叫做 `localhost` 的主机名。使用这个 IP 或主机名时，数据包不会流向网络。\n\n### IP 分片与重组\n\n每种数据链路的最大传输单元 `MTU` 都是不相同的，如 FDDI 数据链路 MTU 4352、以太网的 MTU 是 1500 字节等。\n\n每种数据链路的 MTU 之所以不同，是因为每个不同类型的数据链路的使用目的不同。使用目的不同，可承载的 MTU 也就不同。\n\n其中，我们最常见数据链路是以太网，它的 MTU 是 `1500` 字节。\n\n那么当 IP 数据包大小大于 MTU 时， IP 数据包就会被分片。\n\n经过分片之后的 IP 数据报在被重组的时候，只能由目标主机进行，路由器是不会进行重组的。\n\n\n假设发送方发送一个 4000 字节的大数据报，若要传输在以太网链路，则需要把数据报分片成 3 个小数据报进行传输，再交由接收方重组成大数据报。\n\n![分片与重组](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/26.jpg)\n\n\n在分片传输中，一旦某个分片丢失，则会造成整个 IP 数据报作废，所以 TCP 引入了 `MSS` 也就是在 TCP 层进行分片不由 IP 层分片，那么对于 UDP 我们尽量不要发送一个大于 `MTU` 的数据报文。\n\n\n### IPv6 基本认识\n\nIPv4 的地址是 32 位的，大约可以提供 42 亿个地址，但是早在 2011 年 IPv4 地址就已经被分配完了。\n\n但是 IPv6 的地址是 `128` 位的，这可分配的地址数量是大的惊人，说个段子 **IPv6 可以保证地球上的每粒沙子都能被分配到一个 IP 地址。**\n\n但 IPv6 除了有更多的地址之外，还有更好的安全性和扩展性，说简单点就是 IPv6 相比于 IPv4 能带来更好的网络体验。\n\n但是因为 IPv4 和 IPv6 不能相互兼容，所以不但要我们电脑、手机之类的设备支持，还需要网络运营商对现有的设备进行升级，所以这可能是 IPv6 普及率比较慢的一个原因。\n\n\u003e IPv6 的亮点\n\nIPv6 不仅仅只是可分配的地址变多了，它还有非常多的亮点。\n\n- IPv6 可自动配置，即使没有 DHCP 服务器也可以实现自动分配IP地址，真是**便捷到即插即用**啊。\n- IPv6 包头包首部长度采用固定的值 `40` 字节，去掉了包头校验和，简化了首部结构，减轻了路由器负荷，大大**提高了传输的性能**。\n- IPv6 有应对伪造 IP 地址的网络安全功能以及防止线路窃听的功能，大大**提升了安全性**。\n- **...** （由你发现更多的亮点）\n\n\n\u003e IPv6 地址的标识方法\n\nIPv4 地址长度共 32 位，是以每 8 位作为一组，并用点分十进制的表示方式。\n\nIPv6 地址长度是 128 位，是以每 16 位作为一组，每组用冒号 「:」 隔开。\n\n![IPv6 地址表示方法](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/27.jpg)\n\n\n如果出现连续的 0 时还可以将这些 0 省略，并用两个冒号 「::」隔开。但是，一个 IP 地址中只允许出现一次两个连续的冒号。\n\n![Pv6 地址缺省表示方](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/28.jpg)\n\n\u003e IPv6 地址的结构\n\nIPv6 类似 IPv4，也是通过 IP 地址的前几位标识 IP 地址的种类。\n\nIPv6 的地址主要有以下类型地址：\n\n- 单播地址，用于一对一的通信\n- 组播地址，用于一对多的通信\n- 任播地址，用于通信最近的节点，最近的节点是由路由协议决定\n- 没有广播地址\n\n![IPv6地址结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/29.jpg)\n\n\u003e IPv6 单播地址类型\n\n对于一对一通信的 IPv6 地址，主要划分了三类单播地址，每类地址的有效范围都不同。\n\n- 在同一链路单播通信，不经过路由器，可以使用**链路本地单播地址**，IPv4 没有此类型\n- 在内网里单播通信，可以使用**唯一本地地址**，相当于 IPv4 的私有 IP\n- 在互联网通信，可以使用**全局单播地址**，相当于 IPv4 的公有 IP\n\n![ IPv6 中的单播通信](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/30.jpg)\n\n\n### IPv4 首部与 IPv6 首部\n\nIPv4 首部与 IPv6 首部的差异如下图：\n\n![IPv4 首部与 IPv6 首部的差异](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/31.jpg)\n\nIPv6 相比 IPv4 的首部改进：\n\n- **取消了首部校验和字段。** 因为在数据链路层和传输层都会校验，因此 IPv6 直接取消了 IP 的校验。\n- **取消了分片/重新组装相关字段。** 分片与重组是耗时的过程，IPv6 不允许在中间路由器进行分片与重组，这种操作只能在源与目标主机，这将大大提高了路由器转发的速度。\n- **取消选项字段。** 选项字段不再是标准 IP 首部的一部分了，但它并没有消失，而是可能出现在 IPv6 首部中的「下一个首部」指出的位置上。删除该选项字段使的 IPv6 的首部成为固定长度的 `40` 字节。\n\n---\n\n## 点心 —— IP 协议相关技术\n\n跟 IP 协议相关的技术也不少，接下来说说与 IP 协议相关的重要且常见的技术。\n\n- DNS 域名解析\n- ARP 与 RARP 协议\n- DHCP 动态获取 IP 地址\n- NAT 网络地址转换\n- ICMP 互联网控制报文协议\n- IGMP 因特网组管理协\n\n\n### DNS\n\n我们在上网的时候，通常使用的方式是域名，而不是 IP 地址，因为域名方便人类记忆。\n\n那么实现这一技术的就是 **DNS 域名解析**，DNS 可以将域名网址自动转换为具体的 IP 地址。\n\n\u003e 域名的层级关系\n\nDNS 中的域名都是用**句点**来分隔的，比如 `www.server.com`，这里的句点代表了不同层次之间的**界限**。\n\n在域名中，**越靠右**的位置表示其层级**越高**。\n\n毕竟域名是外国人发明，所以思维和中国人相反，比如说一个城市地点的时候，外国喜欢从小到大的方式顺序说起（如 XX 街道 XX 区 XX 市 XX 省），而中国则喜欢从大到小的顺序（如 XX 省 XX 市 XX 区 XX 街道）。\n\n根域是在最顶层，它的下一层就是 com 顶级域，再下面是 server.com。\n\n所以域名的层级关系类似一个树状结构：\n\n- 根 DNS 服务器\n- 顶级域 DNS 服务器（com）\n- 权威 DNS 服务器（server.com）\n\n![DNS 树状结构](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/32.jpg)\n\n\n根域的 DNS 服务器信息保存在互联网中所有的 DNS 服务器中。这样一来，任何 DNS 服务器就都可以找到并访问根域 DNS 服务器了。\n\n因此，客户端只要能够找到任意一台 DNS 服务器，就可以通过它找到根域 DNS 服务器，然后再一路顺藤摸瓜找到位于下层的某台目标 DNS 服务器。\n\n\u003e 域名解析的工作流程\n\n浏览器首先看一下自己的缓存里有没有，如果没有就向操作系统的缓存要，还没有就检查本机域名解析文件 `hosts`，如果还是没有，就会 DNS 服务器进行查询，查询的过程如下：\n\n1. 客户端首先会发出一个 DNS 请求，问 www.server.com 的 IP 是啥，并发给本地 DNS 服务器（也就是客户端的 TCP/IP 设置中填写的 DNS 服务器地址）。\n2. 本地域名服务器收到客户端的请求后，如果缓存里的表格能找到 www.server.com，则它直接返回 IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大， 能告诉我 www.server.com 的 IP 地址吗？” 根域名服务器是最高层次的，它不直接用于域名解析，但能指明一条道路。 \n3. 根 DNS 收到来自本地 DNS 的请求后，发现后置是 .com，说：“www.server.com 这个域名归 .com 区域管理”，我给你 .com 顶级域名服务器地址给你，你去问问它吧。”\n4. 本地 DNS 收到顶级域名服务器的地址后，发起请求问“老二， 你能告诉我 www.server.com  的 IP 地址吗？”\n5. 顶级域名服务器说：“我给你负责 www.server.com 区域的权威 DNS 服务器的地址，你去问它应该能问到”。\n6. 本地 DNS 于是转向问权威 DNS 服务器：“老三，www.server.com对应的IP是啥呀？” server.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。\n7. 权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。\n8. 本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。\n\n\n至此，我们完成了 DNS 的解析过程。现在总结一下，整个过程我画成了一个图。\n\n![域名解析的工作流程](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/33.jpg)\n\nDNS 域名解析的过程蛮有意思的，整个过程就和我们日常生活中找人问路的过程类似，**只指路不带路**。\n\n\n### ARP\n\n在传输一个 IP 数据报的时候，确定了源 IP 地址和目标 IP 地址后，就会通过主机「路由表」确定 IP 数据包下一跳。然而，网络层的下一层是数据链路层，所以我们还要知道「下一跳」的 MAC 地址。\n\n由于主机的路由表中可以找到下一跳的 IP 地址，所以可以通过 **ARP 协议**，求得下一跳的 MAC 地址。\n\n\u003e 那么 ARP 又是如何知道对方 MAC 地址的呢？\n\n简单地说，ARP 是借助 **ARP 请求与 ARP 响应**两种类型的包确定 MAC 地址的。\n\n![ARP 广播](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/34.jpg)\n\n- 主机会通过**广播发送 ARP 请求**，这个包中包含了想要知道的 MAC 地址的主机 IP 地址。\n- 当同个链路中的所有设备收到 ARP 请求时，会去拆开 ARP 请求包里的内容，如果 ARP 请求包中的目标 IP 地址与自己的 IP 地址一致，那么这个设备就将自己的 MAC 地址塞入 **ARP 响应包**返回给主机。\n\n操作系统通常会把第一次通过 ARP 获取的 MAC 地址缓存起来，以便下次直接从缓存中找到对应 IP 地址的 MAC 地址。\n\n不过，MAC 地址的缓存是有一定期限的，超过这个期限，缓存的内容将被清除。\n\n\u003e RARP 协议你知道是什么吗？\n\nARP 协议是已知 IP 地址求 MAC 地址，那 RARP 协议正好相反，它是**已知 MAC 地址求 IP 地址**。例如将打印机服务器等小型嵌入式设备接入到网络时就经常会用得到。\n\n通常这需要架设一台 `RARP` 服务器，在这个服务器上注册设备的 MAC 地址及其 IP 地址。然后再将这个设备接入到网络，接着：\n\n- 该设备会发送一条「我的 MAC 地址是XXXX，请告诉我，我的IP地址应该是什么」的请求信息。\n- RARP 服务器接到这个消息后返回「MAC地址为 XXXX 的设备，IP地址为 XXXX」的信息给这个设备。\n\n最后，设备就根据从 RARP 服务器所收到的应答信息设置自己的 IP 地址。\n\n![RARP](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/35.jpg)\n\n\n### DHCP\n\nDHCP 在生活中我们是很常见的了，我们的电脑通常都是通过 DHCP 动态获取 IP 地址，大大省去了配 IP 信息繁琐的过程。\n\n接下来，我们来看看我们的电脑是如何通过 4 个步骤的过程，获取到 IP 的。\n\n![DHCP 工作流程](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/36.jpg)\n\n先说明一点，DHCP 客户端进程监听的是 68 端口号，DHCP 服务端进程监听的是 67 端口号。\n\n这 4 个步骤：\n\n- 客户端首先发起 **DHCP 发现报文（DHCP DISCOVER）** 的 IP 数据报，由于客户端没有 IP 地址，也不知道 DHCP 服务器的地址，所以使用的是 UDP **广播**通信，其使用的广播目的地址是 255.255.255.255（端口 67） 并且使用 0.0.0.0（端口 68） 作为源 IP 地址。DHCP 客户端将该 IP 数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。\n- DHCP 服务器收到 DHCP 发现报文时，用 **DHCP 提供报文（DHCP OFFER）** 向客户端做出响应。该报文仍然使用 IP 广播地址 255.255.255.255，该报文信息携带服务器提供可租约的 IP 地址、子网掩码、默认网关、DNS 服务器以及 **IP 地址租用期**。\n- 客户端收到一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向选中的服务器发送 **DHCP 请求报文（DHCP REQUEST**进行响应，回显配置的参数。\n- 最后，服务端用 **DHCP ACK 报文**对 DHCP 请求报文进行响应，应答所要求的参数。\n\n一旦客户端收到 DHCP ACK 后，交互便完成了，并且客户端能够在租用期内使用 DHCP 服务器分配的 IP 地址。\n\n如果租约的 DHCP IP 地址快期后，客户端会向服务器发送 DHCP 请求报文：\n\n- 服务器如果同意继续租用，则用 DHCP ACK 报文进行应答，客户端就会延长租期。\n- 服务器如果不同意继续租用，则用 DHCP NACK 报文，客户端就要停止使用租约的 IP 地址。\n\n\n可以发现，DHCP 交互中，**全程都是使用 UDP 广播通信**。\n\n\u003e 咦，用的是广播，那如果 DHCP 服务器和客户端不是在同一个局域网内，路由器又不会转发广播包，那不是每个网络都要配一个 DHCP 服务器？\n\n所以，为了解决这一问题，就出现了 **DHCP 中继代理**。有了 DHCP 中继代理以后，**对不同网段的 IP 地址分配也可以由一个 DHCP 服务器统一进行管理。**\n\n![ DHCP 中继代理](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/37.jpg)\n\n- DHCP 客户端会向 DHCP 中继代理发送 DHCP 请求包，而 DHCP 中继代理在收到这个广播包以后，再以**单播**的形式发给 DHCP 服务器。\n\n- 服务器端收到该包以后再向 DHCP 中继代理返回应答，并由 DHCP 中继代理将此包广播给 DHCP 客户端 。\n\n因此，DHCP 服务器即使不在同一个链路上也可以实现统一分配和管理IP地址。\n\n\n### NAT\n\nIPv4 的地址是非常紧缺的，在前面我们也提到可以通过无分类地址来减缓 IPv4 地址耗尽的速度，但是互联网的用户增速是非常惊人的，所以 IPv4 地址依然有被耗尽的危险。\n\n于是，提出了一种**网络地址转换 NAT** 的方法，再次缓解了 IPv4 地址耗尽的问题。\n\n简单的来说 NAT 就是同个公司、家庭、教室内的主机对外部通信时，把私有 IP 地址转换成公有 IP 地址。\n\n![NAT](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/38.jpg)\n\n\n\u003e 那不是 N 个私有 IP 地址，你就要 N 个公有 IP 地址？这怎么就缓解了 IPv4 地址耗尽的问题？这不瞎扯吗？\n\n确实是，普通的 NAT 转换没什么意义。\n\n由于绝大多数的网络应用都是使用传输层协议 TCP 或 UDP 来传输数据的。\n\n因此，可以把 IP 地址 + 端口号一起进行转换。\n\n这样，就用一个全球 IP 地址就可以了，这种转换技术就叫**网络地址与端口转换 NAPT。**\n\n很抽象？来，看下面的图解就能瞬间明白了。\n\n![NAPT](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/39.jpg)\n\n\n图中有两个客户端 192.168.1.10 和 192.168.1.11 同时与服务器 183.232.231.172 进行通信，并且这两个客户端的本地端口都是 1025。\n\n此时，**两个私有 IP 地址都转换 IP 地址为公有地址 120.229.175.121，但是以不同的端口号作为区分。**\n\n于是，生成一个 NAPT 路由器的转换表，就可以正确地转换地址跟端口的组合，令客户端 A、B 能同时与服务器之间进行通信。\n\n这种转换表在 NAT 路由器上自动生成。例如，在 TCP 的情况下，建立 TCP 连接首次握手时的 SYN 包一经发出，就会生成这个表。而后又随着收到关闭连接时发出 FIN 包的确认应答从表中被删除。\n\n\u003e NAT 那么牛逼，难道就没缺点了吗？\n\n当然有缺陷，肯定没有十全十美的方案。\n\n由于 NAT/NAPT 都依赖于自己的转换表，因此会有以下的问题：\n\n- 外部无法主动与 NAT 内部服务器建立连接，因为 NAPT 转换表没有转换记录。\n- 转换表的生成与转换操作都会产生性能开销。\n- 通信过程中，如果 NAT 路由器重启了，所有的 TCP 连接都将被重置。\n\n\u003e 如何解决 NAT 潜在的问题呢？\n\n解决的方法主要有两种方法。\n\n*第一种就是改用 IPv6*\n\nIPv6 可用范围非常大，以至于每台设备都可以配置一个公有 IP 地址，就不搞那么多花里胡哨的地址转换了，但是 IPv6 普及速度还需要一些时间。\n\n*第二种 NAT 穿透技术*\n\nNAT 穿越技术拥有这样的功能，它能够让网络应用程序主动发现自己位于 NAT 设备之后，并且会主动获得 NAT 设备的公有 IP，并为自己建立端口映射条目，注意这些都是 NAT设备后的应用程序自动完成的。\n\n也就是说，在 NAT 穿透技术中，NAT设备后的应用程序处于主动地位，它已经明确地知道 NAT 设备要修改它外发的数据包，于是它主动配合 NAT 设备的操作，主动地建立好映射，这样就不像以前由 NAT 设备来建立映射了。\n\n说人话，就是客户端主动从 NAT 设备获取公有 IP 地址，然后自己建立端口映射条目，然后用这个条目对外通信，就不需要 NAT 设备来进行转换了。\n\n\n### ICMP\n\nICMP 全称是 **Internet Control Message Protocol**，也就是**互联网控制报文协议**。\n\n里面有个关键词 —— **控制**，如何控制的呢？\n\n网络包在复杂的网络传输环境里，常常会遇到各种问题。\n\n当遇到问题的时候，总不能死个不明不白，没头没脑的作风不是计算机网络的风格。所以需要传出消息，报告遇到了什么问题，这样才可以调整传输策略，以此来控制整个局面。\n\n\u003e ICMP 功能都有啥？\n\n`ICMP` 主要的功能包括：**确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。**\n\n在 `IP` 通信中如果某个 `IP` 包因为某种原因未能达到目标地址，那么这个具体的原因将**由 ICMP 负责通知**。\n\n![ICMP 目标不可达消息](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/40.jpg)\n\n如上图例子，主机 `A` 向主机 `B` 发送了数据包，由于某种原因，途中的路由器 `2` 未能发现主机 `B` 的存在，这时，路由器 `2` 就会向主机 `A` 发送一个 `ICMP` 目标不可达数据包，说明发往主机 `B` 的包未能成功。\n\nICMP 的这种通知消息会使用 `IP` 进行发送 。\n\n因此，从路由器 `2` 返回的 ICMP 包会按照往常的路由控制先经过路由器 `1` 再转发给主机 `A` 。收到该 ICMP 包的主机 `A` 则分解 ICMP 的首部和数据域以后得知具体发生问题的原因。\n\n\u003e ICMP 类型\n\nICMP 大致可以分为两大类：\n\n- 一类是用于诊断的查询消息，也就是「**查询报文类型**」\n- 另一类是通知出错原因的错误消息，也就是「**差错报文类型**」\n\n![常见的 ICMP 类型](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/41.jpg)\n\n### IGMP\n\nICMP 跟 IGMP 是一点关系都没有的，就好像周杰与周杰伦的区别，大家不要混淆了。\n\n在前面我们知道了组播地址，也就是 D 类地址，既然是组播，那就说明是只有一组的主机能收到数据包，不在一组的主机不能收到数组包，怎么管理是否是在一组呢？那么，就需要 `IGMP` 协议了。\n\n![组播模型](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/42.jpg)\n\n**IGMP 是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间**，如上图中的蓝色部分。\n\n- IGMP 报文向路由器申请加入和退出组播组，默认情况下路由器是不会转发组播包到连接中的主机，除非主机通过 IGMP 加入到组播组，主机申请加入到组播组时，路由器就会记录 IGMP 路由器表，路由器后续就会转发组播包到对应的主机了。\n- IGMP 报文采用 IP 封装，IP 头部的协议号为 2，而且 TTL 字段值通常为 1，因为 IGMP 是工作在主机与连接的路由器之间。\n\n\u003e IGMP 工作机制\n\nIGMP 分为了三个版本分别是，IGMPv1、IGMPv2、IGMPv3。\n\n接下来，以 `IGMPv2` 作为例子，说说**常规查询与响应和离开组播组**这两个工作机制。\n\n*常规查询与响应工作机制*\n\n![ IGMP 常规查询与响应工作机制](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/43.jpg)\n\n\n1. 路由器会周期性发送目的地址为 `224.0.0.1`（表示同一网段内所有主机和路由器） **IGMP 常规查询报文**。\n2. 主机1 和 主机 3 收到这个查询，随后会启动「报告延迟计时器」，计时器的时间是随机的，通常是 0~10 秒，计时器超时后主机就会发送 **IGMP 成员关系报告报文**（源 IP 地址为自己主机的 IP 地址，目的 IP 地址为组播地址）。如果在定时器超时之前，收到同一个组内的其他主机发送的成员关系报告报文，则自己不再发送，这样可以减少网络中多余的 IGMP 报文数量。\n3. 路由器收到主机的成员关系报文后，就会在 IGMP 路由表中加入该组播组，后续网络中一旦该组播地址的数据到达路由器，它会把数据包转发出去。\n\n*离开组播组工作机制*\n\n离开组播组的情况一，网段中仍有该组播组：\n\n![ IGMPv2 离开组播组工作机制 情况1](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/44.jpg)\n\n\n1. 主机 1 要离开组 224.1.1.1，发送 IGMPv2 离组报文，报文的目的地址是 224.0.0.2（表示发向网段内的所有路由器）\n2. 路由器 收到该报文后，以 1 秒为间隔连续发送 IGMP 特定组查询报文（共计发送 2 个），以便确认该网络是否还有 224.1.1.1 组的其他成员。\n3. 主机 3 仍然是组 224.1.1.1 的成员，因此它立即响应这个特定组查询。路由器知道该网络中仍然存在该组播组的成员，于是继续向该网络转发 224.1.1.1 的组播数据包。\n\n离开组播组的情况二，网段中没有该组播组：\n\n![ IGMPv2 离开组播组工作机制 情况2](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/45.jpg)\n\n1. 主机 1 要离开组播组 224.1.1.1，发送 IGMP 离组报文。\n2. 路由器收到该报文后，以 1 秒为间隔连续发送 IGMP 特定组查询报文（共计发送 2 个）。此时在该网段内，组 224.1.1.1 已经没有其他成员了，因此没有主机响应这个查询。\n3. 一定时间后，路由器认为该网段中已经没有 224.1.1.1 组播组成员了，将不会再向这个网段转发该组播地址的数据包。\n\n---\n\n参考资料：\n\n[1] 计算机网络-自顶向下方法.陈鸣 译.机械工业出版社\n\n[2] TCP/IP详解 卷1：协议.范建华 译.机械工业出版社\n\n[3] 图解TCP/IP.竹下隆史.人民邮电出版社\n\n---\n\n## 读者问答\n\n\u003e 读者问题：“组播不太懂。。。假设一台机器加入组播地址，需要把IP改成组播地址吗？如果离开某个组播地址，需要dhcp重新请求个IP吗？”\n\n组播地址不是用于机器ip地址的，因为组播地址没有网络号和主机号，所以跟dhcp没关系。组播地址一般是用于udp协议，机器发送UDP组播数据时，目标地址填的是组播地址，那么在组播组内的机器都能收到数据包。\n\n是否加入组播组和离开组播组，是由socket一个接口实现的，主机ip是不用改变的。\n\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，别忘记关注我哦！\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/4_ip/ping":{"title":"ping","content":"# 5.2 ping 的工作原理\n\n\n在日常生活或工作中，我们在判断与对方**网络是否畅通**，使用的最多的莫过于 `ping` 命令了。\n\n“**那你知道 `ping` 是如何工作的吗？**” —— 来自小林的灵魂拷问\n\n可能有的小伙伴奇怪的问：“我虽然不明白它的工作，但 ping 我也用的贼 6 啊！”\n\n你用的是 6 ，但你在面试官面前，你就 6 不起来了，毕竟他们也爱问。\n\n所以，我们要抱有「**知其然，知其所以然**」的态度，这样就能避免面试过程中，出门右拐的情况了。\n\n![来自面试官的灵魂拷问](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/2.jpg)\n\n不知道的小伙伴也没关系，今天我们就来搞定它，搞懂它。消除本次的问号，**让问号少一点**。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/3.jpg)\n\n----\n\n## IP协议的助手 —— ICMP 协议\n\nping 是基于 `ICMP` 协议工作的，所以要明白 ping 的工作，首先我们先来熟悉 **ICMP 协议**。\n\n\u003e ICMP 是什么？\n\nICMP 全称是 **Internet Control Message Protocol**，也就是**互联网控制报文协议**。\n\n里面有个关键词 —— **控制**，如何控制的呢？\n\n网络包在复杂的网络传输环境里，常常会遇到各种问题。当遇到问题的时候，总不能死的不明不白，没头没脑的作风不是计算机网络的风格。所以需要传出消息，报告遇到了什么问题，这样才可以调整传输策略，以此来控制整个局面。\n\n\u003e ICMP 功能都有啥？\n\n`ICMP` 主要的功能包括：**确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。**\n\n在 `IP` 通信中如果某个 `IP` 包因为某种原因未能达到目标地址，那么这个具体的原因将**由 ICMP 负责通知**。\n\n![ICMP 目标不可达消息](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/4.jpg)\n\n如上图例子，主机 `A` 向主机 `B` 发送了数据包，由于某种原因，途中的路由器 `2` 未能发现主机 `B` 的存在，这时，路由器 `2` 就会向主机 `A` 发送一个 `ICMP` 目标不可达数据包，说明发往主机 `B` 的包未能成功。\n\nICMP 的这种通知消息会使用 `IP` 进行发送 。\n\n因此，从路由器 `2` 返回的 ICMP 包会按照往常的路由控制先经过路由器 `1` 再转发给主机 `A` 。收到该 ICMP 包的主机 `A` 则分解 ICMP 的首部和数据域以后得知具体发生问题的原因。\n\n\u003e ICMP 包头格式\n\nICMP 报文是封装在 IP 包里面，它工作在网络层，是 IP 协议的助手。\n\n![ICMP 报文](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/5.jpg)\n\nICMP 包头的**类型**字段，大致可以分为两大类：\n\n- 一类是用于诊断的查询消息，也就是「**查询报文类型**」\n- 另一类是通知出错原因的错误消息，也就是「**差错报文类型**」\n\n![常见的 ICMP 类型](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/6.jpg)\n\n\n## 查询报文类型\n\n\u003e 回送消息 —— 类型 `0` 和 `8`\n\n**回送消息**用于进行通信的主机或路由器之间，判断所发送的数据包是否已经成功到达对端的一种消息，`ping` 命令就是利用这个消息实现的。\n\n![ICMP 回送消息](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/7.jpg)\n\n可以向对端主机发送**回送请求**的消息（`ICMP Echo Request Message`，类型 `8`），也可以接收对端主机发回来的**回送应答**消息（`ICMP Echo Reply Message`，类型 `0`）。\n\n![ICMP 回送请求和回送应答报文](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/8.jpg)\n\n相比原生的 ICMP，这里多了两个字段：\n\n- **标识符**：用以区分是哪个应用程序发 ICMP 包，比如用进程 `PID` 作为标识符；\n- **序号**：序列号从 `0` 开始，每发送一次新的回送请求就会加 `1`， 可以用来确认网络包是否有丢失。\n\n在**选项数据**中，`ping` 还会存放发送请求的时间值，来计算往返时间，说明路程的长短。\n\n\n## 差错报文类型\n\n接下来，说明几个常用的 ICMP 差错报文的例子：\n\n- 目标不可达消息 —— 类型 为 `3`\n- 原点抑制消息 —— 类型 `4`\n- 重定向消息 —— 类型 `5`\n- 超时消息 —— 类型 `11`\n\n\n\u003e 目标不可达消息（Destination Unreachable Message） —— 类型为 `3`\n\nIP 路由器无法将 IP 数据包发送给目标地址时，会给发送端主机返回一个**目标不可达**的 ICMP 消息，并在这个消息中显示不可达的具体原因，原因记录在 ICMP 包头的**代码**字段。\n\n由此，根据 ICMP 不可达的具体消息，发送端主机也就可以了解此次发送**不可达的具体原因**。\n\n举例 6 种常见的目标不可达类型的**代码**：\n\n![目标不可达类型的常见代码号](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/9.jpg)\n\n- 网络不可达代码为 `0`\n- 主机不可达代码为 `1`\n- 协议不可达代码为 `2`\n- 端口不可达代码为 `3`\n- 需要进行分片但设置了不分片位代码为 `4`\n\n\n为了给大家说清楚上面的目标不可达的原因，**小林牺牲自己给大家送 5 次外卖。**\n\n为什么要送外卖？别问，问就是为 `35` 岁的老林做准备 ...\n\n![外卖员 —— 小林](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/10.jpg)\n\n#### a. 网络不可达代码为 0\n\n*外卖版本：*\n\n小林第一次送外卖时，小区里只有 A 和 B 区两栋楼，但送餐地址写的是 C 区楼，小林表示头上很多问号，压根就没这个地方。\n\n*正常版本：*\n\nIP 地址是分为网络号和主机号的，所以当路由器中的路由器表匹配不到接收方 IP 的网络号，就通过 ICMP 协议以**网络不可达**（`Network Unreachable`）的原因告知主机。\n\n自从不再有网络分类以后，网络不可达也渐渐不再使用了。\n\n\n####  b. 主机不可达代码为 1\n\n*外卖版本：*\n\n小林第二次送外卖时，这次小区有 5 层楼高的 C 区楼了，找到地方了，但送餐地址写的是 C 区楼 601 号房 ，说明找不到这个房间。\n\n*正常版本：*\n\n当路由表中没有该主机的信息，或者该主机没有连接到网络，那么会通过 ICMP 协议以**主机不可达**（`Host Unreachable`）的原因告知主机。\n\n#### c. 协议不可达代码为 2\n\n*外卖版本：*\n\n小林第三次送外卖时，这次小区有 C 区楼，也有 601 号房，找到地方了，也找到房间了，但是一开门人家是外国人说的是英语，我说的是中文！语言不通，外卖送达失败~\n\n*正常版本：*\n\n当主机使用 TCP 协议访问对端主机时，能找到对端的主机了，可是对端主机的防火墙已经禁止 TCP 协议访问，那么会通过 ICMP 协议以**协议不可达**的原因告知主机。\n\n#### d. 端口不可达代码为 3\n\n*外卖版本：*\n\n小林第四次送外卖时，这次小区有 C 区楼，也有 601 号房，找到地方了，也找到房间了，房间里的人也是说中文的人了，但是人家说他要的不是外卖，而是快递。。。\n\n*正常版本：*\n\n当主机访问对端主机 8080 端口时，这次能找到对端主机了，防火墙也没有限制，可是发现对端主机没有进程监听 8080 端口，那么会通过 ICMP 协议以**端口不可达**的原因告知主机。\n\n####  e. 需要进行分片但设置了不分片位代码为 4\n\n*外卖版本：*\n\n小林第五次送外卖时，这次是个吃播博主点了 100 份外卖，但是吃播博主要求一次性要把全部外卖送达，小林的一台电动车装不下呀，这样就没办法送达了。\n\n*正常版本：*\n\n发送端主机发送 IP 数据报时，将 IP 首部的**分片禁止标志位**设置为`1`。根据这个标志位，途中的路由器遇到超过 MTU 大小的数据包时，不会进行分片，而是直接抛弃。\n\n随后，通过一个 ICMP 的不可达消息类型，**代码为 4** 的报文，告知发送端主机。\n\n\n\u003e 原点抑制消息（ICMP Source Quench Message） —— 类型 `4`\n\n在使用低速广域线路的情况下，连接 WAN 的路由器可能会遇到网络拥堵的问题。\n\n`ICMP` 原点抑制消息的目的就是**为了缓和这种拥堵情况**。\n\n当路由器向低速线路发送数据时，其发送队列的缓存变为零而无法发送出去时，可以向 IP 包的源地址发送一个 ICMP **原点抑制消息**。\n\n收到这个消息的主机借此了解在整个线路的某一处发生了拥堵的情况，从而增大 IP 包的传输间隔，减少网络拥堵的情况。\n\n然而，由于这种 ICMP 可能会引起不公平的网络通信，一般不被使用。\n\n\u003e 重定向消息（ICMP Redirect Message） —— 类型 `5`\n\n如果路由器发现发送端主机使用了「不是最优」的路径发送数据，那么它会返回一个 ICMP **重定向消息**给这个主机。\n\n在这个消息中包含了**最合适的路由信息和源数据**。这主要发生在路由器持有更好的路由信息的情况下。路由器会通过这样的 ICMP 消息告知发送端，让它下次发给另外一个路由器。\n\n好比，小林本可以过条马路就能到的地方，但小林不知道，所以绕了一圈才到，后面小林知道后，下次小林就不会那么**傻**再绕一圈了。\n\n\u003e 超时消息（ICMP Time Exceeded Message） —— 类型 `11`\n\nIP 包中有一个字段叫做 `TTL` （`Time To Live`，生存周期），它的**值随着每经过一次路由器就会减 1，直到减到 0 时该 IP 包会被丢弃。**\n\n此时，路由器将会发送一个 ICMP **超时消息**给发送端主机，并通知该包已被丢弃。\n\n设置 IP 包生存周期的主要目的，是为了在路由控制遇到问题发生循环状况时，避免 IP 包无休止地在网络上被转发。\n\n![ICMP 时间超过消息](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/11.jpg)\n\n此外，有时可以用 TTL 控制包的到达范围，例如设置一个**较小的 TTL 值**。\n\n----\n\n\n## ping —— 查询报文类型的使用\n\n接下来，我们重点来看 `ping` 的**发送和接收过程**。\n\n同个子网下的主机 A 和 主机 B，主机 A 执行`ping` 主机 B 后，我们来看看其间发送了什么？\n\n![主机 A ping 主机 B](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/12.jpg)\n\nping 命令执行的时候，源主机首先会构建一个 **ICMP 回送请求消息**数据包。\n\nICMP 数据包内包含多个字段，最重要的是两个：\n\n- 第一个是**类型**，对于回送请求消息而言该字段为 `8`；\n- 另外一个是**序号**，主要用于区分连续 ping 的时候发出的多个数据包。\n\n每发出一个请求数据包，序号会自动加 `1`。为了能够计算往返时间 `RTT`，它会在报文的数据部分插入发送时间。\n\n![主机 A 的 ICMP 回送请求报文](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/13.jpg)\n\n然后，由 ICMP 协议将这个数据包连同地址 192.168.1.2 一起交给 IP 层。IP 层将以 192.168.1.2 作为**目的地址**，本机 IP 地址作为**源地址**，**协议**字段设置为 `1` 表示是 `ICMP` 协议，再加上一些其他控制信息，构建一个 `IP` 数据包。\n\n![主机 A 的 IP 层数据包](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/14.jpg)\n\n接下来，需要加入 `MAC` 头。如果在本地 ARP 映射表中查找出 IP 地址 192.168.1.2 所对应的 MAC 地址，则可以直接使用；如果没有，则需要发送 `ARP` 协议查询 MAC 地址，获得 MAC 地址后，由数据链路层构建一个数据帧，目的地址是 IP 层传过来的 MAC 地址，源地址则是本机的 MAC 地址；还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送出去。\n\n![主机 A 的 MAC 层数据包](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/15.jpg)\n\n主机 `B` 收到这个数据帧后，先检查它的目的 MAC 地址，并和本机的 MAC 地址对比，如符合，则接收，否则就丢弃。\n\n接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层。同样，IP 层检查后，将有用的信息提取后交给 ICMP 协议。\n\n主机 `B` 会构建一个 **ICMP 回送响应消息**数据包，回送响应数据包的**类型**字段为 `0`，**序号**为接收到的请求数据包中的序号，然后再发送出去给主机 A。\n\n![主机 B 的 ICMP 回送响应报文](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/16.jpg)\n\n在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 回送响应消息，则说明目标主机可达。\n\n此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。\n\n针对上面发送的事情，总结成了如下图：\n\n![主机 A ping 主机 B 期间发送的事情](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/17.png)\n\n当然这只是最简单的，同一个局域网里面的情况。如果跨网段的话，还会涉及网关的转发、路由器的转发等等。\n\n但是对于 ICMP 的头来讲，是没什么影响的。会影响的是根据目标 IP 地址，选择路由的下一跳，还有每经过一个路由器到达一个新的局域网，需要换 MAC 头里面的 MAC 地址。\n\n说了这么多，可以看出 ping 这个程序是**使用了 ICMP 里面的 ECHO REQUEST（类型为 8 ） 和 ECHO REPLY （类型为 0）**。\n\n\n## traceroute —— 差错报文类型的使用\n\n有一款充分利用 ICMP **差错报文类型**的应用叫做 `traceroute`（在UNIX、MacOS中是这个命令，而在Windows中对等的命令叫做 tracert ）。 \n\n*1. traceroute 作用一*\n\ntraceroute 的第一个作用就是**故意设置特殊的 TTL，来追踪去往目的地时沿途经过的路由器。**\n\ntraceroute 的参数指向某个**目的 IP 地址**：\n\n```bash\ntraceroute 192.168.1.100\n```\n\n\u003e 这个作用是如何工作的呢？\n\n它的原理就是利用 IP 包的**生存期限** 从 `1` 开始按照顺序递增的同时发送 **UDP 包**，强制接收 **ICMP 超时消息**的一种方法。\n\n比如，将 TTL 设置 为 `1`，则遇到第一个路由器，就牺牲了，接着返回 ICMP 差错报文网络包，类型是**时间超时**。\n\n接下来将 TTL 设置为 `2`，第一个路由器过了，遇到第二个路由器也牺牲了，也同时返回了 ICMP 差错报文数据包，如此往复，直到到达目的主机。\n\n这样的过程，traceroute 就可以拿到了所有的路由器 IP。\n\n当然有的路由器根本就不会返回这个 ICMP，所以对于有的公网地址，是看不到中间经过的路由的。\n\n\u003e 发送方如何知道发出的 UDP 包是否到达了目的主机呢？\n\ntraceroute 在发送 `UDP` 包时，会填入一个**不可能的端口号**值作为 UDP 目标端口号（大于 `3000` ）。当目的主机，收到 UDP 包后，会返回 ICMP 差错报文消息，但这个差错报文消息的类型是「**端口不可达**」。\n\n所以，**当差错报文类型是端口不可达时，说明发送方发出的 UDP 包到达了目的主机。**\n\n*2. traceroute 作用二*\n\ntraceroute 还有一个作用是**故意设置不分片，从而确定路径的 MTU**。\n\n\u003e 这么做是为了什么？\n\n这样做的目的是为了**路径MTU发现**。\n\n因为有的时候我们并不知道路由器的 `MTU` 大小，以太网的数据链路上的 `MTU` 通常是 `1500` 字节，但是非以外网的 `MTU` 值就不一样了，所以我们要知道 `MTU` 的大小，从而控制发送的包大小。\n\n![MTU 路径发现（UDP的情况下）](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ping/18.jpg)\n\n它的工作原理如下：\n\n首先在发送端主机发送 `IP` 数据报时，将 `IP` 包首部的**分片禁止标志位设置为 1**。根据这个标志位，途中的路由器不会对大数据包进行分片，而是将包丢弃。\n\n随后，通过一个 ICMP 的不可达消息将**数据链路上 MTU 的值**一起给发送主机，不可达消息的类型为「**需要进行分片但设置了不分片位**」。\n\n发送主机端每次收到 ICMP 差错报文时就**减少**包的大小，以此来定位一个合适的 `MTU` 值，以便能到达目标主机。\n\n----\n\n参考资料：\n\n[1] 竹下隆史.图解TCP/IP.人民邮电出版社.\n\n[2] 刘超.趣谈网络协议.极客时间.\n\n---\n\n## 读者问答\n\n\u003e 读者问：“有个问题就是A的icmp到了B后，B为啥会自动给A一个回执0？这是操作系统的底层设计吗？”\n\n你说的“回执0”是指 ICMP 类型为 0 吗？如果是的话，那么 B 收到 A 的回送请求（类型为8） ICMP 报文，B 主机操作系统协议栈发现是个回送请求 ICMP 报文，那么协议栈就会组装一个回送应答（类型为0）的 IMCP 回应给 A。\n\n----\n\n哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，别忘记关注我哦！\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/4_ip/ping_lo":{"title":"ping_lo","content":"# 5.3 断网了，还能 ping 通 127.0.0.1 吗？\n\n\u003e 来源：公众号@小白debug\n\u003e\n\u003e 原文地址：[断网了，还能 ping 通 127.0.0.1 吗？](https://mp.weixin.qq.com/s/qqfnyw4wKFjJqnV1eoRDhw)\n\n你**女神爱不爱你**，你问她，她可能不会告诉你。\n\n但**网通不通**，你 `ping` 一下就知道了。\n\n可能看到标题，你就知道答案了，但是你了解背后的原因吗？\n\n那如果把 `127.0.0.1` 换成 `0.0.0.0` 或 `localhost` 会怎么样呢？你知道这几个`IP`有什么区别吗？\n\n以前面试的时候就遇到过这个问题，大家看个动图了解下面试官和我当时的场景，求当时我的心里阴影面积。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c3a97f36607c0e7ef6805bfba482a060.gif)\n\n话不多说，我们直接开车。\n\n拔掉网线，断网。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/5137bc7bce08dc60cfa2f8152b738dfd.jpeg)\n\n然后在控制台输入`ping 127.0.0.1`。\n\n```shell\n$ ping 127.0.0.1\nPING 127.0.0.1 (127.0.0.1): 56 data bytes\n64 bytes from 127.0.0.1: icmp_seq=0 ttl=64 time=0.080 ms\n64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.093 ms\n64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.074 ms\n64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.079 ms\n64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.079 ms\n^C\n--- 127.0.0.1 ping statistics ---\n5 packets transmitted, 5 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 0.074/0.081/0.093/0.006 ms\n```\n\n说明，拔了网线，`ping 127.0.0.1` 是**能ping通的**。\n\n其实这篇文章看到这里，标题前半个问题已经被回答了。但是我们可以再想深一点。\n\n为什么断网了还能 `ping` 通 `127.0.0.1` 呢？\n\n**这能说明你不用交网费就能上网吗？**\n\n**不能。**\n\n首先我们需要进入基础科普环节。\n\n不懂的同学看了就懂了，懂的看了就当查漏补缺吧。\n\n## 什么是127.0.0.1\n\n首先，这是个 `IPV4` 地址。\n\n`IPV4` 地址有 `32` 位，一个字节有 `8` 位，共 `4` 个字节。\n\n其中**127 开头的都属于回环地址**，也是 `IPV4` 的特殊地址，没什么道理，就是人为规定的。\n\n而`127.0.0.1`是**众多**回环地址中的一个。之所以不是 `127.0.0.2` ，而是 `127.0.0.1`，是因为源码里就是这么定义的，也没什么道理。\n\n```c\n/* Address to loopback in software to local host.  */\n#define    INADDR_LOOPBACK     0x7f000001  /* 127.0.0.1   */\n```\n\n![图片](https://img-blog.csdnimg.cn/img_convert/fa904fbcf66cc7abf510a8dc16f867fa.png)\n\n`IPv4` 的地址是 `32` 位的，2的32次方，大概是`40+亿`。地球光人口就76亿了，40亿IP这点量，**塞牙缝都不够**，实际上**IP也确实用完**了。\n\n所以就有了`IPV6`， `IPv6` 的地址是 `128` 位的，大概是2的128次方≈**10的38次方**。据说地球的沙子数量大概是 **10的23次方**，所以IPV6的IP可以认为用不完。\n\nIPV4以8位一组，每组之间用 **.** 号隔开。\n\nIPV6就以16位为一组，每组之间用 **:** 号隔开。如果全是0，那么可以省略不写。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/e841adeeecf9451e1aca296d5c7a7f30.png)\n\n在IPV4下的回环地址是 `127.0.0.1`，在`IPV6`下，表达为 `::1` 。中间把**连续的0**给省略了，之所以不是**7个 冒号**，而是**2个冒号:** ， 是因为一个 IPV6 地址中**只允许出现⼀次两个连续的冒号**。\n\n\u003e 多说一句：在IPV4下用的是 **ping 127.0.0.1** 命令。在IPV6下用的是 **ping6  ::1** 命令。\n\n## 什么是 ping\n\nping 是应用层命令，可以理解为它跟游戏或者聊天软件属于同一层。只不过聊天软件可以收发消息，还能点个赞什么的，有很多复杂的功能。而 ping 作为一个小软件，它的功能比较简单，就是**尝试**发送一个小小的消息到目标机器上，判断目的机器是否**可达**，其实也就是判断目标机器网络是否能连通。\n\nping应用的底层，用的是网络层的**ICMP协议**。\n\nIP和ICMP和Ping所在分层\n\n虽然ICMP协议和IP协议**都属于网络层协议**，但其实**ICMP也是利用了IP协议进行消息的传输**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/8e0aba146432baeb407ab445292c8019.png)\n\n所以，大家在这里完全可以简单的理解为 ping 某个IP 就是往某个IP地址发个消息。\n\n## TCP发数据和ping的区别\n\n一般情况下，我们会使用 TCP 进行网络数据传输，那么我们可以看下它和 ping 的区别。\n\n*PS：下图中有一处画错了，右边是 tcp 数据，而不是 ping 数据，我偷懒就不重画了*。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/eb0963a11439dff361dbe0e7a8876abd.png)\n\nping和其他应用层软件都属于**应用层**。\n\n那么我们横向对比一下，比方说聊天软件，如果用的是TCP的方式去发送消息。\n\n为了发送消息，那就得先知道往哪发。linux里万物皆文件，那你要发消息的目的地，也是个文件，这里就引出了socket 的概念。\n\n要使用 `socket` , 那么首先需要创建它。\n\n在 TCP 传输中创建的方式是  `socket(AF_INET, SOCK_STREAM, 0);`，其中 `AF_INET` 表示将使用 IPV4 里 **host:port** 的方式去解析待会你输入的网络地址。`SOCK_STREAM` 是指使用面向字节流的 TCP 协议，**工作在传输层**。\n\n创建好了 `socket` 之后，就可以愉快的把要传输的数据写到这个文件里。调用 socket 的`sendto`接口的过程中进程会从**用户态进入到内核态**，最后会调用到 `sock_sendmsg` 方法。\n\n然后进入传输层，带上`TCP`头。网络层带上`IP`头，数据链路层带上 `MAC`头等一系列操作后。进入网卡的**发送队列 ring buffer** ，顺着网卡就发出去了。\n\n回到 `ping` ， 整个过程也基本跟 `TCP` 发数据类似，差异的地方主要在于，创建 `socket` 的时候用的是  `socket(AF_INET,SOCK_RAW,IPPROTO_ICMP)`，`SOCK_RAW` 是原始套接字 ，**工作在网络层**， 所以构建`ICMP`（网络层协议）的数据，是再合适不过了。ping 在进入内核态后最后也是调用的  `sock_sendmsg` 方法，进入到网络层后加上**ICMP和IP头**后，数据链路层加上**MAC头**，也是顺着网卡发出。因此 本质上ping 跟 普通应用发消息 在程序流程上没太大差别。\n\n这也解释了**为什么当你发现怀疑网络有问题的时候，别人第一时间是问你能ping通吗？** 因为可以简单理解为ping就是自己组了个数据包，让系统按着其他软件发送数据的路径往外发一遍，能通的话说明其他软件发的数据也能通。\n\n## 为什么断网了还能 ping 通 127.0.0.1\n\n前面提到，有网的情况下，ping 最后是**通过网卡**将数据发送出去的。\n\n那么断网的情况下，网卡已经不工作了，ping 回环地址却一切正常，我们可以看下这种情况下的工作原理。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c1019a8be584b27c4fc8b8abda9d3cf1.png)\n\n从应用层到传输层再到网络层。这段路径跟ping外网的时候是几乎是一样的。到了网络层，系统会根据目的IP，在路由表中获取对应的**路由信息**，而这其中就包含选择**哪个网卡**把消息发出。\n\n当发现**目标IP是外网IP**时，会从\"真网卡\"发出。\n\n当发现**目标IP是回环地址**时，就会选择**本地网卡**。\n\n本地网卡，其实就是个 **\"** **假网卡** **\"**，它不像\"真网卡\"那样有个`ring buffer`什么的，\"假网卡\"会把数据推到一个叫 `input_pkt_queue` 的 **链表** 中。这个链表，其实是所有网卡共享的，上面挂着发给本机的各种消息。消息被发送到这个链表后，会再触发一个**软中断**。\n\n专门处理软中断的工具人 **\"** **ksoftirqd** **\"**（这是个**内核线程**），它在收到软中断后就会立马去链表里把消息取出，然后顺着数据链路层、网络层等层层往上传递最后给到应用程序。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/a207c14a5416f44e9dbf0fe0a41179e4.png)\n\nping 回环地址和**通过TCP等各种协议发送数据到回环地址**都是走这条路径。整条路径从发到收，都没有经过\"真网卡\"。**之所以127.0.0.1叫本地回环地址，可以理解为，消息发出到这个地址上的话，就不会出网络，在本机打个转就又回来了。** 所以断网，依然能 `ping` 通 `127.0.0.1`。\n\n## ping回环地址和ping本机地址有什么区别\n\n我们在mac里执行 `ifconfig` 。\n\n```shell\n$ ifconfig\nlo0: flags=8049\u003cUP,LOOPBACK,RUNNING,MULTICAST\u003e mtu 16384\n    inet 127.0.0.1 netmask 0xff000000\n    ...\nen0: flags=8863\u003cUP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST\u003e mtu 1500\n    inet 192.168.31.6 netmask 0xffffff00 broadcast 192.168.31.255\n    ...\n```\n\n能看到 **lo0**，表示本地回环接口，对应的地址，就是我们前面提到的 **127.0.0.1** ，也就是**回环地址**。\n\n和 **eth0**，表示本机第一块网卡，对应的IP地址是**192.168.31.6**，管它叫**本机IP**。\n\n之前一直认为ping本机IP的话会通过\"真网卡\"出去，然后遇到第一个路由器，再发回来到本机。\n\n为了验证这个说法，可以进行抓包，但结果跟上面的说法并不相同。\n\nping 127.0.0.1:\n\n![图片](https://img-blog.csdnimg.cn/img_convert/bc2765b1d6d3e37a5663f98085198926.png)\n\nping 本机地址:\n\n![图片](https://img-blog.csdnimg.cn/img_convert/50cd584f9f82aee8d3d9bfaf7d910cb8.png)\n\n可以看到 ping 本机IP 跟 ping 回环地址一样，相关的网络数据，都是走的  **lo0**，本地回环接口，也就是前面提到的**\"假网卡\"**。\n\n只要走了本地回环接口，那数据都不会发送到网络中，在本机网络协议栈中兜一圈，就发回来了。因此 **ping回环地址和ping本机地址没有区别**。\n\n## 127.0.0.1 和 localhost 以及 0.0.0.0 有区别吗\n\n回到文章开头动图里的提问，算是面试八股文里的老常客了。\n\n以前第一次用 `nginx` 的时候，发现用这几个 `IP`，都能正常访问到 `nginx` 的欢迎网页。一度认为这几个 `IP` 都是一样的。\n\n访问127.0.0.1:80\n\n![图片](https://img-blog.csdnimg.cn/img_convert/12e13316a18009ce8b2983846819e270.png)\n\n访问localhost:80\n\n![图片](https://img-blog.csdnimg.cn/img_convert/2c35f573e91e94733d009384a4657859.png)\n\n访问0.0.0.0:80\n\n![图片](https://img-blog.csdnimg.cn/img_convert/ba534fdc5f21b3ab26d0b8c890bb02c3.png)\n\n访问本机的IP地址\n\n![图片](https://img-blog.csdnimg.cn/img_convert/9b31572ced19805fab02a23b22819b92.png)\n\n但本质上还是有些区别的。\n\n首先 `localhost` 就不叫 `IP`，它是一个域名，就跟 `\"baidu.com\"`,是一个形式的东西，只不过默认会把它解析为 `127.0.0.1` ，当然这可以在 `/etc/hosts` 文件下进行修改。\n\n所以默认情况下，使用 `localhost` 跟使用  `127.0.0.1` 确实是没区别的。\n\n其次就是 `0.0.0.0`，执行 ping 0.0.0.0  ，是会失败的，因为它在`IPV4`中表示的是无效的**目标地址**。\n\n```shell\n$ ping 0.0.0.0\nPING 0.0.0.0 (0.0.0.0): 56 data bytes\nping: sendto: No route to host\nping: sendto: No route to host\n```\n\n但它还是很有用处的，回想下，我们启动服务器的时候，一般会 `listen` 一个 IP 和端口，等待客户端的连接。\n\n如果此时 `listen` 的是本机的 `0.0.0.0` , 那么它表示本机上的**所有IPV4地址**。\n\n```c\n/* Address to accept any incoming messages. */\n#define    INADDR_ANY      ((unsigned long int) 0x00000000) /* 0.0.0.0   */\n```\n\n举个例子。刚刚提到的 `127.0.0.1` 和 `192.168.31.6` ，都是本机的IPV4地址，如果监听 `0.0.0.0` ，那么用上面两个地址，都能访问到这个服务器。\n\n当然， 客户端 `connect` 时，不能使用 `0.0.0.0` 。必须指明要连接哪个服务器IP。\n\n## 总结\n\n- `127.0.0.1` 是**回环地址**。`localhost`是**域名**，但默认等于 `127.0.0.1`。\n- `ping` 回环地址和 `ping` 本机地址，是一样的，走的是**lo0 \"假网卡\"**，都会经过网络层和数据链路层等逻辑，最后在快要出网卡前**狠狠拐了个弯**， 将数据插入到一个**链表**后就**软中断**通知 **ksoftirqd** 来进行**收数据**的逻辑，**压根就不出网络**。所以断网了也能 `ping` 通回环地址。\n- 如果服务器 `listen` 的是 `0.0.0.0`，那么此时用`127.0.0.1`和本机地址**都可以**访问到服务。\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，别忘记关注我哦！\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/5_learn/draw":{"title":"draw","content":"# 6.2 画图经验分享\n\n小林写这么多篇图解文章，你们猜我收到的最多的读者问题是什么？没错，就是问我是使用什么**画图**工具，看来对这一点大家都相当好奇，那干脆不如写一篇介绍下我是怎么画图的。\n\n如果我的文章缺少了自己画的图片，相当于失去了灵魂，技术文章本身就很枯燥，如果文章中没有几张图片，读者被劝退的概率飙飙升，剩下没被劝退的估计看着看着就睡着了。所以，精美的图片可以说是必不可少的一部分，不仅在阅读时能带来视觉的冲击，而且图片相比文字能涵盖更多的信息，不然怎会有一图胜千言的说法呢？\n\n这时，可能有的读者会说自己不写文章呀，是不是没有必要了解画图了？我觉得这是不对，画图在我们工作中其实也是有帮助的，比如如果你想跟领导汇报一个业务流程的问题，把业务流程画出来，肯定用图的方式比用文字的方式交流起来会更有效率，更轻松些；如果你参与了一个比较复杂的项目开发，你也可以把代码的流程图给画出来，不仅能帮助自己加深理解，也能帮助后面参与的同事能更快的接手这个项目；甚至如果你要晋升级别了，演讲 PTT 里的配图也是必不可少的。\n\n不过很多人都是纠结用什么画图工具，其实小林觉得再烂的画图工具，只要你思路清晰，确定自己要表达出什么信息，也是能把图画好的，所以不必纠结哪款画图工具，挑一款自己画起来舒服的就行了。\n\n\u003e “小林，你说的我都懂，我就是喜欢你的画图风格嘛，你就说说你用啥画的？”\n\n咳咳，没问题，直接坦白讲，我用的是一个在线的画图网址，地址是：\n- *https://draw.io*\n\n用它的原因是使用方便和简单，当然最重要的是它完全免费，没有什么限制，甚至还能直接把图片保存到 GoogleDrive 、 OneDrive 和 Github，我就是保存到 Github，然后用 Github 作为我的图床。\n\n\n既然要认识它，那就先来看看它长什么样子，它主要分为三个区域，从左往右的顺序是「图形选择区域、绘图区域、属性设置区域」。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/正面图.png)\n\n其中，最左边的「图形选择区域」可以选择的图案有很多种，常见的流程图、时序图、表格图都有，甚至还可以在最左下角的「更多图形」找到其他种类的图形，比如网络设备图标等。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/图形选择.png)\n\n再来，最右边「属性设置区域」可以设置文字的大小，图片颜色、线条形状等，而我最常用颜色板块是下面这三种，都是比较浅色的，这样看起来舒服些。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/浅色风格2.png)\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/深浅色风格.png)\n\n我最近常用的一个图形是圆角方块图，它的位置如下图，但是它默认的颜色过于深色，如果要在方框图中描述文字，则可能看不清楚，这时我会在最右侧的「属性设置区域」把方块颜色设置成浅色系列的。另外，还有一点需要注意的是，默认的字体大小比较小，我一般会调成 `16px` 大小。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/常用的方块.png)\n\n如果你不喜欢上图的带有「划痕」的圆角方块图形，可以选择下图中这个最简洁的圆角方框图形。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/圆角方块图形.png)\n\n这个简洁的圆角方框图形，再搭配颜色，能组合成很多结构图，比如我用过它组成过 CPU Cache 的结构图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/存储结构/CPU-Cache.png)\n\n那直角方框图形，我主要是用来组成「表格」，原因自带的表格不好看，也不方便调。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/表格.png)\n\n如果觉得直直的线条太死板，你可以把图片属性中的「*Comic*」勾上，于是就会变成歪歪扭扭的效果啦，有点像手绘风格，挺多人喜欢这种风格。\n\n比如，我用过这种风格画过 TCP 三次握手流程的图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/14.jpg)\n\n方块图形再加上菱形，就可以组合成简单程序流程图了，比如我画过存储器缓存流程图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/存储结构/缓存体系.png)\n\n所以，不要小看这些基本图形，只要构思清晰，再基本的图形，也是能构成层次分明并且好看的图。\n\n基本的图形介绍完后，相信你画一些简单程序流程图等图形是没问题的了，接下来就是各种**图形 + 线条**的组合的了。\n\n通过一些基本的图形组合，你还可以画出时序图，时序图可以用来描述多个对象之间的交互流程，比如我画过多个线程获取互斥锁的时序图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/操作系统/锁/互斥锁工作流程.png)\n\n再来，为了更好表达零拷贝技术的过程，那么用图的方式会更清晰。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)\n\n前面也提到，图形不只是简单图形，还有其他自带的设备类图形，比如我用网络设备图画过单播、广播、多播通信的区别图。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/IP/13.jpg)\n\n你要说，我画过最复杂的图，那就是写 TCP 流量控制的时候，把整个交互过程 + 文字描述 + 滑动窗口状况都画出来了，现在回想起来还是觉得累人。\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost2/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8F%AF%E9%9D%A0%E7%89%B9%E6%80%A7/22.jpg)\n\n还有好多好多，我就比一一列举，这半年下来，小林至少画了 `500+` 张图了，每一张图其实还是挺费时间的，相信画过图的朋友后，都能体会到这种感觉了。但没办法，谁叫小林是图解工具人呢，画图可以更好的诠释文章内容，但最重要的是，把你们吸引过来了，这是件让我非常高兴的事情，也是让我感觉画图这个事情值得认真做。\n\n另外，细心的读者也发现了，小林贴代码的时候，使用的是图片的形式，原因是代码通常都是比较长，在手机看文章用图片的呈现的方式会更舒服清晰。\n\n在这里也推荐下这个代码截图网址：\n- *https://carbon.now.sh/*\n\n网站页面如下图，代码显示的效果是不是很美观？\n\n![](https://cdn.jsdelivr.net/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/carbon.png)\n\n文字的分享有局限性，关键还是要你自己动手摸索摸索，形成自己一套画图的方法论，练习的时候可以先从模仿画起，后面再结合工作或文章的需求画出自己心中的那个图。","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/%E7%BD%91%E7%BB%9C/README":{"title":"README","content":"\n# 图解网络介绍\n\n大家好，我是小林，是《图解网络》的作者，本站的内容都是整理于我[公众号](https://mp.weixin.qq.com/s/FYH1I8CRsuXDSybSGY_AFA)里的图解文章。\n\n还没关注的朋友，可以微信搜索「**小林coding**」，关注我的公众号，**后续最新版本的 PDF 会在我的公众号第一时间发布**，而且会有更多其他系列的图解文章，比如操作系统、计算机组成、数据库、算法等等。\n\n简单介绍下《图解网络》，整个内容共有 **`20W` 字 + `500` 张图**，每一篇都自己手绘了很多图，目的也很简单，击破大家对于「八股文」的恐惧。\n\n## 适合什么群体？\n\n《图解网络》写的网络知识主要是**面向程序员**的，因为小林本身也是个程序员，所以涉及到的知识主要是关于程序员日常工作或者面试的网络知识。\n\n非常适合有一点网络基础，但是又不怎么扎实，或者知识点串不起来的同学，说白**这本图解网络就是为了拯救半桶水的同学而出来的**。\n\n因为小林写的图解网络就四个字，**通俗易懂**！\n\n相信你在看这本图解网络的时候，你心里的感受会是：\n\n- 「卧槽，原来是这样，大学老师教知识原来是这么理解」\n- 「卧槽，我的网络知识串起来了」\n- 「卧槽，我感觉面试稳了」\n- 「卧槽，相见恨晚」\n\n当然，也适合面试突击网络知识时拿来看。图解网络里的内容基本是面试常见的协议，比如 HTTP、HTTPS、TCP、UDP、IP 等等，也有很多面试常问的问题，比如：\n\n- TCP 为什么三次握手？四次挥手？\n- TCP 为什么要有 TIME_WAIT 状态？\n- TCP 为什么是可靠传输协议，而 UDP 不是？\n- 键入网址到网页显示，期间发生了什么？\n- HTTPS 握手过程是怎样的？\n- ….\n\n不敢说 100 % 涵盖了面试的网络问题，但是至少 90% 是有的，而且内容的深度应对大厂也是绰绰有余，有非常多的读者跑来感激小林的图解网络，帮助他们拿到了国内很多一线大厂的 offer。\n\n## 要怎么阅读？\n\n很诚恳的告诉你，《图解网络》不是教科书，而是我写的图解网络文章的整合，所以肯定是没有教科书那么细致和全面，当然也就不会有很多废话，都是直击重点，不绕弯，而且有的知识点书上看不到。\n\n阅读的顺序可以不用从头读到尾，你可以根据你想要了解的知识点，通过本站的搜索功能，去看哪个章节的内容就好，可以随意阅读任何章节。\n\n本站的左侧边拦就是《图解网络》的目录结构（别看篇章不多，每一章都是很长很长的文章哦 :laughing:）：\n\n- **网络基础篇** :point_down:\n  - [TCP/IP 网络模型有哪几层？](网络/1_base/tcp_ip_model.md) \n  - [键入网址到网页显示，期间发生了什么？](网络/1_base/what_happen_url.md) \n  - [Linux 系统是如何收发网络包的？](网络/1_base/how_os_deal_network_package.md) \n- **HTTP 篇** :point_down:\n\t- [HTTP 常见面试题](网络/2_http/http_interview.md) \n\t- [HTTP/1.1 如何优化？](网络/2_http/http_optimize.md) \n\t- [HTTPS RSA 握手解析](网络/2_http/https_rsa.md) \n\t- [HTTPS ECDHE 握手解析](网络/2_http/https_ecdhe.md) \n\t- [HTTPS 如何优化？](网络/2_http/https_optimize.md) \n\t- [HTTP/2 牛逼在哪？](网络/2_http/http2.md) \n\t- [HTTP/3 强势来袭](网络/2_http/http3.md) \n\t- [既然有 HTTP 协议，为什么还要有 RPC？](网络/2_http/http_rpc.md) \n\t- [既然有 HTTP 协议，为什么还要有 websocket？](网络/2_http/http_websocket.md) \n- **TCP 篇** :point_down:\n\t- [TCP 三次握手与四次挥手面试题](网络/3_tcp/tcp_interview.md) \n\t- [TCP 重传、滑动窗口、流量控制、拥塞控制](网络/3_tcp/tcp_feature.md) \n\t- [TCP 实战抓包分析](网络/3_tcp/tcp_tcpdump.md) \n\t- [TCP 半连接队列和全连接队列](网络/3_tcp/tcp_queue.md) \n\t- [如何优化 TCP?](网络/3_tcp/tcp_optimize.md) \n\t- [如何理解是 TCP 面向字节流协议？](网络/3_tcp/tcp_stream.md) \n\t- [为什么 TCP 每次建立连接时，初始化序列号都要不一样呢？](网络/3_tcp/isn_deff.md) \n\t- [SYN 报文什么时候情况下会被丢弃？](网络/3_tcp/syn_drop.md) \n\t- [四次挥手中收到乱序的 FIN 包会如何处理？](网络/3_tcp/out_of_order_fin.md) \n\t- [在 TIME_WAIT 状态的 TCP 连接，收到 SYN 后会发生什么？](网络/3_tcp/time_wait_recv_syn.md) \n\t- [TCP 连接，一端断电和进程崩溃有什么区别？](网络/3_tcp/tcp_down_and_crash.md) \n\t- [拔掉网线后， 原本的 TCP 连接还存在吗？](网络/3_tcp/tcp_unplug_the_network_cable.md) \n\t- [tcp_tw_reuse 为什么默认是关闭的？](网络/3_tcp/tcp_tw_reuse_close.md) \n\t- [HTTPS 中 TLS 和 TCP 能同时握手吗？](网络/3_tcp/tcp_tls.md) \n\t- [TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？](网络/3_tcp/tcp_http_keepalive.md) \n\t- [TCP 有什么缺陷？](网络/3_tcp/tcp_problem.md)\n\t- [如何基于 UDP 协议实现可靠传输？](网络/3_tcp/quic.md)\n\t- [TCP 和 UDP 可以使用同一个端口吗？](网络/3_tcp/port.md)\n\t- [服务端没有 listen，客户端发起连接建立，会发生什么？](网络/3_tcp/tcp_no_listen.md)\n\t- [没有 accept，可以建立 TCP 连接吗？](网络/3_tcp/tcp_no_accpet.md)\n\t- [用了 TCP 协议，数据一定不会丢吗？](网络/3_tcp/tcp_drop.md)\n\t- [TCP 四次挥手，可以变成三次吗？](网络/3_tcp/tcp_three_fin.md)\n- **IP 篇** :point_down:\n\t- [IP 基础知识全家桶](网络/4_ip/ip_base.md) \t\n\t- [ping 的工作原理](网络/4_ip/ping.md) \t\n\t- [断网了，还能 ping 通 127.0.0.1 吗？](网络/4_ip/ping_lo.md)\n- **学习心得** :point_down:\n\t- [计算机网络怎么学？](网络/1_base/计算机网络怎么学.md) \t\n  - [画图经验分享](网络/5_learn/draw.md) \t\n\n## 质量如何？\n\n图解网络的质量小林说的不算，读者说的算！\n\n图解网络的第一个版本自去年发布以来，每隔一段时间，就会有不少的读者跑来感激小林。\n\n他们说看了我的图解网络，轻松应对大厂的网络面试题，而且每次面试时问到网络问题，他们一点都不慌，甚至暗暗窃喜。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/160f55b965cf4c42ba160e327178a783.png)\n\n## 有错误怎么办？\n\n小林是个手残党，时常写出错别字。\n\n如果你在学习的过程中，**如果你发现有任何错误或者疑惑的地方，欢迎你通过邮箱或者底部留言给小林**，勘误邮箱：xiaolincoding@163.com\n\n小林抽时间会逐个修正，然后发布新版本的图解网络 PDF，一起迭代出更好的图解网络！\n\n新的图解文章都在公众号首发，别忘记关注了哦！如果你想加入百人技术交流群，扫码下方二维码回复「加群」。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/其他/公众号介绍.png)\n\n","lastmodified":"2023-08-01T09:41:32.868106574Z","tags":[]},"/Obsidian/Front-Matter":{"title":"Front Matter","content":"\n\n使用 Front Matter 可以保存 note 待元数据，推荐使用 Hugo 的配置 [Front matter | Hugo (gohugo.io)](https://gohugo.io/content-management/front-matter/)\n\n","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/Obsidian/Obsidian-plugin":{"title":"Obsidian-plugin","content":"\n* [advanced-table](https://github.com/tgrosinger/advanced-tables-obsidian)\n* [banners](https://github.com/noatpad/obsidian-banners)\n* [calendar](https://github.com/liamcain/obsidian-calendar-plugin)\n* [commander](https://github.com/phibr0/obsidian-commander)\n* [dataview](https://github.com/blacksmithgu/obsidian-dataview)\n* [emoji-shortcodes](https://github.com/phibr0/obsidian-emoji-shortcodes)\n* [emoji-toolbar](https://github.com/oliveryh/obsidian-emoji-toolbar)\n* [excel-to-markdown-table](https://github.com/ganesshkumar/obsidian-excel-to-markdown-table)\n* [homepage](https://github.com/mirnovov/obsidian-homepage)\n* [hover-editor](https://github.com/nothingislost/obsidian-hover-editor)\n* [icon-folder)](https://github.com/FlorianWoelki/obsidian-icon-folder)\n* [icons](https://github.com/visini/obsidian-icons-plugin)\n* [image-toolkit](https://github.com/sissilab/obsidian-image-toolkit)\n* [minimal-settings](https://github.com/kepano/obsidian-minimal-settings)\n* [obsidian-git](https://github.com/denolehov/obsidian-git)\n* [recent-files](https://github.com/tgrosinger/recent-files-obsidian)\n* [settings-search](https://github.com/javalent/settings-search)\n* [style-settings](https://github.com/mgmeyers/obsidian-style-settings)\n* [tag-wrangler](https://github.com/pjeby/tag-wrangler)\n* [excalidraw](https://github.com/zsviczian/obsidian-excalidraw-plugin)","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/Obsidian/dataview":{"title":"dataview","content":"\n\n# 官方地址\n\n* [代码仓库](https://github.com/blacksmithgu/obsidian-dataview)\n* [文档地址](https://blacksmithgu.github.io/obsidian-dataview/)\n\n# 其他教程\n*  [Obsidian DataView 入门保姆级引导手册](https://zhuanlan.zhihu.com/p/614881764)\n\n# 元数据\n\n元数据是一系列的键值对,可以给笔记，可以给note,list item ,task 添加元数据\n\n## 如何添加元数据\n\n### Frontmatter\n\n* frontmatter 是markdown的一种扩展，可以使用yaml 来添加元数据\n\n```\n --- \n alias: \"document\" \n last-reviewed: 2021-08-17 \n thoughts: \n\t rating: 8 \n\t reviewable: false \n ---\n```\n\n###  inline fields\n* 使用方法为在文件的任意位置添加\n```text\n\nBasic Field:: Some random Value \n**Bold Field**:: Nice!\n  \n```\n\n* 如果你需要标注list itme 或者 task 需要使用中括号\n```\n- [ ] Send an mail to David about the deadline [due:: 2022-04-05].\n```\n\n\n# 另外还有隐含的元数据\n\n## page 中的元数据\n\n\n[# Metadata on Pages](https://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-pages/)\n\n\n| Field Name       | Data Type      | Description                                                                                                                                                                   |\n|------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| file.name        | Text           | The file name as seen in Obsidians sidebar.                                                                                                                                   |\n| file.folder      | Text           | The path of the folder this file belongs to.                                                                                                                                  |\n| file.path        | Text           | The full file path, including the files name.                                                                                                                                 |\n| file.ext         | Text           | The extension of the file type; generally md.                                                                                                                                 |\n| file.link        | Link           | A link to the file.                                                                                                                                                           |\n| file.size        | Number         | The size (in bytes) of the file.                                                                                                                                              |\n| file.ctime       | Date with Time | The date that the file was created.                                                                                                                                           |\n| file.cday        | Date           | The date that the file was created.                                                                                                                                           |\n| file.mtime       | Date with Time | The date that the file was last modified.                                                                                                                                     |\n| file.mday        | Date           | The date that the file was last modified.                                                                                                                                     |\n| file.tags        | List           | A list of all unique tags in the note. Subtags are broken down by each level, so #Tag/1/A will be stored in the list as [#Tag, #Tag/1, #Tag/1/A].                             |\n| file.etags       | List           | A list of all explicit tags in the note; unlike file.tags, does not break subtags down, i.e. [#Tag/1/A]                                                                       |\n| file.inlinks     | List           | A list of all incoming links to this file, meaning all files that contain a link to this file.                                                                                |\n| file.outlinks    | List           | A list of all outgoing links from this file, meaning all links the file contains.                                                                                             |\n| file.aliases     | List           | A list of all aliases for the note as defined via the YAML frontmatter.                                                                                                       |\n| file.tasks       | List           | A list of all tasks (I.e., \\| [ ] some task) in this file.                                                                                                                    |\n| file.lists       | List           | A list of all list elements in the file (including tasks); these elements are effectively tasks and can be rendered in task views.                                            |\n| file.frontmatter | List           | Contains the raw values of all frontmatter in form of key \\| value text values; mainly useful for checking raw frontmatter values or for dynamically listing frontmatter keys. |\n| file.day         | Date           | Only available if the file has a date inside its file name (of form yyyy-mm-dd or yyyymmdd), or has a Date field/inline field.                                                |\n| file.starred     | Boolean        | if this file has been starred via the Obsidian Core Plugin \"Starred Files\".                                                                                                   |\n\n\n## 列表和任务中的元数据\n\n[# Metadata on Tasks and Lists](https://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-tasks/)\n\n| Field name     | Data Type | Description                                                                                                                                                                                                                                                                                               |\n|----------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| status         | Text      | The completion status of this task, as determined by the character inside the [ ] brackets. Generally a space \" \" for incomplete tasks and a \"x\" for complete tasks, but allows for plugins which support alternative task statuses.                                                                      |\n| checked        | Boolean   | Whether or not this task status is empty, meaning it has a space in its [ ] brackets                                                                                                                                                                                                                      |\n| completed      | Boolean   | Whether or not this specific task has been completed; this does not consider the completionnon-completion of any child tasks. A task is explicitly considered \"completed\" if it has been marked with an 'x'. If you use a custom status, i.e. [-], checked will be true, whereas completed will be false. |\n| fullyCompleted | Boolean   | Whether or not this task and all of its subtasks are completed.                                                                                                                                                                                                                                           |\n| text           | Text      | The plain text of this task, including any metadata field annotations.                                                                                                                                                                                                                                    |\n| visual         | Text      | The text of this task, which is rendered by Dataview. It can be modified to render arbitary text.                                                                                                                                                                                                         |\n| line           | Number    | The line of the file this task shows up on.                                                                                                                                                                                                                                                               |\n| lineCount      | Number    | The number of Markdown lines that this task takes up.                                                                                                                                                                                                                                                     |\n| path           | Text      | The full path of the file this task is in. Equals to file.path for pages                                                                                                                                                                                                                                  |\n| section        | Link      | link to the section this task is contained in.                                                                                                                                                                                                                                                            |\n| tags           | List      | Any tags inside of the text task.                                                                                                                                                                                                                                                                         |\n| outlinks       | List      | Any links defined in this task.                                                                                                                                                                                                                                                                           |\n| link           | Link      | link to the closest linkable block near this task; useful for making links which go to the task.                                                                                                                                                                                                          |\n| children       | List      | ny subtasks or sublists of this task.                                                                                                                                                                                                                                                                     |\n| task           | Boolean   | If true, this is a task; otherwise, it is a regular list element.                                                                                                                                                                                                                                         |\n| annotated      | Boolean   | True if the task text contains any metadata fields, false otherwise.                                                                                                                                                                                                                                      |\n| parent         | Number    | The line number of the task above this task, if present; will be null if this is a root-level task.                                                                                                                                                                                                       |\n| blockId        | Text      | The block ID of this task / list element, if one has been defined with the ^blockId syntax; otherwise null.                                                                                                                                                                                               |\n\n\n# DQL\n\n比较类似于sql, 可是实现以下的功能\n\n- Choosing an **output format** of your output (the [Query Type](https://blacksmithgu.github.io/obsidian-dataview/queries/query-types/))\n- Fetch pages **from a certain [source](https://blacksmithgu.github.io/obsidian-dataview/reference/sources/)**, i.e. a tag, folder or link\n- **Filtering pages/data** by simple operations on fields, like comparison, existence checks, and so on\n- **Transforming fields** for displaying, i.e. with calculations or splitting up multi-value fields\n- **Sorting** results based on fields\n- **Grouping** results based on fields\n- **Limiting** your result count\n\n## 查询语法\n\n```text\n```dataview \n\t\u003cQUERY-TYPE\u003e \u003cfields\u003e \n\tFROM \u003csource\u003e \n\t\u003cDATA-COMMAND\u003e \u003cexpression\u003e \n\t\u003cDATA-COMMAND\u003e \u003cexpression\u003e \n\t...\n```\n```\t\n```\n\n\n## 输出类型\n\n* **TABLE**: A table of results with one row per result and one or many columns of **field data**.\n* **LIST**: A bullet point list of **pages** which match the query. You can output one field for each page alongside their file links.\n* **TASK**: An interactive task list of **tasks** that match the given query.\n* **CALENDAR**: A calendar view displaying each hit via a dot on its referred date.\n\n\n```text\nLists all pages in your vault as a bullet point list\n\t```dataview \n\tLIST \n\t```\n\t\nLists all tasks (completed or not) in your vault \n\t```dataview \n\tTASK \n\t```\n\t\nRenders a Calendar view where each page is represented as a dot on its creation date. \n\t```dataview \n\tCALENDAR file.cday \n\t```\n\t\nShows a table with all pages of your vault, their field value of due, the files' tags and an average of the values of multi-value field working-hours \n\t```dataview \n\tTABLE due, file.tags AS \"tags\", average(working-hours)\n\t ```\n\n```\n\n\n## 数据来源\n\n* tags\n* folders\n* note\n* lint\n\n```\nLists all pages inside the folder Books and its sub folders \n\t```dataview \n\tLIST FROM \"Books\" \n\t``` \n\t\nLists all pages that include the tag #status/open or #status/wip \n\t```dataview \n\tLIST FROM #status/open OR #status/wip \n\t``` \n\t\nLists all pages that have either the tag #assignment and are inside folder \"30 School\" (or its sub folders), or are inside folder \"30 School/32 Homeworks\" and are linked on the page School Dashboard Current To Dos \n\n\t```dataview \n\tLIST FROM (#assignment AND \"30 School\") OR (\"30 School/32 Homeworks\" AND outgoing([[School Dashboard Current To Dos]])) \n\t```\n\n```\n\n\n## Filter, sort, group or limit results\n\n* ***FROM** like explained [above](https://blacksmithgu.github.io/obsidian-dataview/queries/structure/#choose-your-source).\n*  **WHERE**: Filter notes based on information **inside** notes, the meta data fields.\n*  **SORT**: Sorts your results depending on a field and a direction.\n*  **GROUP BY**: Bundles up several results into one result row per group.\n*  **LIMIT**: Limits the result count of your query to the given number.\n*  **FLATTEN**: Splits up one result into multiple results based on a field or calculation.\n\n\n```\nLists all pages that have a metadata field `due` and where `due` is before today \n\n\t```dataview \n\tLIST WHERE due AND due \u003c date(today) \n\t``` \nLists the 10 most recently created pages in your vault that have the tag #status/open \n\t```dataview \n\tLIST FROM #status/open SORT file.ctime DESC LIMIT 10 \n\t``` \nLists the 10 oldest and incompleted tasks of your vault as an interactive task list, grouped by their containing file and sorted from oldest to newest file. \n\t```dataview \n\tTASK WHERE !completed SORT created ASC LIMIT 10 GROUP BY file.link SORT rows.file.ctime ASC \n\t```\n\n\n```","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/Obsidian/excalidraw":{"title":"excalidraw","content":"\n*  [代码仓库](https://github.com/zsviczian/obsidian-excalidraw-plugin)\n* note 中插入excalidraw 语法\n\n```\n![[excalidraw]]\n```","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/Obsidian/obsidian-overview":{"title":"obsidian overview","content":"\n# 主页内容\n\nobsidian 相关内容，包括插件\n\n\n# 结构\n\n","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/Obsidian/publish":{"title":"publish","content":"\n\n\n[obsidian 目前最完美的免费发布方案 渐进式教程 by oldwinter](https://publish.obsidian.md/chinesehelp/01+2021%E6%96%B0%E6%95%99%E7%A8%8B/obsidian+%E7%9B%AE%E5%89%8D%E6%9C%80%E5%AE%8C%E7%BE%8E%E7%9A%84%E5%85%8D%E8%B4%B9%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88+%E6%B8%90%E8%BF%9B%E5%BC%8F%E6%95%99%E7%A8%8B+by+oldwinter#%E5%87%A0%E4%B8%AA%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88%E5%AF%B9%E6%AF%94)","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/Obsidian/template":{"title":"template","content":"\n* *[模板的使用方法](https://publish.obsidian.md/help-zh/%E6%8F%92%E4%BB%B6/%E6%A8%A1%E6%9D%BF)\n* 默认存放的文件夹 `/template`\n\n","lastmodified":"2023-08-01T09:41:29.504058336Z","tags":["Obsidian"]},"/lua/Lua%E9%AB%98%E7%BA%A7":{"title":"Lua高级","content":"\n# 元表\n\n元表 _(metatable)_ 的**表现行为类似于 C++ 语言中的操作符重载**，例如我们可以重载 \"__add\" 元方法 _(metamethod)_，来计算两个 Lua 数组的并集；或者重载 \"__index\" 方法，来定义我们自己的 Hash 函数。Lua 提供了两个十分重要的用来处理元表的方法\n\n- setmetatable(table, metatable)：此方法用于为一个表设置元表。\n    \n- getmetatable(table)：此方法用于获取表的元表对象\n    \n\n设置元表\n\n```Lua\nlocal mytable = {}\nlocal mymetatable = {}\nsetmetatable(mytable, mymetatable)\n```\n\n  \n\n## **修改表的操作符行为**\n\n  \n\n通过重载 \"__add\" 元方法来计算集合的并集实例\n\n```Lua\nlocal set1 = {10, 20, 30}   -- 集合\nlocal set2 = {20, 40, 50}   -- 集合\n\n-- 将用于重载__add的函数，注意第一个参数是self\nlocal union = function (self, another)\n    local set = {}\n    local result = {}\n\n    -- 利用数组来确保集合的互异性\n    for i, j in pairs(self) do set[j] = true end\n    for i, j in pairs(another) do set[j] = true end\n\n    -- 加入结果集合\n    for i, j in pairs(set) do table.insert(result, i) end\n    return result\nend\nsetmetatable(set1, {__add = union}) -- 重载 set1 表的 __add 元方法\n\nlocal set3 = set1 + set2\nfor _, j in pairs(set3) do\n    io.write(j..\" \")               --\u003eoutput：30 50 20 40 10\nend\n```\n\n除了加法可以被重载之外，Lua 提供的所有操作符都可以被重载：\n| 元方法        | 含义                                                                         |\n|------------|----------------------------------------------------------------------------|\n| \"__add    | #NAME?                                                                     |\n| \"__sub   | - 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__mul    | * 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__div   | / 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__mod   | % 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__pow    | ^ （幂）操作 其行为类似于 \"add\" 操作                                                    |\n| \"__unm\"   | 一元 - 操作                                                                    |\n| \"__concat\" | .. （字符串连接）操作                                                               |\n| \"__len\"    | # 操作                                                                       |\n| \"__eq\"     | == 操作 函数 getcomphandler 定义了 Lua 怎样选择一个处理器来作比较操作 仅在两个对象类型相同且有对应操作相同的元方法时才起效 |\n| \"__lt\"     | \u003c 操作                                                                       |\n| \"__le\"     | \u003c= 操作                                                                      |\n\n\n除了操作符之外，如下元方法也可以被重载，下面会依次解释使用方法：\n\n|   |   |\n|---|---|\n|元方法|含义|\n|\"__index\"|取下标操作用于访问 table[key]|\n|\"__newindex\"|赋值给指定下标 table[key] = value|\n|\"__tostring\"|转换成字符串|\n|\"__call\"|当 Lua 调用一个值时调用|\n|\"__mode\"|用于弱表(week table)|\n|\"__metatable\"|用于保护metatable不被访问|\n\n## **__index 元方法**\n\n```Lua\nmytable = setmetatable({key1 = \"value1\"},   --原始表\n{__index = function(self, key)            --重载函数\n    if key == \"key2\" then\n        return \"metatablevalue\"\n    end\nend\n})\n\nprint(mytable.key1,mytable.key2)  --\u003e output：value1 metatablevalue\n```\n\n关于 __index 元方法，有很多比较高阶的技巧，例如：__index 的元方法不需要非是一个函数，他也可以是一个表。\n\n```Lua\nt = setmetatable({[1] = \"hello\"}, {__index = {[2] = \"world\"}})\nprint(t[1], t[2])   --\u003ehello wor\n```\n\n## **__tostring 元方法**\n\n  \n\n与 Java 中的 toString() 函数类似，可以实现自定义的字符串转换。\n\n```Lua\narr = {1, 2, 3, 4}\narr = setmetatable(arr, {__tostring = function (self)\n    local result = '{'\n    local sep = ''\n    for _, i in pairs(self) do\n        result = result ..sep .. i\n        sep = ', '\n    end\n    result = result .. '}'\n    return result\nend})\nprint(arr)  --\u003e {1, 2, 3, 4}\n```\n\n## **__call 元方法**\n\n__call 元方法的功能类似于 C++ 中的仿函数，使得普通的表也可以被调用。\n\n  \n\n```Lua\nfunctor = {}\nfunction func1(self, arg)\n    print (\"called from\", arg)\nend\nsetmetatable(functor, {__call = func1})\n\nfunctor(\"functor\")  --\u003e called from functor\nprint(functor)      --\u003e output：0x00076fc8 （后面这串数字可能不一样）\n```\n\n## **__metatable 元方法**\n\n假如我们想保护我们的对象使其使用者既看不到也不能修改 metatables。我**们可以对 metatable 设置了 __metatable 的值，getmetatable 将返回这个域的值，而调用 setmetatable 将会出错**：\n\n```Lua\nbject = setmetatable({}, {__metatable = \"You cannot access here\"})\n\nprint(getmetatable(Object)) --\u003e You cannot access heresetmetatable(Object, {})    --\u003e 引发编译器报错\n```\n\n  \n\n# 面向对象\n\n## 类\n\n在 Lua 中，我们可以使用表和函数实现面向对象。**将函数和相关的数据放置于同一个表中就形成了一个对象。**\n\n```Plaintext\nlocal _M = {}\n\nlocal mt = { __index = _M }\n\nfunction _M.deposit (self, v)\n    self.balance = self.balance + v\nend\n\nfunction _M.withdraw (self, v)\n    if self.balance \u003e v then\n        self.balance = self.balance - v\n    else\n        error(\"insufficient funds\")\n    end\nend\n\nfunction _M.new (self, balance)\n    balance = balance or 0\n    return setmetatable({balance = balance}, mt)\nend\n\nreturn _M\n```\n\n引用\n\n```Lua\nlocal account = require(\"account\")\n\nlocal a = account:new()\na:deposit(100)\n\nlocal b = account:new()\nb:deposit(50)\n\nprint(a.balance)  --\u003e output: 100\nprint(b.balance)  --\u003e output: 50\n```\n\n上面这段代码 \"setmetatable({balance = balance}, mt)\"，其中 mt 代表 `{ __index = _M }` ，这句话值得注意。根据我们在元表这一章学到的知识，我们明白，setmetatable 将 `_M` 作为新建表的原型，所以在自己的表内找不到 'deposit'、'withdraw' 这些方法和变量的时候，便会到 __index 所指定的 _M 类型中去寻找。\n\n  \n\n## 继承\n\n继承可以用元表实现，它提供了在父类中查找存在的方法和变量的机制。在 Lua 中是不推荐使用继承方式完成构造的，这样做引入的问题可能比解决的问题要多，下面一个是字符串操作类库，给大家演示一下。\n\n```Lua\n---------- s_base.lualocal _M = {}\n\nlocal mt = { __index = _M }\n\nfunction _M.upper (s)return string.upper(s)\nendreturn _M\n\n---------- s_more.lualocal s_base = require(\"s_base\")\n\nlocal _M = {}\n_M = setmetatable(_M, { __index = s_base })\n\n\nfunction _M.lower (s)return string.lower(s)\nendreturn _M\n\n---------- test.lualocal s_more = require(\"s_more\")\n\nprint(s_more.upper(\"Hello\"))   -- output: HELLOprint(s_more.lower(\"Hello\"))   -- output: hello\n```\n\n  \n\n## 成员私有性\n\n在动态语言中引入成员私有性并没有太大的必要，反而会显著增加运行时的开销，毕竟这种检查无法像许多静态语言那样在编译期完成。下面的技巧把对象作为各方法的 upvalue，本身是很巧妙的，但会让子类继承变得困难，同时构造函数动态创建了函数，会导致构造函数无法被 JIT 编译。\n\n在 Lua 中，成员的私有性，使用类似于函数闭包的形式来实现。在我们之前的银行账户的例子中，我们使用一个工厂方法来创建新的账户实例，通过工厂方法对外提供的闭包来暴露对外接口。而不想暴露在外的例如 balance 成员变量，则被很好的隐藏起来。\n\n```Lua\nfunction newAccount (initialBalance)\n    local self = {balance = initialBalance}\n    local withdraw = function (v)\n        self.balance = self.balance - v\n    end\n    local deposit = function (v)\n        self.balance = self.balance + v\n    end\n    local getBalance = function () \n        return self.balance \n    end\n    \n    return {\n        withdraw = withdraw,\n        deposit = deposit,\n        getBalance = getBalance\n    }\nend\n\na = newAccount(100)\na.deposit(100)\nprint(a.getBalance()) --\u003e 200print(a.balance)      --\u003e nil\n```\n\n  \n\n# 局部变量\n\nLua 的设计有一点很奇怪，**在一个 block 中的变量，如果之前没有定义过，那么认为它是一个全局变量**，**而不是这个 block 的局部变量**。这一点和别的语言不同。**容易造成不小心覆盖了全局同名变量的错误**。\n\n## **定义**\n\nLua 中的局部变量要用 local 关键字来显式定义，不使用 local 显式定义的变量就是全局变量\n\n```Lua\ng_var = 1         -- global var\nlocal l_var = 2   -- local var\n```\n\n## **作用域**\n\n**局部变量的生命周期是有限的，它的作用域仅限于声明它的块（block）**。一个块是一个控制结构的执行体、或者是一个函数的执行体再或者是一个程序块（chunk）。\n\n```Lua\nx = 10\nlocal i = 1         -- 程序块中的局部变量 i\n\nwhile i \u003c=x do\n    local x = i * 2   -- while 循环体中的局部变量 x\n    print(x)          -- output： 2, 4, 6, 8, ...\n    i = i + 1\nend\n\nif i \u003e 20 then\n    local x           -- then 中的局部变量 x\n    x = 20\n    print(x + 2)      -- 如果i \u003e 20 将会打印 22，此处的 x 是局部变量\nelse\n    print(x)          -- 打印 10，这里 x 是全局变量\nend\n\nprint(x)            -- 打印 10\n```\n\n  \n\n## 使用局部变量的好处\n\n  \n\n1. 局部变量可以避免因为命名问题污染了全局环境\n    \n2. local 变量的访问比全局变量更快\n    \n3. 由于局部变量出了作用域之后生命周期结束，这样可以被垃圾回收器及时释放\n    \n\n  \n\n  \n\n## 检测模块的函数使用局部变量\n\nfoo.lua\n\n```Lua\nlocal _M = { _VERSION = '0.01' }\n\nfunction _M.add(a, b)     --两个number型变量相加\n    return a + b\nend\n\nfunction _M.update_A()    --更新变量值\n    A = 365               -- A 是全局变量\nend\n\nreturn _M\n```\n\nuse_foo.lua\n\n```Lua\nA = 360     --定义全局变量\n\nlocal foo = require(\"foo\")\n\nlocal b = foo.add(A, A)\nprint(\"b = \", b)\n\nfoo.update_A()\nprint(\"A = \", A)\n```\n\n因为A 是全局变量，改变了A的值\n\nLua 上下文中应当严格避免使用自己定义的全局变量。**可以使用一个 lj-releng 工具来扫描 Lua 代码，定位使用 Lua 全局变量的地方**。lj-releng 的相关链接：[https://github.com/openresty/openresty-devel-utils/blob/master/lj-releng](https://github.com/openresty/openresty-devel-utils/blob/master/lj-releng)\n\nWindows 用户把 lj-releng 文件所在的目录的绝对路径添加进 PATH 环境变量。然后进入你自己的 Lua 文件所在的工作目录，得到如下结果：\n\n```Lua\n#  lj-releng\nfoo.lua: 0.01 (0.01)\nChecking use of Lua global variables in file foo.lua...\nop no.  line  instruction args  ; code\n2  [8] SETGLOBAL 0 -1  ; A\nChecking line length exceeding 80...\nWARNING: No \"_VERSION\" or \"version\" field found in `use_foo.lua`.\nChecking use of Lua global variables in file use_foo.lua...\nop no.  line  instruction args  ; code\n2  [1] SETGLOBAL 0 -1  ; A\n7  [4] GETGLOBAL 2 -1  ; A\n8  [4] GETGLOBAL 3 -1  ; A\n18 [8] GETGLOBAL 4 -1  ; A\n```\n\n当然，更推荐采用 **luacheck 来检查项目中全局变量，之后的“代码静态分析”一节，我们还会讲到如何使用 luacheck**。\n\n  \n\n# 判断数组的大小\n\n- table.getn(t) 等价于 t 但**计算的是数组元素，不包括 hash 键值**。而且数组是以第一个 nil 元素来判断数组结束。\n    \n- `#` 只计算 array 的元素个数，它实际上调用了对象的 metatable 的 `__len` 函数。对于有 `__len` 方法的函数返回函数返回值，不然就返回数组成员数目\n    \n- _Lua_ 内部实际采用哈希表和数组分别保存键值对、普通值，所以不推荐混合使用这两种赋值方式。\n    \n- Lua 数组中允许 nil 值的存在，但是数组默认结束标志却是 nil。这类比于 C 语言中的字符串，字符串中允许 '\\0' 存在，但当读到 '\\0' 时，就认为字符串已经结束了。\n    \n- 初始化是例外，在 Lua 相关源码中，初始化数组时首先判断数组的长度，若长度大于 0 ，并且最后一个值不为 nil，返回包括 nil 的长度；若最后一个值为 nil，则返回截至第一个非 nil 值的长度。\n    \n- **如果你要删除一个数组中的元素，请使用 remove 函数，而不是用 nil 赋值**\n    \n\n```Lua\n-- test.lua\nlocal tblTest1 = { 1, a = 2, 3 }\nprint(\"Test1 \" .. #(tblTest1))\n\nlocal tblTest2 = { 1, nil }\nprint(\"Test2 \" .. #(tblTest2))\n\nlocal tblTest3 = { 1, nil, 2 }\nprint(\"Test3 \" .. #(tblTest3))\n\nlocal tblTest4 = { 1, nil, 2, nil }\nprint(\"Test4 \" .. #(tblTest4))\n\nlocal tblTest5 = { 1, nil, 2, nil, 3, nil }\nprint(\"Test5 \" .. #(tblTest5))\n\nlocal tblTest6 = { 1, nil, 2, nil, 3, nil, 4, nil }\nprint(\"Test6 \" .. #(tblTest6))\n```\n\n我们分别使用 Lua 和 LuaJIT 来执行一下：\n\n```Lua\n➜ luajit test.lua\nTest1 2\nTest2 1\nTest3 1\nTest4 1\nTest5 1\nTest6 1\n\n➜ lua test.lua\nTest1 2\nTest2 1\nTest3 3\nTest4 1\nTest5 3\nTest6 1\n```\n\n这一段的输出结果，就是这么 **匪夷所思**。不要在 Lua 的 table 中使用 nil 值，**如果一个元素要删除，直接 remove，不要用 nil 去代替**。\n\n  \n\n# 非空判断\n\n  \n\n有时候不小心引用了一个没有赋值的变量，这时它的值默认为 nil。如果对一个 nil 进行索引的话，会导致异常。\n\n```Plaintext\nlocal person = {name = \"Bob\", sex = \"M\"}\n\n-- do something\nperson = nil\n-- do something\nprint(person.name)\n```\n\n会报错\n\n```Lua\nstdin:1:attempt to index global 'person' (a nil value)\nstack traceback:\n   stdin:1: in main chunk\n   [C]: ?\n```\n\n在实际的工程代码中，我们很难这么轻易地发现我们引用了 nil 变量。因此，在很多情况下我们在访问一些 table 型变量时，需要先判断该变量是否为 nil，例如将上面的代码改成\n\n```Lua\nlocal person = {name = \"Bob\", sex = \"M\"}\n\n-- do something\nperson = nil\n-- do something\nif person ~= nil and person.name ~= nil then\n    print(person.name)\nelse\n-- do somethingend\n```\n\n对于简单类型的变量，我们可以用 _if (var == nil) then_ 这样的简单句子来判断。**但是对于 table 型的 Lua 对象，就不能这么简单判断它是否为空了。一个 table 型变量的值可能是** **`{}`**，这时它不等于 nil。我们来看下面这段代码：\n\n```Lua\nlocal next = next\nlocal a = {}\nlocal b = {name = \"Bob\", sex = \"Male\"}\nlocal c = {\"Male\", \"Female\"}\nlocal d = nil\n\nprint(#a)\nprint(#b)\nprint(#c)\n--print(#d)    -- error\n\nif a == nil then\n    print(\"a == nil\")\nend\n\nif b == nil then\n    print(\"b == nil\")\nend\n\nif c == nil then\n    print(\"c == nil\")\nend\n\nif d== nil then\n    print(\"d == nil\")\nend\n\nif next(a) == nil then\n    print(\"next(a) == nil\")\nend\n\nif next(b) == nil then\n    print(\"next(b) == nil\")\nend\n\nif next(c) == nil then\n    print(\"next(c) == nil\")\nend\n```\n\n输出\n\n```Lua\n0\n0\n2\nd == nil\nnext(a) == nil\n```\n\n因此，我们要判断一个 table 是否为 `{}`，不能采用 `#table == 0` 的方式来判断。可以用下面这样的方法来判断：\n\n```Plaintext\nfunction isTableEmpty(t)\n    return t == nil or next(t) == nil\nend\n```\n\n注意：**`next`** **指令是不能被 LuaJIT 的 JIT 编译优化，并且 LuaJIT 貌似没有明确计划支持这个指令优化，在不是必须的情况下，尽量少用。**\n\n  \n\n# 正则表达式\n\n同时存在两套正则表达式规范：_Lua_ 语言的规范和 `ngx.re.*` 的规范，即使您对 _Lua_ 语言中的规范非常熟悉，我们仍不建议使用 _Lua_ 中的正则表达式。\n\n- 一是因为 _Lua_ 中正则表达式的性能并不如 `ngx.re.*` 中的正则表达式优秀；\n    \n- 二是 _Lua_ 中的正则表达式并不符合 _POSIX_ 规范，而 `ngx.re.*` 中实现的是标准的 _POSIX_ 规范，后者明显更具备通用性。\n    \n\n`ngx.re.*` 中的 `o` 选项，指明该参数，被编译的 Pattern 将会在工作进程中缓存，并且被当前工作进程的每次请求所共享。Pattern 缓存的上限值通过 `lua_regex_cache_max_entries` 来修改，它的默认值为1024。\n\n`ngx.re.*` 中的 `j` 选项，指明该参数，如果使用的 PCRE 库支持 JIT，OpenResty 会在编译 Pattern 时启用 JIT。启用 JIT 后正则匹配会有明显的性能提升。较新的平台，自带的 PCRE 库均支持 JIT。如果系统自带的 PCRE 库不支持 JIT，出于性能考虑，最好自己编译一份 libpcre.so，然后在编译 OpenResty 时链接过去。要想验证当前 PCRE 库是否支持 JIT，可以这么做\n\n1. 编译 OpenResty 时在 `./configure` 中指定 `--with-debug` 选项\n    \n2. 在 `error_log` 指令中指定日志级别为 `debug`\n    \n3. 运行正则匹配代码，查看日志中是否有 `pcre JIT compiling result: 1`\n    \n\n即使运行在不支持 JIT 的 OpenResty 上，加上 `j` 选项也不会带来坏的影响。在 OpenResty 官方的 Lua 库中，正则匹配至少都会带上 `jo` 这两个选项。\n\n```Lua\nlocation /test {\n    content_by_lua_block {\n        local regex = [[\\d+]]\n\n        -- 参数 \"j\" 启用 JIT 编译，参数 \"o\" 是开启缓存必须的\n        local m = ngx.re.match(\"hello, 1234\", regex, \"jo\")\n        if m then\n            ngx.say(m[0])\n        else\n            ngx.say(\"not matched!\")\n        end\n    }\n}\n```\n\n#### **Lua 正则简单汇总**\n\n_Lua_ 中正则表达式语法上最大的区别，_Lua_ 使用 _'%'_ 来进行转义，而其他语言的正则表达式使用 _'\\'_ 符号来进行转义。其次，_Lua_ 中并不使用 _'?'_ 来表示非贪婪匹配，而是定义了不同的字符来表示是否是贪婪匹配。定义如下：\n\n|符号|匹配次数|匹配模式|\n|---|---|---|\n|+|匹配前一字符 1 次或多次|非贪婪|\n|`*`|匹配前一字符 0 次或多次|贪婪|\n|-|匹配前一字符 0 次或多次|非贪婪|\n|?|匹配前一字符 0 次或1次|仅用于此，不用于标识是否贪婪|\n\n|符号|匹配模式|\n|---|---|\n|.|任意字符|\n|%a|字母|\n|%c|控制字符|\n|%d|数字|\n|%l|小写字母|\n|%p|标点字符|\n|%s|空白符|\n|%u|大写字母|\n|%w|字母和数字|\n|%x|十六进制数字|\n|%z|代表 0 的字符|\n\n  \n\n# 虚变量\n\n当一个方法返回多个值时，有些返回值有时候用不到，要是声明很多变量来一一接收，显然不太合适（不是不能）。**Lua 提供了一个虚变量(dummy variable)的概念， 按照****[惯例](https://www.lua.org/pil/1.3.html)****以一个下划线（“_”）来命名，用它来表示丢弃不需要的数值，仅仅起到占位的作用。**\n\n  \n\n## 返回值\n\n```Lua\n-- string.find (s,p) 从string 变量s的开头向后匹配 string\n-- p，若匹配不成功，返回nil，若匹配成功，返回第一次匹配成功\n-- 的起止下标。\n\nlocal start, finish = string.find(\"hello\", \"he\") --start 值为起始下标，finish\n--值为结束下标\nprint ( start, finish )                          --输出 1   2\n\nlocal start = string.find(\"hello\", \"he\")      -- start值为起始下标\nprint ( start )                               -- 输出 1\n\n\nlocal _,finish = string.find(\"hello\", \"he\")   --采用虚变量（即下划线），接收起\n--始下标值，然后丢弃，finish接收\n--结束下标值\nprint ( finish )                              --输出 2\nprint ( _ )    \n```\n\n  \n\n## 迭代\n\n```Lua\n-- test.lua 文件\nlocal t = {1, 3, 5}\n\nprint(\"all  data:\")\nfor i,v in ipairs(t) do\n    print(i,v)\nend\n\nprint(\"\")\nprint(\"part data:\")\nfor _,v in ipairs(t) do\n    print(v)\nend\n```\n\n输出\n\n```Lua\n# luajit test.lua\nall  data:\n1   1\n2   3\n3   5\n\npart data:\n1\n3\n5\n```\n\n# **抵制使用 module() 定义模块**\n\n旧式的模块定义方式是通过 `module(\"filename\"[,package.seeall])*` 来显式声明一个包，现在官方不推荐再使用这种方式\n\n这种方式将会返回一个由 `filename` 模块函数组成的 `table`，并且还会定义一个包含该 `table` 的全局变量。\n\n  \n\n1. `package.seeall` 这种方式破坏了模块的高内聚，原本引入 \"filename\" 模块只想调用它的 _foobar()_ 函数，但是它却可以读写全局属性，例如 `\"filename.os\"`。\n    \n2. `module` 函数压栈操作引发的副作用，污染了全局环境变量。例如 `module(\"filename\")` 会创建一个 `filename` 的 `table`，并将这个 `table` 注入全局环境变量中，这样使得没有引用它的文件也能调用 `filename` 模块的方法。\n    \n\n  \n\n推荐的模块定义\n\n```Lua\n-- square.lua 长方形模块\nlocal _M = {}           -- 局部的变量\n_M._VERSION = '1.0'     -- 模块版本\n\nlocal mt = { __index = _M }\n\nfunction _M.new(self, width, height)\n    return setmetatable({ width=width, height=height }, mt)\nend\n\nfunction _M.get_square(self)\n    return self.width * self.height\nend\n\nfunction _M.get_circumference(self)\n    return (self.width + self.height) * 2\nend\n\nreturn _M\n```\n\n使用\n\n```Lua\nlocal square = require \"square\"\nlocal s1 = square:new(1, 2)\nprint(s1:get_square())          --output: 2\nprint(s1:get_circumference())   --output: 6\n```\n\n另一个跟 Lua 的 module 模块相关需要注意的点是，当 lua_code_cache on 开启时，require 加载的模块是会被缓存下来的，这样我们的模块就会以最高效的方式运行，直到被显式地调用如下语句（这里有点像模块卸载）：\n\n```Plaintext\npackage.loaded[\"square\"] = nil\n```\n\n  \n\n## 调用函数前先定义函数\n\nLua 里面的函数必须放在调用的代码之前，下面的代码是一个常见的错误：\n\n```Lua\n-- test.lua 文件local i = 100\ni = add_one(i)\n\nfunction add_one(i)\n    return i + 1\nend\n```\n\n因此在函数定义之前使用函数相当于在变量赋值之前使用变量，Lua 世界对于没有赋值的变量，默认都是 nil，所以这里也就产生了一个 nil 的错误。\n\n  \n\n# 点号操作符和冒号操作符的区别\n\n```Plaintext\nlocal str = \"abcde\"\n\nprint(\"case 1:\", str:sub(1, 2))\nprint(\"case 2:\", str.sub(str, 1, 2))\n```\n\n输出\n\n```Lua\ncase 1: ab\ncase 2: ab\n```\n\n- **冒号操作会带入一个** **`self`** **参数，用来代表** **`自己`****。**\n    \n- 而点号操作，只是 `内容` 的展开。\n    \n\n在函数定义时，使用冒号将默认接收一个 `self` 参数，而使用点号则需要显式传入 `self` 参数\n\n示例代码：\n\n```Plaintext\nobj = { x = 20 }\n\nfunction obj:fun1()\n    print(self.x)\nend\n```\n\n等价于\n\n```Plaintext\nobj = { x = 20 }\n\nfunction obj.fun1(self)\n    print(self.x)\nend\n```\n\n# module的缺点\n\n由于 `lua_code_cache off` 情况下，缓存的代码会伴随请求完结而释放。module 的最大好处缓存这时候是无法发挥的，所以本章的内容都是基于 `lua_code_cache on` 的情况下。\n\n先看看下面代码：\n\n```Plaintext\nlocal ngx_socket_tcp = ngx.socket.tcp           -- ①\n\nlocal _M = { _VERSION = '0.06' }                -- ②\nlocal mt = { __index = _M }                     -- ③\n\nfunction _M.new(self)\n    local sock, err = ngx_socket_tcp()          -- ④\n    if not sock then\n        return nil, err\n    end\n    return setmetatable({ sock = sock }, mt)    -- ⑤\nend\n\nfunction _M.set_timeout(self, timeout)\n    local sock = self.sock\n    if not sock then\n        return nil, \"not initialized\"\n    end\n\n    return sock:settimeout(timeout)\nend\n\n-- ... 其他功能代码，这里简略\n\nreturn _M\n```\n\n1. 对于比较底层的模块，内部使用到的非本地函数，都需要 local 本地化，这样做的好处：\n    \n    1. 避免命名冲突：防止外部是 `require(...)` 的方法调用造成全局变量污染\n        \n    2. 访问局部变量的速度比全局变量更快、更快、更快（重要事情说三遍）\n        \n\n  \n\n2. 每个基础模块最好有自己 `_VERSION` 标识，方便后期利用 `_VERSION` 完成热代码部署等高级特性，也便于使用者对版本有整体意识。\n    \n3. 其实 `_M` 和 `mt` 对于不同的请求实例（require 方法得到的对象）是相同的，因为 module 会被缓存到全局环境中。所以在这个位置千万不要放单请求内个性信息，例如 ngx.ctx 等变量。\n    \n4. **这里需要实现的是给每个实例绑定不同的 tcp 对象**，后**面 setmetatable 确保了每个实例拥有自己的 socket 对象，所以必须放在 new 函数中**。如果放在 ③ 的下面，那么这时候所有的不同实例内部将绑定了同一个 socket 对象。\n    \n\n```Plaintext\nlocal mt = { __index = _M }                     -- ③\nlocal sock = ngx_socket_tcp()                   -- ④ 错误的\n\nfunction _M.new(self)\n    return setmetatable({ sock = sock }, mt)    -- ⑤\nend\n```\n\n5. Lua 的 module 有两种类型：\n    \n    1. 支持面向对象痕迹可以保留私有属性；静态方法提供者，没有任何私有属性。\n        \n    2. 真正起到区别作用的就是 setmetatable 函数，是否有自己的个性元表，最终导致两种不同的形态。\n        \n\n# FFI\n\nhttps://moonbingbing.gitbooks.io/openresty-best-practices/content/lua/FFI.html\n\nFFI 库，是 LuaJIT 中最重要的一个扩展库。它允许从纯 Lua 代码调用外部 C 函数，使用 C 数据结构。\n\n  \n\nFFI 库最大限度的省去了使用 C 手工编写繁重的 `Lua/C` 绑定的需要。不需要学习一门独立/额外的绑定语言——它解析普通 C 声明。这样可以从 C 头文件或参考手册中，直接剪切，粘贴。它的任务就是绑定很大的库，但不需要捣鼓脆弱的绑定生成器。\n\nFFI 紧紧的整合进了 LuaJIT（几乎不可能作为一个独立的模块）。`JIT` 编译器在 C 数据结构上所产生的代码，等同于一个 C 编译器应该生产的代码。在 `JIT` 编译过的代码中，调用 C 函数，可以被内连处理，不同于基于 `Lua/C API` 函数调用。\n\n  \n\n## **ffi 库 词汇**\n\n|   |   |\n|---|---|\n|noun|Explanation|\n|cdecl|A definition of an abstract C type(actually, is a lua string)|\n|ctype|C type object|\n|cdata|C data object|\n|ct|C type format, is a template object, may be cdecl, cdata, ctype|\n|cb|callback object|\n|VLA|An array of variable length|\n|VLS|A structure of variable length|\n\n## **ffi.* API**\n\n**功能：** _Lua ffi 库的 API，与 LuaJIT 不可分割。_\n\n毫无疑问，在 `lua` 文件中使用 `ffi` 库的时候，必须要有下面的一行。\n\n```Plaintext\nlocal ffi = require \"ffi\"\n```\n\n# JIT\n\n看一下 LuaJIT 官方的解释：LuaJIT is a Just-In-Time Compilerfor the Lua programming language。\n\n**LuaJIT 的运行时环境包括一个用手写汇编实现的 Lua 解释器和一个可以直接生成机器代码的 JIT 编译器**\n\n- 一开始的时候，Lua 字节码总是被 LuaJIT 的解释器解释执行。LuaJIT 的解释器会在执行字节码时同时记录一些运行时的统计信息，比如每个 Lua 函数调用入口的实际运行次数，还有每个 Lua 循环的实际执行次数。\n    \n- 当这些次数超过某个预设的阈值时，便认为对应的 Lua 函数入口或者对应的 Lua 循环足够的“热”，这时便会触发 JIT 编译器开始工作。\n    \n- JIT 编译器会从热函数的入口或者热循环的某个位置开始尝试编译对应的 Lua 代码路径。编译的过程是把 LuaJIT 字节码先转换成 LuaJIT 自己定义的中间码（IR），然后再生成针对目标体系结构的机器码（比如 x86_64 指令组成的机器码）\n    \n- 如果当前 Lua 代码路径上的所有的操作都可以被 JIT 编译器顺利编译，则这条编译过的代码路径便被称为一个“trace”，在物理上对应一个 `trace` 类型的 GC 对象（即参与 Lua GC 的对象）。\n    \n\n  \n\nJIT 编译器不支持的原语被称为 **NYI（Not Yet Implemented）原语**。比较完整的 NYI 列表在这篇文档里面：\n\n```Plaintext\nhttp://wiki.luajit.org/NYI\n```\n\n所谓“让更多的 Lua 代码被 JIT 编译”，其实就是帮助更多的 Lua 代码路径能为 JIT 编译器所接受。这一般通过两种途径来实现：\n\n1. 调整对应的 Lua 代码，**避免使用 NYI 原语**。\n    \n2. 增强 JIT 编译器，让越来越多的 NYI 原语能够被编译。\n    \n\n## **可以被 JIT 编译的元操作**\n\n下面给大家列一下截止到目前已经可以被 JIT 编译的元操作。 其他还有 IO、Bit、FFI、Coroutine、OS、Package、Debug、JIT 等分类，使用频率相对较低，这里就不罗列了，可以参考官网：[http://wiki.luajit.org/NYI](http://wiki.luajit.org/NYI)。\n\n### **基础库的支持情况**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|assert|yes||\n|collectgarbage|no||\n|dofile|never||\n|error|never||\n|getfenv|2.1 partial|只有 getfenv(0) 能编译|\n|getmetatable|yes||\n|ipairs|yes||\n|load|never||\n|loadfile|never||\n|loadstring|never||\n|next|no||\n|pairs|no||\n|pcall|yes||\n|print|no||\n|rawequal|yes||\n|rawget|yes||\n|rawlen (5.2)|yes||\n|rawset|yes||\n|select|partial|第一个参数是静态变量的时候可以编译|\n|setfenv|no||\n|setmetatable|yes||\n|tonumber|partial|不能编译非10进制，非预期的异常输入|\n|tostring|partial|只能编译：字符串、数字、布尔、nil 以及支持 __tostring元方法的类型|\n|type|yes||\n|unpack|no||\n|xpcall|yes||\n\n### **字符串库**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|string.byte|yes||\n|string.char|2.1||\n|string.dump|never||\n|string.find|2.1 partial|只有字符串样式查找（没有样式）|\n|string.format|2.1 partial|不支持 %p 或 非字符串参数的 %s|\n|string.gmatch|no||\n|string.gsub|no||\n|string.len|yes||\n|string.lower|2.1||\n|string.match|no||\n|string.rep|2.1||\n|string.reverse|2.1||\n|string.sub|yes||\n|string.upper|2.1||\n\n### **表**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|table.concat|2.1||\n|table.foreach|no|2.1: 内部编译，但还没有外放|\n|table.foreachi|2.1||\n|table.getn|yes||\n|table.insert|partial|只有 push 操作|\n|table.maxn|no||\n|table.pack (5.2)|no||\n|table.remove|2.1|部分，只有 pop 操作|\n|table.sort|no||\n|table.unpack (5.2)|no||\n\n### **math 库**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|math.abs|yes||\n|math.acos|yes||\n|math.asin|yes||\n|math.atan|yes||\n|math.atan2|yes||\n|math.ceil|yes||\n|math.cos|yes||\n|math.cosh|yes||\n|math.deg|yes||\n|math.exp|yes||\n|math.floor|yes||\n|math.fmod|no||\n|math.frexp|no||\n|math.ldexp|yes||\n|math.log|yes||\n|math.log10|yes||\n|math.max|yes||\n|math.min|yes||\n|math.modf|yes||\n|math.pow|yes||\n|math.rad|yes||\n|math.random|yes||\n|math.randomseed|no||\n|math.sin|yes||\n|math.sinh|yes||\n|math.sqrt|yes||\n|math.tan|yes||\n|math.tanh|yes||","lastmodified":"2023-08-01T09:41:32.852106344Z","tags":["lua"]},"/lua/lua%E5%9F%BA%E7%A1%80":{"title":"lua基础","content":"\n\n# Lua 简介\n\nLua 是一个小巧的脚本语言。是巴西里约热内卢天主教大学（Pontifical Catholic University of Rio de Janeiro）里的一个研究小组并于 1993 年开发。**其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能**。Lua 由标准 C 编写而成，几乎在所有操作系统和平台上都可以编译、运行。Lua 并没有提供强大的库，这是由它的定位决定的。所以 Lua 不适合作为开发独立应用程序的语言。**Lua 有一个同时进行的 JIT 项目，提供在特定平台上的即时编译功能**。\n\n- **Lua 脚本可以很容易的被 C/C++ 代码调用，也可以反过来调用 C/C++ 的函数，这使得 Lua 在应用程序中可以被广泛应用**。\n    \n- **不仅仅作为扩展脚本，也可以作为普通的配置文件，代替 XML、ini 等文件格式，并且更容易理解和维护**。\n    \n- 标准 Lua 5.1 解释器由标准 C 编写而成，代码简洁优美，几乎在所有操作系统和平台上都可以编译和运行；\n    \n- 一个完整的标准 Lua 5.1 解释器不足 200 KB。而本书推荐使用的 LuaJIT 2 的代码大小也只有不足 500 KB\n    \n- 同时也支持大部分常见的体系结构。在目前所有脚本语言引擎中，LuaJIT 2 实现的速度应该算是最快的之一。这一切都决定了 Lua 是作为嵌入式脚本的最佳选择。\n    \n\nLua 语言的各个版本是不相兼容的。因此本书只介绍 Lua 5.1 语言，这是为标准 Lua 5.1 解释器和 LuaJIT 2 所共同支持的。LuaJIT 支持的对 Lua 5.1 向后兼容的 Lua 5.2 和 Lua 5.3 的特性，我们也会在方便的时候予以介绍。\n\n  \n\n# Lua 环境搭建\n\n[http://openresty.org](http://openresty.org/)\n\n  \n\n## Helloworld\n\n```Go\n# cat hello.lua\nprint(\"hello world\")\n# luajit hello.lua\nhello world\n```\n\n  \n\n# 基本数据类型\n\n  \n\n```Go\nprint(type(\"helloworld\"))\nprint(type('helloworld'))\nprint(type('true'))\nprint(type(1))\nprint(type(2.1))\nprint(type(nil))\nfunction hello()\n    print(\"hello\")\nend\nprint(type(hello))\n```\n\n输出\n\n```Go\nstring\nstring\nstring\nnumber\nnumber\nnil\nfunction\n```\n\n## Nil\n\nNil 是一种类型，Lua 将 nil 用于表示“无效值”。\n\n- 一个变量在第一次赋值前的默认值是 nil，\n    \n- 将 nil 赋予给一个全局变量就等同于删除它。\n    \n\n```Go\nlocal num\nprint(num)        --\u003eoutput:nil\n\nnum = 100\nprint(num)        --\u003eoutput:100\n```\n\n## Boolean (布尔)\n\n布尔类型，可选值 true/false；\n\n- Lua 中 nil 和 false 为“假”\n    \n- 其它所有值均为“真”。比如 0 和空字符串就是“真”；\n    \n\n```Go\nlocal a = true\nlocal b = 0\nlocal c = nil\nif a then\n    print(\"a\")        --\u003eoutput:a\nelse\n    print(\"not a\")    --这个没有执行\nend\n\nif b then\n    print(\"b\")        --\u003eoutput:b\nelse\n    print(\"not b\")    --这个没有执行\nend\n\nif c then\n    print(\"c\")        --这个没有执行\nelse\n    print(\"not c\")    --\u003eoutput:not c\nend\n```\n\n## **number（数字）**\n\nNumber 类型用于表示实数，和 C/C++ 里面的 double 类型很类似。可以使用数学函数 math. Floor（向下取整）和 math. Ceil（向上取整）进行取整操作。\n\n一般地，Lua 的 number 类型就是用双精度浮点数来实现的。值得一提的是，LuaJIT 支持所谓的“dual-number”（双数）模式，\n\n- 即 **LuaJIT 会根据上下文用整型来存储整数，而用双精度浮点数来存放浮点数。**\n    \n\n```Go\nlocal order = 3.99\nlocal score = 98.01\nprint(math.floor(order))   --\u003eoutput:3\nprint(math.ceil(score))    --\u003eoutput:99\nprint(9223372036854775807LL - 1)  --\u003eoutput:9223372036854775806LL\n```\n\n## String（字符串）\n\nLua 中有三种方式表示字符串:\n\n1. 使用一对匹配的单引号。例：'hello'。\n    \n2. 使用一对匹配的双引号。例：\"abclua\"。\n    \n3. 字符串还可以用一种长括号（即 [[ ]]）括起来的方式定义\n    \n    1. 我们把两个正的方括号（即[[）间插入 n 个等号定义为第 n 级正长括号。\n        \n    2. 0 级正的长括号写作 [[ ，一级正的长括号写作 [=[\n        \n    3. 反的长括号也作类似定义；举个例子，4 级反的长括号写作 ]====]\n        \n    4. **一个长字符串可以由任何一级的正的长括号开始，而由第一个碰到的同级反的长括号结束**。整个词法分析过程将**不受分行限制，不处理任何转义符，并且忽略掉任何不同级别的长括号**\n        \n\n  \n\n```Plaintext\nlocal str1 = 'hello world'\nlocal str2 = \"hello lua\"\nlocal str3 = [[\"add\\name\",'hello']]\nlocal str4 = [=[string have a [[]].]=]\nlocal str5 = [=[asdfasd]=]\n\nprint(str1)    --\u003eoutput:hello world\nprint(str2)    --\u003eoutput:hello lua\nprint(str3)    --\u003eoutput:\"add\\name\",'hello'\nprint(str4)    --\u003eoutput:string have a [[]].\nprint(str5)    --\u003eoutput:asdfasd\n```\n\n在 Lua 实现中，Lua 字符串一般都会经历一个“内化”（intern）的过程，**即两个完全一样的 Lua 字符串在 Lua 虚拟机中只会存储一份**。每一个 Lua 字符串在创建时都会**插入到 Lua 虚拟机内部的一个全局的哈希表**中\n\n1. 创建相同的 Lua 字符串并不会引入新的动态内存分配操作，所以相对便宜（但仍有全局哈希表查询的开销），\n    \n2. 内容相同的 Lua 字符串不会占用多份存储空间，\n    \n3. 已经创建好的 Lua 字符串之间进行相等性比较时是 `O(1)` 时间度的开销，而不是通常见到的 `O(n)`.\n    \n\n## Table (表)\n\nTable 类型实现了一种抽象的“关联数组”。“关联数组”是一种具有特殊索引方式的数组，\n\n- 索引通常是**字符串（string）或者 number 类型，但也可以是除** **`nil`** **以外的任意类型的值**\n    \n\n```Go\n\nlocal corp = {\n    web = \"www.google.com\",   --索引为字符串，key = \"web\",\n    --            value = \"www.google.com\"\n    telephone = \"12345678\",   --索引为字符串\n    staff = {\"Jack\", \"Scott\", \"Gary\"}, --索引为字符串，值也是一个表\n    100876,              --相当于 [1] = 100876，此时索引为数字\n    --      key = 1, value = 100876\n    100191,              --相当于 [2] = 100191，此时索引为数字\n    [10] = 360,          --直接把数字索引给出\n    [\"city\"] = \"Beijing\" --索引为字符串\n}\n\nprint(corp.web)               --\u003eoutput:www.google.com\nprint(corp[\"web\"])               --\u003eoutput:www.google.com\nprint(corp[\"telephone\"])      --\u003eoutput:12345678\nprint(corp[2])                --\u003eoutput:100191\nprint(corp[\"city\"])           --\u003eoutput:\"Beijing\"\nprint(corp.staff[1])          --\u003eoutput:Jack\nprint(corp[\"staff\"][1])          --\u003eoutput:Jack\nprint(corp[10])               --\u003eoutput:360\n```\n\n在内部实现上，table 通常实现为一个哈希表、一个数组、或者两者的混合。具体的实现为何种形式，动态依赖于具体的 table 的键分布特点。\n\n## Function (函数)\n\n在 Lua 中，**函数** 也是一种数据类型，函数可以存储在变量中，可以通过参数传递给其他函数，还可以作为其他函数的返回值\n\n```Go\nlocal function foo()\n    print(\"in the function\")\n    --dosomething()\n    local x = 10\n    local y = 20\n    return x + y\nend\n\nlocal a = foo    --把函数赋给变量\n\nprint(a())\n\n--output:\n--in the function\n--30\n\nfunction foo()\nend\n--等价于\n\nfoo = function ()\nend\n\nlocal function foo()\nend\n-- 等价于\n\nlocal foo = function ()\nend\n```\n\n  \n\n# 表达式\n\n## 算术运算符\n\n|            |      |\n| ---------- | ---- |\n| 算术运算符 | 说明 |\n| +          | 加法 |\n| -          | 减法 |\n| *          | 乘法 |\n| /          | 除法 |\n| ^          | 指数 |\n| %          | 取模 |\n\n```Go\nprint(1 + 2)       --\u003e打印 3\nprint(5 / 10)      --\u003e打印 0.5。 这是Lua不同于c语言的\nprint(5.0 / 10)    --\u003e打印 0.5。 浮点数相除的结果是浮点数\n-- print(10 / 0)   --\u003e注意除数不能为0，计算的结果会出错\nprint(2 ^ 10)      --\u003e打印 1024。 求2的10次方\n\nlocal num = 1357\nprint(num % 2)       --\u003e打印 1\nprint((num % 2) == 1) --\u003e打印 true。 判断num是否为奇数\n```\n\n## 关系运算符\n\n  \n\n|            |          |\n| ---------- | -------- |\n| 关系运算符 | 说明     |\n| \u003c          | 小于     |\n| \u003e          | 大于     |\n| \u003c=         | 小于等于 |\n| \u003e=         | 大于等于 |\n| ==         | 等于     |\n| ~=         | 不等于   |\n\n  \n\n```Go\nprint(1 \u003c 2)    --\u003e打印 true\nprint(1 == 2)   --\u003e打印 false\nprint(1 ~= 2)   --\u003e打印 true\nlocal a, b = true, false\nprint(a == b)  --\u003e打印 false\n```\n\n- 在使用“==”做等于判断时，要注意对于 table, userdate 和函数， Lua 是作引用比较的。也就是说，只有当两个变量引用同一个对象时，才认为它们相等\n    \n\n```Go\nlocal a = { x = 1, y = 0}\nlocal b = { x = 1, y = 0}\nif a == b then\n    print(\"a==b\")\nelse\n    print(\"a~=b\")\nend\n---output:\na~=b\n```\n\n- Lua 字符串总是会被“内化”，即相同内容的字符串只会被保存一份，因此 Lua 字符串之间的相等性比较可以简化为其内部存储地址的比较。\n    \n- 这意味着 Lua 字符串的相等性比较总是为 O (1)\n    \n\n## 逻辑运算符\n\n|            |        |\n| ---------- | ------ |\n| 逻辑运算符 | 说明   |\n| and        | 逻辑与 |\n| or         | 逻辑或 |\n| not        | 逻辑非 |\n\n在 c 语言中，and 和 or 只得到两个值 1 和 0，其中 1 表示真，0 表示假。而 Lua 中 and 的执行过程是这样的：\n\n- `a and b` 如果 a 为 nil，则返回 a，否则返回 b;\n    \n- `a or b` 如果 a 为 nil，则返回 b，否则返回 a。\n    \n- **所有逻辑操作符将 false 和 nil 视作假，其他任何值视作真，对于 and 和 or，“短路求值”，对于 not，永远只返回 true 或者 false。**\n    \n\n```Go\nlocal c = nil\nlocal d = 0\nlocal e = 100\nprint(c and d)  --\u003e打印 nil\nprint(c and e)  --\u003e打印 nil\nprint(d and e)  --\u003e打印 100\nprint(c or d)   --\u003e打印 0\nprint(c or e)   --\u003e打印 100\nprint(not c)    --\u003e打印 true\nprint(not d)    --\u003e打印 false\n```\n\n## 字符串连接\n\nLua 中连接两个字符串，可以使用操作符“..”（两个点）\n\n- 如果其任意一个操作数是数字的话，Lua 会将这个数字转换成字符串。\n    \n- 注意，连接操作符只会创建一个新字符串，而不会改变原操作数\n    \n- 也可以使用 string 库函数 `string.format` 连接字符串\n    \n\n```Go\nprint(\"Hello \" .. \"World\")    --\u003e打印 Hello Worldprint(0 .. 1)                 --\u003e打印 01\n\nstr1 = string.format(\"%s-%s\",\"hello\",\"world\")\nprint(str1)              --\u003e打印 hello-world\n\nstr2 = string.format(\"%d-%s-%.2f\",123,\"world\",1.21)\nprint(str2)              --\u003e打印 123-world-1.21\n```\n\n于 Lua 字符串本质上是只读的，**因此字符串连接运算符几乎总会创建一个新的（更大的）字符串**。这意味着如果有很多这样的连接操作（比如在循环中使用 .. 来拼接最终结果），则性能损耗会非常大。在这种情况下，推荐使用 table 和 `table.concat()` 来进行很多字符串的拼接\n\n```Go\nlocal pieces = {}\nfor i, elem in ipairs(my_list) do\n    pieces[i] = my_process(elem)\nend\nlocal res = table.concat(pieces)\n```\n\n上面的例子还可以使用 LuaJIT 独有的 `table.new` 来恰当地初始化 `pieces` 表的空间，以避免该表的动态生长。\n\n## 优先级\n\n| f               |     |\n| --------------- | --- |\n| ^               |     |\n| not # -         |     |\n| * / %           |     |\n| + -             |     |\n| ..              |     |\n| \u003c \u003e \u003c= \u003e= == ~= |     |\n| and             |     |\n| or              |     |\n  \n\n```Go\nlocal a, b = 1, 2\nlocal x, y = 3, 4\nlocal i = 10\nlocal res = 0\nres = a + i \u003c b/2 + 1  --\u003e等价于res =  (a + i) \u003c ((b/2) + 1)\nres = 5 + x^2*8        --\u003e等价于res =  5 + ((x^2) * 8)\nres = a \u003c y and y \u003c=x  --\u003e等价于res =  (a \u003c y) and (y \u003c= x)\n```\n\n  \n\n# 控制结构\n\n## If-else\n\n### **单个 if 分支型**\n\n```Go\nx = 10\nif x \u003e 0 then\n    print(\"x is a positive number\")\nend\n```\n\n### **两个分支 if-else 型**\n\n```Go\nx = 10\nif x \u003e 0 then\n    print(\"x is a positive number\")\nelse\n    print(\"x is a non-positive number\")\nend\n```\n\n### 多个分支的 if-elseif-else\n\n```Go\n\nscore = 90\nif score == 100 then\n    print(\"Very good!Your score is 100\")\nelseif score \u003e= 60 then\n    print(\"Congratulations, you have passed it,your score greater or equal to 60\")\n    --此处可以添加多个elseif\nelse\n    print(\"Sorry, you do not pass the exam! \")\nend\n```\n\n与 C 语言的不同之处是 else 与 if 是连在一起的，若将 else 与 if 写成 \"else if\" 则相当于在 else 里嵌套另一个 if 语句，如下代码：\n\n```Go\nscore = 0\nif score == 100 then\n    print(\"Very good!Your score is 100\")\nelseif score \u003e= 60 then\n    print(\"Congratulations, you have passed it,your score greater or equal to 60\")\nelse\n    if score \u003e 0 then\n        print(\"Your score is better than 0\")\n    else\n        print(\"My God, your score turned out to be 0\")\n    end --与上一示例代码不同的是，此处要添加一个end\nend\n```\n\n## While\n\n```Go\nwhile 表达式 do\n    --body\nend\n```\n\n  \n\n## Repeat\n\nLua 中的 repeat 控制结构类似于其他语言（如：C++ 语言）中的 do-while，但是控制方式是刚好相反的。简单点说，**执行 repeat 循环体后，直到 until 的条件为真时才结束**\n\n```Lua\n-- 以下代码会死循环\nx = 10\nrepeat\n    print(x)\nuntil false\n```\n\n  \n\n## For\n\n### **for 数字型**\n\n```Lua\nfor var = begin, finish, step do\n    --body\nend\n```\n\n1. Var 从 begin 变化到 finish，每次变化都以 step 作为步长递增 var\n    \n2. Begin、finish、step 三个表达式只会在循环开始时执行一次\n    \n3. 第三个表达式 step 是可选的，默认为 1\n    \n4. 控制变量 var 的作用域仅在 for 循环内，需要在外面控制，则需将值赋给一个新的变量\n    \n5. 循环过程中不要改变控制变量的值，那样会带来不可预知的影响\n    \n\n```Lua\nfor i = 1, 5 do\n    print(i)\nend\n-- output:\n1\n2\n3\n4\n5\n\nfor i = 1, 10, 2 do\n    print(i)\nend\n-- output:\n1\n3\n5\n7\n9\n```\n\n## For 泛型\n\n泛型 for 循环通过一个迭代器（iterator）函数来遍历所有值：\n\n```Lua\n-- 打印数组a的所有值local a = {\"a\", \"b\", \"c\", \"d\"}\nfor i, v in ipairs(a) do\n    print(\"index:\", i, \" value:\", v)\nend\n-- output:\nindex:  1  value: a\nindex:  2  value: b\nindex:  3  value: c\nindex:  4  value: d\n```\n\nLua 的基础库提供了 **ipairs，这是一个用于遍历数组的迭代器函数**。在每次循环中，i 会被赋予一个索引值，同时 v 被赋予一个对应于该索引的数组元素值。\n\n```Lua\n-- 打印table t中所有的\nkeyfor k in pairs(t) do\n    print(k)\nend\n```\n\n通过不同的迭代器，几乎可以遍历所有的东西，而且写出的代码极具可读性。标准库提供了几种迭代器，包括用于迭代文件中每行的（io. Lines）、迭代 table 元素的（pairs）、迭代数组元素的（ipairs）、迭代字符串中单词的（string. Gmatch）\n\n泛型 for 循环与数字型 for 循环有两个相同点：\n\n1. 循环变量是循环体的局部变量；\n    \n2. 决不应该对循环变量作任何赋值。\n    \n\n在 LuaJIT 2.1 中，**`ipairs()`** **内建函数是可以被 JIT 编译的，而** **`pairs()`** **则只能被解释执行。因此在性能敏感的场景，应当合理安排数据结构，避免对哈希表进行遍历**\n\n  \n\n## Break\n\n语句 `break` 用来终止 `while`、`repeat` 和 `for` 三种循环的执行，并跳出当前循环体，继续执行当前循环之后的语句\n\n```Lua\n-- 计算最小的x,使从1到x的所有数相加和大于100\nsum = 0\ni = 1while true do\n    sum = sum + i\n    if sum \u003e 100 then\n        break\n    end\n    i = i + 1\nend\nprint(\"The result is \" .. i)  \n--\u003eoutput:The result is 14\n```\n\n## Return\n\n  \n\n`return` 主要用于从函数中返回结果，或者用于简单的结束一个函数的执行。\n\n```Lua\nlocal function add(x, y)\n    return x + y\n    --print(\"add: I will return the result \" .. (x + y))\n    --因为前面有个return，若不注释该语句，则会报错\nend\n\nlocal function is_positive(x)\n    if x \u003e 0 then\n        return x .. \" is positive\"\n    else\n        return x .. \" is non-positive\"\n    end\n\n    --由于return只出现在前面显式的语句块，所以此语句不注释也不会报错\n    --，但是不会被执行，此处不会产生输出\n    print(\"function end!\")\nend\n\nlocal sum = add(10, 20)\nprint(\"The sum is \" .. sum)  --\u003eoutput:The sum is 30\nlocal answer = is_positive(-10)\nprint(answer)                --\u003eoutput:-10 is non-positive\n```\n\n  \n\n## Goto\n\n有了 `goto`，我们可以实现 `continue` 的功能：\n\n```Lua\nfor i=1, 3 do\n    if i \u003c= 2 then\n        print(i, \"yes continue\")\n        goto continue\n    end\n    print(i, \" no continue\")\n\n    ::continue::\n    print([[i'm end]])\nend\n```\n\n输出结果\n\n```Lua\n$ luajit test.lua\n1   yes continue\ni'm end\n2   yes continue\ni'm end\n3    no continue\ni'm end\n```\n\n# 函数\n\n## 定义\n\n```Lua\nfunction function_name (arc)  -- arc 表示参数列表，函数的参数列表可以为空\n    -- body\nend\n```\n\n上面的语法定义了一个全局函数，名为 `function_name`. 全局函数本质上就是函数类型的值赋给了一个全局变量，即上面的语法等价于\n\n```Lua\nfunction_name = function (arc)\n     -- body\nend\n```\n\n由于全局变量一般会污染全局名字空间，同时也有性能损耗（即查询全局环境表的开销），因此我们应当尽量使用“局部函数”，其记法是类似的，只是开头加上 `local` 修饰符：\n\n```Lua\nlocal function function_name (arc)\n    -- body\nend\n```\n\n定义函数\n\n1. 利用名字来解释函数、变量的目的，使人通过名字就能看出来函数、变量的作用。\n    \n2. 每个函数的长度要尽量控制在一个屏幕内，一眼可以看明白。\n    \n3. 让代码自己说话，不需要注释最好。\n    \n\n  \n\n由于函数定义等价于变量赋值，我们也可以把函数名替换为某个 Lua 表的某个字段，例如\n\n```Lua\nlocal foo = {}\nfunction foo.pr()\n    print(\"ssss\")\nend\n\nfoo.pr()\n```\n\n  \n\n## 参数\n\n### 按值传递\n\n**Lua 函数的参数大部分是按值传递的**。**当函数参数是 table 类型时，传递进来的是实际参数的引用**\n\n值传递就是调用函数时，实参把它的值通过赋值运算传递给形参，然后形参的改变和实参就没有关系了。在这个过程中，实参是通过它在参数表中的位置与形参匹配起来的。\n\n```Lua\nlocal function swap(a, b) --定义函数swap,函数内部进行交换两个变量的值\n    local temp = a\n    a = b\n    b = temp\n    print(a, b)\nend\n\nlocal x = \"hello\"\nlocal y = 20\nprint(x, y)\nswap(x, y)    --调用swap函数\nprint(x, y)   --调用swap函数后，x和y的值并没有交换\n\n--\u003eoutput\nhello 20\n20  hello\nhello 20\n```\n\n在调用函数的时候，**若形参个数和实参个数不同时，Lua 会自动调整实参个数**。调整规则：\n\n- 若实参个数大于形参个数，从左向右，多余的实参被忽略；\n    \n- 若实参个数小于形参个数，从左向右，**没有被实参初始化的形参会被初始化为 nil**\n    \n\n```Lua\nlocal function fun1(a, b)       --两个形参，多余的实参被忽略掉\n    print(a, b)\nend\n\nlocal function fun2(a, b, c, d) --四个形参，没有被实参初始化的形参，用nil初始化\n    print(a, b, c, d)\nend\n\nlocal x = 1\nlocal y = 2\nlocal z = 3\n\nfun1(x, y, z)         -- z被函数fun1忽略掉了，参数变成 x, y\nfun2(x, y, z)         -- 后面自动加上一个nil，参数变成 x, y, z, nil\n\n--\u003eoutput\n1   2\n1   2   3   nil\n```\n\n### 变长参数\n\n其实 Lua 还支持变长参数。若形参为 `...`，表示该函数可以接收不同长度的参数。访问参数的时候也要使用 `...`\n\n```Lua\n\nlocal function func( ... )                -- 形参为 ... ,表示函数采用变长参数\n\n    local temp = {...}                     -- 访问的时候也要使用 ...\n    local ans = table.concat(temp, \" \")    -- 使用 table.concat 库函数对数\n    -- 组内容使用 \" \" 拼接成字符串。\n    print(ans)\nend\n\nfunc(1, 2)        -- 传递了两个参数\nfunc(1, 2, 3, 4)  -- 传递了四个参数\n\n--\u003eoutput\n1 2\n\n1 2 3 4\n```\n\n### **具名参数**\n\nLua 还支持通过名称来指定实参，这时候要把所有的实参组织到一个 table 中，并将这个 table 作为唯一的实参传给函数。\n\n```Lua\nlocal function change(arg) -- change 函数，改变长方形的长和宽，使其各增长一倍\n  arg.width = arg.width * 2\n  arg.height = arg.height * 2return arg\nendlocal rectangle = { width = 20, height = 15 }\nprint(\"before change:\", \"width  =\", rectangle.width,\n                        \"height =\", rectangle.height)\nrectangle = change(rectangle)\nprint(\"after  change:\", \"width  =\", rectangle.width,\n                        \"height =\", rectangle.height)\n\n--\u003eoutput\nbefore change: width = 20  height =  15\nafter  change: width = 40  height =  30\n```\n\n  \n\n### 按引用传递\n\n**当函数参数是 table 类型时，传递进来的是实际参数的引用**，此时在函数内部对该 table 所做的修改，会直接对调用者所传递的实际参数生效，而无需自己返回结果和让调用者进行赋值\n\n```Plaintext\nfunction change(arg) --change函数，改变长方形的长和宽，使其各增长一倍\n  arg.width = arg.width * 2  --表arg不是表rectangle的拷贝，他们是同一个表\n  arg.height = arg.height * 2end                  -- 没有return语句了local rectangle = { width = 20, height = 15 }\nprint(\"before change:\", \"width = \", rectangle.width,\n                        \" height = \", rectangle.height)\nchange(rectangle)\nprint(\"after change:\", \"width = \", rectangle.width,\n                       \" height =\", rectangle.height)\n\n--\u003e output\nbefore change: width = 20  height = 15\nafter  change: width = 40  height = 30\n```\n\n## 函数返回值\n\nLua 具有一项与众不同的特性，允许函数返回多个值。\n\n```Lua\nlocal function swap(a, b)   \n    -- 定义函数 swap，实现两个变量交换值\n    return b, a              \n    -- 按相反顺序返回变量的值\nend\n\nlocal x = 1\nlocal y = 20\nx, y = swap(x, y)           -- 调用 swap 函数\nprint(x, y)                 --\u003e output   20     1\n```\n\n  \n\n当函数返回值的个数和接收返回值的变量的个数不一致时，Lua 也会自动调整参数个数调整规则：\n\n- 若返回值个数大于接收变量的个数，多余的返回值会被忽略掉；\n    \n- 若返回值个数小于参数个数，从左向右，没有被返回值初始化的变量会被初始化为 nil。\n    \n\n```Lua\nfunction init()             \n    --init 函数 返回两个值 1 和 \"lua\"\n    return 1, \"lua\"\nend\n\nx = init()\nprint(x)\n\nx, y, z = init()\nprint(x, y, z)\n\n--output\n1\n1 lua nil\n```\n\n  \n\n当一个函数有一个以上返回值，且函数调用不是一个列表表达式的最后一个元素，那么函数调用只会产生一个返回值, 也就是第一个返回值。\n\n```Lua\nlocal function init()       -- init 函数 返回两个值 1 和 \"lua\"\n    return 1, \"lua\"\nend\n\nlocal x, y, z = init(), 2   -- init 函数的位置不在最后，此时只返回 1\nprint(x, y, z)              --\u003eoutput  1  2  nil\n\nlocal a, b, c = 2, init()   -- init 函数的位置在最后，此时返回 1 和 \"lua\"\nprint(a, b, c)              --\u003eoutput  2  1  lua\n```\n\n函数调用的实参列表也是一个列表表达式。考虑下面的例子：\n\n```Lua\nlocal function init()\n    return 1, \"lua\"\nend\n\nprint(init(), 2)   --\u003eoutput  1  2\nprint(2, init())   --\u003eoutput  2  1  lua\n```\n\n如果你确保只取函数返回值的第一个值，可以使用括号运算符\n\n```Lua\nlocal function init()\n    return 1, \"lua\"\nend\nprint((init()), 2)   --\u003eoutput  1  2\nprint(2, (init()))   --\u003eoutput  2  1\n```\n\n**值得一提的是，如果实参列表中某个函数会返回多个值，同时调用者又没有显式地使用括号运算符来筛选和过滤，则这样的表达式是不能被 LuaJIT 2 所 JIT 编译的，而只能被解释执行。**\n\n  \n\n  \n\n# 全动态函数调用\n\n调用回调函数，并把一个数组参数作为回调函数的参数。\n\n```Lua\nlocal args = {...} or {}\nmethod_name(unpack(args, 1, table.maxn(args)))\n```\n\n```Lua\nlocal function run(x, y)\n    print('run', x, y)\nend\n\nlocal function attack(targetId)\n    print('targetId', targetId)\nend\n\nlocal function do_action(method, ...)\n    local args = {...} or {}\n    method(unpack(args, 1, table.maxn(args)))\nend\n\ndo_action(run, 1, 2)         -- output: run 1 2\ndo_action(attack, 1111)      -- output: targetId    1111\n```\n\n  \n\n# 模块\n\n从 Lua 5.1 语言添加了对模块和包的支持。一**个 Lua 模块的数据结构是用一个 Lua 值（通常是一个 Lua 表或者 Lua 函数）**。**一个 Lua 模块代码就是一个会返回这个 Lua 值的代码块**\n\n- 可以使用内建函数 `require()` 来加载和缓存模块。\n    \n- 简单的说，一个代码模块就是一个程序库，可以通过 `require` 来加载。**模块加载后的结果通过是一个 Lua table**\n    \n- **这个表就像是一个命名空间**，其内容就是模块中导出的所有东西，**比如函数和变量**。`require` 函数会返回 Lua 模块加载后的结果，即用于表示该 Lua 模块的 Lua 值。\n    \n\n  \n\n  \n\nLua 提供了一个名为 `require` 的函数用来加载模块。**要加载一个模块，只需要简单地调用** **`require`** **\"file\" 就可以了，file 指模块所在的文件名**。这个调用会返回一个由模块函数组成的 table，并且还会定义一个包含该 table 的全局变量。\n\n在 Lua 中创建一个模块最简单的方法是：**创建一个 table，并将所有需要导出的函数放入其中，最后返回这个 table 就可以了。相当于将导出的函数作为 table 的一个字段，在 Lua 中函数是第一类值，提供了天然的优势。**\n\n- 创建 my. Lua\n    \n\n```Lua\nlocal _M = {}\n\nlocal function get_name()\n    return \"Lucy\"\n    end\nfunction _M.greeting()\n    print(\"hello \" .. get_name())\nend\n\nreturn _M\n```\n\n- 把下面代码保存在文件 main. Lua 中，然后执行 main. Lua，调用上述模块。\n    \n\n```Lua\nlocal my_module = require(\"my\")\nmy_module.greeting()     --\u003eoutput: hello Lucy\n```\n\n  \n\n\u003e - 对于需要导出给外部使用的公共模块，处于安全考虑，**是要避免全局变量的出现**。我们可以使用 lj-releng 或 luacheck 工具完成全局变量的检测。至于如何做，到后面再讲。\n\u003e     \n\u003e - 另一个要注意的是，由于在 LuaJIT 中，**require 函数内不能进行上下文切换**，**所以不能够在模块的顶级上下文中调用 cosocket 一类的 API**。否则会报 `attempt to yield across C-call boundary` 错误。\n\u003e     \n\n  \n\n# String\n\nLua 字符串总是由字节构成的。Lua 核心并不尝试理解具体的字符集编码（比如 GBK 和 UTF-8 这样的多字节字符编码）\n\nLua 字符串内部用来标识各个组成字节的下标是从 1 开始的，这不同于像 C 和 Perl 这样的编程语言。这样数字符串位置的时候再也不用调整，对于非专业的开发者来说可能也是一个好事情，**string.Sub (str, 3, 7) 直接表示从第三个字符开始到第七个字符（含）为止的子串。**\n\n## **string.Byte (s [, i [, j ]])**\n\n返回字符 s[i]、s[i + 1]、s[i + 2]、······、s[j] 所对应的 ASCII 码\n\n```Lua\nprint(string.byte(\"abc\", 1, 3))\nprint(string.byte(\"abc\", 3)) -- 缺少第三个参数，第三个参数默认与第二个相同，此时为 3\nprint(string.byte(\"abc\"))    -- 缺少第二个和第三个参数，此时这两个参数都默认为 1\n\n--\u003eoutput\n97    98    99\n99\n97\n```\n\n## **string. Char (...)**\n\n接收 0 个或更多的整数（整数范围：0~255），返回这些整数所对应的 ASCII 码字符组成的字符串。当参数为空时，默认是一个 0。\n\n```Lua\nprint(string.char(96, 97, 98))\nprint(string.char())        -- 参数为空，默认是一个0，-- 你可以用string.byte(string.char())测试一下print(string.char(65, 66))\n\n--\u003e output\n`ab\n\nAB\n```\n\n## **string.Upper (s)**\n\n接收一个字符串 s，返回一个把所有小写字母变成大写字母的字符串。\n\n```Lua\nprint(string.upper(\"Hello Lua\"))  --\u003eoutput  HELLO LUA\n```\n\n## **string.Lower (s)**\n\n接收一个字符串 s，返回一个把所有大写字母变成小写字母的字符串。\n\n```Lua\nprint(string.lower(\"Hello Lua\"))  --\u003eoutput   hello lua\n```\n\n## **string.Len (s)**\n\n接收一个字符串，返回它的长度。\n\n```Lua\nprint(string.len(\"hello lua\")) --\u003eoutput  9\n```\n\n使用此函数是不推荐的。应当总是使用 `#` 运算符来获取 Lua 字符串的长度\n\n## **string.Find (s, p [, init [, plain]])**\n\n在 s 字符串中第一次匹配 p 字符串。若匹配成功，则返回 p 字符串在 s 字符串中出现的开始位置和结束位置；若匹配失败，则返回 nil,\n\n第三个参数第三个参数 init 默认为 1，并且可以为负整数，\n\n当 init 为负数时，表示从 s 字符串的 string.Len (s) + init + 1 索引处开始向后匹配字符串 p 。\n\n第四个参数默认为 false，当其为 true 时，只会把 p 看成一个字符串对待。\n\n```Lua\nlocal find = string.find\nprint(find(\"abc cba\", \"ab\"))\nprint(find(\"abc cba\", \"ab\", 2))     -- 从索引为2的位置开始匹配字符串：ab\nprint(find(\"abc cba\", \"ba\", -1))    -- 从索引为7的位置开始匹配字符串：ba\nprint(find(\"abc cba\", \"ba\", -3))    -- 从索引为5的位置开始匹配字符串：ba\nprint(find(\"abc cba\", \"(%a+)\", 1))  -- 从索引为1处匹配最长连续且只含字母的字符串\nprint(find(\"abc cba\", \"(%a+)\", 1, true)) --从索引为1的位置开始匹配字符串：(%a+)\n\n--\u003eoutput\n1   2\nnil\nnil\n6   7\n1   3   abc\nnil\n```\n\n## **string.Format (formatstring, ...)**\n\n按照格式化参数 formatstring，返回后面 `...` 内容的格式化版本\n\n```Plaintext\nprint(string.format(\"%.4f\", 3.1415926))     -- 保留4位小数\nprint(string.format(\"%d %x %o\", 31, 31, 31))-- 十进制数31转换成不同进制\nd = 29; m = 7; y = 2015                     -- 一行包含几个语句，用；分开\nprint(string.format(\"%s %02d/%02d/%d\", \"today is:\", d, m, y))\n\n--\u003eoutput\n3.1416\n31 1f 37\ntoday is: 29/07/2015\n```\n\n## **string.Match (s, p [, init])**\n\n在字符串 s 中匹配（模式）字符串 p，若匹配成功，则返回目标字符串中与模式匹配的子串；否则返回 nil。第三个参数 init 默认为 1，并且可以为负整数，当 init 为负数时，表示从 s 字符串的 string.Len (s) + init + 1 索引处开始向后匹配字符串 p。\n\n```Lua\nprint(string.match(\"hello lua\", \"lua\"))\nprint(string.match(\"lua lua\", \"lua\", 2))  --匹配后面那个luaprint(string.match(\"lua lua\", \"hello\"))\nprint(string.match(\"today is 27/7/2015\", \"%d+/%d+/%d+\"))\n\n--\u003eoutput\nlua\nlua\nnil27/7/2015\n```\n\n## **string.Gmatch (s, p)**\n\n返回一个迭代器函数，通过这个迭代器函数可以遍历到在字符串 s 中出现模式串 p 的所有地方。\n\n```Lua\ns = \"hello world from Lua\"\nfor w in string.gmatch(s, \"%a+\") do  --匹配最长连续且只含字母的字符串\n    print(w)\nend\n\n--\u003eoutput\nhello\nworld\nfrom\nLua\n\n\nt = {}\ns = \"from=world, to=Lua\"\nfor k, v in string.gmatch(s, \"(%a+)=(%a+)\") do  --匹配两个最长连续且只含字母的\n    t[k] = v                                    --字符串，它们之间用等号连接\nend\nfor k, v in pairs(t) do\n    print (k,v)\nend\n\n--\u003eoutput\nto      Lua\nfrom    worl\n```\n\n## **string.Rep (s, n)**\n\n返回字符串 s 的 n 次拷贝。\n\n```Lua\nprint(string.rep(\"abc\", 3)) \n\n--拷贝3次\"abc\"--\u003eoutput  abcabcabc\n```\n\n## **string.Sub (s, i [, j])**\n\n返回字符串 s 中，索引 i 到索引 j 之间的子字符串。当 j 缺省时，默认为 -1，也就是字符串 s 的最后位置。I 可以为负数。当索引 i 在字符串 s 的位置在索引 j 的后面时，将返回一个空字符串。\n\n```Lua\nprint(string.sub(\"Hello Lua\", 4, 7))\nprint(string.sub(\"Hello Lua\", 2))\nprint(string.sub(\"Hello Lua\", 2, 1))    --看到返回什么了吗print(string.sub(\"Hello Lua\", -3, -1))\n\n--\u003eoutput\nlo L\nello Lua\n\nLua\n```\n\n## **string.Gsub (s, p, r [, n])**\n\n将目标字符串 s 中所有的子串 p 替换成字符串 r。可选参数 n，表示限制替换次数。返回值有两个，第一个是被替换后的字符串，第二个是替换了多少次。\n\n```Plaintext\nprint(string.gsub(\"Lua Lua Lua\", \"Lua\", \"hello\"))\nprint(string.gsub(\"Lua Lua Lua\", \"Lua\", \"hello\", 2)) --指明第四个参数--\u003eoutput\nhello hello hello   3\nhello hello Lua     2\n```\n\n## **string. Reverse (s)**\n\n接收一个字符串 s，返回这个字符串的反转\n\n```Lua\nprint(string.reverse(\"Hello Lua\"))  --\u003e output: auL olleH\n```\n\n  \n\n# Table\n\n## **下标从 1 开始**\n\n数组下标从 1 开始计数。\n\n而 Lua 最初设计是一种类似 XML 的数据描述语言，所以索引（index）反应的是数据在里面的位置，而不是偏移量。\n\n  \n\n在初始化一个数组的时候，**若不显式地用键值对方式赋值，则会默认用数字作为下标**，从 1 开始。由于在 _Lua_ 内部实际采用哈希表和数组分别保存键值对、普通值，所以不推荐混合使用这两种赋值方式\n\n```Lua\nlocal color={first=\"red\", \"blue\", third=\"green\", \"yellow\"}\nprint(color[\"first\"])                 --\u003e output: red\nprint(color[1])                       --\u003e output: blue\nprint(color[\"third\"])                 --\u003e output: green\nprint(color[2])                       --\u003e output: yellow\nprint(color[3])                       --\u003e output: nil\n```\n\n- **当我们把 table 当作栈或者队列使用的时候，容易犯错，追加到 table 的末尾用的是** **`s[#s+1] = something`****, 而不是** **`s[#s] = something`**\n    \n- 而且如果这个 something 是一个 nil 的话**，会导致这一次压栈（或者入队列）没有存入任何东西**， s 的值没有变\n    \n- 如果 `s = { 1, 2, 3, 4, 5, 6 }`，你令 `s[4] = nil`， s 会令你“匪夷所思”地变成 3。\n    \n\n## **table. Getn 获取长度**\n\n取长度操作符写作一元操作 。字符串的长度是它的字节数（就是以一个字符一个字节计算的字符串长度）\n\n- 对于常规的数组，里面从 1 到 n 放着一些非空的值的时候，它的长度就精确的为 n，即最后一个值的下标\n    \n- 如果数组有一个“空洞”（**就是说，nil 值被夹在非空值之间**），**那么 t 可能是指向任何一个是 nil 值的前一个位置的下标**\n    \n- 这也就说明对于有“空洞”的情况，table 的长度存在一定的 **不可确定性**\n    \n\n```Lua\nlocal tblTest1 = { 1, a = 2, 3 }\nprint(\"Test1 \" .. table.getn(tblTest1))\n\nlocal tblTest2 = { 1, nil }\nprint(\"Test2 \" .. table.getn(tblTest2))\n\nlocal tblTest3 = { 1, nil, 2 }\nprint(\"Test3 \" .. table.getn(tblTest3))\n\nlocal tblTest4 = { 1, nil, 2, nil }\nprint(\"Test4 \" .. table.getn(tblTest4))\n\nlocal tblTest5 = { 1, nil, 2, nil, 3, nil }\nprint(\"Test5 \" .. table.getn(tblTest5))\n\nlocal tblTest6 = { 1, nil, 2, nil, 3, nil, 4, nil }\nprint(\"Test6 \" .. table.getn(tblTest6))\n```\n\n我们使用 Lua 5.1 和 LuaJIT 2.1 分别执行这个用例，结果如下：\n\n```Lua\n# lua test.lua\nTest1 2\nTest2 1\nTest3 3\nTest4 1\nTest5 3\nTest6 1\n# luajit test.lua\nTest1 2\nTest2 1\nTest3 1\nTest4 1\nTest5 1\nTest6 1\n```\n\n不要在 Lua 的 table 中使用 nil 值，**如果一个元素要删除，直接 remove，不要用 nil 去代替**。\n\n## **table. Concat (table [, sep [, i [, j ] ] ])**\n\n对于元素是 string 或者 number 类型的表 table，返回 `table[i]..sep..table[i+1] ··· sep..table[j]` 连接成的字符串。填充字符串 sep 默认为空白字符串。起始索引位置 i 默认为 1，结束索引位置 j 默认是 table 的长度。\n\n```Lua\nlocal a = {1, 3, 5, \"hello\" }\nprint(table.concat(a))              -- output: 135hello\nprint(table.concat(a, \"|\"))         -- output: 1|3|5|hello\nprint(table.concat(a, \" \", 4, 2))   -- output:\nprint(table.concat(a, \" \", 2, 4))   -- output: 3 5 hello\n```\n\n## **table. Insert (table, [pos ,] value)**\n\n在（数组型）表 table 的 pos 索引位置插入 value，其它元素向后移动到空的地方。Pos 的默认值是表的长度加一，即默认是插在表的最后\n\n```Lua\nlocal a = {1, 8}             --a[1] = 1,a[2] = 8\ntable.insert(a, 1, 3)   --在表索引为1处插入3\nprint(a[1], a[2], a[3])\ntable.insert(a, 10)    --在表的最后插入10\nprint(a[1], a[2], a[3], a[4])\n\n--\u003eoutput\n3    1    8\n3    1    8    10\n```\n\n## **table. Maxn (table)**\n\n返回（数组型）表 table 的最大索引编号；如果此表没有正的索引编号，返回 0。\n\n```Lua\nlocal a = {}\na[-1] = 10\nprint(table.maxn(a))\na[5] = 10\nprint(table.maxn(a))\n\n--\u003eoutput05\n```\n\n## **table. Remove (table [, pos])**\n\n在表 table 中删除索引为 pos（pos 只能是 number 型）的元素，并返回这个被删除的元素，它后面所有元素的索引值都会减一。Pos 的默认值是表的长度，即默认是删除表的最后一个元素。\n\n```Lua\nlocal a = { 1, 2, 3, 4}\nprint(table.remove(a, 1)) --删除速索引为1的元素print(a[1], a[2], a[3], a[4])\n\nprint(table.remove(a))   --删除最后一个元素print(a[1], a[2], a[3], a[4])\n\n--\u003eoutput12    3    4    nil42    3    nil    nil\n```\n\n## **table. Sort (table [, comp])**\n\n按照给定的比较函数 comp 给表 table 排序，也就是从 table[1] 到 table[n]，这里 n 表示 table 的长度。比较函数有两个参数，如果希望第一个参数排在第二个的前面，就应该返回 true，否则返回 false。如果比较函数 comp 没有给出，默认从小到大排序。\n\n```Lua\n\nlocal function compare(x, y) --从大到小排序\n    return x \u003e y         --如果第一个参数大于第二个就返回true，否则返回false\nend\n\nlocal a = { 1, 7, 3, 4, 25}\ntable.sort(a)           --默认从小到大排序\nprint(a[1], a[2], a[3], a[4], a[5])\ntable.sort(a, compare) --使用比较函数进行排序\nprint(a[1], a[2], a[3], a[4], a[5])\n\n--\u003eoutput\n1    3    4    7    25\n25    7    4    3    1\n```\n\n## 其他\n\nLuaJIT 2.1 新增加的 `table.new` 和 `table.clear` 函数是非常有用的。前者主要用来预分配 Lua table 空间，后者主要用来高效的释放 table 空间，并且它们都是可以被 JIT 编译的\n\n  \n\n# 日期时间\n\n函数 time、date 和 difftime 提供了所有的日期和时间功能。\n\n在 OpenResty 的世界里，不推荐使用这里的标准时间函数，因为这些函数通常会引发不止一个昂贵的系统调用，同时无法为 LuaJIT JIT 编译，对性能造成较大影响。推荐使用 ngx_lua 模块提供的带缓存的时间接口，如 `ngx.today`, `ngx.time`, `ngx.utctime`, `ngx.localtime`, `ngx.now`, `ngx.http_time`，以及 `ngx.cookie_time` 等。\n\n  \n\n## **os. Time ([table])**\n\n如果不使用参数 table 调用 time 函数，\n\n- 它会返回当前的时间和日期（它表示从某一时刻到现在的秒数）。\n    \n- 如果用 table 参数，它会返回一个数字，表示该 table 中所描述的日期和时间（它表示从某一时刻到 table 中描述日期和时间的秒数）。Table 的字段如下：\n    \n\n|          |                            |\n| -------- | -------------------------- |\n| 字段名称 | 取值范围                   |\n| year     | 四位数字                   |\n| month    | 1--12                      |\n| day      | 1--31                      |\n| hour     | 0--23                      |\n| min      | 0--59                      |\n| sec      | 0--61                      |\n| isdst    | boolean（true 表示夏令时） |\n\n对于 time 函数，如果参数为 table，那么 table 中必须含有 year、month、day 字段。其他字缺省时段默认为中午（12:00:00）。\n\n\u003e 示例代码：（地点为北京）\n\n```Plaintext\nprint(os.time())    --\u003eoutput  1438243393\na = { year = 1970, month = 1, day = 1, hour = 8, min = 1 }\nprint(os.time(a))   --\u003eoutput  60\n```\n\n## **os. Difftime (t 2, t 1)**\n\n返回 t 1 到 t 2 的时间差，单位为秒。\n\n\u003e 示例代码:\n\n```Plaintext\nlocal day1 = { year = 2015, month = 7, day = 30 }\nlocal t1 = os.time(day1)\n\nlocal day2 = { year = 2015, month = 7, day = 31 }\nlocal t2 = os.time(day2)\nprint(os.difftime(t2, t1))   --\u003eoutput  86400\n```\n\n## **os. Date ([format [, time]])**\n\n把一个表示日期和时间的数值，转换成更高级的表现形式。\n\n- 其第一个参数 format 是一个格式化字符串，描述了要返回的时间形式。\n    \n- 第二个参数 time 就是日期和时间的数字表示，缺省时默认为当前的时间。\n    \n- 使用格式字符 \"*t\"，创建一个时间表。\n    \n\n\u003e 示例代码：\n\n```Plaintext\nlocal tab1 = os.date(\"*t\")  --返回一个描述当前日期和时间的表\nlocal ans1 = \"{\"\nfor k, v in pairs(tab1) do  --把tab1转换成一个字符串\n    ans1 = string.format(\"%s %s = %s,\", ans1, k, tostring(v))\nend\n\nans1 = ans1 .. \"}\"\nprint(\"tab1 = \", ans1)\n\n\nlocal tab2 = os.date(\"*t\", 360)  --返回一个描述日期和时间数为360秒的表\nlocal ans2 = \"{\"\nfor k, v in pairs(tab2) do      --把tab2转换成一个字符串\n    ans2 = string.format(\"%s %s = %s,\", ans2, k, tostring(v))\nend\n\nans2 = ans2 .. \"}\"\nprint(\"tab2 = \", ans2)\n\n--\u003eoutput\ntab1 = { hour = 17, min = 28, wday = 5, day = 30, month = 7, year = 2015, sec = 10, yday = 211, isdst = false,}\ntab2 = { hour = 8, min = 6, wday = 5, day = 1, month = 1, year = 1970, sec = 0, yday = 1, isdst = false,}\n```\n\n该表中除了使用到了 time 函数参数 table 的字段外，这还提供了星期（wday，星期天为 1）和一年中的第几天（yday，一月一日为 1）。除了使用 \"*t\" 格式字符串外，如果使用带标记（见下表）的特殊字符串，os. Date 函数会将相应的标记位以时间信息进行填充，得到一个包含时间的字符串。表如下：\n\n|          |                                           |\n| -------- | ----------------------------------------- |\n| 格式字符 | 含义                                      |\n| %a       | 一星期中天数的简写（例如：Wed）           |\n| %A       | 一星期中天数的全称（例如：Wednesday）     |\n| %b       | 月份的简写（例如：Sep）                   |\n| %B       | 月份的全称（例如：September）             |\n| %c       | 日期和时间（例如：07/30/15 16:57:24）     |\n| %d       | 一个月中的第几天[01 ~ 31]                 |\n| %H       | 24 小时制中的小时数[00 ~ 23]              |\n| %I       | 12 小时制中的小时数[01 ~ 12]              |\n| %j       | 一年中的第几天[001 ~ 366]                 |\n| %M       | 分钟数[00 ~ 59]                           |\n| %m       | 月份数[01 ~ 12]                           |\n| %p       | “上午（am）”或“下午（pm）”                |\n| %S       | 秒数[00 ~ 59]                             |\n| %w       | 一星期中的第几天[1 ~ 7 = 星期天 ~ 星期六] |\n| %x       | 日期（例如：07/30/15）                    |\n| %X       | 时间（例如：16:57:24）                    |\n| %y       | 两位数的年份[00 ~ 99]                     |\n| %Y       | 完整的年份（例如：2015）                  |\n| %%       | 字符'%'                                   |\n\n\u003e 示例代码：\n\n```Plaintext\nprint(os.date(\"today is %A, in %B\"))\nprint(os.date(\"now is %x %X\"))\n\n--\u003eoutput\ntoday is Thursday, in July\nnow is 07/30/15 17:39:22\n```\n\n  \n\n# 数学库\n\nUa 数学库由一组标准的数学函数构成。数学库的引入丰富了 Lua 编程语言的功能，同时也方便了程序的编写。常用数学函数见下表：\n\n|               asd           |                                                                                        sdfa                                                                                                      | \n| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 函数名                   | 函数功能                                                                                                                                                                                     |\n| math.Rad (x)             | 角度 x 转换成弧度                                                                                                                                                                            |\n| math.Deg (x)             | 弧度 x 转换成角度                                                                                                                                                                            |\n| math.Max (x, ...)        | 返回参数中值最大的那个数，参数必须是 number 型                                                                                                                                               |\n| math.Min (x, ...)        | 返回参数中值最小的那个数，参数必须是 number 型                                                                                                                                               |\n| math. Random ([m [, n]]) | 不传入参数时，返回一个在区间[0,1)内均匀分布的伪随机实数；只使用一个整数参数 m 时，返回一个在区间[1, m]内均匀分布的伪随机整数；使用两个整数参数时，返回一个在区间[m, n]内均匀分布的伪随机整数 |\n| math. Randomseed (x)     | 为伪随机数生成器设置一个种子 x，相同的种子将会生成相同的数字序列                                                                                                                             |\n| math.Abs (x)             | 返回 x 的绝对值                                                                                                                                                                              |\n| math.Fmod (x, y)         | 返回 x 对 y 取余数                                                                                                                                                                           |\n| math.Pow (x, y)          | 返回 x 的 y 次方                                                                                                                                                                             |\n| math.Sqrt (x)            | 返回 x 的算术平方根                                                                                                                                                                          |\n| math.Exp (x)             | 返回自然数 e 的 x 次方                                                                                                                                                                       |\n| math.Log (x)             | 返回 x 的自然对数                                                                                                                                                                            |\n| math. Log 10 (x)         | 返回以 10 为底，x 的对数                                                                                                                                                                     |\n| math.Floor (x)           | 返回最大且不大于 x 的整数                                                                                                                                                                    |\n| math.Ceil (x)            | 返回最小且不小于 x 的整数                                                                                                                                                                    |\n| math. Pi                 | 圆周率                                                                                                                                                                                       |\n| math.Sin (x)             | 求弧度 x 的正弦值                                                                                                                                                                            |\n| math.Cos (x)             | 求弧度 x 的余弦值                                                                                                                                                                            |\n| math.Tan (x)             | 求弧度 x 的正切值                                                                                                                                                                            |\n| math.Asin (x)            | 求 x 的反正弦值                                                                                                                                                                              |\n| math.Acos (x)            | 求 x 的反余弦值                                                                                                                                                                              |\n| math.Atan (x)            | 求 x 的反正切值                                                                                                                                                                              |\n\n```Lua\nprint(math.pi)           --\u003eoutput  3.1415926535898\nprint(math.rad(180))     --\u003eoutput  3.1415926535898\nprint(math.deg(math.pi)) --\u003eoutput  180\n\nprint(math.sin(1))       --\u003eoutput  0.8414709848079\nprint(math.cos(math.pi)) --\u003eoutput  -1\nprint(math.tan(math.pi / 4))  --\u003eoutput  1\n\nprint(math.atan(1))      --\u003eoutput  0.78539816339745\nprint(math.asin(0))      --\u003eoutput  0\n\nprint(math.max(-1, 2, 0, 3.6, 9.1))     --\u003eoutput  9.1\nprint(math.min(-1, 2, 0, 3.6, 9.1))     --\u003eoutput  -1\n\nprint(math.fmod(10.1, 3))   --\u003eoutput  1.1\nprint(math.sqrt(360))      --\u003eoutput  18.97366596101\n\nprint(math.exp(1))         --\u003eoutput  2.718281828459\nprint(math.log(10))        --\u003eoutput  2.302585092994\nprint(math.log10(10))      --\u003eoutput  1\n\nprint(math.floor(3.1415))  --\u003eoutput  3\nprint(math.ceil(7.998))    --\u003eoutput  8\n```\n\n使用 `math.random()` 函数获得伪随机数时，如果不使用 `math.randomseed()` 设置伪随机数生成种子或者设置相同的伪随机数生成种子，那么得得到的伪随机数序列是一样的。\n\n```Lua\nmath.randomseed (100) --把种子设置为100\nprint(math.random())         --\u003eoutput  0.0012512588885159\nprint(math.random(100))      --\u003eoutput  57\nprint(math.random(100, 360)) --\u003eoutput  150\n```\n\n稍等片刻，再次运行上面的代码。\n\n```Lua\nmath.randomseed (100) --把种子设置为100\nprint(math.random())         --\u003eoutput  0.0012512588885159\nprint(math.random(100))      --\u003eoutput  57\nprint(math.random(100, 360)) --\u003eoutput  150\n```\n\n两次运行的结果一样。为了避免每次程序启动时得到的都是相同的伪随机数序列，通常是使用当前时间作为种子。\n\n\u003e 修改上例中的代码：\n\n```Lua\nmath.randomseed (os.time())   --把100换成os.time()\nprint(math.random())          --\u003eoutput 0.88369396038697\nprint(math.random(100))       --\u003eoutput 66\nprint(math.random(100, 360))  --\u003eoutput 228\n```\n\n稍等片刻，再次运行上面的代码。\n\n```Plaintext\nmath.randomseed (os.time())   --把100换成os.time()\nprint(math.random())          --\u003eoutput 0.88946195867794\nprint(math.random(100))       --\u003eoutput 68\nprint(math.random(100, 360))  --\u003eoutput 129\n```\n\n  \n\n# 文件\n\nLua I/O 库提供两种不同的方式处理文件：隐式文件描述，显式文件描述。\n\n这些文件 I/O 操作，**在 OpenResty 的上下文中对事件循环是会产生阻塞效应**。OpenResty 比较擅长的是高并发网络处理，在这个环境中，任何文件的操作，都将阻塞其他并行执行的请求。**实际中的应用，在 OpenResty 项目中应尽可能让网络处理部分、文件 I/0 操作部分相互独立，不要揉和在一起**。\n\n## **隐式文件描述**\n\n设置一个默认的输入或输出文件，然后在这个文件上进行所有的输入或输出操作。所有的操作函数由 io 表提供。\n\n\u003e 打开已经存在的 `test1.txt` 文件，并读取里面的内容\n\n```Plaintext\nfile = io.input(\"test1.txt\")    -- 使用 io.input() 函数打开文件repeat\n    line = io.read()            -- 逐行读取内容，文件结束时返回nil\n    if nil == line then\n        break\n    end\n    print(line)\nuntil (false)\n\nio.close(file)                  -- 关闭文件--\u003e output\nmy test file\nhello\nlua\n```\n\n\u003e 在 `test1.txt` 文件的最后添加一行 \"hello world\"\n\n```Plaintext\nfile = io.open(\"test1.txt\", \"a+\")   -- 使用 io.open() 函数，以添加模式打开文件\nio.output(file)                     -- 使用 io.output() 函数，设置默认输出文件\nio.write(\"\\nhello world\")           -- 使用 io.write() 函数，把内容写到文件\nio.close(file)\n```\n\n在相应目录下打开 `test1.txt` 文件，查看文件内容发生的变化。\n\n## **显式文件描述**\n\n使用 file: XXX () 函数方式进行操作, 其中 file 为 io.Open () 返回的文件句柄。\n\n\u003e 打开已经存在的 test 2. Txt 文件，并读取里面的内容\n\n```Plaintext\nfile = io.open(\"test2.txt\", \"r\")    -- 使用 io.open() 函数，以只读模式打开文件\n\nfor line in file:lines() do         -- 使用 file:lines() 函数逐行读取文件\n    print(line)\nend\n\nfile:close()\n\n--\u003eoutput\nmy test2\nhello lua\n```\n\n\u003e 在 test 2. Txt 文件的最后添加一行 \"hello world\"\n\n```Plaintext\nfile = io.open(\"test2.txt\", \"a\")  -- 使用 io.open() 函数，以添加模式打开文件\nfile:write(\"\\nhello world\")       -- 使用 file:write() 函数，在文件末尾追加内容\nfile:close()\n```\n\n在相应目录下打开 `test2.txt` 文件，查看文件内容发生的变化。\n\n## **文件操作函数**\n\n#### **io. Open (filename [, mode])**\n\n按指定的模式 mode，打开一个文件名为 `filename` 的文件，成功则返回文件句柄，失败则返回 nil 加错误信息。模式：\n\n|      |                                                |                     | \n| ---- | ---------------------------------------------- | ------------------- |\n| 模式 | 含义                                           | 文件不存在时        |\n| \"r\"  | 读模式 (默认)                                  | 返回 nil 加错误信息 |\n| \"w\"  | 写模式                                         | 创建文件            |\n| \"a\"  | 添加模式                                       | 创建文件            |\n| \"r+\" | 更新模式，保存之前的数据                       | 返回 nil 加错误信息 |\n| \"w+\" | 更新模式，清除之前的数据                       | 创建文件            |\n| \"a+\" | 添加更新模式，保存之前的数据, 在文件尾进行添加 | 创建文件            |\n\n模式字符串后面可以有一个 'b'，用于在某些系统中打开二进制文件。\n\n注意 \"w\" 和 \"wb\" 的区别\n\n- \"w\" 表示文本文件。某些文件系统 (如 Linux 的文件系统)认为 0 x 0 A 为文本文件的换行符，Windows 的文件系统认为 0 x 0 D 0 A 为文本文件的换行符。为了兼容其他文件系统（如从 Linux 拷贝来的文件），Windows 的文件系统在写文件时，会在文件中 0 x 0 A 的前面加上 0 x 0 D。使用 \"w\"，其属性要看所在的平台。\n    \n- \"wb\" 表示二进制文件。文件系统会按纯粹的二进制格式进行写操作，因此也就不存在格式转换的问题。（Linux 文件系统下 \"w\" 和 \"wb\" 没有区别）\n    \n\n#### **file: close ()**\n\n关闭文件。注意：当文件句柄被垃圾收集后，文件将自动关闭。句柄将变为一个不可预知的值。\n\n#### **io. Close ([file])**\n\n关闭文件，和 file: close () 的作用相同。没有参数 file 时，关闭默认输出文件。\n\n#### **file: flush ()**\n\n把写入缓冲区的所有数据写入到文件 file 中。\n\n#### **io. Flush ()**\n\n相当于 file: flush ()，把写入缓冲区的所有数据写入到默认输出文件。\n\n#### **io. Input ([file])**\n\n当使用一个文件名调用时，打开这个文件（以文本模式），并设置文件句柄为默认输入文件；当使用一个文件句柄调用时，设置此文件句柄为默认输入文件；当不使用参数调用时，返回默认输入文件句柄。\n\n#### **file: lines ()**\n\n返回一个迭代函数, 每次调用将获得文件中的一行内容, 当到文件尾时，将返回 nil，但不关闭文件。\n\n#### **io. Lines ([filename])**\n\n打开指定的文件 filename 为读模式并返回一个迭代函数, 每次调用将获得文件中的一行内容, 当到文件尾时，将返回 nil，并自动关闭文件。若不带参数时 io.Lines () 等价于 io.Input (): lines () 读取默认输入设备的内容，结束时不关闭文件。\n\n#### **io. Output ([file])**\n\n类似于 io. Input，但操作在默认输出文件上。\n\n#### **file: read (...)**\n\n按指定的格式读取一个文件。按每个格式将返回一个字符串或数字, 如果不能正确读取将返回 nil，若没有指定格式将指默认按行方式进行读取。格式：\n\n|        |                                                                                                        |\n| ------ | ------------------------------------------------------------------------------------------------------ |\n| 格式   | 含义                                                                                                   |\n| \"*n\"   | 读取一个数字                                                                                           |\n| \"*a\"   | 从当前位置读取整个文件。若当前位置为文件尾，则返回空字符串                                             |\n| \"*l\"   | 读取下一行的内容。若为文件尾，则返回 nil。(默认)                                                       |\n| number | 读取指定字节数的字符。若为文件尾，则返回 nil。如果 number 为 0, 则返回空字符串，若为文件尾, 则返回 nil |\n\n#### **io. Read (...)**\n\n相当于 io.Input ():read\n\n#### **io. Type (obj)**\n\n检测 obj 是否一个可用的文件句柄。如果 obj 是一个打开的文件句柄，则返回 \"file\" 如果 obj 是一个已关闭的文件句柄，则返回 \"closed file\" 如果 obj 不是一个文件句柄，则返回 nil。\n\n#### **file: write (...)**\n\n把每一个参数的值写入文件。参数必须为字符串或数字，若要输出其它值，则需通过 tostring 或 string. Format 进行转换。\n\n#### **io. Write (...)**\n\n相当于 io.Output (): write。\n\n#### **file: seek ([whence] [, offset])**\n\n设置和获取当前文件位置，成功则返回最终的文件位置 (按字节，相对于文件开头), 失败则返回 nil 加错误信息。缺省时，whence 默认为 \"cur\"，offset 默认为 0 。参数 whence：\n\n|        |                     |\n| ------ | ------------------- |\n| whence | 含义                |\n| \"set\"  | 文件开始            |\n| \"cur\"  | 文件当前位置 (默认) |\n| \"end\"  | 文件结束            |\n\n#### **file: setvbuf (mode [, size])**\n\n设置输出文件的缓冲模式。模式：\n\n|        |                                                              |\n| ------ | ------------------------------------------------------------ |\n| 模式   | 含义                                                         |\n| \"no\"   | 没有缓冲，即直接输出                                         |\n| \"full\" | 全缓冲，即当缓冲满后才进行输出操作 (也可调用 flush 马上输出) |\n| \"line\" | 以行为单位，进行输出                                         |\n\n最后两种模式，size 可以指定缓冲的大小（按字节），忽略 size 将自动调整为最佳的大小。","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":["lua"]},"/statistic/%E5%BA%94%E7%94%A8%E5%B1%82.png":{"title":"应用层.png","content":"","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]},"/statistic/%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E5%B1%82.png":{"title":"网络接口层.png","content":"","lastmodified":"2023-08-01T09:41:32.856106402Z","tags":[]}}