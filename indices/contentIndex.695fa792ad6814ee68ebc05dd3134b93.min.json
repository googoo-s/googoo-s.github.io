{"/":{"title":"_index","content":"\n\nHello !😀😘😆😂😁\n\n\n\n\n\n\n# Obsidian\n\n* [[Obsidian/dataview]]\n\n- [[Obsidian/excalidraw]]\n\n- [[Obsidian/Front Matter]]\n\n- [[Obsidian/obsidian overview]]\n\n- [[Obsidian/Obsidian-plugin]]\n\n- [[Obsidian/publish]]\n\n- [[Obsidian/template]]\n\n\n# Lua\n\n- [[lua/lua基础]]\n\n- [[lua/Lua高级]]\n\n","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":[]},"/%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%B9%B6%E5%8F%91%E9%AB%98%E6%80%A7%E8%83%BD%E9%AB%98%E5%8F%AF%E7%94%A8/%E6%97%A5%E5%BF%97/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86":{"title":"服务治理：分布式下如何进行日志管理？","content":"![](statistic/Pasted%20image%2020230802010117.png)","lastmodified":"2023-08-01T17:34:29.624260868Z","tags":[]},"/%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%B9%B6%E5%8F%91%E9%AB%98%E6%80%A7%E8%83%BD%E9%AB%98%E5%8F%AF%E7%94%A8/%E7%9B%91%E6%8E%A7/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E5%A6%82%E4%BD%95%E5%81%9A":{"title":"服务治理：监控系统如何做？","content":"![](statistic/Pasted%20image%2020230802010009.png)","lastmodified":"2023-08-01T17:34:29.624260868Z","tags":[]},"/%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%B9%B6%E5%8F%91%E9%AB%98%E6%80%A7%E8%83%BD%E9%AB%98%E5%8F%AF%E7%94%A8/%E9%AB%98%E5%8F%AF%E7%94%A8%E8%AE%BE%E8%AE%A1/%E8%B6%85%E6%97%B6%E9%87%8D%E8%AF%95%E8%AF%A6%E8%A7%A3":{"title":"超时\u0026重试详解","content":"由于网络问题、系统或者服务内部的 Bug、服务器宕机、操作系统崩溃等问题的不确定性，我们的系统或者服务永远不可能保证时刻都是可用的状态。\n\n为了最大限度的减小系统或者服务出现故障之后带来的影响，我们需要用到的 **超时（Timeout）** 和 **重试（Retry）** 机制。\n\n想要把超时和重试机制讲清楚其实很简单，因为它俩本身就不是什么高深的概念。\n\n虽然超时和重试机制的思想很简单，但是它俩是真的非常实用。你平时接触到的绝大部分涉及到远程调用的系统或者服务都会应用超时和重试机制。尤其是对于微服务系统来说，正确设置超时和重试非常重要。单体服务通常只涉及数据库、缓存、第三方 API、中间件等的网络调用，而微服务系统内部各个服务之间还存在着网络调用。\n\n## [#](#超时机制) 超时机制\n\n### [#](#什么是超时机制) 什么是超时机制？\n\n超时机制说的是当一个请求超过指定的时间（比如 1s）还没有被处理的话，这个请求就会直接被取消并抛出指定的异常或者错误（比如 `504 Gateway Timeout`）。\n\n我们平时接触到的超时可以简单分为下面 2 种：\n\n- **连接超时（ConnectTimeout）**：客户端与服务端建立连接的最长等待时间。\n- **读取超时（ReadTimeout）**：客户端和服务端已经建立连接，客户端等待服务端处理完请求的最长时间。实际项目中，我们关注比较多的还是读取超时。\n\n一些连接池客户端框架中可能还会有获取连接超时和空闲连接清理超时。\n\n如果没有设置超时的话，就可能会导致服务端连接数爆炸和大量请求堆积的问题。\n\n这些堆积的连接和请求会消耗系统资源，影响新收到的请求的处理。严重的情况下，甚至会拖垮整个系统或者服务。\n\n我之前在实际项目就遇到过类似的问题，整个网站无法正常处理请求，服务器负载直接快被拉满。后面发现原因是项目超时设置错误加上客户端请求处理异常，导致服务端连接数直接接近 40w+，这么多堆积的连接直接把系统干趴了。\n\n### [#](#超时时间应该如何设置) 超时时间应该如何设置？\n\n超时到底设置多长时间是一个难题！超时值设置太高或者太低都有风险。如果设置太高的话，会降低超时机制的有效性，比如你设置超时为 10s 的话，那设置超时就没啥意义了，系统依然可能会出现大量慢请求堆积的问题。如果设置太低的话，就可能会导致在系统或者服务在某些处理请求速度变慢的情况下（比如请求突然增多），大量请求重试（超时通常会结合重试）继续加重系统或者服务的压力，进而导致整个系统或者服务被拖垮的问题。\n\n通常情况下，我们建议读取超时设置为 **1500ms** ,这是一个比较普适的值。如果你的系统或者服务对于延迟比较敏感的话，那读取超时值可以适当在 **1500ms** 的基础上进行缩短。反之，读取超时值也可以在 **1500ms** 的基础上进行加长，不过，尽量还是不要超过 **1500ms** 。连接超时可以适当设置长一些，建议在 **1000ms ~ 5000ms** 之内。\n\n没有银弹！超时值具体该设置多大，还是要根据实际项目的需求和情况慢慢调整优化得到。\n\n更上一层，参考[美团的 Java 线程池参数动态配置open in new window](https://tech.meituan.com/2020/04/02/java-pooling-pratice-in-meituan.html)思想，我们也可以将超时弄成可配置化的参数而不是固定的，比较简单的一种办法就是将超时的值放在配置中心中。这样的话，我们就可以根据系统或者服务的状态动态调整超时值了。\n\n## [#](#重试机制) 重试机制\n\n### [#](#什么是重试机制) 什么是重试机制？\n\n重试机制一般配合超时机制一起使用，指的是多次发送相同的请求来避免瞬态故障和偶然性故障。\n\n瞬态故障可以简单理解为某一瞬间系统偶然出现的故障，并不会持久。偶然性故障可以理解为哪些在某些情况下偶尔出现的故障，频率通常较低。\n\n重试的核心思想是通过消耗服务器的资源来尽可能获得请求更大概率被成功处理。由于瞬态故障和偶然性故障是很少发生的，因此，重试对于服务器的资源消耗几乎是可以被忽略的。\n\n### [#](#重试的次数如何设置) 重试的次数如何设置？\n\n重试的次数不宜过多，否则依然会对系统负载造成比较大的压力。\n\n重试的次数通常建议设为 3 次。并且，我们通常还会设置重试的间隔，比如说我们要重试 3 次的话，第 1 次请求失败后，等待 1 秒再进行重试，第 2 次请求失败后，等待 2 秒再进行重试，第 3 次请求失败后，等待 3 秒再进行重试。\n\n### [#](#重试幂等) 重试幂等\n\n超时和重试机制在实际项目中使用的话，需要注意保证同一个请求没有被多次执行。\n\n什么情况下会出现一个请求被多次执行呢？客户端等待服务端完成请求完成超时但此时服务端已经执行了请求，只是由于短暂的网络波动导致响应在发送给客户端的过程中延迟了。\n\n举个例子：用户支付购买某个课程，结果用户支付的请求由于重试的问题导致用户购买同一门课程支付了两次。对于这种情况，我们在执行用户购买课程的请求的时候需要判断一下用户是否已经购买过。这样的话，就不会因为重试的问题导致重复购买了。\n\n## [#](#参考) 参考\n\n- 微服务之间调用超时的设置治理：[https://www.infoq.cn/article/eyrslar53l6hjm5yjgyxopen in new window](https://www.infoq.cn/article/eyrslar53l6hjm5yjgyx)\n- 超时、重试和抖动回退：[https://aws.amazon.com/cn/builders-library/timeouts-retries-and-backoff-with-jitter/](https://aws.amazon.com/cn/builders-library/timeouts-retries-and-backoff-with-jitter/)\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/high-availability/timeout-and-retry.html#%E8%B6%85%E6%97%B6%E6%97%B6%E9%97%B4%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE","lastmodified":"2023-08-01T17:34:29.624260868Z","tags":[]},"/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/git/git":{"title":"git","content":"\n## 版本控制\n\n### 什么是版本控制\n\n版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。除了项目源代码，你可以对任何类型的文件进行版本控制。\n\n### 为什么要版本控制\n\n有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态，你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。\n\n### 本地版本控制系统\n\n许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单，但是特别容易犯错。有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。\n\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。\n\n![本地版本控制系统](statistic/本地版本控制系统.png)\n\n### 集中化的版本控制系统\n\n接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 于是，集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生。\n\n集中化的版本控制系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。\n\n![集中化的版本控制系统](statistic/集中化的版本控制系统.png)\n\n这么做虽然解决了本地版本控制系统无法让在不同系统上的开发者协同工作的诟病，但也还是存在下面的问题：\n\n- **单点故障：** 中央服务器宕机，则其他人无法使用；如果中心数据库磁盘损坏又没有进行备份，你将丢失所有数据。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。\n- **必须联网才能工作：** 受网络状况、带宽影响。\n\n### 分布式版本控制系统\n\n于是分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。 Git 就是一个典型的分布式版本控制系统。\n\n这类系统，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。\n\n![分布式版本控制系统](statistic/分布式版本控制系统.png)\n\n分布式版本控制系统可以不用联网就可以工作，因为每个人的电脑上都是完整的版本库，当你修改了某个文件后，你只需要将自己的修改推送给别人就可以了。但是，在实际使用分布式版本控制系统的时候，很少会直接进行推送修改，而是使用一台充当“中央服务器”的东西。这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。\n\n分布式版本控制系统的优势不单是不必联网这么简单，后面我们还会看到 Git 极其强大的分支管理等功能。\n\n## 认识 Git\n\n### Git 简史\n\nLinux 内核项目组当时使用分布式版本控制系统 BitKeeper 来管理和维护代码。但是，后来开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）基于使用 BitKeeper 时的经验教训，开发出自己的版本系统，而且对新的版本控制系统做了很多改进。\n\n### Git 与其他版本管理系统的主要区别\n\nGit 在保存和对待各种信息的时候与其它版本控制系统有很大差异，尽管操作起来的命令形式非常相近，理解这些差异将有助于防止你使用中的困惑。\n\n下面我们主要说一个关于 Git 与其他版本管理系统的主要差别：**对待数据的方式**。\n\n**Git 采用的是直接记录快照的方式，而非差异比较。我后面会详细介绍这两种方式的差别。**\n\n大部分版本控制系统（CVS、Subversion、Perforce、Bazaar 等等）都是以文件变更列表的方式存储信息，这类系统**将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。**\n\n具体原理如下图所示，理解起来其实很简单，每当我们提交更新一个文件之后，系统都会记录这个文件做了哪些更新，以增量符号 Δ(Delta)表示。\n\n![](statistic/2019-3deltas.png)\n\n**我们怎样才能得到一个文件的最终版本呢？**\n\n很简单，高中数学的基本知识，我们只需要将这些原文件和这些增加进行相加就行了。\n\n**这种方式有什么问题呢？**\n\n比如我们的增量特别特别多的话，如果我们要得到最终的文件是不是会耗费时间和性能。\n\nGit 不按照以上方式对待或保存数据。反之，Git 更像是把数据看作是对小型文件系统的一组快照。每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 **快照流**。\n\n![](statistic/2019-3snapshots.png)\n\n### Git 的三种状态\n\nGit 有三种状态，你的文件可能处于其中之一：\n\n1. **已提交（committed）**：数据已经安全的保存在本地数据库中。\n2. **已修改（modified）**：已修改表示修改了文件，但还没保存到数据库中。\n3. **已暂存（staged）**：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。\n\n由此引入 Git 项目的三个工作区域的概念：**Git 仓库 (. Git directory)**、**工作目录 (Working Directory)** 以及 **暂存区域 (Staging Area)** 。\n\n![](statistic/2019-3areas.png)\n\n**基本的 Git 工作流程如下：**\n\n1. 在工作目录中修改文件。\n2. 暂存文件，将文件的快照放入暂存区域。\n3. 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。\n\n## Git 使用快速入门\n\n### 获取 Git 仓库\n\n有两种取得 Git 项目仓库的方法。\n\n1. 在现有目录中初始化仓库: 进入项目目录运行 `git init` 命令, 该命令将创建一个名为 `.git` 的子目录。\n2. 从一个服务器克隆一个现有的 Git 仓库: `git clone [url]` 自定义本地仓库的名字: `git clone [url] directoryname`\n\n### 记录每次更新到仓库\n\n1. **检测当前文件状态** : `git status`\n2. **提出更改（把它们添加到暂存区**）：`git add filename` (针对特定文件)、`git add *` (所有文件)、`git add *.txt`（支持通配符，所有 .txt 文件）\n3. **忽略文件**：`.gitignore` 文件\n4. **提交更新:** `git commit -m \"代码提交信息\"` （每次准备提交前，先用 `git status` 看下，是不是都已暂存起来了，然后再运行提交命令 `git commit`）\n5. **跳过使用暂存区域更新的方式** : `git commit -a -m \"代码提交信息\"`。 `git commit` 加上 `-a` 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 `git add` 步骤。\n6. **移除文件**：`git rm filename` （从暂存区域移除，然后提交。）\n7. **对文件重命名**：`git mv README.md README` (这个命令相当于 `mv README.md README`、`git rm README.md`、`git add README` 这三条命令的集合)\n\n### 一个好的 Git 提交消息\n\n一个好的 Git 提交消息如下：\n\n```\n标题行：用这一行来描述和解释你的这次提交\n\n主体部分可以是很少的几行，来加入更多的细节来解释提交，最好是能给出一些相关的背景或者解释这个提交能修复和解决什么问题。\n\n主体部分当然也可以有几段，但是一定要注意换行和句子不要太长。因为这样在使用 \"git log\" 的时候会有缩进比较好看。\n```\n\n提交的标题行描述应该尽量的清晰和尽量的一句话概括。这样就方便相关的 Git 日志查看工具显示和其他人的阅读。\n\n### 推送改动到远程仓库\n\n- 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：`git remote add origin \u003cserver\u003e` ,比如我们要让本地的一个仓库和 GitHub 上创建的一个仓库关联可以这样 `git remote add origin https://github.com/Snailclimb/test.git`\n- 将这些改动提交到远端仓库：`git push origin master` (可以把 _master_ 换成你想要推送的任何分支)\n\n  如此你就能够将你的改动推送到所添加的服务器上去了。\n\n### 远程仓库的移除与重命名\n\n- 将 test 重命名为 test 1：`git remote rename test test 1`\n- 移除远程仓库 test 1:`git remote rm test 1`\n\n### 查看提交历史\n\n在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。完成这个任务最简单而又有效的工具是 `git log` 命令。`git log` 会按提交时间列出所有的更新，最近的更新排在最上面。\n\n**可以添加一些参数来查看自己希望看到的内容：**\n\n只看某个人的提交记录：\n\n```shell\nGit log --author=bob\n```\n\n### 撤销操作\n\n有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。此时，可以运行带有 `--amend` 选项的提交命令尝试重新提交：\n\n```shell\nGit commit --amend\n```\n\n取消暂存的文件\n\n```shell\nGit reset filename\n```\n\n撤消对文件的修改:\n\n```shell\nGit checkout -- filename\n```\n\n假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：\n\n```shell\nGit fetch origin\nGit reset --hard origin/master\n```\n\n### 分支\n\n分支是用来将特性开发绝缘开来的。在你创建仓库的时候，_master_ 是“默认”的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。\n\n我们通常在开发新功能、修复一个紧急 bug 等等时候会选择创建分支。单分支开发好还是多分支开发好，还是要看具体场景来说。\n\n创建一个名字叫做 test 的分支\n\n```shell\nGit branch test\n```\n\n切换当前分支到 test（当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样）\n\n```shell\nGit checkout test\n```\n\n![](statistic/2019-3%E5%88%87%E6%8D%A2%E5%88%86%E6%94%AF.png)\n\n你也可以直接这样创建分支并切换过去 (上面两条命令的合写)\n\n```shell\nGit checkout -b feature_x\n```\n\n切换到主分支\n\n```shell\nGit checkout master\n```\n\n合并分支 (可能会有冲突)\n\n```shell\n Git merge test\n```\n\n把新建的分支删掉\n\n```shell\nGit branch -d feature_x\n```\n\n将分支推送到远端仓库（推送成功后其他人可见）：\n\n```shell\nGit push origin\n```\n\n\n---\nTitle: Github 实用小技巧总结\nCategory: 开发工具\nTag:\n  - Git\n---\n\n我使用 Github 已经有 6 年多了，今天毫无保留地把自己觉得比较有用的 Github 小技巧送给关注 JavaGuide 的各位小伙伴。\n\n## 一键生成 Github 简历 \u0026 Github 年报\n\n通过 [https://resume.github.io/](https://resume.github.io/) 这个网站你可以一键生成一个在线的 Github 简历。\n\n当时我参加的校招的时候，个人信息那里就放了一个在线的 Github 简历。我觉得这样会让面试官感觉你是一个内行，会提高一些印象分。\n\n但是，如果你的 Github 没有什么项目的话还是不要放在简历里面了。生成后的效果如下图所示。\n\n![Github简历](statistic/Github简历.png)\n\n通过 \u003chttps://www.githubtrends.io/wrapped\u003e 这个网站，你可以生成一份 Github 个人年报，这个年报会列举出你在这一年的项目贡献情况、最常使用的编程语言、详细的贡献信息。\n\n![](statistic/image-20211226144607457.png)\n\n## 个性化 Github 首页\n\nGithub 目前支持在个人主页自定义展示一些内容。展示效果如下图所示。\n\n![个性化首页展示效果](statistic/个性化首页展示效果.png)\n\n想要做到这样非常简单，你只需要创建一个和你的 Github 账户同名的仓库，然后自定义 `README.md` 的内容即可。\n\n展示在你主页的自定义内容就是 `README.md` 的内容（_不会 Markdown 语法的小伙伴自行面壁 5 分钟_）。\n\n![创建一个和你的Github账户同名的仓库](statistic/创建一个和你的Github账户同名的仓库.png)\n\n这个也是可以玩出花来的！比如说：通过 [github-readme-stats](https://hellogithub.com/periodical/statistics/click/?target=https://github.com/anuraghazra/github-readme-stats) 这个开源项目，你可以 README 中展示动态生成的 GitHub 统计信息。展示效果如下图所示。\n\n![通过github-readme-stats动态生成GitHub统计信息 ](statistic/通过github-readme-stats动态生成GitHub统计信息_.png)\n\n关于个性化首页这个就不多提了，感兴趣的小伙伴自行研究一下。\n\n## 自定义项目徽章\n\n你在 Github 上看到的项目徽章都是通过 [https://shields.io/](https://shields.io/) 这个网站生成的。我的 JavaGuide 这个项目的徽章如下图所示。\n\n![项目徽章](statistic/项目徽章.png)\n\n并且，你不光可以生成静态徽章，shield. Io 还可以动态读取你项目的状态并生成对应的徽章。\n\n![自定义项目徽章](statistic/自定义项目徽章.png)\n\n生成的描述项目状态的徽章效果如下图所示。\n\n![描述项目状态的徽章](statistic/描述项目状态的徽章.png)\n\n## 自动为项目添加贡献情况图标\n\n通过 repobeats 这个工具可以为 Github 项目添加如下图所示的项目贡献基本情况图表，挺不错的 👍\n\n![](statistic/repobeats.png)\n\n地址：\u003chttps://repobeats.axiom.co/\u003e 。\n\n## Github 表情\n\n![Github表情](statistic/Github表情.png)\n\n如果你想要在 Github 使用表情的话，可以在这里找找：[www.webfx.com/tools/emoji-cheat-sheet/](https://www.webfx.com/tools/emoji-cheat-sheet/)。\n\n![在线Github表情](statistic/在线Github表情.png)\n\n## 高效阅读 Github 项目的源代码\n\nGithub 前段时间推出的 Codespaces 可以提供类似 VS Code 的在线 IDE，不过目前还没有完全开发使用。\n\n简单介绍几种我最常用的阅读 Github 项目源代码的方式。\n\n### Chrome 插件 Octotree\n\n这个已经老生常谈了，是我最喜欢的一种方式。使用了 Octotree 之后网页侧边栏会按照树形结构展示项目，为我们带来 IDE 般的阅读源代码的感受。\n\n![Chrome插件Octotree](statistic/Chrome插件Octotree.png)\n\n### Chrome 插件 SourceGraph\n\n我不想将项目 clone 到本地的时候一般就会使用这种方式来阅读项目源代码。SourceGraph 不仅可以让我们在 Github 优雅的查看代码，它还支持一些骚操作，比如：类之间的跳转、代码搜索等功能。\n\n当你下载了这个插件之后，你的项目主页会多出一个小图标如下图所示。点击这个小图标即可在线阅读项目源代码。\n\n![](statistic/image-20201107145749659.png)\n\n使用 SourceGraph 阅读代码的就像下面这样，同样是树形结构展示代码，但是我个人感觉没有 Octotree 的手感舒服。不过，SourceGraph 内置了很多插件，而且还支持类之间的跳转！\n\n![](statistic/image-20201107150307314.png)\n\n### 克隆项目到本地\n\n先把项目克隆到本地，然后使用自己喜欢的 IDE 来阅读。可以说是最酸爽的方式了！\n\n如果你想要深入了解某个项目的话，首选这种方式。一个 `git clone` 就完事了。\n\n## 扩展 Github 的功能\n\n**Enhanced GitHub** 可以让你的 Github 更好用。这个 Chrome 插件可以可视化你的 Github 仓库大小，每个文件的大小并且可以让你快速下载单个文件。\n\n![](statistic/image-20201107160817672.png)\n\n## 自动为 Markdown 文件生成目录\n\n如果你想为 Github 上的 Markdown 文件生成目录的话，通过 VS Code 的 **Markdown Preview Enhanced** 这个插件就可以了。\n\n生成的目录效果如下图所示。你直接点击目录中的链接即可跳转到文章对应的位置，可以优化阅读体验。\n\n![](\u003chttps://oss.javaguide.cn/2020-11/iShot2020-11-07%2016.14.14%20(1).png\u003e)\n\n不过，目前 Github 已经自动为 Markdown 文件生成了目录，只是需要通过点击的方式才能显示出来。\n\n![](statistic/image-20211227093215005.png)\n\n## 善用 Github Explore\n\n其实，Github 自带的 Explore 是一个非常强大且好用的功能。不过，据我观察，国内很多 Github 用户都不知道这个到底是干啥的。\n\n简单来说，Github Explore 可以为你带来下面这些服务：\n\n1. 可以根据你的个人兴趣为你推荐项目；\n2. Githunb Topics 按照类别/话题将一些项目进行了分类汇总。比如 [Data visualization](https://github.com/topics/data-visualization) 汇总了数据可视化相关的一些开源项目，[Awesome Lists](https://github.com/topics/awesome) 汇总了 Awesome 系列的仓库；\n3. 通过 Github Trending 我们可以看到最近比较热门的一些开源项目，我们可以按照语言类型以及时间维度对项目进行筛选；\n4. Github Collections 类似一个收藏夹集合。比如 [Teaching materials for computational social science](https://github.com/collections/teaching-computational-social-science) 这个收藏夹就汇总了计算机课程相关的开源资源，[Learn to Code](https://github.com/collections/learn-to-code) 这个收藏夹就汇总了对你学习编程有帮助的一些仓库；\n5. ......\n\n![](statistic/github-explore.png)\n\n## GitHub Actions 很强大\n\n你可以简单地将 GitHub Actions 理解为 Github 自带的 CI/CD ，通过 GitHub Actions 你可以直接在 GitHub 构建、测试和部署代码，你还可以对代码进行审查、管理 API、分析项目依赖项。总之，GitHub Actions 可以自动化地帮你完成很多事情。\n\n关于 GitHub Actions 的详细介绍，推荐看一下阮一峰老师写的 [GitHub Actions 入门教程](https://www.ruanyifeng.com/blog/2019/09/getting-started-with-github-actions.html) 。\n\nGitHub Actions 有一个官方市场，上面有非常多别人提交的 Actions ，你可以直接拿来使用。\n\n![](statistic/image-20211227100147433.png)\n\n## 后记\n\n这一篇文章，我毫无保留地把自己这些年总结的 Github 小技巧分享了出来，真心希望对大家有帮助，真心希望大家一定要利用好 Github 这个专属程序员的宝藏。\n\n另外，这篇文章中，我并没有提到 Github 搜索技巧。在我看来，Github 搜索技巧不必要记网上那些文章说的各种命令啥的，真没啥卵用。你会发现你用的最多的还是关键字搜索以及 Github 自带的筛选功能。\n\n\n\n## 学习资料推荐\n\n**在线演示学习工具：**\n\n「补充，来自 [issue729](https://github.com/Snailclimb/JavaGuide/issues/729) 」Learn Git Branching \u003chttps://oschina.gitee.io/learn-git-branching/\u003e 。该网站可以方便的演示基本的 git 操作，讲解得明明白白。每一个基本命令的作用和结果。\n\n**推荐阅读：**\n\n- [Git 入门图文教程(1.5W 字 40 图)](https://www.cnblogs.com/anding/p/16987769.html)：超用心的一篇文章，内容全面且附带详细的图解，强烈推荐！\n- [Git - 简明指南](https://rogerdudler.github.io/git-guide/index.zh.html)：涵盖 Git 常见操作，非常清晰。\n- [图解 Git](https://marklodato.github.io/visual-git-guide/index-zh-cn.html)：图解 Git 中的最常用命令。如果你稍微理解 git 的工作原理，这篇文章能够让你理解的更透彻。\n- [猴子都能懂得 Git 入门](https://backlog.com/git-tutorial/cn/intro/intro1_1.html)：有趣的讲解。\n- [Pro Git book](https://git-scm.com/book/zh/v2)：国外的一本 Git 书籍，被翻译成多国语言，质量很高。","lastmodified":"2023-08-01T17:34:29.624260868Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Untitled":{"title":"Untitled","content":"### Kafka 是什么？主要应用场景有哪些？\n\nKafka 是一个分布式流式处理平台。这到底是什么意思呢？\n\n流平台具有三个关键功能：\n\n1. **消息队列**：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。\n2. **容错的持久方式存储记录消息流**：Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。\n3. **流式处理平台：** 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。\n\nKafka 主要有两大应用场景：\n\n1. **消息队列**：建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。\n2. **数据处理：** 构建实时的流数据处理程序来转换或处理数据流。\n\n### [#](#和其他消息队列相比-kafka-的优势在哪里) 和其他消息队列相比,Kafka 的优势在哪里？\n\n我们现在经常提到 Kafka 的时候就已经默认它是一个非常优秀的消息队列了，我们也会经常拿它跟 RocketMQ、RabbitMQ 对比。我觉得 Kafka 相比其他消息队列主要的优势如下：\n\n1. **极致的性能**：基于 Scala 和 Java 语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。\n2. **生态系统兼容性无可匹敌**：Kafka 与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域。\n\n实际上在早期的时候 Kafka 并不是一个合格的消息队列，早期的 Kafka 在消息队列领域就像是一个衣衫褴褛的孩子一样，功能不完备并且有一些小问题比如丢失消息、不保证消息可靠性等等。当然，这也和 LinkedIn 最早开发 Kafka 用于处理海量的日志有很大关系，哈哈哈，人家本来最开始就不是为了作为消息队列滴，谁知道后面误打误撞在消息队列领域占据了一席之地。\n\n随着后续的发展，这些短板都被 Kafka 逐步修复完善。所以，**Kafka 作为消息队列不可靠这个说法已经过时！**\n\n### [#](#队列模型了解吗-kafka-的消息模型知道吗) 队列模型了解吗？Kafka 的消息模型知道吗？\n\n\u003e 题外话：早期的 JMS 和 AMQP 属于消息服务领域权威组织所做的相关的标准，我在 [JavaGuideopen in new window](https://github.com/Snailclimb/JavaGuide)的 [《消息队列其实很简单》open in new window](https://github.com/Snailclimb/JavaGuide#%E6%95%B0%E6%8D%AE%E9%80%9A%E4%BF%A1%E4%B8%AD%E9%97%B4%E4%BB%B6)这篇文章中介绍过。但是，这些标准的进化跟不上消息队列的演进速度，这些标准实际上已经属于废弃状态。所以，可能存在的情况是：不同的消息队列都有自己的一套消息模型。\n\n#### [#](#队列模型-早期的消息模型) 队列模型：早期的消息模型\n\n![队列模型](statistic/队列模型.png)\n\n队列模型\n\n**使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。** 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）\n\n**队列模型存在的问题：**\n\n假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完整的消息内容。\n\n这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。\n\n#### [#](#发布-订阅模型-kafka-消息模型) 发布-订阅模型:Kafka 消息模型\n\n发布-订阅模型主要是为了解决队列模型存在的问题。\n\n![发布订阅模型](statistic/发布订阅模型.png)\n\n发布订阅模型\n\n发布订阅模型（Pub-Sub） 使用**主题（Topic）** 作为消息通信载体，类似于**广播模式**；发布者发布一条消息，该消息通过主题传递给所有的订阅者，**在一条消息广播之后才订阅的用户则是收不到该条消息的**。\n\n**在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。**\n\n**Kafka 采用的就是发布 - 订阅模型。**\n\n\u003e **RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区）。**\n\n### [#](#什么是-producer、consumer、broker、topic、partition) 什么是 Producer、Consumer、Broker、Topic、Partition？\n\nKafka 将生产者发布的消息发送到 **Topic（主题）** 中，需要这些消息的消费者可以订阅这些 **Topic（主题）**，如下图所示：\n\n![](statistic/message-queue20210507200944439.png)\n\n上面这张图也为我们引出了，Kafka 比较重要的几个概念：\n\n1. **Producer（生产者）** : 产生消息的一方。\n2. **Consumer（消费者）** : 消费消息的一方。\n3. **Broker（代理）** : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。\n\n同时，你一定也注意到每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念：\n\n- **Topic（主题）** : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。\n- **Partition（分区）** : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。这正如我上面所画的图一样。\n\n\u003e 划重点：**Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列。这样是不是更好理解一点？**\n\n### [#](#kafka-的多副本机制了解吗-带来了什么好处) Kafka 的多副本机制了解吗？带来了什么好处？\n\n还有一点我觉得比较重要的是 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。\n\n\u003e 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。\n\n**Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？**\n\n1. Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。\n2. Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。\n\n### [#](#zookeeper-在-kafka-中的作用知道吗) Zookeeper 在 Kafka 中的作用知道吗？\n\n\u003e **要想搞懂 zookeeper 在 Kafka 中的作用 一定要自己搭建一个 Kafka 环境然后自己进 zookeeper 去看一下有哪些文件夹和 Kafka 有关，每个节点又保存了什么信息。** 一定不要光看不实践，这样学来的也终会忘记！这部分内容参考和借鉴了这篇文章：https://www.jianshu.com/p/a036405f989c 。\n\n下图就是我的本地 Zookeeper ，它成功和我本地的 Kafka 关联上（以下文件夹结构借助 idea 插件 Zookeeper tool 实现）。\n\n![](statistic/zookeeper-kafka.jpg)\n\nZooKeeper 主要为 Kafka 提供元数据的管理的功能。\n\n从图中我们可以看出，Zookeeper 主要为 Kafka 做了下面这些事情：\n\n1. **Broker 注册**：在 Zookeeper 上会有一个专门**用来进行 Broker 服务器列表记录**的节点。每个 Broker 在启动时，都会到 Zookeeper 上进行注册，即到 `/brokers/ids` 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去\n2. **Topic 注册**：在 Kafka 中，同一个**Topic 的消息会被分成多个分区**并将其分布在多个 Broker 上，**这些分区信息及与 Broker 的对应关系**也都是由 Zookeeper 在维护。比如我创建了一个名字为 my-topic 的主题并且它有两个分区，对应到 zookeeper 中会创建这些文件夹：`/brokers/topics/my-topic/Partitions/0`、`/brokers/topics/my-topic/Partitions/1`\n3. **负载均衡**：上面也说过了 Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力。 对于同一个 Topic 的不同 Partition，Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。\n4. ......\n\n### [#](#kafka-如何保证消息的消费顺序) Kafka 如何保证消息的消费顺序？\n\n我们在使用消息队列的过程中经常有业务场景需要严格保证消息的消费顺序，比如我们同时发了 2 个消息，这 2 个消息对应的操作分别对应的数据库操作是：\n\n1. 更改用户会员等级。\n2. 根据会员等级计算订单价格。\n\n假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。\n\n我们知道 Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。\n\n![](statistic/KafkaTopicPartionsLayout.png)\n\n每次添加消息到 Partition(分区) 的时候都会采用尾加法，如上图所示。 **Kafka 只能为我们保证 Partition(分区) 中的消息有序。**\n\n\u003e 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。\n\n所以，我们就有一种很简单的保证消息消费顺序的方法：**1 个 Topic 只对应一个 Partition**。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。\n\nKafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以采用表/对象的 id 来作为 key 。\n\n总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法：\n\n1. 1 个 Topic 只对应一个 Partition。\n2. （推荐）发送消息的时候指定 key/Partition。\n\n当然不仅仅只有上面两种方法，上面两种方法是我觉得比较好理解的，\n\n### [#](#kafka-如何保证消息不丢失) Kafka 如何保证消息不丢失\n\n#### [#](#生产者丢失消息的情况) 生产者丢失消息的情况\n\n生产者(Producer) 调用`send`方法发送消息之后，消息可能因为网络问题并没有发送过去。\n\n所以，我们不能默认在调用`send`方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 `send` 方法发送消息实际上是异步的操作，我们可以通过 `get()`方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下：\n\n\u003e **详细代码见我的这篇文章：[Kafka 系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列?open in new window](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==\u0026mid=2247486269\u0026idx=2\u0026sn=ec00417ad641dd8c3d145d74cafa09ce\u0026chksm=cea244f6f9d5cde0c8eb233fcc4cf82e11acd06446719a7af55230649863a3ddd95f78d111de\u0026token=1633957262\u0026lang=zh_CN#rd)**\n\n```\nSendResult\u003cString, Object\u003e sendResult = kafkaTemplate.send(topic, o).get();\nif (sendResult.getRecordMetadata() != null) {\n  logger.info(\"生产者成功发送消息到\" + sendResult.getProducerRecord().topic() + \"-\u003e \" + sendRe\n              sult.getProducerRecord().value().toString());\n}\n```\n\n但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下：\n\n```\n        ListenableFuture\u003cSendResult\u003cString, Object\u003e\u003e future = kafkaTemplate.send(topic, o);\n        future.addCallback(result -\u003e logger.info(\"生产者成功发送消息到topic:{} partition:{}的消息\", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()),\n                ex -\u003e logger.error(\"生产者发送消失败，原因：{}\", ex.getMessage()));\n```\n\n如果消息发送失败的话，我们检查失败的原因之后重新发送即可！\n\n**另外这里推荐为 Producer 的`retries` （重试次数）设置一个比较合理的值，一般是 3 ，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次你 3 次一下子就重试完了**\n\n#### [#](#消费者丢失消息的情况) 消费者丢失消息的情况\n\n我们知道消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。\n\n![kafka offset](statistic/kafka_offset.jpg)\n\nkafka offset\n\n当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。\n\n**解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。** 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。\n\n#### [#](#kafka-弄丢了消息) Kafka 弄丢了消息\n\n我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。\n\n**试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。**\n\n**设置 acks = all**\n\n解决办法就是我们设置 **acks = all**。acks 是 Kafka 生产者(Producer) 很重要的一个参数。\n\nacks 的默认值即为 1，代表我们的消息被 leader 副本接收之后就算被成功发送。当我们配置 **acks = all** 表示只有所有 ISR 列表的副本全部收到消息时，生产者才会接收到来自服务器的响应. 这种模式是最高级别的，也是最安全的，可以确保不止一个 Broker 接收到了消息. 该模式的延迟会很高.\n\n**设置 replication.factor \u003e= 3**\n\n为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 **replication.factor \u003e= 3**。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。\n\n**设置 min.insync.replicas \u003e 1**\n\n一般情况下我们还需要设置 **min.insync.replicas\u003e 1** ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。**min.insync.replicas** 的默认值为 1 ，在实际生产中应尽量避免默认值 1。\n\n但是，为了保证整个 Kafka 服务的高可用性，你需要确保 **replication.factor \u003e min.insync.replicas** 。为什么呢？设想一下假如两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 **replication.factor = min.insync.replicas + 1**。\n\n**设置 unclean.leader.election.enable = false**\n\n\u003e **Kafka 0.11.0.0 版本开始 unclean.leader.election.enable 参数的默认值由原来的 true 改为 false**\n\n我们最开始也说了我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的消息同步情况不一样，当我们配置了 **unclean.leader.election.enable = false** 的话，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。\n\n### [#](#kafka-如何保证消息不重复消费) Kafka 如何保证消息不重复消费\n\n**kafka 出现消息重复消费的原因：**\n\n- 服务端侧已经消费的数据没有成功提交 offset（根本原因）。\n- Kafka 侧 由于服务端处理业务时间长或者网络链接等等原因让 Kafka 认为服务假死，触发了分区 rebalance。\n\n**解决方案：**\n\n- 消费消息服务做幂等校验，比如 Redis 的 set、MySQL 的主键等天然的幂等功能。这种方法最有效。\n- 将 **`enable.auto.commit`** 参数设置为 false，关闭自动提交，开发者在代码中手动提交 offset。那么这里会有个问题：**什么时候提交 offset 合适？**\n    - 处理完消息再提交：依旧有消息重复消费的风险，和自动提交一样\n    - 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。\n\n### [#](#reference) Reference\n\n- Kafka 官方文档：https://kafka.apache.org/documentation/\n- 极客时间—《Kafka 核心技术与实战》第 11 节：无消息丢失配置怎么实现？\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/high-performance/message-queue/kafka-questions-01.html#kafka-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1","lastmodified":"2023-08-01T17:34:29.628260994Z","tags":[]},"/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RocketMQ/RocketMQ%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93":{"title":"RocketMQ常见问题总结","content":"## 消息队列扫盲\n\n消息队列顾名思义就是存放消息的队列，队列我就不解释了，别告诉我你连队列都不知道是啥吧？\n\n所以问题并不是消息队列是什么，而是 **消息队列为什么会出现？消息队列能用来干什么？用它来干这些事会带来什么好处？消息队列会带来副作用吗？**\n\n### [#](#消息队列为什么会出现) 消息队列为什么会出现？\n\n消息队列算是作为后端程序员的一个必备技能吧，因为**分布式应用必定涉及到各个系统之间的通信问题**，这个时候消息队列也应运而生了。可以说分布式的产生是消息队列的基础，而分布式怕是一个很古老的概念了吧，所以消息队列也是一个很古老的中间件了。\n\n### [#](#消息队列能用来干什么) 消息队列能用来干什么？\n\n#### [#](#异步) 异步\n\n你可能会反驳我，应用之间的通信又不是只能由消息队列解决，好好的通信为什么中间非要插一个消息队列呢？我不能直接进行通信吗？\n\n很好 👍，你又提出了一个概念，**同步通信**。就比如现在业界使用比较多的 `Dubbo` 就是一个适用于各个系统之间同步通信的 `RPC` 框架。\n\n我来举个 🌰 吧，比如我们有一个购票系统，需求是用户在购买完之后能接收到购买完成的短信。\n\n![](statistic/16ef37fee7e09230.jpg)\n\n我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms = 350ms。\n\n当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短信系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 **头重脚轻** 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？\n\n![](statistic/16ef380429cf373e.jpg)\n\n这样整个系统的调用链又变长了，整个时间就变成了 550ms。\n\n当我们在学生时代需要在食堂排队的时候，我们和食堂大妈就是一个同步的模型。\n\n我们需要告诉食堂大妈：“姐姐，给我加个鸡腿，再加个酸辣土豆丝，帮我浇点汁上去，多打点饭哦 😋😋😋” 咦~~~ 为了多吃点，真恶心。\n\n然后大妈帮我们打饭配菜，我们看着大妈那颤抖的手和掉落的土豆丝不禁咽了咽口水。\n\n最终我们从大妈手中接过饭菜然后去寻找座位了...\n\n回想一下，我们在给大妈发送需要的信息之后我们是 **同步等待大妈给我配好饭菜** 的，上面我们只是加了鸡腿和土豆丝，万一我再加一个番茄牛腩，韭菜鸡蛋，这样是不是大妈打饭配菜的流程就会变长，我们等待的时间也会相应的变长。\n\n![](statistic/006APoFYly1fvd9cwjlfrj30as0b03ym.jpg)\n\n那后来，我们工作赚钱了有钱去饭店吃饭了，我们告诉服务员来一碗牛肉面加个荷包蛋 **(传达一个消息)** ，然后我们就可以在饭桌上安心的玩手机了 **(干自己其他事情)** ，等到我们的牛肉面上了我们就可以吃了。这其中我们也就传达了一个消息，然后我们又转过头干其他事情了。这其中虽然做面的时间没有变短，但是我们只需要传达一个消息就可以干其他事情了，这是一个 **异步** 的概念。\n\n所以，为了解决这一个问题，聪明的程序员在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。\n\n![](statistic/16ef38124f55eaea.jpg)\n\n这样，我们在将消息存入消息队列之后我们就可以直接返回了(我们告诉服务员我们要吃什么然后玩手机)，所以整个耗时只是 150ms + 10ms = 160ms。\n\n\u003e 但是你需要注意的是，整个流程的时长是没变的，就像你仅仅告诉服务员要吃什么是不会影响到做面的速度的。\n\n#### [#](#解耦) 解耦\n\n回到最初同步调用的过程，我们写个伪代码简单概括一下。\n\n![](statistic/16ef381a505d3e1f.jpg.png)\n\n那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？\n\n![](statistic/16ef381c4e1b1ac7.jpg.png)\n\n如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？\n\n![](statistic/16ef381f273a66bd.jpg)\n\n这样改来改去是不是很麻烦，那么 **此时我们就用一个消息队列在中间进行解耦** 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 `result` ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 **“广播消息”** 来实现。\n\n我上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 **订阅** 特定的主题。比如我们这里的主题就可以叫做 `订票` ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 **生产消息到指定主题中** ，而 **消费者只需要关注从指定主题中拉取消息** 就行了。\n\n![](statistic/16ef382674b66892.jpg)\n\n\u003e 如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。\n\n#### [#](#削峰) 削峰\n\n我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？\n\n![](statistic/16ef382a9756bb1c.jpg)\n\n如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 **直接崩溃** 了？\n\n短信业务又不是我们的主业务，我们能不能 **折中处理** 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 **尽自己所能地去消息队列中取消息和消费消息** ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。\n\n留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？\n\n#### [#](#消息队列能带来什么好处) 消息队列能带来什么好处？\n\n其实上面我已经说了。**异步、解耦、削峰。** 哪怕你上面的都没看懂也千万要记住这六个字，因为他不仅是消息队列的精华，更是编程和架构的精华。\n\n#### [#](#消息队列会带来副作用吗) 消息队列会带来副作用吗？\n\n没有哪一门技术是“银弹”，消息队列也有它的副作用。\n\n比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 **降低了系统的可用性** ？\n\n那这样是不是要保证 HA(高可用)？是不是要搞集群？那么我 **整个系统的复杂度是不是上升了** ？\n\n抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。\n\n或者我消费端处理失败了，请求重发，这样也会产生重复的消息。\n\n对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？\n\n那么，又 **如何解决重复消费消息的问题** 呢？\n\n如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个 id 为 1 的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？\n\n那么，又 **如何解决消息的顺序消费问题** 呢？\n\n就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 `Spring` 的话我们在上面伪代码中加入 `@Transactional` 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。\n\n那么，又如何 **解决分布式事务问题** 呢？\n\n我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？\n\n那么，又如何 **解决消息堆积的问题** 呢？\n\n可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊 😵？\n\n![](statistic/16ef382d709abc9d.png.jpg)\n\n别急，办法总是有的。\n\n## [#](#rocketmq-是什么) RocketMQ 是什么？\n\n![](statistic/16ef383014430799.jpg)\n\n哇，你个混蛋！上面给我抛出那么多问题，你现在又讲 `RocketMQ` ，还让不让人活了？！🤬\n\n别急别急，话说你现在清楚 `MQ` 的构造吗，我还没讲呢，我们先搞明白 `MQ` 的内部构造，再来看看如何解决上面的一系列问题吧，不过你最好带着问题去阅读和了解喔。\n\n`RocketMQ` 是一个 **队列模型** 的消息中间件，具有**高性能、高可靠、高实时、分布式** 的特点。它是一个采用 `Java` 语言开发的分布式的消息系统，由阿里巴巴团队开发，在 2016 年底贡献给 `Apache`，成为了 `Apache` 的一个顶级项目。 在阿里内部，`RocketMQ` 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 `RocketMQ` 流转。\n\n废话不多说，想要了解 `RocketMQ` 历史的同学可以自己去搜寻资料。听完上面的介绍，你只要知道 `RocketMQ` 很快、很牛、而且经历过双十一的实践就行了！\n\n## [#](#队列模型和主题模型是什么) 队列模型和主题模型是什么？\n\n在谈 `RocketMQ` 的技术架构之前，我们先来了解一下两个名词概念——**队列模型** 和 **主题模型** 。\n\n首先我问一个问题，消息队列为什么要叫消息队列？\n\n你可能觉得很弱智，这玩意不就是存放消息的队列嘛？不叫消息队列叫什么？\n\n的确，早期的消息中间件是通过 **队列** 这一模型来实现的，可能是历史原因，我们都习惯把消息中间件成为消息队列。\n\n但是，如今例如 `RocketMQ`、`Kafka` 这些优秀的消息中间件不仅仅是通过一个 **队列** 来实现消息存储的。\n\n### [#](#队列模型) 队列模型\n\n就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。我画一张图给大家理解。\n\n![](statistic/16ef3834ae653469.jpg)\n\n在一开始我跟你提到了一个 **“广播”** 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。\n\n当然你可以让 `Producer` 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 **解耦** 这一原则。\n\n### [#](#主题模型) 主题模型\n\n那么有没有好的方法去解决这一个问题呢？有，那就是 **主题模型** 或者可以称为 **发布订阅模型** 。\n\n\u003e 感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。\n\n在主题模型中，消息的生产者称为 **发布者(Publisher)** ，消息的消费者称为 **订阅者(Subscriber)** ，存放消息的容器称为 **主题(Topic)** 。\n\n其中，发布者将消息发送到指定主题中，订阅者需要 **提前订阅主题** 才能接受特定主题的消息。\n\n![](statistic/16ef3837887d9a54sds.jpg)\n\n### [#](#rocketmq-中的消息模型) RocketMQ 中的消息模型\n\n`RocketMQ` 中的消息模型就是按照 **主题模型** 所实现的。你可能会好奇这个 **主题** 到底是怎么实现的呢？你上面也没有讲到呀！\n\n其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 `Kafka` 中的 **分区** ，`RocketMQ` 中的 **队列** ，`RabbitMQ` 中的 `Exchange` 。我们可以理解为 **主题模型/发布订阅模型** 就是一个标准，那些中间件只不过照着这个标准去实现而已。\n\n所以，`RocketMQ` 中的 **主题模型** 到底是如何实现的呢？首先我画一张图，大家尝试着去理解一下。\n\n![](statistic/16ef383d3e8c9788.jpg)\n\n我们可以看到在整个图中有 `Producer Group`、`Topic`、`Consumer Group` 三个角色，我来分别介绍一下他们。\n\n- `Producer Group` 生产者组：代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 `Producer Group` 生产者组，它们一般生产相同的消息。\n- `Consumer Group` 消费者组：代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 `Consumer Group` 消费者组，它们一般消费相同的消息。\n- `Topic` 主题：代表一类消息，比如订单消息，物流消息等等。\n\n你可以看到图中生产者组中的生产者会向主题发送消息，而 **主题中存在多个队列**，生产者每次生产消息之后是指定主题中的某个队列发送消息的。\n\n每个主题中都有多个队列(分布在不同的 `Broker`中，如果是集群的话，`Broker`又分布在不同的服务器中)，集群消费模式下，一个消费者集群多台机器共同消费一个 `topic` 的多个队列，**一个队列只会被一个消费者消费**。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 `Consumer1` 和 `Consumer2` 分别对应着两个队列，而 `Consumer3` 是没有队列对应的，所以一般来讲要控制 **消费者组中的消费者个数和主题中队列个数相同** 。\n\n当然也可以消费者个数小于队列个数，只不过不太建议。如下图。\n\n![](statistic/16ef3850c808d707.jpg)\n\n**每个消费组在每个队列上维护一个消费位置** ，为什么呢？\n\n因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个 **消费位移(offset)** ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。\n\n![](statistic/16ef3857fefaa079.jpg)\n\n可能你还有一个问题，**为什么一个主题中需要维护多个队列** ？\n\n答案是 **提高并发能力** 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 **发布订阅模式** 。如下图。\n\n![](statistic/16ef38600cdb6d4b.jpg)\n\n但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 `Consumer` 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。\n\n所以总结来说，`RocketMQ` 通过**使用在一个 `Topic` 中配置多个队列并且每个队列维护每个消费者组的消费位置** 实现了 **主题模式/发布订阅模式** 。\n\n## [#](#rocketmq-的架构图) RocketMQ 的架构图\n\n讲完了消息模型，我们理解起 `RocketMQ` 的技术架构起来就容易多了。\n\n`RocketMQ` 技术架构中有四大角色 `NameServer`、`Broker`、`Producer`、`Consumer` 。我来向大家分别解释一下这四个角色是干啥的。\n\n- `Broker`：主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 `Broker` ，消费者从 `Broker` 拉取消息并消费。\n    \n    这里，我还得普及一下关于 `Broker`、`Topic` 和 队列的关系。上面我讲解了 `Topic` 和队列的关系——一个 `Topic` 中存在多个队列，那么这个 `Topic` 和队列存放在哪呢？\n    \n    **一个 `Topic` 分布在多个 `Broker`上，一个 `Broker` 可以配置多个 `Topic` ，它们是多对多的关系**。\n    \n    如果某个 `Topic` 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 **尽量多分布在不同 `Broker` 上，以减轻某个 `Broker` 的压力** 。\n    \n    `Topic` 消息量都比较均匀的情况下，如果某个 `broker` 上的队列越多，则该 `broker` 压力越大。\n    \n    ![](statistic/16ef38687488a5a4.jpg)\n    \n    \u003e 所以说我们需要配置多个 Broker。\n    \n- `NameServer`：不知道你们有没有接触过 `ZooKeeper` 和 `Spring Cloud` 中的 `Eureka` ，它其实也是一个 **注册中心** ，主要提供两个功能：**Broker 管理** 和 **路由信息管理** 。说白了就是 `Broker` 会将自己的信息注册到 `NameServer` 中，此时 `NameServer` 就存放了很多 `Broker` 的信息(Broker 的路由表)，消费者和生产者就从 `NameServer` 中获取路由表然后照着路由表的信息和对应的 `Broker` 进行通信(生产者和消费者定期会向 `NameServer` 去查询相关的 `Broker` 的信息)。\n    \n- `Producer`：消息发布的角色，支持分布式集群方式部署。说白了就是生产者。\n    \n- `Consumer`：消息消费的角色，支持分布式集群方式部署。支持以 push 推，pull 拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。\n    \n\n听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？\n\n![](statistic/16ef386c6d1e8bdb.jpg)\n\n嗯？你可能会发现一个问题，这老家伙 `NameServer` 干啥用的，这不多余吗？直接 `Producer`、`Consumer` 和 `Broker` 直接进行生产消息，消费消息不就好了么？\n\n但是，我们上文提到过 `Broker` 是需要保证高可用的，如果整个系统仅仅靠着一个 `Broker` 来维持的话，那么这个 `Broker` 的压力会不会很大？所以我们需要使用多个 `Broker` 来保证 **负载均衡** 。\n\n如果说，我们的消费者和生产者直接和多个 `Broker` 相连，那么当 `Broker` 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 `NameServer` 注册中心就是用来解决这个问题的。\n\n\u003e 如果还不是很理解的话，可以去看我介绍 `Spring Cloud` 的那篇文章，其中介绍了 `Eureka` 注册中心。\n\n当然，`RocketMQ` 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。\n\n![](statistic/16ef386fa3be1e53.jpg)\n\n其实和我们最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听我细细道来 🤨。\n\n第一、我们的 `Broker` **做了集群并且还进行了主从部署** ，由于消息分布在各个 `Broker` 上，一旦某个 `Broker` 宕机，则该`Broker` 上的消息读写都会受到影响。所以 `Rocketmq` 提供了 `master/slave` 的结构， `salve` 定时从 `master` 同步数据(同步刷盘或者异步刷盘)，如果 `master` 宕机，**则 `slave` 提供消费服务，但是不能写入消息** (后面我还会提到哦)。\n\n第二、为了保证 `HA` ，我们的 `NameServer` 也做了集群部署，但是请注意它是 **去中心化** 的。也就意味着它没有主节点，你可以很明显地看出 `NameServer` 的所有节点是没有进行 `Info Replicate` 的，在 `RocketMQ` 中是通过 **单个 Broker 和所有 NameServer 保持长连接** ，并且在每隔 30 秒 `Broker` 会向所有 `Nameserver` 发送心跳，心跳包含了自身的 `Topic` 配置信息，这个步骤就对应这上面的 `Routing Info` 。\n\n第三、在生产者需要向 `Broker` 发送消息的时候，**需要先从 `NameServer` 获取关于 `Broker` 的路由信息**，然后通过 **轮询** 的方法去向每个队列中生产数据以达到 **负载均衡** 的效果。\n\n第四、消费者通过 `NameServer` 获取所有 `Broker` 的路由信息后，向 `Broker` 发送 `Pull` 请求来获取消息数据。`Consumer` 可以以两种模式启动—— **广播（Broadcast）和集群（Cluster）**。广播模式下，一条消息会发送给 **同一个消费组中的所有消费者** ，集群模式下消息只会发送给一个消费者。\n\n## [#](#如何解决顺序消费和重复消费) 如何解决顺序消费和重复消费？\n\n其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于 `RocketMQ` ，而是应该每个消息中间件都需要去解决的。\n\n在上面我介绍 `RocketMQ` 的技术架构的时候我已经向你展示了 **它是如何保证高可用的** ，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的 `RocketMQ` 集群。\n\n\u003e 其实 `Kafka` 的架构基本和 `RocketMQ` 类似，只是它注册中心使用了 `Zookeeper`、它的 **分区** 就相当于 `RocketMQ` 中的 **队列** 。还有一些小细节不同会在后面提到。\n\n### [#](#顺序消费) 顺序消费\n\n在上面的技术架构介绍中，我们已经知道了 **`RocketMQ` 在主题上是无序的、它只有在队列层面才是保证有序** 的。\n\n这又扯到两个概念——**普通顺序** 和 **严格顺序** 。\n\n所谓普通顺序是指 消费者通过 **同一个消费队列收到的消息是有顺序的** ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 `Broker` **重启情况下不会保证消息顺序性** (短暂时间) 。\n\n所谓严格顺序是指 消费者收到的 **所有消息** 均是有顺序的。严格顺序消息 **即使在异常情况下也会保证消息的顺序性** 。\n\n但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，`Broker` 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 `binlog` 同步。\n\n一般而言，我们的 `MQ` 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。\n\n那么，我们现在使用了 **普通顺序模式** ，我们从上面学习知道了在 `Producer` 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 **三个消息会被发送到不同队列** ，因为在不同的队列此时就无法使用 `RocketMQ` 带来的队列有序特性来保证消息有序性了。\n\n![](statistic/16ef3874585e096e.jpg)\n\n那么，怎么解决呢？\n\n其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 **Hash 取模法** 来保证同一个订单在同一个队列中就行了。\n\n### [#](#重复消费) 重复消费\n\nemmm，就两个字—— **幂等** 。在编程中一个_幂等_ 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如 Broker 意外重启等等)，这条回应没有发送成功。\n\n那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？\n\n所以我们需要给我们的消费者实现 **幂等** ，也就是对同一个消息的处理结果，执行多少次都不变。\n\n那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 **写入 `Redis`** 来保证，因为 `Redis` 的 `key` 和 `value` 就是天然支持幂等的。当然还有使用 **数据库插入法** ，基于数据库的唯一键来保证重复数据不会被插入多条。\n\n不过最主要的还是需要 **根据特定场景使用特定的解决方案** ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。\n\n而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，**在其他场景中来解决重复请求或者重复调用的问题** 。比如将 HTTP 服务设计成幂等的，**解决前端或者 APP 重复提交表单数据的问题** ，也可以将一个微服务设计成幂等的，解决 `RPC` 框架自动重试导致的 **重复调用问题** 。\n\n## [#](#rocketmq-如何实现分布式事务) RocketMQ 如何实现分布式事务？\n\n如何解释分布式事务呢？事务大家都知道吧？**要么都执行要么都不执行** 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现 A 系统下了订单，但是 B 系统增加积分失败或者 A 系统没有下订单，B 系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。\n\n那么，如何去解决这个问题呢？\n\n如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，**都不是完美的解决方案**。\n\n在 `RocketMQ` 中使用的是 **事务消息加上事务反查机制** 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。\n\n![](statistic/16ef38798d7a987f.png.jpg)\n\n在第一步发送的 half 消息 ，它的意思是 **在事务提交之前，对于消费者来说，这个消息是不可见的** 。\n\n\u003e 那么，如何做到写入消息但是对用户不可见呢？RocketMQ 事务消息的做法是：如果消息是 half 消息，将备份原消息的主题与消息消费队列，然后 **改变主题** 为 RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费 half 类型的消息，**然后 RocketMQ 会开启一个定时任务，从 Topic 为 RMQ_SYS_TRANS_HALF_TOPIC 中拉取消息进行消费**，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。\n\n你可以试想一下，如果没有从第 5 步开始的 **事务反查机制** ，如果出现网路波动第 4 步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 `RocketMQ` 中就是使用的上述的事务反查来解决的，而在 `Kafka` 中通常是直接抛出一个异常让用户来自行解决。\n\n你还需要注意的是，在 `MQ Server` 指向系统 B 的操作已经和系统 A 不相关了，也就是说在消息队列中的分布式事务是——**本地事务和存储消息到消息队列才是同一个事务**。这样也就产生了事务的**最终一致性**，因为整个过程是异步的，**每个系统只要保证它自己那一部分的事务就行了**。\n\n## [#](#如何解决消息堆积问题) 如何解决消息堆积问题？\n\n在上面我们提到了消息队列一个很重要的功能——**削峰** 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？\n\n其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。\n\n我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 **限流降级** 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 **是否是消费者出现了大量的消费错误** ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。\n\n\u003e 当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 **同时你还需要增加每个主题的队列数量** 。\n\u003e \n\u003e 别忘了在 `RocketMQ` 中，**一个队列只会被一个消费者消费** ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。\n\n![](statistic/16ef387d939ab66d.jpg)\n\n## [#](#什么事回溯消费) 什么事回溯消费？\n\n回溯消费是指 `Consumer` 已经消费成功的消息，由于业务上需求需要重新消费，在`RocketMQ` 中， `Broker` 在向`Consumer` 投递成功消息后，**消息仍然需要保留** 。并且重新消费一般是按照时间维度，例如由于 `Consumer` 系统故障，恢复后需要重新消费 1 小时前的数据，那么 `Broker` 要提供一种机制，可以按照时间维度来回退消费进度。`RocketMQ` 支持按照时间回溯消费，时间维度精确到毫秒。\n\n这是官方文档的解释，我直接照搬过来就当科普了 😁😁😁。\n\n## [#](#rocketmq-的刷盘机制) RocketMQ 的刷盘机制\n\n上面我讲了那么多的 `RocketMQ` 的架构和设计原理，你有没有好奇\n\n在 `Topic` 中的 **队列是以什么样的形式存在的？**\n\n**队列中的消息又是如何进行存储持久化的呢？**\n\n我在上文中提到的 **同步刷盘** 和 **异步刷盘** 又是什么呢？它们会给持久化带来什么样的影响呢？\n\n下面我将给你们一一解释。\n\n### [#](#同步刷盘和异步刷盘) 同步刷盘和异步刷盘\n\n![](statistic/16ef387fba311cda.jpg)\n\n如上图所示，在同步刷盘中需要等待一个刷盘成功的 `ACK` ，同步刷盘对 `MQ` 消息可靠性来说是一种不错的保障，但是 **性能上会有较大影响** ，一般地适用于金融等特定业务场景。\n\n而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， **降低了读写延迟** ，提高了 `MQ` 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。\n\n一般地，**异步刷盘只有在 `Broker` 意外宕机的时候会丢失部分数据**，你可以设置 `Broker` 的参数 `FlushDiskType` 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。\n\n### [#](#同步复制和异步复制) 同步复制和异步复制\n\n上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 `Borker` 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。\n\n- 同步复制：也叫 “同步双写”，也就是说，**只有消息同步双写到主从节点上时才返回写入成功** 。\n- 异步复制：**消息写入主节点之后就直接返回写入成功** 。\n\n然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。\n\n那么，**异步复制会不会也像异步刷盘那样影响消息的可靠性呢？**\n\n答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 **可用性** 。为什么呢？其主要原因**是 `RocketMQ` 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了**。\n\n比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，**消费者可以自动切换到从节点进行消费**(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。\n\n在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？**一个主从不行那就多个主从的呗**，别忘了在我们最初的架构图中，每个 `Topic` 是分布在不同 `Broker` 中的。\n\n![](statistic/16ef38687488a5a4.jpg)\n\n但是这种复制方式同样也会带来一个问题，那就是无法保证 **严格顺序** 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 `Topic` 下的队列来保证顺序性的。如果此时我们主节点 A 负责的是订单 A 的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点 A 的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。\n\n而在 `RocketMQ` 中采用了 `Dledger` 解决这个问题。他要求在写入消息的时候，要求**至少消息复制到半数以上的节点之后**，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。\n\n\u003e 也不是说 `Dledger` 是个完美的方案，至少在 `Dledger` 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制半数以上节点的效率和直接异步复制还是有一定的差距的。\n\n### [#](#存储机制) 存储机制\n\n还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。\n\n但是，在 `Topic` 中的 **队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？** 还未解决，其实这里涉及到了 `RocketMQ` 是如何设计它的存储结构了。我首先想大家介绍 `RocketMQ` 消息存储架构中的三大角色——`CommitLog`、`ConsumeQueue` 和 `IndexFile` 。\n\n- `CommitLog`：**消息主体以及元数据的存储主体**，存储 `Producer` 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认 1G ，文件名长度为 20 位，左边补零，剩余为起始偏移量，比如 00000000000000000000 代表了第一个文件，起始偏移量为 0，文件大小为 1G=1073741824；当第一个文件写满了，第二个文件为 00000000001073741824，起始偏移量为 1073741824，以此类推。消息主要是**顺序写入日志文件**，当文件满了，写入下一个文件。\n- `ConsumeQueue`：消息消费队列，**引入的目的主要是提高消息消费的性能**(我们再前面也讲了)，由于`RocketMQ` 是基于主题 `Topic` 的订阅模式，消息消费是针对主题进行的，如果要遍历 `commitlog` 文件中根据 `Topic` 检索消息是非常低效的。`Consumer` 即可根据 `ConsumeQueue` 来查找待消费的消息。其中，`ConsumeQueue`（逻辑消费队列）**作为消费消息的索引**，保存了指定 `Topic` 下的队列消息在 `CommitLog` 中的**起始物理偏移量 `offset` **，消息大小 `size` 和消息 `Tag` 的 `HashCode` 值。**`consumequeue` 文件可以看成是基于 `topic` 的 `commitlog` 索引文件**，故 `consumequeue` 文件夹的组织方式如下：topic/queue/file 三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 `consumequeue` 文件采取定长设计，每一个条目共 20 个字节，分别为 8 字节的 `commitlog` 物理偏移量、4 字节的消息长度、8 字节 tag `hashcode`，单个文件由 30W 个条目组成，可以像数组一样随机访问每一个条目，每个 `ConsumeQueue`文件大小约 5.72M；\n- `IndexFile`：`IndexFile`（索引文件）提供了一种可以通过 key 或时间区间来查询消息的方法。这里只做科普不做详细介绍。\n\n总结来说，整个消息存储的结构，最主要的就是 `CommitLoq` 和 `ConsumeQueue` 。而 `ConsumeQueue` 你可以大概理解为 `Topic` 中的队列。\n\n![](statistic/16ef3884c02acc72.png.jpg)\n\n`RocketMQ` 采用的是 **混合型的存储结构** ，即为 `Broker` 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 `Kafka` 中会为每个 `Topic` 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，`RockeMQ` 是不分书的种类直接成批的塞上去的，而 `Kafka` 是将书本放入指定的分类区域的。\n\n而 `RocketMQ` 为什么要这么做呢？原因是 **提高数据的写入效率** ，不分 `Topic` 意味着我们有更大的几率获取 **成批** 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。\n\n所以，在 `RocketMQ` 中又使用了 `ConsumeQueue` 作为每个队列的索引文件来 **提升读取消息的效率**。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度 20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。\n\n讲到这里，你可能对 `RockeMQ` 的存储架构还有些模糊，没事，我们结合着图来理解一下。\n\n![](statistic/16ef388763c25c62.jpg.png)\n\nemmm，是不是有一点复杂 🤣，看英文图片和英文文档的时候就不要怂，硬着头皮往下看就行。\n\n\u003e 如果上面没看懂的读者一定要认真看下面的流程分析！\n\n首先，在最上面的那一块就是我刚刚讲的你现在可以直接 **把 `ConsumerQueue` 理解为 `Queue`**。\n\n在图中最左边说明了红色方块代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 `Topic`、`QueueId` 和具体消息内容，而在 `Broker` 中管你是哪门子消息，他直接 **全部顺序存储到了 CommitLog**。而根据生产者指定的 `Topic` 和 `QueueId` 将这条消息本身在 `CommitLog` 的偏移(offset)，消息本身大小，和 tag 的 hash 值存入对应的 `ConsumeQueue` 索引文件中。而在每个队列中都保存了 `ConsumeOffset` 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 `ConsumeOffset` 获取下一个未被消费的消息就行了。\n\n上述就是我对于整个消息存储架构的大概理解(这里不涉及到一些细节讨论，比如稀疏索引等等问题)，希望对你有帮助。\n\n因为有一个知识点因为写嗨了忘讲了，想想在哪里加也不好，所以我留给大家去思考 🤔🤔 一下吧。\n\n![](statistic/e314ee45gy1g05zgr67bbj20gp0b3aba.jpg)\n\n为什么 `CommitLog` 文件要设计成固定大小的长度呢？提醒：**内存映射机制**。\n\n## [#](#总结) 总结\n\n总算把这篇博客写完了。我讲的你们还记得吗 😅？\n\n这篇文章中我主要想大家介绍了\n\n1. 消息队列出现的原因\n2. 消息队列的作用(异步，解耦，削峰)\n3. 消息队列带来的一系列问题(消息堆积、重复消费、顺序消费、分布式事务等等)\n4. 消息队列的两种消息模型——队列和主题模式\n5. 分析了 `RocketMQ` 的技术架构(`NameServer`、`Broker`、`Producer`、`Comsumer`)\n6. 结合 `RocketMQ` 回答了消息队列副作用的解决方案\n7. 介绍了 `RocketMQ` 的存储机制和刷盘策略。\n\n等等。。。\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/high-performance/message-queue/rocketmq-questions.html#%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6","lastmodified":"2023-08-01T17:34:29.628260994Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%9C%BA%E6%99%AF%E9%A2%98/%E5%A6%82%E4%BD%95":{"title":"如何","content":"\n![](statistic/Pasted%20image%2020230802012455.png)","lastmodified":"2023-08-01T17:34:29.628260994Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%9C%BA%E6%99%AF%E9%A2%98/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E6%8E%92%E8%A1%8C%E6%A6%9C":{"title":"如何设计一个排行榜？","content":"![](statistic/Pasted%20image%2020230802012459.png)","lastmodified":"2023-08-01T17:34:29.628260994Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/%E5%B8%B8%E8%A7%81%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93":{"title":"常见加密算法总结","content":"加密算法是一种用数学方法对数据进行变换的技术，目的是保护数据的安全，防止被未经授权的人读取或修改。加密算法可以分为三大类：对称加密算法、非对称加密算法和哈希算法（也叫摘要算法）。\n\n日常开发中常见的需要用到的加密算法的场景：\n\n1. 保存在数据库中的密码需要加盐之后使用哈希算法（比如 BCrypt）进行加密。\n2. 保存在数据库中的银行卡号、身份号这类敏感数据需要使用对称加密算法（比如 AES）保存。\n3. 网络传输的敏感数据比如银行卡号、身份号需要用 HTTPS + 非对称加密算法（如 RSA）来保证传输数据的安全性。\n4. ......\n\n## [#](#哈希算法) 哈希算法\n\n哈希算法也叫哈希函数、散列函数或摘要算法，它的作用是对任意长度的数据生成一个固定长度的唯一标识，也叫哈希值、散列值或消息摘要（后文统称为哈希值）。\n\n![哈希算法效果演示](statistic/哈希算法效果演示.png)\n\n哈希算法效果演示\n\n哈希值的作用是可以用来验证数据的完整性和一致性。\n\n举两个实际的例子：\n\n- 我们下载一个文件时，可以通过比较文件的哈希值和官方提供的哈希值是否一致，来判断文件是否被篡改或损坏；\n- 保存密码到数据库时使用哈希算法进行加密，可以通过比较用户输入密码的哈希值和数据库保存的哈希值是否一致，来判断密码是否正确。\n\n这种算法的特点是不可逆：\n\n- 不能从哈希值还原出原始数据。\n- 原始数据的任何改变都会导致哈希值的巨大变化。\n\n哈希算法主要下面几类：\n\n- MD（Message Digest，消息摘要算法）：比如 MD5。\n- SHA（Secure Hash Algorithm，安全哈希算法）：比如 SHA-1、SHA-256。\n- MAC（Message Authentication Code，消息认证码算法）：比如 HMAC(Hash Message Authentication Code)。\n- 其他：国密算法（SM3）、密码哈希算法（Bcrypt）。\n- ......\n\n国密算法常见的如 SM2、SM3、SM4，其中 SM2 为非对称加密算法，SM4 为对称加密算法，SM3 为哈希算法（安全性及效率和 SHA-256 相当，但更适合国内的应用环境）。\n\n哈希算法一般是不需要密钥的，但也存在部分特殊哈希算法需要密钥。例如，MAC 算法就是一种基于密钥的哈希算法，它在哈希算法的基础上增加了一个密钥，使得只有知道密钥的人才能验证数据的完整性和来源。\n\n### [#](#md) MD\n\nMD 算法有多个版本，包括 MD2、MD4、MD5 等，其中 MD5 是最常用的版本，它可以生成一个 128 位（16 字节）的哈希值。从安全性上说：MD5 \u003e MD4 \u003e MD2。除了这些版本，还有一些基于 MD4 或 MD5 改进的算法，如 RIPEMD、HAVAL 等。\n\n即使是最安全 MD 算法 MD5 也存在被破解的风险，攻击者可以通过暴力破解或彩虹表攻击等方式，找到与原始数据相同的哈希值，从而破解数据。\n\n为了增加破解难度，通常可以选择加盐。盐（Salt）在密码学中，是指通过在密码任意固定位置插入特定的字符串，让哈希后的结果和使用原始密码的哈希结果不相符，这种过程称之为“加盐”。\n\n加盐之后就安全了吗？并不一定，这只是增加了破解难度，不代表无法破解。而且，MD5 算法本身就存在弱碰撞（Collision）问题，即多个不同的输入产生相同的 MD5 值。\n\n因此，MD 算法已经不被推荐使用，建议使用更安全的哈希算法比如 SHA-2、Bcrypt。\n\nJava 提供了对 MD 算法系列的支持，包括 MD2、MD5。\n\nMD5 代码示例（未加盐）：\n\n```\nString originalString = \"Java学习 + 面试指南：javaguide.cn\";\n// 创建MD5摘要对象\nMessageDigest messageDigest = MessageDigest.getInstance(\"MD5\");\nmessageDigest.update(originalString.getBytes(StandardCharsets.UTF_8));\n// 计算哈希值\nbyte[] result = messageDigest.digest();\n// 将哈希值转换为十六进制字符串\nString hexString = new HexBinaryAdapter().marshal(result);\nSystem.out.println(\"Original String: \" + originalString);\nSystem.out.println(\"SHA-256 Hash: \" + hexString.toLowerCase());\n```\n\n输出：\n\n```\nOriginal String: Java学习 + 面试指南：javaguide.cn\nSHA-256 Hash: fb246796f5b1b60d4d0268c817c608fa\n```\n\n### [#](#sha) SHA\n\nSHA（Secure Hash Algorithm）系列算法是一组密码哈希函数，用于将任意长度的数据映射为固定长度的哈希值。SHA 系列算法由美国国家安全局（NSA）于 1993 年设计，目前共有 SHA-1、SHA-2、SHA-3 三种版本。\n\nSHA-1 算法将任意长度的数据映射为 160 位的哈希值。然而，SHA-1 算法存在一些严重的缺陷，比如安全性低，容易受到碰撞攻击和长度扩展攻击。因此，SHA-1 算法已经不再被推荐使用。 SHA-2 家族（如 SHA-256、SHA-384、SHA-512 等）和 SHA-3 系列是 SHA-1 算法的替代方案，它们都提供了更高的安全性和更长的哈希值长度。\n\nSHA-2 家族是在 SHA-1 算法的基础上改进而来的，它们采用了更复杂的运算过程和更多的轮次，使得攻击者更难以通过预计算或巧合找到碰撞。\n\n为了寻找一种更安全和更先进的密码哈希函数，美国国家标准与技术研究院（National Institute of Standards and Technology，简称 NIST）在 2007 年公开征集 SHA-3 的候选算法。NIST 一共收到了 64 个算法方案，经过多轮的评估和筛选，最终在 2012 年宣布 Keccak 算法胜出，成为 SHA-3 的标准算法（SHA-3 与 SHA-2 算法没有直接的关系）。 Keccak 算法具有与 MD 和 SHA-1/2 完全不同的设计思路，即海绵结构（Sponge Construction），使得传统攻击方法无法直接应用于 SHA-3 的攻击中（能够抵抗目前已知的所有攻击方式包括碰撞攻击、长度扩展攻击、差分攻击等）。\n\n由于 SHA-2 算法还没有出现重大的安全漏洞，而且在软件中的效率更高，所以大多数人还是倾向于使用 SHA-2 算法。\n\n相比 MD5 算法，SHA-2 算法之所以更强，主要有两个原因：\n\n- 哈希值长度更长：例如 SHA-256 算法的哈希值长度为 256 位，而 MD5 算法的哈希值长度为 128 位，这就提高了攻击者暴力破解或者彩虹表攻击的难度。\n- 更强的碰撞抗性：SHA 算法采用了更复杂的运算过程和更多的轮次，使得攻击者更难以通过预计算或巧合找到碰撞。目前还没有找到任何两个不同的数据，它们的 SHA-256 哈希值相同。\n\n当然，SHA-2 也不是绝对安全的，也有被暴力破解或者彩虹表攻击的风险，所以，在实际的应用中，加盐还是必不可少的。\n\nJava 提供了对 SHA 算法系列的支持，包括 SHA-1、SHA-256、SHA-384 和 SHA-512。\n\nSHA-256 代码示例（未加盐）：\n\n```\nString originalString = \"Java学习 + 面试指南：javaguide.cn\";\n// 创建SHA-256摘要对象\nMessageDigest messageDigest = MessageDigest.getInstance(\"SHA-256\");\nmessageDigest.update(originalString.getBytes());\n// 计算哈希值\nbyte[] result = messageDigest.digest();\n// 将哈希值转换为十六进制字符串\nString hexString = new HexBinaryAdapter().marshal(result);\nSystem.out.println(\"Original String: \" + originalString);\nSystem.out.println(\"SHA-256 Hash: \" + hexString.toLowerCase());\n```\n\n输出：\n\n```\nOriginal String: Java学习 + 面试指南：javaguide.cn\nSHA-256 Hash: 184eb7e1d7fb002444098c9bde3403c6f6722c93ecfac242c0e35cd9ed3b41cd\n```\n\n### [#](#bcrypt) Bcrypt\n\nBcrypt 算法是一种基于 Blowfish 加密算法的密码哈希函数，专门为密码加密而设计，安全性高。\n\n由于 Bcrypt 采用了 salt（盐） 和 cost（成本） 两种机制，它可以有效地防止彩虹表攻击和暴力破解攻击，从而保证密码的安全性。salt 是一个随机生成的字符串，用于和密码混合，增加密码的复杂度和唯一性。cost 是一个数值参数，用于控制 Bcrypt 算法的迭代次数，增加密码哈希的计算时间和资源消耗。\n\nBcrypt 算法可以根据实际情况进行调整加密的复杂度，可以设置不同的 cost 值和 salt 值，从而满足不同的安全需求，灵活性很高。\n\nJava 应用程序的安全框架 Spring Security 支持多种密码编码器，其中 `BCryptPasswordEncoder` 是官方推荐的一种，它使用 BCrypt 算法对用户的密码进行加密存储。\n\n```\n@Bean\npublic PasswordEncoder passwordEncoder(){\n    return new BCryptPasswordEncoder();\n}\n```\n\n## [#](#对称加密) 对称加密\n\n对称加密算法是指加密和解密使用同一个密钥的算法，也叫共享密钥加密算法。\n\n![对称加密](statistic/对称加密.png)\n\n对称加密\n\n常见的对称加密算法有 DES、3DES、AES 等。\n\n### [#](#des-和-3des) DES 和 3DES\n\nDES（Data Encryption Standard）使用 64 位的密钥(有效秘钥长度为 56 位,8 位奇偶校验位)和 64 位的明文进行加密。\n\n虽然 DES 一次只能加密 64 位，但我们只需要把明文划分成 64 位一组的块，就可以实现任意长度明文的加密。如果明文长度不是 64 位的倍数，必须进行填充，常用的模式有 PKCS5Padding, PKCS7Padding, NOPADDING。\n\nDES 加密算法的基本思想是将 64 位的明文分成两半，然后对每一半进行多轮的变换，最后再合并成 64 位的密文。这些变换包括置换、异或、选择、移位等操作，每一轮都使用了一个子密钥，而这些子密钥都是由同一个 56 位的主密钥生成的。DES 加密算法总共进行了 16 轮变换，最后再进行一次逆置换，得到最终的密文。\n\n![DES（Data Encryption Standard）](statistic/DES（Data_Encryption_Standard）.jpg)\n\nDES（Data Encryption Standard）\n\n这是一个经典的对称加密算法，但也有明显的缺陷，即 56 位的密钥安全性不足，已被证实可以在短时间内破解。\n\n为了提高 DES 算法的安全性，人们提出了一些变种或者替代方案，例如 3DES（Triple DES）。\n\n3DES（Triple DES）是 DES 向 AES 过渡的加密算法，它使用 2 个或者 3 个 56 位的密钥对数据进行三次加密。3DES 相当于是对每个数据块应用三次 DES 的对称加密算法。\n\n为了兼容普通的 DES，3DES 并没有直接使用 加密-\u003e加密-\u003e加密 的方式，而是采用了加密-\u003e解密-\u003e加密 的方式。当三种密钥均相同时，前两步相互抵消，相当于仅实现了一次加密，因此可实现对普通 DES 加密算法的兼容。3DES 比 DES 更为安全，但其处理速度不高。\n\n### [#](#aes) AES\n\nAES（Advanced Encryption Standard）算法是一种更先进的对称密钥加密算法，它使用 128 位、192 位或 256 位的密钥对数据进行加密或解密，密钥越长，安全性越高。\n\nAES 也是一种分组(或者叫块)密码，分组长度只能是 128 位，也就是说，每个分组为 16 个字节。AES 加密算法有多种工作模式（mode of operation），如：ECB、CBC、OFB、CFB、CTR、XTS、OCB、GCM（目前使用最广泛的模式）。不同的模式参数和加密流程不同，但是核心仍然是 AES 算法。\n\n和 DES 类似，对于不是 128 位倍数的明文需要进行填充，常用的填充模式有 PKCS5Padding, PKCS7Padding, NOPADDING。不过，AES-GCM 是流加密算法，可以对任意长度的明文进行加密，所以对应的填充模式为 NoPadding，即无需填充。\n\nAES 的速度比 3DES 快，而且更安全。\n\n![AES（Advanced Encryption Standard）](statistic/AES（Advanced_Encryption_Standard）.jpg)\n\nAES（Advanced Encryption Standard）\n\nDES 算法和 AES 算法简单对比（图片来自于：[RSA vs. AES Encryption: Key Differences Explainedopen in new window](https://cheapsslweb.com/blog/rsa-vs-aes-encryption)）：\n\n![DES 和 AES 对比](statistic/DES_和_AES_对比.png)\n\nDES 和 AES 对比\n\n基于 Java 实现 AES 算法代码示例：\n\n```\nprivate static final String AES_ALGORITHM = \"AES\";\n// AES密钥\nprivate static final String AES_SECRET_KEY = \"4128D9CDAC7E2F82951CBAF7FDFE675B\";\n// AES加密模式为GCM，填充方式为NoPadding\n// AES-GCM 是流加密（Stream cipher）算法，所以对应的填充模式为 NoPadding，即无需填充。\nprivate static final String AES_TRANSFORMATION = \"AES/GCM/NoPadding\";\n// 加密器\nprivate static Cipher encryptionCipher;\n// 解密器\nprivate static Cipher decryptionCipher;\n\n/**\n * 完成一些初始化工作\n */\npublic static void init() throws Exception {\n    // 将AES密钥转换为SecretKeySpec对象\n    SecretKeySpec secretKeySpec = new SecretKeySpec(AES_SECRET_KEY.getBytes(), AES_ALGORITHM);\n    // 使用指定的AES加密模式和填充方式获取对应的加密器并初始化\n    encryptionCipher = Cipher.getInstance(AES_TRANSFORMATION);\n    encryptionCipher.init(Cipher.ENCRYPT_MODE, secretKeySpec);\n    // 使用指定的AES加密模式和填充方式获取对应的解密器并初始化\n    decryptionCipher = Cipher.getInstance(AES_TRANSFORMATION);\n    decryptionCipher.init(Cipher.DECRYPT_MODE, secretKeySpec, new GCMParameterSpec(128, encryptionCipher.getIV()));\n}\n\n/**\n * 加密\n */\npublic static String encrypt(String data) throws Exception {\n    byte[] dataInBytes = data.getBytes();\n    // 加密数据\n    byte[] encryptedBytes = encryptionCipher.doFinal(dataInBytes);\n    return Base64.getEncoder().encodeToString(encryptedBytes);\n}\n\n/**\n * 解密\n */\npublic static String decrypt(String encryptedData) throws Exception {\n    byte[] dataInBytes = Base64.getDecoder().decode(encryptedData);\n    // 解密数据\n    byte[] decryptedBytes = decryptionCipher.doFinal(dataInBytes);\n    return new String(decryptedBytes, StandardCharsets.UTF_8);\n}\n\npublic static void main(String[] args) throws Exception {\n    String originalString = \"Java学习 + 面试指南：javaguide.cn\";\n    init();\n    String encryptedData = encrypt(originalString);\n    String decryptedData = decrypt(encryptedData);\n    System.out.println(\"Original String: \" + originalString);\n    System.out.println(\"AES Encrypted Data : \" + encryptedData);\n    System.out.println(\"AES Decrypted Data : \" + decryptedData);\n}\n```\n\n输出：\n\n```\nOriginal String: Java学习 + 面试指南：javaguide.cn\nAES Encrypted Data : E1qTkK91suBqToag7WCyoFP9uK5hR1nSfM6p+oBlYj71bFiIVnk5TsQRT+zpjv8stha7oyKi3jQ=\nAES Decrypted Data : Java学习 + 面试指南：javaguide.cn\n```\n\n## [#](#非对称加密) 非对称加密\n\n非对称加密算法是指加密和解密使用不同的密钥的算法，也叫公开密钥加密算法。这两个密钥互不相同，一个称为公钥，另一个称为私钥。公钥可以公开给任何人使用，私钥则要保密。\n\n如果用公钥加密数据，只能用对应的私钥解密（加密）；如果用私钥加密数据，只能用对应的公钥解密（签名）。这样就可以实现数据的安全传输和身份认证。\n\n![非对称加密](statistic/非对称加密.png)\n\n非对称加密\n\n常见的非对称加密算法有 RSA、DSA、ECC 等。\n\n### [#](#rsa) RSA\n\nRSA（Rivest–Shamir–Adleman algorithm）算法是一种基于大数分解的困难性的非对称加密算法，它需要选择两个大素数作为私钥的一部分，然后计算出它们的乘积作为公钥的一部分（寻求两个大素数比较简单，而将它们的乘积进行因式分解却极其困难）。RSA 算法原理的详细介绍，可以参考这篇文章：[你真的了解 RSA 加密算法吗？ - 小傅哥open in new window](https://www.cnblogs.com/xiaofuge/p/16954187.html)。\n\nRSA 算法的安全性依赖于大数分解的难度，目前已经有 512 位和 768 位的 RSA 公钥被成功分解，因此建议使用 2048 位或以上的密钥长度。\n\nRSA 算法的优点是简单易用，可以用于数据加密和数字签名；缺点是运算速度慢，不适合大量数据的加密。\n\nRSA 算法是是目前应用最广泛的非对称加密算法，像 SSL/TLS、SSH 等协议中就用到了 RSA 算法。\n\n![HTTPS 证书签名算法中带RSA 加密的SHA-256](statistic/HTTPS_证书签名算法中带RSA_加密的SHA-256.png)\n\nHTTPS 证书签名算法中带RSA 加密的SHA-256\n\n基于 Java 实现 RSA 算法代码示例：\n\n```\nprivate static final String RSA_ALGORITHM = \"RSA\";\n\n/**\n * 生成RSA密钥对\n */\npublic static KeyPair generateKeyPair() throws NoSuchAlgorithmException {\n    KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(RSA_ALGORITHM);\n    // 密钥大小为2048位\n    keyPairGenerator.initialize(2048);\n    return keyPairGenerator.generateKeyPair();\n}\n\n/**\n * 使用公钥加密数据\n */\npublic static String encrypt(String data, PublicKey publicKey) throws Exception {\n    Cipher cipher = Cipher.getInstance(RSA_ALGORITHM);\n    cipher.init(Cipher.ENCRYPT_MODE, publicKey);\n    byte[] encryptedData = cipher.doFinal(data.getBytes(StandardCharsets.UTF_8));\n    return Base64.getEncoder().encodeToString(encryptedData);\n}\n\n/**\n * 使用私钥解密数据\n */\npublic static String decrypt(String encryptedData, PrivateKey privateKey) throws Exception {\n    byte[] decodedData = Base64.getDecoder().decode(encryptedData);\n    Cipher cipher = Cipher.getInstance(RSA_ALGORITHM);\n    cipher.init(Cipher.DECRYPT_MODE, privateKey);\n    byte[] decryptedData = cipher.doFinal(decodedData);\n    return new String(decryptedData, StandardCharsets.UTF_8);\n}\n\npublic static void main(String[] args) throws Exception {\n    KeyPair keyPair = generateKeyPair();\n    PublicKey publicKey = keyPair.getPublic();\n    PrivateKey privateKey = keyPair.getPrivate();\n    String originalString = \"Java学习 + 面试指南：javaguide.cn\";\n    String encryptedData = encrypt(originalString, publicKey);\n    String decryptedData = decrypt(encryptedData, privateKey);\n    System.out.println(\"Original String: \" + originalString);\n    System.out.println(\"RSA Encrypted Data : \" + encryptedData);\n    System.out.println(\"RSA Decrypted Data : \" + decryptedData);\n}\n```\n\n输出：\n\n```\nOriginal String: Java学习 + 面试指南：javaguide.cn\nRSA Encrypted Data : T9ey/CEPUAhZm4UJjuVNIg8RPd1fQ32S9w6+rvOKxmuMumkJY2daFfWuCn8A73Mk5bL6TigOJI0GHfKOt/W2x968qLM3pBGCcPX17n4pR43f32IIIz9iPdgF/INOqDxP5ZAtCDvTiuzcSgDHXqiBSK5TDjtj7xoGjfudYAXICa8pWitnqDgJYoo2J0F8mKzxoi8D8eLE455MEx8ZT1s7FUD/z7/H8CfShLRbO9zq/zFI06TXn123ufg+F4lDaq/5jaIxGVEUB/NFeX4N6OZCFHtAV32mw71BYUadzI9TgvkkUr1rSKmQ0icNhnRdKedJokGUh8g9QQ768KERu92Ibg==\nRSA Decrypted Data : Java学习 + 面试指南：javaguide.cn\n```\n\n### [#](#dsa) DSA\n\nDSA（Digital Signature Algorithm）算法是一种基于离散对数的困难性的非对称加密算法，它需要选择一个素数 q 和一个 q 的倍数 p 作为私钥的一部分，然后计算出一个模 p 的原根 g 和一个模 q 的整数 y 作为公钥的一部分。DSA 算法的安全性依赖于离散对数的难度，目前已经有 1024 位的 DSA 公钥被成功破解，因此建议使用 2048 位或以上的密钥长度。\n\nDSA 算法的优点是数字签名速度快，适合生成数字证书；缺点是不能用于数据加密，且签名过程需要随机数。\n\nDSA 算法签名过程：\n\n1. 使用消息摘要算法对要发送的数据进行加密，生成一个信息摘要，也就是一个短的、唯一的、不可逆的数据表示。\n2. 发送方用自己的 DSA 私钥对信息摘要再进行加密，形成一个数字签名，也就是一个可以证明数据来源和完整性的数据附加。\n3. 将原始数据和数字签名一起通过互联网传送给接收方。\n4. 接收方用发送方的公钥对数字签名进行解密，得到信息摘要。同时，接收方也用消息摘要算法对收到的原始数据进行加密，得到另一个信息摘要。接收方将两个信息摘要进行比较，如果两者一致，则说明在传送过程中数据没有被篡改或损坏；否则，则说明数据已经失去了安全性和保密性。\n\n![DSA 算法签名过程](statistic/DSA_算法签名过程.png)\n\nDSA 算法签名过程\n\n## [#](#总结) 总结\n\n这篇文章介绍了三种加密算法：哈希算法、对称加密算法和非对称加密算法。\n\n- 哈希算法是一种用数学方法对数据生成一个固定长度的唯一标识的技术，可以用来验证数据的完整性和一致性，常见的哈希算法有 MD、SHA、MAC 等。\n- 对称加密算法是一种加密和解密使用同一个密钥的算法，可以用来保护数据的安全性和保密性，常见的对称加密算法有 DES、3DES、AES 等。\n- 非对称加密算法是一种加密和解密使用不同的密钥的算法，可以用来实现数据的安全传输和身份认证，常见的非对称加密算法有 RSA、DSA、ECC 等。\n\n## [#](#参考) 参考\n\n- 奇妙的安全旅行之 DSA 算法：[https://zhuanlan.zhihu.com/p/347025157open in new window](https://zhuanlan.zhihu.com/p/347025157)\n- AES-GCM 加密简介：[https://juejin.cn/post/6844904122676690951open in new window](https://juejin.cn/post/6844904122676690951)\n- Java AES 256 GCM Encryption and Decryption Example | JCE Unlimited Strength：[https://www.javainterviewpoint.com/java-aes-256-gcm-encryption-and-decryption/](https://www.javainterviewpoint.com/java-aes-256-gcm-encryption-and-decryption/)\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/encryption-algorithms.html#dsa","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/%E6%95%8F%E6%84%9F%E8%AF%8D%E8%BF%87%E6%BB%A4%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93":{"title":"敏感词过滤方案总结","content":"系统需要对用户输入的文本进行敏感词过滤如色情、政治、暴力相关的词汇。\n\n敏感词过滤用的使用比较多的 **Trie 树算法** 和 **DFA 算法**。\n\n## [#](#算法实现) 算法实现\n\n### [#](#trie-树) Trie 树\n\n**Trie 树** 也称为字典树、单词查找树，哈系树的一种变种，通常被用于字符串匹配，用来解决在一组字符串集合中快速查找某个字符串的问题。像浏览器搜索的关键词提示一般就是基于 Trie 树来做的。\n\n![浏览器 Trie 树效果展示](https://oss.javaguide.cn/github/javaguide/system-design/security/brower-trie.png)\n\n浏览器 Trie 树效果展示\n\n假如我们的敏感词库中有以下敏感词：\n\n- 高清视频\n- 高清 CV\n- 东京冷\n- 东京热\n\n我们构造出来的敏感词 Trie 树就是下面这样的：\n\n![敏感词 Trie 树](https://oss.javaguide.cn/github/javaguide/system-design/security/sensitive-word-trie.png)\n\n敏感词 Trie 树\n\n当我们要查找对应的字符串“东京热”的话，我们会把这个字符串切割成单个的字符“东”、“京”、“热”，然后我们从 Trie 树的根节点开始匹配。\n\n可以看出， **Trie 树的核心原理其实很简单，就是通过公共前缀来提高字符串匹配效率。**\n\n[Apache Commons Collecionsopen in new window](https://mvnrepository.com/artifact/org.apache.commons/commons-collections4) 这个库中就有 Trie 树实现：\n\n![Apache Commons Collecions 中的 Trie 树实现](https://oss.javaguide.cn/github/javaguide/system-design/security/common-collections-trie.png)\n\nApache Commons Collecions 中的 Trie 树实现\n\n```\nTrie\u003cString, String\u003e trie = new PatriciaTrie\u003c\u003e();\ntrie.put(\"Abigail\", \"student\");\ntrie.put(\"Abi\", \"doctor\");\ntrie.put(\"Annabel\", \"teacher\");\ntrie.put(\"Christina\", \"student\");\ntrie.put(\"Chris\", \"doctor\");\nAssertions.assertTrue(trie.containsKey(\"Abigail\"));\nassertEquals(\"{Abi=doctor, Abigail=student}\", trie.prefixMap(\"Abi\").toString());\nassertEquals(\"{Chris=doctor, Christina=student}\", trie.prefixMap(\"Chr\").toString());\n```\n\nAho-Corasick（AC）自动机是一种建立在 Trie 树上的一种改进算法，是一种多模式匹配算法，由贝尔实验室的研究人员 Alfred V. Aho 和 Margaret J.Corasick 发明。\n\nAC 自动机算法使用 Trie 树来存放模式串的前缀，通过失败匹配指针（失配指针）来处理匹配失败的跳转。\n\n相关阅读：[地铁十分钟 | AC 自动机open in new window](https://zhuanlan.zhihu.com/p/146369212)\n\n### [#](#dfa) DFA\n\n**DFA**（Deterministic Finite Automata)即确定有穷自动机，与之对应的是 NFA（Non-Deterministic Finite Automata，不确定有穷自动机)。\n\n关于 DFA 的详细介绍可以看这篇文章：[有穷自动机 DFA\u0026NFA (学习笔记) - 小蜗牛的文章 - 知乎open in new window](https://zhuanlan.zhihu.com/p/30009083) 。\n\n[Hutoolopen in new window](https://hutool.cn/docs/#/dfa/%E6%A6%82%E8%BF%B0) 提供了 DFA 算法的实现：\n\n![Hutool 的 DFA 算法](https://oss.javaguide.cn/github/javaguide/system-design/security/hutool-dfa.png)\n\nHutool 的 DFA 算法\n\n```\nWordTree wordTree = new WordTree();\nwordTree.addWord(\"大\");\nwordTree.addWord(\"大憨憨\");\nwordTree.addWord(\"憨憨\");\nString text = \"那人真是个大憨憨！\";\n// 获得第一个匹配的关键字\nString matchStr = wordTree.match(text);\nSystem.out.println(matchStr);\n// 标准匹配，匹配到最短关键词，并跳过已经匹配的关键词\nList\u003cString\u003e matchStrList = wordTree.matchAll(text, -1, false, false);\nSystem.out.println(matchStrList);\n//匹配到最长关键词，跳过已经匹配的关键词\nList\u003cString\u003e matchStrList2 = wordTree.matchAll(text, -1, false, true);\nSystem.out.println(matchStrList2);\n```\n\n输出：\n\n```\n大\n[大, 憨憨]\n[大, 大憨憨]\n```\n\n## [#](#开源项目) 开源项目\n\n- [ToolGood.Wordsopen in new window](https://github.com/toolgood/ToolGood.Words)：一款高性能敏感词(非法词/脏字)检测过滤组件，附带繁体简体互换，支持全角半角互换，汉字转拼音，模糊搜索等功能。\n- [sensitive-words-filteropen in new window](https://github.com/hooj0/sensitive-words-filter)：敏感词过滤项目，提供 TTMP、DFA、DAT、hash bucket、Tire 算法支持过滤。可以支持文本的高亮、过滤、判词、替换的接口支持。\n\n## [#](#论文) 论文\n\n- [一种敏感词自动过滤管理系统open in new window](https://patents.google.com/patent/CN101964000B)\n- [一种网络游戏中敏感词过滤方法及系统](https://patents.google.com/patent/CN103714160A/zh)\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/sentive-words-filter.html#dfa","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/%E6%95%B0%E6%8D%AE%E8%84%B1%E6%95%8F":{"title":"数据脱敏","content":"数据脱敏说的就是我们根据特定的规则对敏感信息数据进行变形，比如我们把手机号、身份证号某些位数使用 * 来代替。","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E8%AF%A6%E8%A7%A3":{"title":"权限系统设计详解","content":"## 老权限系统的问题与现状\n\n转转公司在过去并没有一个统一的权限管理系统，权限管理由各业务自行研发或是使用其他业务的权限系统，权限管理的不统一带来了不少问题：\n\n1. 各业务重复造轮子，维护成本高\n2. 各系统只解决部分场景问题，方案不够通用，新项目选型时没有可靠的权限管理方案\n3. 缺乏统一的日志管理与审批流程，在授权信息追溯上十分困难\n\n基于上述问题，去年底公司启动建设转转统一权限系统，目标是开发一套灵活、易用、安全的权限管理系统，供各业务使用。\n\n## [#](#业界权限系统的设计方式) 业界权限系统的设计方式\n\n目前业界主流的权限模型有两种，下面分别介绍下：\n\n- **基于角色的访问控制（RBAC）**\n- **基于属性的访问控制（ABAC）**\n\n### [#](#rbac-模型) RBAC 模型\n\n**基于角色的访问控制（Role-Based Access Control，简称 RBAC）** 指的是通过用户的角色（Role）授权其相关权限，实现了灵活的访问控制，相比直接授予用户权限，要更加简单、高效、可扩展。\n\n一个用户可以拥有若干角色，每一个角色又可以被分配若干权限这样，就构造成“用户-角色-权限” 的授权模型。在这种模型中，用户与角色、角色与权限之间构成了多对多的关系。\n\n用一个图来描述如下：\n\n![RBAC 权限模型示意图](statistic/RBAC_权限模型示意图.png)\n\nRBAC 权限模型示意图\n\n当使用 `RBAC模型` 时，通过分析用户的实际情况，基于共同的职责和需求，授予他们不同角色。这种 `用户 -\u003e 角色 -\u003e 权限` 间的关系，让我们可以不用再单独管理单个用户权限，用户从授予的角色里面获取所需的权限。\n\n以一个简单的场景（Gitlab 的权限系统）为例，用户系统中有 `Admin`、`Maintainer`、`Operator` 三种角色，这三种角色分别具备不同的权限，比如只有 `Admin` 具备创建代码仓库、删除代码仓库的权限，其他的角色都不具备。我们授予某个用户 `Admin` 这个角色，他就具备了 **创建代码仓库** 和 **删除代码仓库** 这两个权限。\n\n通过 `RBAC模型` ，当存在多个用户拥有相同权限时，我们只需要创建好拥有该权限的角色，然后给不同的用户分配不同的角色，后续只需要修改角色的权限，就能自动修改角色内所有用户的权限。\n\n### [#](#abac-模型) ABAC 模型\n\n**基于属性的访问控制（Attribute-Based Access Control，简称 ABAC）** 是一种比 `RBAC模型` 更加灵活的授权模型，它的原理是通过各种属性来动态判断一个操作是否可以被允许。这个模型在云系统中使用的比较多，比如 AWS，阿里云等。\n\n考虑下面这些场景的权限控制：\n\n1. 授权某个人具体某本书的编辑权限\n2. 当一个文档的所属部门跟用户的部门相同时，用户可以访问这个文档\n3. 当用户是一个文档的拥有者并且文档的状态是草稿，用户可以编辑这个文档\n4. 早上九点前禁止 A 部门的人访问 B 系统\n5. 在除了上海以外的地方禁止以管理员身份访问 A 系统\n6. 用户对 2022-06-07 之前创建的订单有操作权限\n\n可以发现上述的场景通过 `RBAC模型` 很难去实现，因为 `RBAC模型` 仅仅描述了用户可以做什么操作，但是操作的条件，以及操作的数据，`RBAC模型` 本身是没有这些限制的。但这恰恰是 `ABAC模型` 的长处，`ABAC模型` 的思想是基于用户、访问的数据的属性、以及各种环境因素去动态计算用户是否有权限进行操作。\n\n#### [#](#abac-模型的原理) ABAC 模型的原理\n\n在 `ABAC模型` 中，一个操作是否被允许是基于对象、资源、操作和环境信息共同动态计算决定的。\n\n- **对象**：对象是当前请求访问资源的用户。用户的属性包括 ID，个人资源，角色，部门和组织成员身份等\n- **资源**：资源是当前用户要访问的资产或对象，例如文件，数据，服务器，甚至 API\n- **操作**：操作是用户试图对资源进行的操作。常见的操作包括“读取”，“写入”，“编辑”，“复制”和“删除”\n- **环境**：环境是每个访问请求的上下文。环境属性包含访问的时间和位置，对象的设备，通信协议和加密强度等\n\n在 `ABAC模型` 的决策语句的执行过程中，决策引擎会根据定义好的决策语句，结合对象、资源、操作、环境等因素动态计算出决策结果。每当发生访问请求时，`ABAC模型` 决策系统都会分析属性值是否与已建立的策略匹配。如果有匹配的策略，访问请求就会被通过。\n\n## [#](#新权限系统的设计思想) 新权限系统的设计思想\n\n结合转转的业务现状，`RBAC模型` 满足了转转绝大部分业务场景，并且开发成本远低于 `ABAC模型` 的权限系统，所以新权限系统选择了基于 `RBAC模型` 来实现。对于实在无法满足的业务系统，我们选择了暂时性不支持，这样可以保障新权限系统的快速落地，更快的让业务使用起来。\n\n标准的 `RBAC模型` 是完全遵守 `用户 -\u003e 角色 -\u003e 权限` 这个链路的，也就是用户的权限完全由他所拥有的角色来控制，但是这样会有一个缺点，就是给用户加权限必须新增一个角色，导致实际操作起来效率比较低。所以我们在 `RBAC模型` 的基础上，新增了给用户直接增加权限的能力，也就是说既可以给用户添加角色，也可以给用户直接添加权限。最终用户的权限是由拥有的角色和权限点组合而成。\n\n**新权限系统的权限模型**：用户最终权限 = 用户拥有的角色带来的权限 + 用户独立配置的权限，两者取并集。\n\n新权限系统方案如下图：\n\n![新权限系统方案](statistic/新权限系统方案.png)\n\n新权限系统方案\n\n- 首先，将集团所有的用户（包括外部用户），通过 **统一登录与注册** 功能实现了统一管理，同时与公司的组织架构信息模块打通，实现了同一个人员在所有系统中信息的一致，这也为后续基于组织架构进行权限管理提供了可行性。\n- 其次，因为新权限系统需要服务集团所有业务，所以需要支持多系统权限管理。用户进行权限管理前，需要先选择相应的系统，然后配置该系统的 **菜单权限** 和 **数据权限** 信息，建立好系统的各个权限点。_PS：菜单权限和数据权限的具体说明，下文会详细介绍。_\n- 最后，创建该系统下的不同角色，给不同角色配置好权限点。比如店长角色，拥有店员操作权限、本店数据查看权限等，配置好这个角色后，后续只需要给店长增加这个角色，就可以让他拥有对应的权限。\n\n完成上述配置后，就可以进行用户的权限管理了。有两种方式可以给用户加权限：\n\n1. 先选用户，然后添加权限。该方式可以给用户添加任意角色或是菜单/数据权限点。\n2. 先选择角色，然后关联用户。该方式只可给用户添加角色，不能单独添加菜单/数据权限点。\n\n这两种方式的具体设计方案，后文会详细说明。\n\n### [#](#权限系统自身的权限管理) 权限系统自身的权限管理\n\n对于权限系统来说，首先需要设计好系统自身的权限管理，也就是需要管理好 ”谁可以进入权限系统，谁可以管理其他系统的权限“，对于权限系统自身的用户，会分为三类：\n\n1. **超级管理员**：拥有权限系统的全部操作权限，可以进行系统自身的任何操作，也可以管理接入权限的应用系统的管理操作。\n2. **权限操作用户**：拥有至少一个已接入的应用系统的超级管理员角色的用户。该用户能进行的操作限定在所拥有的应用系统权限范围内。权限操作用户是一种身份，无需分配，而是根据规则自动获得的。\n3. **普通用户**：普通用户也可以认为是一种身份，除去上述 2 类人，其余的都为普通用户。他们只能申请接入系统以及访问权限申请页面。\n\n### [#](#权限类型的定义) 权限类型的定义\n\n新权限系统中，我们把权限分为两大类，分别是：\n\n- **菜单功能权限**：包括系统的目录导航、菜单的访问权限，以及按钮和 API 操作的权限\n- **数据权限**：包括定义数据的查询范围权限，在不同系统中，通常叫做 “组织”、”站点“等，在新权限系统中，统一称作 ”组织“ 来管理数据权限\n\n### [#](#默认角色的分类) 默认角色的分类\n\n每个系统中设计了三个默认角色，用来满足基本的权限管理需求，分别如下：\n\n- **超级管理员**：该角色拥有该系统的全部权限，可以修改系统的角色权限等配置，可以给其他用户授权。\n- **系统管理员**：该角色拥有给其他用户授权以及修改系统的角色权限等配置能力，但角色本身不具有任何权限。\n- **授权管理员**：该角色拥有给其他用户授权的能力。但是授权的范围不超出自己所拥有的权限。\n\n\u003e 举个栗子：授权管理员 A 可以给 B 用户添加权限，但添加的范围 小于等于 A 用户已拥有的权限。\n\n经过这么区分，把 **拥有权限** 和 **拥有授权能力** ，这两部分给分隔开来，可以满足所有的权限控制的场景。\n\n## [#](#新权限系统的核心模块设计) 新权限系统的核心模块设计\n\n上面介绍了新权限系统的整体设计思想，接下来分别介绍下核心模块的设计\n\n### [#](#系统-菜单-数据权限管理) 系统/菜单/数据权限管理\n\n把一个新系统接入权限系统有下列步骤：\n\n1. 创建系统\n2. 配置菜单功能权限\n3. 配置数据权限（可选）\n4. 创建系统的角色\n\n其中，1、2、3 的步骤，都是在系统管理模块完成，具体流程如下图:\n\n![系统接入流程图](statistic/系统接入流程图.png)\n\n系统接入流程图\n\n用户可以对系统的基本信息进行增删改查的操作，不同系统之间通过 `系统编码` 作为唯一区分。同时 `系统编码` 也会用作于菜单和数据权限编码的前缀，通过这样的设计保证权限编码全局唯一性。\n\n例如系统的编码为 `test_online`，那么该系统的菜单编码格式便为 `test_online:m_xxx`。\n\n系统管理界面设计如下：\n\n![系统管理界面设计](statistic/系统管理界面设计.png)\n\n系统管理界面设计\n\n#### [#](#菜单管理) 菜单管理\n\n新权限系统首先对菜单进行了分类，分别是 `目录`、`菜单` 和 `操作`，示意如下图\n\n![菜单管理界面](statistic/菜单管理界面.png)\n\n菜单管理界面\n\n它们分别代表的含义是：\n\n- **目录**：指的是应用系统中最顶部的一级目录，通常在系统 Logo 的右边\n- **菜单**：指的是应用系统左侧的多层级菜单，通常在系统 Logo 的下面，也是最常用的菜单结构\n- **操作**：指页面中的按钮、接口等一系列可以定义为操作或页面元素的部分。\n\n菜单管理界面设计如下：\n\n![菜单管理界面设计](statistic/菜单管理界面设计.png)\n\n菜单管理界面设计\n\n菜单权限数据的使用，也提供两种方式：\n\n- **动态菜单模式**：这种模式下，菜单的增删完全由权限系统接管。也就是说在权限系统增加菜单，应用系统会同步增加。这种模式好处是修改菜单无需项目上线。\n- **静态菜单模式**：菜单的增删由应用系统的前端控制，权限系统只控制访问权限。这种模式下，权限系统只能标识出用户是否拥有当前菜单的权限，而具体的显示控制是由前端根据权限数据来决定。\n\n### [#](#角色与用户管理) 角色与用户管理\n\n角色与用户管理都是可以直接改变用户权限的核心模块，整个设计思路如下图：\n\n![角色与用户管理模块设计](statistic/角色与用户管理模块设计.png)\n\n角色与用户管理模块设计\n\n这个模块设计重点是需要考虑到批量操作。无论是通过角色关联用户，还是给用户批量增加/删除/重置权限，批量操作的场景都是系统需要设计好的。\n\n### [#](#权限申请) 权限申请\n\n除了给其他用户添加权限外，新权限系统同时支持了用户自主申请权限。这个模块除了常规的审批流（申请、审批、查看）等，有一个比较特别的功能，就是如何让用户能选对自己要的权限。所以在该模块的设计上，除了直接选择角色外，还支持通过菜单/数据权限点，反向选择角色，如下图：\n\n![权限申请界面](statistic/权限申请界面.png)\n\n权限申请界面\n\n### [#](#操作日志) 操作日志\n\n系统操作日志会分为两大类：\n\n1. **操作流水日志**：用户可看、可查的关键操作日志\n2. **服务 Log 日志**：系统服务运行过程中产生的 Log 日志,其中，服务 Log 日志信息量大于操作流水日志，但是不方便搜索查看。所以权限系统需要提供操作流水日志功能。\n\n在新权限系统中，用户所有的操作可以分为三类，分别为新增、更新、删除。所有的模块也可枚举，例如用户管理、角色管理、菜单管理等。明确这些信息后，那么一条日志就可以抽象为：什么人(Who)在什么时间(When)对哪些人(Target)的哪些模块做了哪些操作。 这样把所有的记录都入库，就可以方便的进行日志的查看和筛选了。\n\n## [#](#总结与展望) 总结与展望\n\n至此，新权限系统的核心设计思路与模块都已介绍完成，新系统在转转内部有大量的业务接入使用，权限管理相比以前方便了许多。权限系统作为每家公司的一个基础系统，灵活且完备的设计可以助力日后业务的发展更加高效。\n\n后续两篇：\n\n- [转转统一权限系统的设计与实现（后端实现篇）open in new window](https://mp.weixin.qq.com/s/hFTDckfxhSnoM_McP18Vkg)\n- [转转统一权限系统的设计与实现（前端实现篇）open in new window](https://mp.weixin.qq.com/s/a_P4JAwxgunhfmJvpBnWYA)\n\n## [#](#参考) 参考\n\n- 选择合适的权限模型：https://docs.authing.cn/v2/guides/access-control/choose-the-right-access-control-model.html\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/design-of-authority-system.html#%E6%9D%83%E9%99%90%E7%94%B3%E8%AF%B7","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E8%AF%A6%E8%A7%A3":{"title":"认证授权基础概念详解","content":"## 认证 (Authentication) 和授权 (Authorization)的区别是什么？\n\n这是一个绝大多数人都会混淆的问题。首先先从读音上来认识这两个名词，很多人都会把它俩的读音搞混，所以我建议你先先去查一查这两个单词到底该怎么读，他们的具体含义是什么。\n\n说简单点就是：\n\n- **认证 (Authentication)：** 你是谁。\n- **授权 (Authorization)：** 你有权限干什么。\n\n稍微正式点（啰嗦点）的说法就是：\n\n- **Authentication（认证）** 是验证您的身份的凭据（例如用户名/用户 ID 和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。\n- **Authorization（授权）** 发生在 **Authentication（认证）** 之后。授权嘛，光看意思大家应该就明白，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如 admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。\n\n认证：\n\n![认证登录](statistic/认证登录.png)\n\n认证登录\n\n授权：\n\n![没有权限](statistic/没有权限.png)\n\n没有权限\n\n这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。\n\n## [#](#rbac-模型了解吗) RBAC 模型了解吗？\n\n系统权限控制最常采用的访问控制模型就是 **RBAC 模型** 。\n\n**什么是 RBAC 呢？**\n\nRBAC 即基于角色的权限访问控制（Role-Based Access Control）。这是一种通过角色关联权限，角色同时又关联用户的授权的方式。\n\n简单地说：一个用户可以拥有若干角色，每一个角色又可以被分配若干权限，这样就构造成“用户-角色-权限” 的授权模型。在这种模型中，用户与角色、角色与权限之间构成了多对多的关系，如下图\n\n![RBAC 权限模型示意图](statistic/RBAC_权限模型示意图.png)\n\nRBAC 权限模型示意图\n\n**在 RBAC 中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。**\n\n本系统的权限设计相关的表如下（一共 5 张表，2 张用户建立表之间的联系）：\n\n![](statistic/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1-%E6%9D%83%E9%99%90.png)\n\n通过这个权限模型，我们可以创建不同的角色并为不同的角色分配不同的权限范围（菜单）。\n\n![](statistic/books%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E6%A8%A1%E5%9D%97.png)\n\n通常来说，如果系统对于权限控制要求比较严格的话，一般都会选择使用 RBAC 模型来做权限控制。\n\n## [#](#什么是-cookie-cookie-的作用是什么) 什么是 Cookie ? Cookie 的作用是什么?\n\n![](statistic/cookie-sessionId.png)\n\n`Cookie` 和 `Session` 都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。\n\n维基百科是这样定义 `Cookie` 的：\n\n\u003e `Cookies` 是某些网站为了辨别用户身份而储存在用户本地终端上的数据（通常经过加密）。\n\n简单来说：**`Cookie` 存放在客户端，一般用来保存用户信息**。\n\n下面是 `Cookie` 的一些应用案例：\n\n1. 我们在 `Cookie` 中保存已经登录过的用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了。除此之外，`Cookie` 还能保存用户首选项，主题和其他设置信息。\n2. 使用 `Cookie` 保存 `SessionId` 或者 `Token` ，向后端发送请求的时候带上 `Cookie`，这样后端就能取到 `Session` 或者 `Token` 了。这样就能记录用户当前的状态了，因为 HTTP 协议是无状态的。\n3. `Cookie` 还可以用来记录和分析用户行为。举个简单的例子你在网上购物的时候，因为 HTTP 协议是没有状态的，如果服务器想要获取你在某个页面的停留状态或者看了哪些商品，一种常用的实现方式就是将这些信息存放在 `Cookie`\n4. ......\n\n## [#](#如何在项目中使用-cookie-呢) 如何在项目中使用 Cookie 呢？\n\n我这里以 Spring Boot 项目为例。\n\n**1)设置 `Cookie` 返回给客户端**\n\n```\n@GetMapping(\"/change-username\")\npublic String setCookie(HttpServletResponse response) {\n    // 创建一个 cookie\n    Cookie cookie = new Cookie(\"username\", \"Jovan\");\n    //设置 cookie过期时间\n    cookie.setMaxAge(7 * 24 * 60 * 60); // expires in 7 days\n    //添加到 response 中\n    response.addCookie(cookie);\n\n    return \"Username is changed!\";\n}\n```\n\n**2) 使用 Spring 框架提供的 `@CookieValue` 注解获取特定的 cookie 的值**\n\n```\n@GetMapping(\"/\")\npublic String readCookie(@CookieValue(value = \"username\", defaultValue = \"Atta\") String username) {\n    return \"Hey! My username is \" + username;\n}\n```\n\n**3) 读取所有的 `Cookie` 值**\n\n```\n@GetMapping(\"/all-cookies\")\npublic String readAllCookies(HttpServletRequest request) {\n\n    Cookie[] cookies = request.getCookies();\n    if (cookies != null) {\n        return Arrays.stream(cookies)\n                .map(c -\u003e c.getName() + \"=\" + c.getValue()).collect(Collectors.joining(\", \"));\n    }\n\n    return \"No cookies\";\n}\n```\n\n更多关于如何在 Spring Boot 中使用 `Cookie` 的内容可以查看这篇文章：[How to use cookies in Spring Bootopen in new window](https://attacomsian.com/blog/cookies-spring-boot) 。\n\n## [#](#cookie-和-session-有什么区别) Cookie 和 Session 有什么区别？\n\n**`Session` 的主要作用就是通过服务端记录用户的状态。** 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 `Session` 之后就可以标识这个用户并且跟踪这个用户了。\n\n`Cookie` 数据保存在客户端(浏览器端)，`Session` 数据保存在服务器端。相对来说 `Session` 安全性更高。如果使用 `Cookie` 的一些敏感信息不要写入 `Cookie` 中，最好能将 `Cookie` 信息加密然后使用到的时候再去服务器端解密。\n\n**那么，如何使用 `Session` 进行身份验证？**\n\n## [#](#如何使用-session-cookie-方案进行身份验证) 如何使用 Session-Cookie 方案进行身份验证？\n\n很多时候我们都是通过 `SessionID` 来实现特定的用户，`SessionID` 一般会选择存放在 Redis 中。举个例子：\n\n1. 用户成功登陆系统，然后返回给客户端具有 `SessionID` 的 `Cookie` 。\n2. 当用户向后端发起请求的时候会把 `SessionID` 带上，这样后端就知道你的身份状态了。\n\n关于这种认证方式更详细的过程如下：\n\n![](statistic/session-cookie-authentication-process.png)\n\n1. 用户向服务器发送用户名、密码、验证码用于登陆系统。\n2. 服务器验证通过后，服务器为用户创建一个 `Session`，并将 `Session` 信息存储起来。\n3. 服务器向用户返回一个 `SessionID`，写入用户的 `Cookie`。\n4. 当用户保持登录状态时，`Cookie` 将与每个后续请求一起被发送出去。\n5. 服务器可以将存储在 `Cookie` 上的 `SessionID` 与存储在内存中或者数据库中的 `Session` 信息进行比较，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。\n\n使用 `Session` 的时候需要注意下面几个点：\n\n- 依赖 `Session` 的关键业务一定要确保客户端开启了 `Cookie`。\n- 注意 `Session` 的过期时间。\n\n另外，Spring Session 提供了一种跨多个应用程序或实例管理用户会话信息的机制。如果想详细了解可以查看下面几篇很不错的文章：\n\n- [Getting Started with Spring Sessionopen in new window](https://codeboje.de/spring-Session-tutorial/)\n- [Guide to Spring Sessionopen in new window](https://www.baeldung.com/spring-Session)\n- [Sticky Sessions with Spring Session \u0026 Redisopen in new window](https://medium.com/@gvnix/sticky-Sessions-with-spring-Session-redis-bdc6f7438cc3)\n\n## [#](#多服务器节点下-session-cookie-方案如何做) 多服务器节点下 Session-Cookie 方案如何做？\n\nSession-Cookie 方案在单体环境是一个非常好的身份认证方案。但是，当服务器水平拓展成多节点时，Session-Cookie 方案就要面临挑战了。\n\n举个例子：假如我们部署了两份相同的服务 A，B，用户第一次登陆的时候 ，Nginx 通过负载均衡机制将用户请求转发到 A 服务器，此时用户的 Session 信息保存在 A 服务器。结果，用户第二次访问的时候 Nginx 将请求路由到 B 服务器，由于 B 服务器没有保存 用户的 Session 信息，导致用户需要重新进行登陆。\n\n**我们应该如何避免上面这种情况的出现呢？**\n\n有几个方案可供大家参考：\n\n1. 某个用户的所有请求都通过特性的哈希策略分配给同一个服务器处理。这样的话，每个服务器都保存了一部分用户的 Session 信息。服务器宕机，其保存的所有 Session 信息就完全丢失了。\n2. 每一个服务器保存的 Session 信息都是互相同步的，也就是说每一个服务器都保存了全量的 Session 信息。每当一个服务器的 Session 信息发生变化，我们就将其同步到其他服务器。这种方案成本太大，并且，节点越多时，同步成本也越高。\n3. 单独使用一个所有服务器都能访问到的数据节点（比如缓存）来存放 Session 信息。为了保证高可用，数据节点尽量要避免是单点。\n\n## [#](#如果没有-cookie-的话-session-还能用吗) 如果没有 Cookie 的话 Session 还能用吗？\n\n这是一道经典的面试题！\n\n一般是通过 `Cookie` 来保存 `SessionID` ，假如你使用了 `Cookie` 保存 `SessionID` 的方案的话， 如果客户端禁用了 `Cookie`，那么 `Session` 就无法正常工作。\n\n但是，并不是没有 `Cookie` 之后就不能用 `Session` 了，比如你可以将 `SessionID` 放在请求的 `url` 里面`https://javaguide.cn/?Session_id=xxx` 。这种方案的话可行，但是安全性和用户体验感降低。当然，为了你也可以对 `SessionID` 进行一次加密之后再传入后端。\n\n## [#](#为什么-cookie-无法防止-csrf-攻击-而-token-可以) 为什么 Cookie 无法防止 CSRF 攻击，而 Token 可以？\n\n**CSRF(Cross Site Request Forgery)** 一般被翻译为 **跨站请求伪造** 。那么什么是 **跨站请求伪造** 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子：\n\n小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了 10000 元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。\n\n```\n\u003ca src=http://www.mybank.com/Transfer?bankId=11\u0026money=10000\u003e科学理财，年盈利率过万\u003c/\u003e\n```\n\n上面也提到过，进行 `Session` 认证的时候，我们一般使用 `Cookie` 来存储 `SessionId`,当我们登陆后后端生成一个 `SessionId` 放在 Cookie 中返回给客户端，服务端通过 Redis 或者其他存储工具记录保存着这个 `SessionId`，客户端登录以后每次请求都会带上这个 `SessionId`，服务端通过这个 `SessionId` 来标示你这个人。如果别人通过 `Cookie` 拿到了 `SessionId` 后就可以代替你的身份访问系统了。\n\n`Session` 认证中 `Cookie` 中的 `SessionId` 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。\n\n但是，我们使用 `Token` 的话就不会存在这个问题，在我们登录成功获得 `Token` 之后，一般会选择存放在 `localStorage` （浏览器本地存储）中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 `Token`,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 `Token` 的，所以这个请求将是非法的。\n\n![](statistic/20210615161108272.png)\n\n需要注意的是：不论是 `Cookie` 还是 `Token` 都无法避免 **跨站脚本攻击（Cross Site Scripting）XSS** 。\n\n\u003e 跨站脚本攻击（Cross Site Scripting）缩写为 CSS 但这会与层叠样式表（Cascading Style Sheets，CSS）的缩写混淆。因此，有人将跨站脚本攻击缩写为 XSS。\n\nXSS 中攻击者会用各种方式将恶意代码注入到其他用户的页面中。就可以通过脚本盗用信息比如 `Cookie` 。\n\n推荐阅读：[如何防止 CSRF 攻击？—美团技术团队open in new window](https://tech.meituan.com/2018/10/11/fe-security-csrf.html)\n\n## [#](#什么是-jwt-jwt-由哪些部分组成) 什么是 JWT?JWT 由哪些部分组成？\n\n[JWT 基础概念详解](/system-design/security/jwt-intro.html)\n\n## [#](#如何基于-jwt-进行身份验证-如何防止-jwt-被篡改) 如何基于 JWT 进行身份验证？ 如何防止 JWT 被篡改？\n\n[JWT 基础概念详解](/system-design/security/jwt-intro.html)\n\n## [#](#什么是-sso) 什么是 SSO?\n\nSSO(Single Sign On)即单点登录说的是用户登陆多个子系统的其中一个就有权访问与其相关的其他系统。举个例子我们在登陆了京东金融之后，我们同时也成功登陆京东的京东超市、京东国际、京东生鲜等子系统。\n\n![SSO 示意图](statistic/SSO_示意图.png)\n\nSSO 示意图\n\n## [#](#sso-有什么好处) SSO 有什么好处？\n\n- **用户角度** :用户能够做到一次登录多次使用，无需记录多套用户名和密码，省心。\n- **系统管理员角度** : 管理员只需维护好一个统一的账号中心就可以了，方便。\n- **新系统开发角度:** 新系统开发时只需直接对接统一的账号中心即可，简化开发流程，省时。\n\n## [#](#如何设计实现一个-sso-系统) 如何设计实现一个 SSO 系统?\n\n[SSO 单点登录详解](/system-design/security/sso-intro.html)\n\n## [#](#什么是-oauth-2-0) 什么是 OAuth 2.0？\n\nOAuth 是一个行业的标准授权协议，主要用来授权第三方应用获取有限的权限。而 OAuth 2.0 是对 OAuth 1.0 的完全重新设计，OAuth 2.0 更快，更容易实现，OAuth 1.0 已经被废弃。详情请见：[rfc6749open in new window](https://tools.ietf.org/html/rfc6749)。\n\n实际上它就是一种授权机制，它的最终目的是为第三方应用颁发一个有时效性的令牌 Token，使得第三方应用能够通过该令牌获取相关的资源。\n\nOAuth 2.0 比较常用的场景就是第三方登录，当你的网站接入了第三方登录的时候一般就是使用的 OAuth 2.0 协议。\n\n另外，现在 OAuth 2.0 也常见于支付场景（微信支付、支付宝支付）和开发平台（微信开放平台、阿里开放平台等等）。\n\n下图是 [Slack OAuth 2.0 第三方登录open in new window](https://api.slack.com/legacy/oauth)的示意图：\n\n![](statistic/20210615151716340.png)\n\n**推荐阅读：**\n\n- [OAuth 2.0 的一个简单解释open in new window](http://www.ruanyifeng.com/blog/2019/04/oauth_design.html)\n- [10 分钟理解什么是 OAuth 2.0 协议open in new window](https://deepzz.com/post/what-is-oauth2-protocol.html)\n- [OAuth 2.0 的四种方式open in new window](http://www.ruanyifeng.com/blog/2019/04/oauth-grant-types.html)\n- [GitHub OAuth 第三方登录示例教程open in new window](http://www.ruanyifeng.com/blog/2019/04/github-oauth.html)\n\n## [#](#参考) 参考\n\n- 不要用 JWT 替代 session 管理（上）：全面了解 Token,JWT,OAuth,SAML,SSO：https://zhuanlan.zhihu.com/p/38942172\n- Introduction to JSON Web Tokens：https://jwt.io/introduction\n- JSON Web Token Claims：https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-token-claims\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/basis-of-authority-certification.html#%E4%BB%80%E4%B9%88%E6%98%AF-oauth-2-0","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/JWT-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E8%AF%A6%E8%A7%A3":{"title":"JWT 基础概念详解","content":"## 什么是 JWT?\n\nJWT （JSON Web Token） 是目前最流行的跨域认证解决方案，是一种基于 Token 的认证授权机制。 从 JWT 的全称可以看出，JWT 本身也是 Token，一种规范化之后的 JSON 结构的 Token。\n\nJWT 自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储 Session 信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。\n\n可以看出，**JWT 更符合设计 RESTful API 时的「Stateless（无状态）」原则** 。\n\n并且， 使用 JWT 认证可以有效避免 CSRF 攻击，因为 JWT 一般是存在在 localStorage 中，使用 JWT 进行身份验证的过程中是不会涉及到 Cookie 的。\n\n我在 [JWT 优缺点分析](/system-design/security/advantages-and-disadvantages-of-jwt.html)这篇文章中有详细介绍到使用 JWT 做身份认证的优势和劣势。\n\n下面是 [RFC 7519open in new window](https://tools.ietf.org/html/rfc7519) 对 JWT 做的较为正式的定义。\n\n\u003e JSON Web Token (JWT) is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted. ——[JSON Web Token (JWT)open in new window](https://tools.ietf.org/html/rfc7519)\n\n## [#](#jwt-由哪些部分组成) JWT 由哪些部分组成？\n\n![此图片来源于：https://supertokens.com/blog/oauth-vs-jwt](https://oss.javaguide.cn/javaguide/system-design/jwt/jwt-composition.png)\n\n此图片来源于：https://supertokens.com/blog/oauth-vs-jwt\n\nJWT 本质上就是一组字串，通过（`.`）切分成三个为 Base64 编码的部分：\n\n- **Header** : 描述 JWT 的元数据，定义了生成签名的算法以及 `Token` 的类型。\n- **Payload** : 用来存放实际需要传递的数据\n- **Signature（签名）**：服务器通过 Payload、Header 和一个密钥(Secret)使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。\n\nJWT 通常是这样的：`xxxxx.yyyyy.zzzzz`。\n\n示例：\n\n```\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.\neyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.\nSflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n```\n\n你可以在 [jwt.ioopen in new window](https://jwt.io/) 这个网站上对其 JWT 进行解码，解码之后得到的就是 Header、Payload、Signature 这三部分。\n\nHeader 和 Payload 都是 JSON 格式的数据，Signature 由 Payload、Header 和 Secret(密钥)通过特定的计算公式和加密算法得到。\n\n![](https://oss.javaguide.cn/javaguide/system-design/jwt/jwt.io.png)\n\n### [#](#header) Header\n\nHeader 通常由两部分组成：\n\n- `typ`（Type）：令牌类型，也就是 JWT。\n- `alg`（Algorithm）：签名算法，比如 HS256。\n\n示例：\n\n```\n{\n  \"alg\": \"HS256\",\n  \"typ\": \"JWT\"\n}\n```\n\nJSON 形式的 Header 被转换成 Base64 编码，成为 JWT 的第一部分。\n\n### [#](#payload) Payload\n\nPayload 也是 JSON 格式数据，其中包含了 Claims(声明，包含 JWT 的相关信息)。\n\nClaims 分为三种类型：\n\n- **Registered Claims（注册声明）**：预定义的一些声明，建议使用，但不是强制性的。\n- **Public Claims（公有声明）**：JWT 签发方可以自定义的声明，但是为了避免冲突，应该在 [IANA JSON Web Token Registryopen in new window](https://www.iana.org/assignments/jwt/jwt.xhtml) 中定义它们。\n- **Private Claims（私有声明）**：JWT 签发方因为项目需要而自定义的声明，更符合实际项目场景使用。\n\n下面是一些常见的注册声明：\n\n- `iss`（issuer）：JWT 签发方。\n- `iat`（issued at time）：JWT 签发时间。\n- `sub`（subject）：JWT 主题。\n- `aud`（audience）：JWT 接收方。\n- `exp`（expiration time）：JWT 的过期时间。\n- `nbf`（not before time）：JWT 生效时间，早于该定义的时间的 JWT 不能被接受处理。\n- `jti`（JWT ID）：JWT 唯一标识。\n\n示例：\n\n```\n{\n  \"uid\": \"ff1212f5-d8d1-4496-bf41-d2dda73de19a\",\n  \"sub\": \"1234567890\",\n  \"name\": \"John Doe\",\n  \"exp\": 15323232,\n  \"iat\": 1516239022,\n  \"scope\": [\"admin\", \"user\"]\n}\n```\n\nPayload 部分默认是不加密的，**一定不要将隐私信息存放在 Payload 当中！！！**\n\nJSON 形式的 Payload 被转换成 Base64 编码，成为 JWT 的第二部分。\n\n### [#](#signature) Signature\n\nSignature 部分是对前两部分的签名，作用是防止 JWT（主要是 payload） 被篡改。\n\n这个签名的生成需要用到：\n\n- Header + Payload。\n- 存放在服务端的密钥(一定不要泄露出去)。\n- 签名算法。\n\n签名的计算公式如下：\n\n```\nHMACSHA256(\n  base64UrlEncode(header) + \".\" +\n  base64UrlEncode(payload),\n  secret)\n```\n\n算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用\"点\"（`.`）分隔，这个字符串就是 JWT 。\n\n## [#](#如何基于-jwt-进行身份验证) 如何基于 JWT 进行身份验证？\n\n在基于 JWT 进行身份验证的的应用程序中，服务器通过 Payload、Header 和 Secret(密钥)创建 JWT 并将 JWT 发送给客户端。客户端接收到 JWT 之后，会将其保存在 Cookie 或者 localStorage 里面，以后客户端发出的所有请求都会携带这个令牌。\n\n![JWT 身份验证示意图](https://oss.javaguide.cn/github/javaguide/system-design/jwt/jwt-authentication process.png)\n\nJWT 身份验证示意图\n\n简化后的步骤如下：\n\n1. 用户向服务器发送用户名、密码以及验证码用于登陆系统。\n2. 如果用户用户名、密码以及验证码校验正确的话，服务端会返回已经签名的 Token，也就是 JWT。\n3. 用户以后每次向后端发请求都在 Header 中带上这个 JWT 。\n4. 服务端检查 JWT 并从中获取用户相关信息。\n\n两点建议：\n\n1. 建议将 JWT 存放在 localStorage 中，放在 Cookie 中会有 CSRF 风险。\n2. 请求服务端并携带 JWT 的常见做法是将其放在 HTTP Header 的 `Authorization` 字段中（`Authorization: Bearer Token`）。\n\n**[spring-security-jwt-guideopen in new window](https://github.com/Snailclimb/spring-security-jwt-guide)** 就是一个基于 JWT 来做身份认证的简单案例，感兴趣的可以看看。\n\n## [#](#如何防止-jwt-被篡改) 如何防止 JWT 被篡改？\n\n有了签名之后，即使 JWT 被泄露或者截获，黑客也没办法同时篡改 Signature、Header、Payload。\n\n这是为什么呢？因为服务端拿到 JWT 之后，会解析出其中包含的 Header、Payload 以及 Signature 。服务端会根据 Header、Payload、密钥再次生成一个 Signature。拿新生成的 Signature 和 JWT 中的 Signature 作对比，如果一样就说明 Header 和 Payload 没有被修改。\n\n不过，如果服务端的秘钥也被泄露的话，黑客就可以同时篡改 Signature、Header、Payload 了。黑客直接修改了 Header 和 Payload 之后，再重新生成一个 Signature 就可以了。\n\n**密钥一定保管好，一定不要泄露出去。JWT 安全的核心在于签名，签名安全的核心在密钥。**\n\n## [#](#如何加强-jwt-的安全性) 如何加强 JWT 的安全性？\n\n1. 使用安全系数高的加密算法。\n2. 使用成熟的开源库，没必要造轮子。\n3. JWT 存放在 localStorage 中而不是 Cookie 中，避免 CSRF 风险。\n4. 一定不要将隐私信息存放在 Payload 当中。\n5. 密钥一定保管好，一定不要泄露出去。JWT 安全的核心在于签名，签名安全的核心在密钥。\n6. Payload 要加入 `exp` （JWT 的过期时间），永久有效的 JWT 不合理。并且，JWT 的过期时间不易过长。\n7. ......\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/jwt-intro.html#%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8E-jwt-%E8%BF%9B%E8%A1%8C%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/JWT-%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81%E4%BC%98%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90":{"title":"JWT 身份认证优缺点分析","content":"在 [JWT 基本概念详解open in new window](https://javaguide.cn/system-design/security/jwt-intro.html) 这篇文章中，我介绍了：\n\n- 什么是 JWT?\n- JWT 由哪些部分组成？\n- 如何基于 JWT 进行身份验证？\n- JWT 如何防止 Token 被篡改？\n- 如何加强 JWT 的安全性？\n\n这篇文章，我们一起探讨一下 JWT 身份认证的优缺点以及常见问题的解决办法。\n\n## [#](#jwt-的优势) JWT 的优势\n\n相比于 Session 认证的方式来说，使用 JWT 进行身份认证主要有下面 4 个优势。\n\n### [#](#无状态) 无状态\n\nJWT 自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储 Session 信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。\n\n不过，也正是由于 JWT 的无状态，也导致了它最大的缺点：**不可控！**\n\n就比如说，我们想要在 JWT 有效期内废弃一个 JWT 或者更改它的权限的话，并不会立即生效，通常需要等到有效期过后才可以。再比如说，当用户 Logout 的话，JWT 也还有效。除非，我们在后端增加额外的处理逻辑比如将失效的 JWT 存储起来，后端先验证 JWT 是否有效再进行处理。具体的解决办法，我们会在后面的内容中详细介绍到，这里只是简单提一下。\n\n### [#](#有效避免了-csrf-攻击) 有效避免了 CSRF 攻击\n\n**CSRF（Cross Site Request Forgery）** 一般被翻译为 **跨站请求伪造**，属于网络攻击领域范围。相比于 SQL 脚本注入、XSS 等安全攻击方式，CSRF 的知名度并没有它们高。但是，它的确是我们开发系统时必须要考虑的安全隐患。就连业内技术标杆 Google 的产品 Gmail 也曾在 2007 年的时候爆出过 CSRF 漏洞，这给 Gmail 的用户造成了很大的损失。\n\n**那么究竟什么是跨站请求伪造呢？** 简单来说就是用你的身份去做一些不好的事情（发送一些对你不友好的请求比如恶意转账）。\n\n举个简单的例子：小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了 10000 元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求，也就是通过你的 Cookie 向银行发出请求。\n\n```\n\u003ca src=\"http://www.mybank.com/Transfer?bankId=11\u0026money=10000\"\n  \u003e科学理财，年盈利率过万\u003c/a\n\u003e\n```\n\nCSRF 攻击需要依赖 Cookie ，Session 认证中 Cookie 中的 `SessionID` 是由浏览器发送到服务端的，只要发出请求，Cookie 就会被携带。借助这个特性，即使黑客无法获取你的 `SessionID`，只要让你误点攻击链接，就可以达到攻击效果。\n\n另外，并不是必须点击链接才可以达到攻击效果，很多时候，只要你打开了某个页面，CSRF 攻击就会发生。\n\n```\n\u003cimg src=\"http://www.mybank.com/Transfer?bankId=11\u0026money=10000\" /\u003e\n```\n\n**那为什么 JWT 不会存在这种问题呢？**\n\n一般情况下我们使用 JWT 的话，在我们登录成功获得 JWT 之后，一般会选择存放在 localStorage 中。前端的每一个请求后续都会附带上这个 JWT，整个过程压根不会涉及到 Cookie。因此，即使你点击了非法链接发送了请求到服务端，这个非法请求也是不会携带 JWT 的，所以这个请求将是非法的。\n\n总结来说就一句话：**使用 JWT 进行身份验证不需要依赖 Cookie ，因此可以避免 CSRF 攻击。**\n\n不过，这样也会存在 XSS 攻击的风险。为了避免 XSS 攻击，你可以选择将 JWT 存储在标记为`httpOnly` 的 Cookie 中。但是，这样又导致了你必须自己提供 CSRF 保护，因此，实际项目中我们通常也不会这么做。\n\n常见的避免 XSS 攻击的方式是过滤掉请求中存在 XSS 攻击风险的可疑字符串。\n\n在 Spring 项目中，我们一般是通过创建 XSS 过滤器来实现的。\n\n```\n@Component\n@Order(Ordered.HIGHEST_PRECEDENCE)\npublic class XSSFilter implements Filter {\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response,\n      FilterChain chain) throws IOException, ServletException {\n        XSSRequestWrapper wrappedRequest =\n          new XSSRequestWrapper((HttpServletRequest) request);\n        chain.doFilter(wrappedRequest, response);\n    }\n\n    // other methods\n}\n```\n\n### [#](#适合移动端应用) 适合移动端应用\n\n使用 Session 进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到 Cookie（需要 Cookie 保存 `SessionId`），所以不适合移动端。\n\n但是，使用 JWT 进行身份认证就不会存在这种问题，因为只要 JWT 可以被客户端存储就能够使用，而且 JWT 还可以跨语言使用。\n\n### [#](#单点登录友好) 单点登录友好\n\n使用 Session 进行身份认证的话，实现单点登录，需要我们把用户的 Session 信息保存在一台电脑上，并且还会遇到常见的 Cookie 跨域的问题。但是，使用 JWT 进行认证的话， JWT 被保存在客户端，不会存在这些问题。\n\n## [#](#jwt-身份认证常见问题及解决办法) JWT 身份认证常见问题及解决办法\n\n### [#](#注销登录等场景下-jwt-还有效) 注销登录等场景下 JWT 还有效\n\n与之类似的具体相关场景有：\n\n- 退出登录;\n- 修改密码;\n- 服务端修改了某个用户具有的权限或者角色；\n- 用户的帐户被封禁/删除；\n- 用户被服务端强制注销；\n- 用户被踢下线；\n- ......\n\n这个问题不存在于 Session 认证方式中，因为在 Session 认证方式中，遇到这种情况的话服务端删除对应的 Session 记录即可。但是，使用 JWT 认证的方式就不好解决了。我们也说过了，JWT 一旦派发出去，如果后端不增加其他逻辑的话，它在失效之前都是有效的。\n\n那我们如何解决这个问题呢？查阅了很多资料，我简单总结了下面 4 种方案：\n\n**1、将 JWT 存入内存数据库**\n\n将 JWT 存入 DB 中，Redis 内存数据库在这里是不错的选择。如果需要让某个 JWT 失效就直接从 Redis 中删除这个 JWT 即可。但是，这样会导致每次使用 JWT 发送请求都要先从 DB 中查询 JWT 是否存在的步骤，而且违背了 JWT 的无状态原则。\n\n**2、黑名单机制**\n\n和上面的方式类似，使用内存数据库比如 Redis 维护一个黑名单，如果想让某个 JWT 失效的话就直接将这个 JWT 加入到 **黑名单** 即可。然后，每次使用 JWT 进行请求的话都会先判断这个 JWT 是否存在于黑名单中。\n\n前两种方案的核心在于将有效的 JWT 存储起来或者将指定的 JWT 拉入黑名单。\n\n虽然这两种方案都违背了 JWT 的无状态原则，但是一般实际项目中我们通常还是会使用这两种方案。\n\n**3、修改密钥 (Secret)** :\n\n我们为每个用户都创建一个专属密钥，如果我们想让某个 JWT 失效，我们直接修改对应用户的密钥即可。但是，这样相比于前两种引入内存数据库带来了危害更大：\n\n- 如果服务是分布式的，则每次发出新的 JWT 时都必须在多台机器同步密钥。为此，你需要将密钥存储在数据库或其他外部服务中，这样和 Session 认证就没太大区别了。\n- 如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方都要重新进行登录，这是不可取的。\n\n**4、保持令牌的有效期限短并经常轮换**\n\n很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户经常登录。\n\n另外，对于修改密码后 JWT 还有效问题的解决还是比较容易的。说一种我觉得比较好的方式：**使用用户的密码的哈希值对 JWT 进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证。**\n\n### [#](#jwt-的续签问题) JWT 的续签问题\n\nJWT 有效期一般都建议设置的不太长，那么 JWT 过期后如何认证，如何实现动态刷新 JWT，避免用户经常需要重新登录？\n\n我们先来看看在 Session 认证中一般的做法：**假如 Session 的有效期 30 分钟，如果 30 分钟内用户有访问，就把 Session 有效期延长 30 分钟。**\n\nJWT 认证的话，我们应该如何解决续签问题呢？查阅了很多资料，我简单总结了下面 4 种方案：\n\n**1、类似于 Session 认证中的做法**\n\n这种方案满足于大部分场景。假设服务端给的 JWT 有效期设置为 30 分钟，服务端每次进行校验时，如果发现 JWT 的有效期马上快过期了，服务端就重新生成 JWT 给客户端。客户端每次请求都检查新旧 JWT，如果不一致，则更新本地的 JWT。这种做法的问题是仅仅在快过期的时候请求才会更新 JWT ,对客户端不是很友好。\n\n**2、每次请求都返回新 JWT**\n\n这种方案的的思路很简单，但是，开销会比较大，尤其是在服务端要存储维护 JWT 的情况下。\n\n**3、JWT 有效期设置到半夜**\n\n这种方案是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。\n\n**4、用户登录返回两个 JWT**\n\n第一个是 accessJWT ，它的过期时间 JWT 本身的过期时间比如半个小时，另外一个是 refreshJWT 它的过期时间更长一点比如为 1 天。客户端登录后，将 accessJWT 和 refreshJWT 保存在本地，每次访问将 accessJWT 传给服务端。服务端校验 accessJWT 的有效性，如果过期的话，就将 refreshJWT 传给服务端。如果有效，服务端就生成新的 accessJWT 给客户端。否则，客户端就重新登录即可。\n\n这种方案的不足是：\n\n- 需要客户端来配合；\n- 用户注销的时候需要同时保证两个 JWT 都无效；\n- 重新请求获取 JWT 的过程中会有短暂 JWT 不可用的情况（可以通过在客户端设置定时器，当 accessJWT 快过期的时候，提前去通过 refreshJWT 获取新的 accessJWT）;\n- 存在安全问题，只要拿到了未过期的 refreshJWT 就一直可以获取到 accessJWT。\n\n## [#](#总结) 总结\n\nJWT 其中一个很重要的优势是无状态，但实际上，我们想要在实际项目中合理使用 JWT 的话，也还是需要保存 JWT 信息。\n\nJWT 也不是银弹，也有很多缺陷，具体是选择 JWT 还是 Session 方案还是要看项目的具体需求。万万不可尬吹 JWT，而看不起其他身份认证方案。\n\n另外，不用 JWT 直接使用普通的 Token(随机生成，不包含具体的信息) 结合 Redis 来做身份认证也是可以的。我在 [「优质开源项目推荐」open in new window](https://javaguide.cn/open-source-project/)的第 8 期推荐过的 [Sa-Tokenopen in new window](https://github.com/dromara/sa-JWT) 这个项目是一个比较完善的 基于 JWT 的身份认证解决方案，支持自动续签、踢人下线、账号封禁、同端互斥登录等功能，感兴趣的朋友可以看看。\n\n![](https://oss.javaguide.cn/javaguide/system-design/jwt/image-20220609170714725.png)\n\n## [#](#参考) 参考\n\n- JWT 超详细分析：[https://learnku.com/articles/17883open in new window](https://learnku.com/articles/17883)\n- How to log out when using JWT：[https://medium.com/devgorilla/how-to-log-out-when-using-jwt-a8c7823e8a6open in new window](https://medium.com/devgorilla/how-to-log-out-when-using-jwt-a8c7823e8a6)\n- CSRF protection with JSON Web JWTs：[https://medium.com/@agungsantoso/csrf-protection-with-json-web-JWTs-83e0f2fcbccopen in new window](https://medium.com/@agungsantoso/csrf-protection-with-json-web-JWTs-83e0f2fcbcc)\n- Invalidating JSON Web JWTs：[https://stackoverflow.com/questions/21978658/invalidating-json-web-JWTs](https://stackoverflow.com/questions/21978658/invalidating-json-web-JWTs)\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/advantages-and-disadvantages-of-jwt.html#%E6%80%BB%E7%BB%93","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/SSO-%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E8%AF%A6%E8%A7%A3":{"title":"SSO 单点登录详解","content":"## SO 介绍\n\n### [#](#什么是-sso) 什么是 SSO？\n\nSSO 英文全称 Single Sign On，单点登录。SSO 是在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统。\n\n例如你登录网易账号中心（https://reg.163.com/ ）之后访问以下站点都是登录状态。\n\n- 网易直播 [https://v.163.comopen in new window](https://v.163.com/)\n- 网易博客 [https://blog.163.comopen in new window](https://blog.163.com/)\n- 网易花田 [https://love.163.comopen in new window](https://love.163.com/)\n- 网易考拉 [https://www.kaola.comopen in new window](https://www.kaola.com/)\n- 网易 Lofter [http://www.lofter.comopen in new window](http://www.lofter.com/)\n\n### [#](#sso-有什么好处) SSO 有什么好处？\n\n1. **用户角度** :用户能够做到一次登录多次使用，无需记录多套用户名和密码，省心。\n2. **系统管理员角度** : 管理员只需维护好一个统一的账号中心就可以了，方便。\n3. **新系统开发角度:** 新系统开发时只需直接对接统一的账号中心即可，简化开发流程，省时。\n\n## [#](#sso-设计与实现) SSO 设计与实现\n\n本篇文章也主要是为了探讨如何设计\u0026实现一个 SSO 系统\n\n以下为需要实现的核心功能：\n\n- 单点登录\n- 单点登出\n- 支持跨域单点登录\n- 支持跨域单点登出\n\n### [#](#核心应用与依赖) 核心应用与依赖\n\n![单点登录（SSO）设计](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-system.png-kblb.png)\n\n单点登录（SSO）设计\n\n|应用/模块/对象|说明|\n|---|---|\n|前台站点|需要登录的站点|\n|SSO 站点-登录|提供登录的页面|\n|SSO 站点-登出|提供注销登录的入口|\n|SSO 服务-登录|提供登录服务|\n|SSO 服务-登录状态|提供登录状态校验/登录信息查询的服务|\n|SSO 服务-登出|提供用户注销登录的服务|\n|数据库|存储用户账户信息|\n|缓存|存储用户的登录信息，通常使用 Redis|\n\n### [#](#用户登录状态的存储与校验) 用户登录状态的存储与校验\n\n常见的 Web 框架对于 Session 的实现都是生成一个 SessionId 存储在浏览器 Cookie 中。然后将 Session 内容存储在服务器端内存中，这个 [ken.ioopen in new window](https://ken.io/) 在之前[Session 工作原理open in new window](https://ken.io/note/session-principle-skill)中也提到过。整体也是借鉴这个思路。\n\n用户登录成功之后，生成 AuthToken 交给客户端保存。如果是浏览器，就保存在 Cookie 中。如果是手机 App 就保存在 App 本地缓存中。本篇主要探讨基于 Web 站点的 SSO。\n\n用户在浏览需要登录的页面时，客户端将 AuthToken 提交给 SSO 服务校验登录状态/获取用户登录信息\n\n对于登录信息的存储，建议采用 Redis，使用 Redis 集群来存储登录信息，既可以保证高可用，又可以线性扩充。同时也可以让 SSO 服务满足负载均衡/可伸缩的需求。\n\n|对象|说明|\n|---|---|\n|AuthToken|直接使用 UUID/GUID 即可，如果有验证 AuthToken 合法性需求，可以将 UserName+时间戳加密生成，服务端解密之后验证合法性|\n|登录信息|通常是将 UserId，UserName 缓存起来|\n\n### [#](#用户登录-登录校验) 用户登录/登录校验\n\n**登录时序图**\n\n![SSO系统设计-登录时序图](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-login-sequence.png-kbrb.png)\n\nSSO系统设计-登录时序图\n\n按照上图，用户登录后 AuthToken 保存在 Cookie 中。 domain=test.com 浏览器会将 domain 设置成 .test.com，\n\n这样访问所有 *.test.com 的 web 站点，都会将 AuthToken 携带到服务器端。 然后通过 SSO 服务，完成对用户状态的校验/用户登录信息的获取\n\n**登录信息获取/登录状态校验**\n\n![SSO系统设计-登录信息获取/登录状态校验](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-logincheck-sequence.png-kbrb.png)\n\nSSO系统设计-登录信息获取/登录状态校验\n\n### [#](#用户登出) 用户登出\n\n用户登出时要做的事情很简单：\n\n1. 服务端清除缓存（Redis）中的登录状态\n2. 客户端清除存储的 AuthToken\n\n**登出时序图**\n\n![SSO系统设计-用户登出](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-logout-sequence.png-kbrb.png)\n\nSSO系统设计-用户登出\n\n### [#](#跨域登录、登出) 跨域登录、登出\n\n前面提到过，核心思路是客户端存储 AuthToken，服务器端通过 Redis 存储登录信息。由于客户端是将 AuthToken 存储在 Cookie 中的。所以跨域要解决的问题，就是如何解决 Cookie 的跨域读写问题。\n\n解决跨域的核心思路就是：\n\n- 登录完成之后通过回调的方式，将 AuthToken 传递给主域名之外的站点，该站点自行将 AuthToken 保存在当前域下的 Cookie 中。\n- 登出完成之后通过回调的方式，调用非主域名站点的登出页面，完成设置 Cookie 中的 AuthToken 过期的操作。\n\n**跨域登录（主域名已登录）**\n\n![SSO系统设计-跨域登录（主域名已登录）](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-crossdomain-login-loggedin-sequence.png-kbrb.png)\n\nSSO系统设计-跨域登录（主域名已登录）\n\n**跨域登录（主域名未登录）**\n\n![SSO系统设计-跨域登录（主域名未登录）](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-crossdomain-login-unlogin-sequence.png-kbrb.png)\n\nSSO系统设计-跨域登录（主域名未登录）\n\n**跨域登出**\n\n![SSO系统设计-跨域登出](https://oss.javaguide.cn/github/javaguide/system-design/security/sso/sso-crossdomain-logout-sequence.png-kbrb.png)\n\nSSO系统设计-跨域登出\n\n## [#](#说明) 说明\n\n- 关于方案：这次设计方案更多是提供实现思路。如果涉及到 APP 用户登录等情况，在访问 SSO 服务时，增加对 APP 的签名验证就好了。当然，如果有无线网关，验证签名不是问题。\n- 关于时序图：时序图中并没有包含所有场景，只列举了核心/主要场景，另外对于一些不影响理解思路的消息能省就省了。\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/security/sso-intro.html#%E8%B7%A8%E5%9F%9F%E7%99%BB%E5%BD%95%E3%80%81%E7%99%BB%E5%87%BA","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/Java-%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E8%AF%A6%E8%A7%A3":{"title":"Java 定时任务详解","content":"## 为什么需要定时任务？\n\n我们来看一下几个非常常见的业务场景：\n\n1. 某系统凌晨要进行数据备份。\n2. 某电商平台，用户下单半个小时未支付的情况下需要自动取消订单。\n3. 某媒体聚合平台，每 10 分钟动态抓取某某网站的数据为自己所用。\n4. 某博客平台，支持定时发送文章。\n5. 某基金平台，每晚定时计算用户当日收益情况并推送给用户最新的数据。\n6. ......\n\n这些场景往往都要求我们在某个特定的时间去做某个事情。\n\n## [#](#单机定时任务技术选型) 单机定时任务技术选型\n\n### [#](#timer) Timer\n\n`java.util.Timer`是 JDK 1.3 开始就已经支持的一种定时任务的实现方式。\n\n`Timer` 内部使用一个叫做 `TaskQueue` 的类存放定时任务，它是一个基于最小堆实现的优先级队列。`TaskQueue` 会按照任务距离下一次执行时间的大小将任务排序，保证在堆顶的任务最先执行。这样在需要执行任务时，每次只需要取出堆顶的任务运行即可！\n\n`Timer` 使用起来比较简单，通过下面的方式我们就能创建一个 1s 之后执行的定时任务。\n\n```\n// 示例代码：\nTimerTask task = new TimerTask() {\n    public void run() {\n        System.out.println(\"当前时间: \" + new Date() + \"n\" +\n                \"线程名称: \" + Thread.currentThread().getName());\n    }\n};\nSystem.out.println(\"当前时间: \" + new Date() + \"n\" +\n        \"线程名称: \" + Thread.currentThread().getName());\nTimer timer = new Timer(\"Timer\");\nlong delay = 1000L;\ntimer.schedule(task, delay);\n\n\n//输出：\n当前时间: Fri May 28 15:18:47 CST 2021n线程名称: main\n当前时间: Fri May 28 15:18:48 CST 2021n线程名称: Timer\n```\n\n不过其缺陷较多，比如一个 `Timer` 一个线程，这就导致 `Timer` 的任务的执行只能串行执行，一个任务执行时间过长的话会影响其他任务（性能非常差），再比如发生异常时任务直接停止（`Timer` 只捕获了 `InterruptedException` ）。\n\n`Timer` 类上的有一段注释是这样写的：\n\n```\n * This class does not offer real-time guarantees: it schedules\n * tasks using the \u003ctt\u003eObject.wait(long)\u003c/tt\u003e method.\n *Java 5.0 introduced the {@code java.util.concurrent} package and\n * one of the concurrency utilities therein is the {@link\n * java.util.concurrent.ScheduledThreadPoolExecutor\n * ScheduledThreadPoolExecutor} which is a thread pool for repeatedly\n * executing tasks at a given rate or delay.  It is effectively a more\n * versatile replacement for the {@code Timer}/{@code TimerTask}\n * combination, as it allows multiple service threads, accepts various\n * time units, and doesn't require subclassing {@code TimerTask} (just\n * implement {@code Runnable}).  Configuring {@code\n * ScheduledThreadPoolExecutor} with one thread makes it equivalent to\n * {@code Timer}.\n```\n\n大概的意思就是：`ScheduledThreadPoolExecutor` 支持多线程执行定时任务并且功能更强大，是 `Timer` 的替代品。\n\n### [#](#scheduledexecutorservice) ScheduledExecutorService\n\n`ScheduledExecutorService` 是一个接口，有多个实现类，比较常用的是 `ScheduledThreadPoolExecutor` 。\n\n![](https://oss.javaguide.cn/javaguide/20210607154324712.png)\n\n`ScheduledThreadPoolExecutor` 本身就是一个线程池，支持任务并发执行。并且，其内部使用 `DelayedWorkQueue` 作为任务队列。\n\n```\n// 示例代码：\nTimerTask repeatedTask = new TimerTask() {\n    @SneakyThrows\n    public void run() {\n        System.out.println(\"当前时间: \" + new Date() + \"n\" +\n                \"线程名称: \" + Thread.currentThread().getName());\n    }\n};\nSystem.out.println(\"当前时间: \" + new Date() + \"n\" +\n        \"线程名称: \" + Thread.currentThread().getName());\nScheduledExecutorService executor = Executors.newScheduledThreadPool(3);\nlong delay  = 1000L;\nlong period = 1000L;\nexecutor.scheduleAtFixedRate(repeatedTask, delay, period, TimeUnit.MILLISECONDS);\nThread.sleep(delay + period * 5);\nexecutor.shutdown();\n//输出：\n当前时间: Fri May 28 15:40:46 CST 2021n线程名称: main\n当前时间: Fri May 28 15:40:47 CST 2021n线程名称: pool-1-thread-1\n当前时间: Fri May 28 15:40:48 CST 2021n线程名称: pool-1-thread-1\n当前时间: Fri May 28 15:40:49 CST 2021n线程名称: pool-1-thread-2\n当前时间: Fri May 28 15:40:50 CST 2021n线程名称: pool-1-thread-2\n当前时间: Fri May 28 15:40:51 CST 2021n线程名称: pool-1-thread-2\n当前时间: Fri May 28 15:40:52 CST 2021n线程名称: pool-1-thread-2\n```\n\n不论是使用 `Timer` 还是 `ScheduledExecutorService` 都无法使用 Cron 表达式指定任务执行的具体时间。\n\n### [#](#spring-task) Spring Task\n\n我们直接通过 Spring 提供的 `@Scheduled` 注解即可定义定时任务，非常方便！\n\n```\n/**\n * cron：使用Cron表达式。　每分钟的1，2秒运行\n */\n@Scheduled(cron = \"1-2 * * * * ? \")\npublic void reportCurrentTimeWithCronExpression() {\n  log.info(\"Cron Expression: The time is now {}\", dateFormat.format(new Date()));\n}\n\n```\n\n我在大学那会做的一个 SSM 的企业级项目，就是用的 Spring Task 来做的定时任务。\n\n并且，Spring Task 还是支持 **Cron 表达式** 的。Cron 表达式主要用于定时作业(定时任务)系统定义执行时间或执行频率的表达式，非常厉害，你可以通过 Cron 表达式进行设置定时任务每天或者每个月什么时候执行等等操作。咱们要学习定时任务的话，Cron 表达式是一定是要重点关注的。推荐一个在线 Cron 表达式生成器：[http://cron.qqe2.com/open in new window](http://cron.qqe2.com/) 。\n\n但是，Spring 自带的定时调度只支持单机，并且提供的功能比较单一。之前写过一篇文章:[《5 分钟搞懂如何在 Spring Boot 中 Schedule Tasks》open in new window](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==\u0026mid=2247485563\u0026idx=1\u0026sn=7419341f04036a10b141b74624a3f8c9\u0026chksm=cea247b0f9d5cea6440759e6d49b4e77d06f4c99470243a10c1463834e873ca90266413fbc92\u0026token=2133161636\u0026lang=zh_CN#rd) ，不了解的小伙伴可以参考一下。\n\nSpring Task 底层是基于 JDK 的 `ScheduledThreadPoolExecutor` 线程池来实现的。\n\n**优缺点总结：**\n\n- 优点：简单，轻量，支持 Cron 表达式\n- 缺点：功能单一\n\n### [#](#时间轮) 时间轮\n\nKafka、Dubbo、ZooKeeper、Netty、Caffeine、Akka 中都有对时间轮的实现。\n\n时间轮简单来说就是一个环形的队列（底层一般基于数组实现），队列中的每一个元素（时间格）都可以存放一个定时任务列表。\n\n时间轮中的每个时间格代表了时间轮的基本时间跨度或者说时间精度，假如时间一秒走一个时间格的话，那么这个时间轮的最高精度就是 1 秒（也就是说 3 s 和 3.9s 会在同一个时间格中）。\n\n下图是一个有 12 个时间格的时间轮，转完一圈需要 12 s。当我们需要新建一个 3s 后执行的定时任务，只需要将定时任务放在下标为 3 的时间格中即可。当我们需要新建一个 9s 后执行的定时任务，只需要将定时任务放在下标为 9 的时间格中即可。\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/one-layers-of-time-wheel.png)\n\n那当我们需要创建一个 13s 后执行的定时任务怎么办呢？这个时候可以引入一叫做 **圈数/轮数** 的概念，也就是说这个任务还是放在下标为 3 的时间格中， 不过它的圈数为 2 。\n\n除了增加圈数这种方法之外，还有一种 **多层次时间轮** （类似手表），Kafka 采用的就是这种方案。\n\n针对下图的时间轮，我来举一个例子便于大家理解。\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/three-layers-of-time-wheel.png)\n\n上图的时间轮，第 1 层的时间精度为 1 ，第 2 层的时间精度为 20 ，第 3 层的时间精度为 400。假如我们需要添加一个 350s 后执行的任务 A 的话（当前时间是 0s），这个任务会被放在第 2 层（因为第二层的时间跨度为 20*20=400\u003e350）的第 350/20=17 个时间格子。\n\n当第一层转了 17 圈之后，时间过去了 340s ，第 2 层的指针此时来到第 17 个时间格子。此时，第 2 层第 17 个格子的任务会被移动到第 1 层。\n\n任务 A 当前是 10s 之后执行，因此它会被移动到第 1 层的第 10 个时间格子。\n\n这里在层与层之间的移动也叫做时间轮的升降级。参考手表来理解就好！\n\n**时间轮比较适合任务数量比较多的定时任务场景，它的任务写入和执行的时间复杂度都是 0（1）。**\n\n## [#](#分布式定时任务技术选型) 分布式定时任务技术选型\n\n上面提到的一些定时任务的解决方案都是在单机下执行的，适用于比较简单的定时任务场景比如每天凌晨备份一次数据。\n\n如果我们需要一些高级特性比如支持任务在分布式场景下的分片和高可用的话，我们就需要用到分布式任务调度框架了。\n\n通常情况下，一个定时任务的执行往往涉及到下面这些角色：\n\n- **任务**：首先肯定是要执行的任务，这个任务就是具体的业务逻辑比如定时发送文章。\n- **调度器**：其次是调度中心，调度中心主要负责任务管理，会分配任务给执行器。\n- **执行器**：最后就是执行器，执行器接收调度器分派的任务并执行。\n\n### [#](#quartz) Quartz\n\n一个很火的开源任务调度框架，完全由Java写成。Quartz 可以说是 Java 定时任务领域的老大哥或者说参考标准，其他的任务调度框架基本都是基于 Quartz 开发的，比如当当网的`elastic-job`就是基于Quartz二次开发之后的分布式调度解决方案。\n\n使用 Quartz 可以很方便地与 Spring集成，并且支持动态添加任务和集群。但是，Quartz 使用起来也比较麻烦，API 繁琐。\n\n并且，Quartz 并没有内置 UI 管理控制台，不过你可以使用 [quartzuiopen in new window](https://github.com/zhaopeiym/quartzui) 这个开源项目来解决这个问题。\n\n另外，Quartz 虽然也支持分布式任务。但是，它是在数据库层面，通过数据库的锁机制做的，有非常多的弊端比如系统侵入性严重、节点负载不均衡。有点伪分布式的味道。\n\n**优缺点总结：**\n\n- 优点：可以与 Spring集成，并且支持动态添加任务和集群。\n- 缺点：分布式支持不友好，没有内置 UI 管理控制台、使用麻烦（相比于其他同类型框架来说）\n\n### [#](#elastic-job) Elastic-Job\n\nElasticJob 当当网开源的一个面向互联网生态和海量任务的分布式调度解决方案，由两个相互独立的子项目 ElasticJob-Lite 和 ElasticJob-Cloud 组成。\n\nElasticJob-Lite 和 ElasticJob-Cloud 两者的对比如下：\n\n|ElasticJob-Lite|ElasticJob-Cloud||\n|---|---|---|\n|无中心化|是|否|\n|资源分配|不支持|支持|\n|作业模式|常驻|常驻 + 瞬时|\n|部署依赖|ZooKeeper|ZooKeeper + Mesos|\n\n`ElasticJob` 支持任务在分布式场景下的分片和高可用、任务可视化管理等功能。\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/elasticjob-feature-list.png)\n\nElasticJob-Lite 的架构设计如下图所示：\n\n![ElasticJob-Lite 的架构设计](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/elasticjob-lite-architecture-design.png)\n\nElasticJob-Lite 的架构设计\n\n从上图可以看出，Elastic-Job没有调度中心这一概念，而是使用 ZooKeeper 作为注册中心，注册中心负责协调分配任务到不同的节点上。\n\nElastic-Job 中的定时调度都是由执行器自行触发，这种设计也被称为去中心化设计（调度和处理都是执行器单独完成）。\n\n```\n@Component\n@ElasticJobConf(name = \"dayJob\", cron = \"0/10 * * * * ?\", shardingTotalCount = 2,\n        shardingItemParameters = \"0=AAAA,1=BBBB\", description = \"简单任务\", failover = true)\npublic class TestJob implements SimpleJob {\n    @Override\n    public void execute(ShardingContext shardingContext) {\n        log.info(\"TestJob任务名：【{}】, 片数：【{}】, param=【{}】\", shardingContext.getJobName(), shardingContext.getShardingTotalCount(),\n                shardingContext.getShardingParameter());\n    }\n}\n```\n\n**相关地址：**\n\n- GitHub 地址：[https://github.com/apache/shardingsphere-elasticjob。open in new window](https://github.com/apache/shardingsphere-elasticjob%E3%80%82)\n- 官方网站：[https://shardingsphere.apache.org/elasticjob/index_zh.htmlopen in new window](https://shardingsphere.apache.org/elasticjob/index_zh.html) 。\n\n**优缺点总结：**\n\n- 优点：可以与 Spring集成、支持分布式、支持集群、性能不错\n- 缺点：依赖了额外的中间件比如 Zookeeper（复杂度增加，可靠性降低、维护成本变高）\n\n### [#](#xxl-job) XXL-JOB\n\n`XXL-JOB` 于 2015 年开源，是一款优秀的轻量级分布式任务调度框架，支持任务可视化管理、弹性扩容缩容、任务失败重试和告警、任务分片等功能，\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/xxljob-feature-list.png)\n\n根据 `XXL-JOB` 官网介绍，其解决了很多 Quartz 的不足。\n\n\u003e Quartz作为开源作业调度中的佼佼者，是作业调度的首选。但是集群环境中Quartz采用API的方式对任务进行管理，从而可以避免上述问题，但是同样存在以下问题：\n\u003e \n\u003e - 问题一：调用API的的方式操作任务，不人性化；\n\u003e - 问题二：需要持久化业务QuartzJobBean到底层数据表中，系统侵入性相当严重。\n\u003e - 问题三：调度逻辑和QuartzJobBean耦合在同一个项目中，这将导致一个问题，在调度任务数量逐渐增多，同时调度任务逻辑逐渐加重的情况下，此时调度系统的性能将大大受限于业务；\n\u003e - 问题四：quartz底层以“抢占式”获取DB锁并由抢占成功节点负责运行任务，会导致节点负载悬殊非常大；而XXL-JOB通过执行器实现“协同分配式”运行任务，充分发挥集群优势，负载各节点均衡。\n\u003e \n\u003e XXL-JOB弥补了quartz的上述不足之处。\n\n`XXL-JOB` 的架构设计如下图所示：\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/xxljob-architecture-design-v2.1.0.png)\n\n从上图可以看出，`XXL-JOB` 由 **调度中心** 和 **执行器** 两大部分组成。调度中心主要负责任务管理、执行器管理以及日志管理。执行器主要是接收调度信号并处理。另外，调度中心进行任务调度时，是通过自研 RPC 来实现的。\n\n不同于 Elastic-Job的去中心化设计， `XXL-JOB` 的这种设计也被称为中心化设计（调度中心调度多个执行器执行任务）。\n\n和 `Quzrtz` 类似 `XXL-JOB` 也是基于数据库锁调度任务，存在性能瓶颈。不过，一般在任务量不是特别大的情况下，没有什么影响的，可以满足绝大部分公司的要求。\n\n不要被 `XXL-JOB` 的架构图给吓着了，实际上，我们要用 `XXL-JOB` 的话，只需要重写 `IJobHandler` 自定义任务执行逻辑就可以了，非常易用！\n\n```\n@JobHandler(value=\"myApiJobHandler\")\n@Component\npublic class MyApiJobHandler extends IJobHandler {\n\n    @Override\n    public ReturnT\u003cString\u003e execute(String param) throws Exception {\n        //......\n        return ReturnT.SUCCESS;\n    }\n}\n```\n\n还可以直接基于注解定义任务。\n\n```\n@XxlJob(\"myAnnotationJobHandler\")\npublic ReturnT\u003cString\u003e myAnnotationJobHandler(String param) throws Exception {\n  //......\n  return ReturnT.SUCCESS;\n}\n```\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/xxljob-admin-task-management.png)\n\n**相关地址：**\n\n- GitHub 地址：[https://github.com/xuxueli/xxl-job/。open in new window](https://github.com/xuxueli/xxl-job/%E3%80%82)\n- 官方介绍：[https://www.xuxueli.com/xxl-job/open in new window](https://www.xuxueli.com/xxl-job/) 。\n\n**优缺点总结：**\n\n- 优点：开箱即用（学习成本比较低）、与 Spring 集成、支持分布式、支持集群、内置了 UI 管理控制台。\n- 缺点：不支持动态添加任务（如果一定想要动态创建任务也是支持的，参见：[xxl-job issue277open in new window](https://github.com/xuxueli/xxl-job/issues/277)）。\n\n### [#](#powerjob) PowerJob\n\n非常值得关注的一个分布式任务调度框架，分布式任务调度领域的新星。目前，已经有很多公司接入比如 OPPO、京东、中通、思科。\n\n这个框架的诞生也挺有意思的，PowerJob 的作者当时在阿里巴巴实习过，阿里巴巴那会使用的是内部自研的 SchedulerX（阿里云付费产品）。实习期满之后，PowerJob 的作者离开了阿里巴巴。想着说自研一个 SchedulerX，防止哪天 SchedulerX 满足不了需求，于是 PowerJob 就诞生了。\n\n更多关于 PowerJob 的故事，小伙伴们可以去看看 PowerJob 作者的视频 [《我和我的任务调度中间件》open in new window](https://www.bilibili.com/video/BV1SK411A7F3/)。简单点概括就是：“游戏没啥意思了，我要扛起了新一代分布式任务调度与计算框架的大旗！”。\n\n由于 SchedulerX 属于人民币产品，我这里就不过多介绍。PowerJob 官方也对比过其和 QuartZ、XXL-JOB 以及 SchedulerX。\n\n![](https://oss.javaguide.cn/github/javaguide/system-design/schedule-task/quartz-xxljob-schedulerx2.0-powerjob-comparison.png)\n\n## [#](#总结) 总结\n\n这篇文章中，我主要介绍了：\n\n- **定时任务的相关概念**：为什么需要定时任务、定时任务中的核心角色、分布式定时任务。\n- **定时任务的技术选型**：XXL-JOB 2015 年推出，已经经过了很多年的考验。XXL-JOB 轻量级，并且使用起来非常简单。虽然存在性能瓶颈，但是，在绝大多数情况下，对于企业的基本需求来说是没有影响的。PowerJob 属于分布式任务调度领域里的新星，其稳定性还有待继续考察。ElasticJob 由于在架构设计上是基于 Zookeeper ，而 XXL-JOB 是基于数据库，性能方面的话，ElasticJob 略胜一筹。\n\n这篇文章并没有介绍到实际使用，但是，并不代表实际使用不重要。我在写这篇文章之前，已经动手写过相应的 Demo。像 Quartz，我在大学那会就用过。不过，当时用的是 Spring 。为了能够更好地体验，我自己又在 Spring Boot 上实际体验了一下。如果你并没有实际使用某个框架，就直接说它并不好用的话，是站不住脚的。\n\n最后，这篇文章要感谢艿艿的帮助，写这篇文章的时候向艿艿询问过一些问题。推荐一篇艿艿写的偏实战类型的硬核文章：[《Spring Job？Quartz？XXL-Job？年轻人才做选择，艿艿全莽~》open in new window](https://mp.weixin.qq.com/s?__biz=MzUzMTA2NTU2Ng==\u0026mid=2247490679\u0026idx=1\u0026sn=25374dbdcca95311d41be5d7b7db454d\u0026chksm=fa4963c6cd3eead055bb9cd10cca13224bb35d0f7373a27aa22a55495f71e24b8273a7603314\u0026scene=27#wechat_redirect)\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/schedule-task.html#powerjob","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E6%B6%88%E6%81%AF%E9%80%9A%E7%9F%A5/Web-%E5%AE%9E%E6%97%B6%E6%B6%88%E6%81%AF%E6%8E%A8%E9%80%81%E8%AF%A6%E8%A7%A3":{"title":"Web 实时消息推送详解","content":"\u003e 原文地址：[https://juejin.cn/post/7122014462181113887，JavaGuideopen in new window](https://juejin.cn/post/7122014462181113887%EF%BC%8CJavaGuide) 对本文进行了完善总结。\n\n我有一个朋友做了一个小破站，现在要实现一个站内信 Web 消息推送的功能，对，就是下图这个小红点，一个很常用的功能。\n\n![站内信 Web 消息推送](statistic/站内信_Web_消息推送.png)\n\n站内信 Web 消息推送\n\n不过他还没想好用什么方式做，这里我帮他整理了一下几种方案，并简单做了实现。\n\n## [#](#什么是消息推送) 什么是消息推送？\n\n推送的场景比较多，比如有人关注我的公众号，这时我就会收到一条推送消息，以此来吸引我点击打开应用。\n\n消息推送通常是指网站的运营工作等人员，通过某种工具对用户当前网页或移动设备 APP 进行的主动消息推送。\n\n消息推送一般又分为 Web 端消息推送和移动端消息推送。\n\n移动端消息推送示例：\n\n![移动端消息推送示例](statistic/移动端消息推送示例.png)\n\n移动端消息推送示例\n\nWeb 端消息推送示例：\n\n![Web 端消息推送示例](statistic/Web_端消息推送示例.png)\n\nWeb 端消息推送示例\n\n在具体实现之前，咱们再来分析一下前边的需求，其实功能很简单，只要触发某个事件（主动分享了资源或者后台主动推送消息），Web 页面的通知小红点就会实时的 `+1` 就可以了。\n\n通常在服务端会有若干张消息推送表，用来记录用户触发不同事件所推送不同类型的消息，前端主动查询（拉）或者被动接收（推）用户所有未读的消息数。\n\n![消息推送表](statistic/消息推送表.png)\n\n消息推送表\n\n消息推送无非是推（push）和拉（pull）两种形式，下边我们逐个了解下。\n\n## [#](#消息推送常见方案) 消息推送常见方案\n\n### [#](#短轮询) 短轮询\n\n**轮询(polling)** 应该是实现消息推送方案中最简单的一种，这里我们暂且将轮询分为短轮询和长轮询。\n\n短轮询很好理解，指定的时间间隔，由浏览器向服务器发出 HTTP 请求，服务器实时返回未读消息数据给客户端，浏览器再做渲染显示。\n\n一个简单的 JS 定时器就可以搞定，每秒钟请求一次未读消息数接口，返回的数据展示即可。\n\n```\nsetInterval(() =\u003e {\n  // 方法请求\n  messageCount().then((res) =\u003e {\n    if (res.code === 200) {\n      this.messageCount = res.data;\n    }\n  });\n}, 1000);\n```\n\n效果还是可以的，短轮询实现固然简单，缺点也是显而易见，由于推送数据并不会频繁变更，无论后端此时是否有新的消息产生，客户端都会进行请求，势必会对服务端造成很大压力，浪费带宽和服务器资源。\n\n### [#](#长轮询) 长轮询\n\n长轮询是对上边短轮询的一种改进版本，在尽可能减少对服务器资源浪费的同时，保证消息的相对实时性。长轮询在中间件中应用的很广泛，比如 Nacos 和 Apollo 配置中心，消息队列 Kafka、RocketMQ 中都有用到长轮询。\n\n[Nacos 配置中心交互模型是 push 还是 pull？open in new window](https://mp.weixin.qq.com/s/94ftESkDoZI9gAGflLiGwg)一文中我详细介绍过 Nacos 长轮询的实现原理，感兴趣的小伙伴可以瞅瞅。\n\n长轮询其实原理跟轮询差不多，都是采用轮询的方式。不过，如果服务端的数据没有发生变更，会 一直 hold 住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。\n\n这次我使用 Apollo 配置中心实现长轮询的方式，应用了一个类`DeferredResult`，它是在 Servlet3.0 后经过 Spring 封装提供的一种异步请求机制，直意就是延迟结果。\n\n![长轮询示意图](statistic/长轮询示意图.png)\n\n长轮询示意图\n\n`DeferredResult`可以允许容器线程快速释放占用的资源，不阻塞请求线程，以此接受更多的请求提升系统的吞吐量，然后启动异步工作线程处理真正的业务逻辑，处理完成调用`DeferredResult.setResult(200)`提交响应结果。\n\n下边我们用长轮询来实现消息推送。\n\n因为一个 ID 可能会被多个长轮询请求监听，所以我采用了 Guava 包提供的`Multimap`结构存放长轮询，一个 key 可以对应多个 value。一旦监听到 key 发生变化，对应的所有长轮询都会响应。前端得到非请求超时的状态码，知晓数据变更，主动查询未读消息数接口，更新页面数据。\n\n```\n@Controller\n@RequestMapping(\"/polling\")\npublic class PollingController {\n\n    // 存放监听某个Id的长轮询集合\n    // 线程同步结构\n    public static Multimap\u003cString, DeferredResult\u003cString\u003e\u003e watchRequests = Multimaps.synchronizedMultimap(HashMultimap.create());\n\n    /**\n     * 设置监听\n     */\n    @GetMapping(path = \"watch/{id}\")\n    @ResponseBody\n    public DeferredResult\u003cString\u003e watch(@PathVariable String id) {\n        // 延迟对象设置超时时间\n        DeferredResult\u003cString\u003e deferredResult = new DeferredResult\u003c\u003e(TIME_OUT);\n        // 异步请求完成时移除 key，防止内存溢出\n        deferredResult.onCompletion(() -\u003e {\n            watchRequests.remove(id, deferredResult);\n        });\n        // 注册长轮询请求\n        watchRequests.put(id, deferredResult);\n        return deferredResult;\n    }\n\n    /**\n     * 变更数据\n     */\n    @GetMapping(path = \"publish/{id}\")\n    @ResponseBody\n    public String publish(@PathVariable String id) {\n        // 数据变更 取出监听ID的所有长轮询请求，并一一响应处理\n        if (watchRequests.containsKey(id)) {\n            Collection\u003cDeferredResult\u003cString\u003e\u003e deferredResults = watchRequests.get(id);\n            for (DeferredResult\u003cString\u003e deferredResult : deferredResults) {\n                deferredResult.setResult(\"我更新了\" + new Date());\n            }\n        }\n        return \"success\";\n    }\n```\n\n当请求超过设置的超时时间，会抛出`AsyncRequestTimeoutException`异常，这里直接用`@ControllerAdvice`全局捕获统一返回即可，前端获取约定好的状态码后再次发起长轮询请求，如此往复调用。\n\n```\n@ControllerAdvice\npublic class AsyncRequestTimeoutHandler {\n\n    @ResponseStatus(HttpStatus.NOT_MODIFIED)\n    @ResponseBody\n    @ExceptionHandler(AsyncRequestTimeoutException.class)\n    public String asyncRequestTimeoutHandler(AsyncRequestTimeoutException e) {\n        System.out.println(\"异步请求超时\");\n        return \"304\";\n    }\n}\n```\n\n我们来测试一下，首先页面发起长轮询请求`/polling/watch/10086`监听消息更变，请求被挂起，不变更数据直至超时，再次发起了长轮询请求；紧接着手动变更数据`/polling/publish/10086`，长轮询得到响应，前端处理业务逻辑完成后再次发起请求，如此循环往复。\n\n长轮询相比于短轮询在性能上提升了很多，但依然会产生较多的请求，这是它的一点不完美的地方。\n\n### [#](#iframe-流) iframe 流\n\niframe 流就是在页面中插入一个隐藏的`\u003ciframe\u003e`标签，通过在`src`中请求消息数量 API 接口，由此在服务端和客户端之间创建一条长连接，服务端持续向`iframe`传输数据。\n\n传输的数据通常是 HTML、或是内嵌的 JavaScript 脚本，来达到实时更新页面的效果。\n\n![iframe 流示意图](statistic/iframe_流示意图.png)\n\niframe 流示意图\n\n这种方式实现简单，前端只要一个`\u003ciframe\u003e`标签搞定了\n\n```\n\u003ciframe src=\"/iframe/message\" style=\"display:none\"\u003e\u003c/iframe\u003e\n```\n\n服务端直接组装 HTML、JS 脚本数据向 response 写入就行了\n\n```\n@Controller\n@RequestMapping(\"/iframe\")\npublic class IframeController {\n    @GetMapping(path = \"message\")\n    public void message(HttpServletResponse response) throws IOException, InterruptedException {\n        while (true) {\n            response.setHeader(\"Pragma\", \"no-cache\");\n            response.setDateHeader(\"Expires\", 0);\n            response.setHeader(\"Cache-Control\", \"no-cache,no-store\");\n            response.setStatus(HttpServletResponse.SC_OK);\n            response.getWriter().print(\" \u003cscript type=\\\"text/javascript\\\"\u003e\\n\" +\n                    \"parent.document.getElementById('clock').innerHTML = \\\"\" + count.get() + \"\\\";\" +\n                    \"parent.document.getElementById('count').innerHTML = \\\"\" + count.get() + \"\\\";\" +\n                    \"\u003c/script\u003e\");\n        }\n    }\n}\n```\n\niframe 流的服务器开销很大，而且 IE、Chrome 等浏览器一直会处于 loading 状态，图标会不停旋转，简直是强迫症杀手。\n\n![iframe 流效果](statistic/iframe_流效果.gif)\n\niframe 流效果\n\niframe 流非常不友好，强烈不推荐。\n\n### [#](#sse-我的方式) SSE (我的方式)\n\n很多人可能不知道，服务端向客户端推送消息，其实除了可以用`WebSocket`这种耳熟能详的机制外，还有一种服务器发送事件(Server-Sent Events)，简称 SSE。这是一种服务器端到客户端(浏览器)的单向消息推送。\n\nSSE 基于 HTTP 协议的，我们知道一般意义上的 HTTP 协议是无法做到服务端主动向客户端推送消息的，但 SSE 是个例外，它变换了一种思路。\n\n![](statistic/1460000042192390.png)\n\nSSE 在服务器和客户端之间打开一个单向通道，服务端响应的不再是一次性的数据包而是`text/event-stream`类型的数据流信息，在有数据变更时从服务器流式传输到客户端。\n\n整体的实现思路有点类似于在线视频播放，视频流会连续不断的推送到浏览器，你也可以理解成，客户端在完成一次用时很长（网络不畅）的下载。\n\n![SSE 示意图](statistic/SSE_示意图.png)\n\nSSE 示意图\n\nSSE 与 WebSocket 作用相似，都可以建立服务端与浏览器之间的通信，实现服务端向客户端推送消息，但还是有些许不同：\n\n- SSE 是基于 HTTP 协议的，它们不需要特殊的协议或服务器实现即可工作；WebSocket 需单独服务器来处理协议。\n- SSE 单向通信，只能由服务端向客户端单向通信；WebSocket 全双工通信，即通信的双方可以同时发送和接受信息。\n- SSE 实现简单开发成本低，无需引入其他组件；WebSocket 传输数据需做二次解析，开发门槛高一些。\n- SSE 默认支持断线重连；WebSocket 则需要自己实现。\n- SSE 只能传送文本消息，二进制数据需要经过编码后传送；WebSocket 默认支持传送二进制数据。\n\n**SSE 与 WebSocket 该如何选择？**\n\n\u003e 技术并没有好坏之分，只有哪个更合适\n\nSSE 好像一直不被大家所熟知，一部分原因是出现了 WebSocket，这个提供了更丰富的协议来执行双向、全双工通信。对于游戏、即时通信以及需要双向近乎实时更新的场景，拥有双向通道更具吸引力。\n\n但是，在某些情况下，不需要从客户端发送数据。而你只需要一些服务器操作的更新。比如：站内信、未读消息数、状态更新、股票行情、监控数量等场景，SEE 不管是从实现的难易和成本上都更加有优势。此外，SSE 具有 WebSocket 在设计上缺乏的多种功能，例如：自动重新连接、事件 ID 和发送任意事件的能力。\n\n前端只需进行一次 HTTP 请求，带上唯一 ID，打开事件流，监听服务端推送的事件就可以了\n\n```\n\u003cscript\u003e\n    let source = null;\n    let userId = 7777\n    if (window.EventSource) {\n        // 建立连接\n        source = new EventSource('http://localhost:7777/sse/sub/'+userId);\n        setMessageInnerHTML(\"连接用户=\" + userId);\n        /**\n         * 连接一旦建立，就会触发open事件\n         * 另一种写法：source.onopen = function (event) {}\n         */\n        source.addEventListener('open', function (e) {\n            setMessageInnerHTML(\"建立连接。。。\");\n        }, false);\n        /**\n         * 客户端收到服务器发来的数据\n         * 另一种写法：source.onmessage = function (event) {}\n         */\n        source.addEventListener('message', function (e) {\n            setMessageInnerHTML(e.data);\n        });\n    } else {\n        setMessageInnerHTML(\"你的浏览器不支持SSE\");\n    }\n\u003c/script\u003e\n```\n\n服务端的实现更简单，创建一个`SseEmitter`对象放入`sseEmitterMap`进行管理\n\n```\nprivate static Map\u003cString, SseEmitter\u003e sseEmitterMap = new ConcurrentHashMap\u003c\u003e();\n\n/**\n * 创建连接\n */\npublic static SseEmitter connect(String userId) {\n    try {\n        // 设置超时时间，0表示不过期。默认30秒\n        SseEmitter sseEmitter = new SseEmitter(0L);\n        // 注册回调\n        sseEmitter.onCompletion(completionCallBack(userId));\n        sseEmitter.onError(errorCallBack(userId));\n        sseEmitter.onTimeout(timeoutCallBack(userId));\n        sseEmitterMap.put(userId, sseEmitter);\n        count.getAndIncrement();\n        return sseEmitter;\n    } catch (Exception e) {\n        log.info(\"创建新的sse连接异常，当前用户：{}\", userId);\n    }\n    return null;\n}\n\n/**\n * 给指定用户发送消息\n */\npublic static void sendMessage(String userId, String message) {\n\n    if (sseEmitterMap.containsKey(userId)) {\n        try {\n            sseEmitterMap.get(userId).send(message);\n        } catch (IOException e) {\n            log.error(\"用户[{}]推送异常:{}\", userId, e.getMessage());\n            removeUser(userId);\n        }\n    }\n}\n```\n\n**注意：** SSE 不支持 IE 浏览器，对其他主流浏览器兼容性做的还不错。\n\n![SSE 兼容性](statistic/SSE_兼容性.png)\n\nSSE 兼容性\n\n### [#](#websocket) Websocket\n\nWebsocket 应该是大家都比较熟悉的一种实现消息推送的方式，上边我们在讲 SSE 的时候也和 Websocket 进行过比较。\n\n是一种在 TCP 连接上进行全双工通信的协议，建立客户端和服务器之间的通信渠道。浏览器和服务器仅需一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。\n\n![Websocket 示意图](statistic/Websocket_示意图.png)\n\nWebsocket 示意图\n\nSpringBoot 整合 Websocket，先引入 Websocket 相关的工具包，和 SSE 相比额外的开发成本。\n\n```\n\u003c!-- 引入websocket --\u003e\n\u003cdependency\u003e\n    \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e\n    \u003cartifactId\u003espring-boot-starter-websocket\u003c/artifactId\u003e\n\u003c/dependency\u003e\n```\n\n服务端使用`@ServerEndpoint`注解标注当前类为一个 WebSocket 服务器，客户端可以通过`ws://localhost:7777/webSocket/10086`来连接到 WebSocket 服务器端。\n\n```\n@Component\n@Slf4j\n@ServerEndpoint(\"/websocket/{userId}\")\npublic class WebSocketServer {\n    //与某个客户端的连接会话，需要通过它来给客户端发送数据\n    private Session session;\n    private static final CopyOnWriteArraySet\u003cWebSocketServer\u003e webSockets = new CopyOnWriteArraySet\u003c\u003e();\n    // 用来存在线连接数\n    private static final Map\u003cString, Session\u003e sessionPool = new HashMap\u003cString, Session\u003e();\n    /**\n     * 链接成功调用的方法\n     */\n    @OnOpen\n    public void onOpen(Session session, @PathParam(value = \"userId\") String userId) {\n        try {\n            this.session = session;\n            webSockets.add(this);\n            sessionPool.put(userId, session);\n            log.info(\"websocket消息: 有新的连接，总数为:\" + webSockets.size());\n        } catch (Exception e) {\n        }\n    }\n    /**\n     * 收到客户端消息后调用的方法\n     */\n    @OnMessage\n    public void onMessage(String message) {\n        log.info(\"websocket消息: 收到客户端消息:\" + message);\n    }\n    /**\n     * 此为单点消息\n     */\n    public void sendOneMessage(String userId, String message) {\n        Session session = sessionPool.get(userId);\n        if (session != null \u0026\u0026 session.isOpen()) {\n            try {\n                log.info(\"websocket消: 单点消息:\" + message);\n                session.getAsyncRemote().sendText(message);\n            } catch (Exception e) {\n                e.printStackTrace();\n            }\n        }\n    }\n}\n```\n\n前端初始化打开 WebSocket 连接，并监听连接状态，接收服务端数据或向服务端发送数据。\n\n```\n\u003cscript\u003e\n    var ws = new WebSocket('ws://localhost:7777/webSocket/10086');\n    // 获取连接状态\n    console.log('ws连接状态：' + ws.readyState);\n    //监听是否连接成功\n    ws.onopen = function () {\n        console.log('ws连接状态：' + ws.readyState);\n        //连接成功则发送一个数据\n        ws.send('test1');\n    }\n    // 接听服务器发回的信息并处理展示\n    ws.onmessage = function (data) {\n        console.log('接收到来自服务器的消息：');\n        console.log(data);\n        //完成通信后关闭WebSocket连接\n        ws.close();\n    }\n    // 监听连接关闭事件\n    ws.onclose = function () {\n        // 监听整个过程中websocket的状态\n        console.log('ws连接状态：' + ws.readyState);\n    }\n    // 监听并处理error事件\n    ws.onerror = function (error) {\n        console.log(error);\n    }\n    function sendMessage() {\n        var content = $(\"#message\").val();\n        $.ajax({\n            url: '/socket/publish?userId=10086\u0026message=' + content,\n            type: 'GET',\n            data: { \"id\": \"7777\", \"content\": content },\n            success: function (data) {\n                console.log(data)\n            }\n        })\n    }\n\u003c/script\u003e\n```\n\n页面初始化建立 WebSocket 连接，之后就可以进行双向通信了，效果还不错。\n\n![](statistic/1460000042192395.png)\n\n### [#](#mqtt) MQTT\n\n**什么是 MQTT 协议？**\n\nMQTT (Message Queue Telemetry Transport)是一种基于发布/订阅（publish/subscribe）模式的轻量级通讯协议，通过订阅相应的主题来获取消息，是物联网（Internet of Thing）中的一个标准传输协议。\n\n该协议将消息的发布者（publisher）与订阅者（subscriber）进行分离，因此可以在不可靠的网络环境中，为远程连接的设备提供可靠的消息服务，使用方式与传统的 MQ 有点类似。\n\n![MQTT 协议示例](statistic/MQTT_协议示例.png)\n\nMQTT 协议示例\n\nTCP 协议位于传输层，MQTT 协议位于应用层，MQTT 协议构建于 TCP/IP 协议上，也就是说只要支持 TCP/IP 协议栈的地方，都可以使用 MQTT 协议。\n\n**为什么要用 MQTT 协议？**\n\nMQTT 协议为什么在物联网（IOT）中如此受偏爱？而不是其它协议，比如我们更为熟悉的 HTTP 协议呢？\n\n- 首先 HTTP 协议它是一种同步协议，客户端请求后需要等待服务器的响应。而在物联网（IOT）环境中，设备会很受制于环境的影响，比如带宽低、网络延迟高、网络通信不稳定等，显然异步消息协议更为适合 IOT 应用程序。\n- HTTP 是单向的，如果要获取消息客户端必须发起连接，而在物联网（IOT）应用程序中，设备或传感器往往都是客户端，这意味着它们无法被动地接收来自网络的命令。\n- 通常需要将一条命令或者消息，发送到网络上的所有设备上。HTTP 要实现这样的功能不但很困难，而且成本极高。\n\n具体的 MQTT 协议介绍和实践，这里我就不再赘述了，大家可以参考我之前的两篇文章，里边写的也都很详细了。\n\n- MQTT 协议的介绍：[我也没想到 SpringBoot + RabbitMQ 做智能家居，会这么简单open in new window](https://mp.weixin.qq.com/s/udFE6k9pPetIWsa6KeErrA)\n- MQTT 实现消息推送：[未读消息（小红点），前端 与 RabbitMQ 实时消息推送实践，贼简单~open in new window](https://mp.weixin.qq.com/s/U-fUGr9i1MVa4PoVyiDFCg)\n\n## [#](#总结) 总结\n\n![](statistic/Pasted%20image%2020230802002008.png)","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%BD%91%E7%BB%9C/HTTP/7-HTTP3-%E5%BC%BA%E5%8A%BF%E6%9D%A5%E8%A2%AD":{"title":"7 HTTP3 强势来袭","content":"\nHTTP/3 现在（2022 年 5 月）还没正式推出，不过自 2017 年起，HTTP/3 已经更新到 34 个草案了，基本的特性已经确定下来了，对于包格式可能后续会有变化。\n\n所以，这次 HTTP/3 介绍不会涉及到包格式，只说它的特性。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/HTTP3提纲.png)\n\n## 美中不足的 HTTP/2 \n\nHTTP/2 通过头部压缩、二进制编码、多路复用、服务器推送等新特性大幅度提升了 HTTP/1.1 的性能，而美中不足的是 HTTP/2 协议是基于 TCP 实现的，于是存在的缺陷有三个。\n\n- 队头阻塞；\n- TCP 与 TLS 的握手时延迟；\n- 网络迁移需要重新连接；\n\n### 队头阻塞\n\nHTTP/2 多个请求是跑在一个 TCP 连接中的，那么当 TCP 丢包时，整个 TCP 都要等待重传，那么就会阻塞该 TCP 连接中的所有请求。\n\n比如下图中，Stream 2 有一个 TCP 报文丢失了，那么即使收到了 Stream 3 和 Stream 4 的 TCP 报文，应用层也是无法读取的，相当于阻塞了 Stream 3 和 Stream 4 请求。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/http2阻塞.jpeg)\n\n因为 TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，从 HTTP 视角看，就是请求被阻塞了。\n\n举个例子，如下图：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/tcp队头阻塞.gif)\n\n图中发送方发送了很多个 Packet，每个 Packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 Packet 3 在网络中丢失了，即使 Packet 4-6 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 Packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。\n\n### TCP 与 TLS 的握手时延迟\n\n发起 HTTP 请求时，需要经过 TCP 三次握手和 TLS 四次握手（TLS 1.2）的过程，因此共需要 3 个 RTT 的时延才能发出请求数据。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/TCP%2BTLS.gif)\n\n另外，TCP 由于具有「拥塞控制」的特性，所以刚建立连接的 TCP 会有个「慢启动」的过程，它会对 TCP 连接产生“减速”效果。\n\n### 网络迁移需要重新连接\n\n一个 TCP 连接是由四元组（源 IP 地址，源端口，目标 IP 地址，目标端口）确定的，这意味着如果 IP 地址或者端口变动了，就会导致需要 TCP 与 TLS 重新握手，这不利于移动设备切换网络的场景，比如 4G 网络环境切换成 WiFi。\n\n这些问题都是 TCP 协议固有的问题，无论应用层的 HTTP/2 在怎么设计都无法逃脱。要解决这个问题，就必须把**传输层协议替换成 UDP**，这个大胆的决定，HTTP/3 做了！\n\n![HTTP/1 ~ HTTP/3](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/计算机网络/HTTP/27-HTTP3.png)\n\n## QUIC 协议的特点\n\n我们深知，UDP 是一个简单、不可靠的传输协议，而且是 UDP 包之间是无序的，也没有依赖关系。\n\n而且，UDP 是不需要连接的，也就不需要握手和挥手的过程，所以天然的就比 TCP 快。\n\n当然，HTTP/3 不仅仅只是简单将传输协议替换成了 UDP，还基于 UDP 协议在「应用层」实现了 **QUIC 协议**，它具有类似 TCP 的连接管理、拥塞窗口、流量控制的网络特性，相当于将不可靠传输的 UDP 协议变成“可靠”的了，所以不用担心数据包丢失的问题。\n\nQUIC 协议的优点有很多，这里举例几个，比如：\n\n- 无队头阻塞；\n- 更快的连接建立；\n- 连接迁移；\n\n\n### 无队头阻塞\n\nQUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。\n\n由于 QUIC 使用的传输协议是 UDP，UDP 不关心数据包的顺序，如果数据包丢失，UDP 也不关心。\n\n不过 QUIC 协议会保证数据包的可靠性，每个数据包都有一个序号唯一标识。当某个流中的一个数据包丢失了，即使该流的其他数据包到达了，数据也无法被 HTTP/3 读取，直到 QUIC 重传丢失的报文，数据才会交给 HTTP/3。\n\n而其他流的数据报文只要被完整接收，HTTP/3 就可以读取到数据。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n\n所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/network/quic/quic无阻塞.jpeg)\n\n### 更快的连接建立\n\n\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、OpenSSL 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。\n\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 **QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS 1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果**。\n\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT：\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/0-rtt.gif)\n\n\n### 连接迁移\n\n在前面我们提到，基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n\n![TCP 四元组](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzEwLmpwZw?x-oss-process=image/format,png)\n\n那么当移动设备的网络从 4G 切换到 WiFi 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接，而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n\n\n而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过**连接 ID** 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了**连接迁移**的功能。\n\n## HTTP/3 协议\n\n了解完 QUIC 协议的特点后，我们再来看看 HTTP/3 协议在 HTTP 这一层做了什么变化。\n\nHTTP/3 同 HTTP/2 一样采用二进制帧的结构，不同的地方在于 HTTP/2 的二进制帧里需要定义 Stream，而  HTTP/3 自身不需要再定义 Stream，直接使用 QUIC 里的 Stream，于是 HTTP/3 的帧的结构也变简单了。 \n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/网络/http3/http3frame.png)\n\n 从上图可以看到，HTTP/3 帧头只有两个字段：类型和长度。\n\n\n根据帧类型的不同，大体上分为数据帧和控制帧两大类，Headers 帧（HTTP 头部）和 DATA 帧（HTTP 包体）属于数据帧。\n\n\nHTTP/3 在头部压缩算法这一方面也做了升级，升级成了 **QPACK**。与 HTTP/2 中的 HPACK 编码方式相似，HTTP/3 中的 QPACK 也采用了静态表、动态表及 Huffman 编码。\n\n对于静态表的变化，HTTP/2 中的 HPACK 的静态表只有 61 项，而 HTTP/3 中的 QPACK 的静态表扩大到 91 项。\n\nHTTP/2 和 HTTP/3 的 Huffman 编码并没有多大不同，但是动态表编解码方式不同。\n\n所谓的动态表，在首次请求-响应后，双方会将未包含在静态表中的 Header 项更新各自的动态表，接着后续传输时仅用 1 个数字表示，然后对方可以根据这 1 个数字从动态表查到对应的数据，就不必每次都传输长长的数据，大大提升了编码效率。\n\n可以看到，**动态表是具有时序性的，如果首次出现的请求发生了丢包，后续的收到请求，对方就无法解码出 HPACK 头部，因为对方还没建立好动态表，因此后续的请求解码会阻塞到首次请求中丢失的数据包重传过来**。\n\n\nHTTP/3 的 QPACK 解决了这一问题，那它是如何解决的呢？\n\nQUIC 会有两个特殊的单向流，所谓的单向流只有一端可以发送消息，双向则指两端都可以发送消息，传输 HTTP 消息时用的是双向流，这两个单向流的用法：\n\n- 一个叫 QPACK Encoder Stream，用于将一个字典（Key-Value）传递给对方，比如面对不属于静态表的 HTTP 请求头部，客户端可以通过这个 Stream 发送字典；\n- 一个叫 QPACK Decoder Stream，用于响应对方，告诉它刚发的字典已经更新到自己的本地动态表了，后续就可以使用这个字典来编码了。\n\n这两个特殊的单向流是用来**同步双方的动态表**，编码方收到解码方更新确认的通知后，才使用动态表编码 HTTP 头部。\n\n## 总结\n\nHTTP/2 虽然具有多个流并发传输的能力，但是传输层是 TCP 协议，于是存在以下缺陷：\n\n- **队头阻塞**，HTTP/2 多个请求跑在一个 TCP 连接中，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，从 HTTP 视角看，就是多个请求被阻塞了；\n- **TCP 和 TLS 握手时延**，TCP 三次握手和 TLS 四次握手，共有 3-RTT 的时延；\n- **连接迁移需要重新连接**，移动设备从 4G 网络环境切换到 WiFi 时，由于 TCP 是基于四元组来确认一条 TCP 连接的，那么网络环境变化后，就会导致 IP 地址或端口变化，于是 TCP 只能断开连接，然后再重新建立连接，切换网络环境的成本高；\n\nHTTP/3 就将传输层从 TCP 替换成了 UDP，并在 UDP 协议上开发了 QUIC 协议，来保证数据的可靠传输。\n\nQUIC 协议的特点：\n\n- **无队头阻塞**，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，也不会有底层协议限制，某个流发生丢包了，只会影响该流，其他流不受影响；\n- **建立连接速度快**，因为 QUIC 内部包含 TLS 1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与 TLS 密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。\n- **连接迁移**，QUIC 协议没有用四元组的方式来“绑定”连接，而是通过「连接 ID 」来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本；\n\n另外 HTTP/3 的 QPACK 通过两个特殊的单向流来同步双方的动态表，解决了 HTTP/2 的 HPACK 队头阻塞问题。\n\n**期待，HTTP/3 正式推出的那一天！**\n\n---\n\n参考资料：\n\n1. https://medium.com/faun/http-2-spdy-and-http-3-quic-bae7d9a3d484\n2. https://developers.google.com/web/fundamentals/performance/http2?hl=zh-cn\n3. https://blog.cloudflare.com/http3-the-past-present-and-future/\n4. https://tools.ietf.org/html/draft-ietf-quic-http-34\n5. https://tools.ietf.org/html/draft-ietf-quic-transport-34#section-17\n6. https://ably.com/topic/http3?amp%3Butm_campaign=evergreen\u0026amp%3Butm_source=reddit\u0026utm_medium=referral\n7. https://www.nginx.org.cn/article/detail/422\n8. https://www.bilibili.com/read/cv793000/\n9. https://www.chinaz.com/2020/1009/1192436.shtml\n\n\n---\n\n哈喽，我是小林，就爱图解计算机基础，如果文章对你有帮助，别忘记关注哦！\n\n![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)\n\n","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/%E7%BD%91%E7%BB%9C/HTTP/9-%E6%97%A2%E7%84%B6%E6%9C%89-HTTP-%E5%8D%8F%E8%AE%AE%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E6%9C%89-WebSocket":{"title":"9 既然有 HTTP 协议，为什么还要有 WebSocket","content":"\n\u003e 来源：公众号@小白debug\n\u003e\n\u003e 原文地址：[既然有 HTTP 协议，为什么还要有 WebSocket？](https://mp.weixin.qq.com/s/jJNdXMNmXcE8wSE0gbtTAQ)\n\n平时我们打开网页，比如购物网站某宝。都是点一下「列表商品」，跳转一下网页就到了「商品详情」。\n\n从 HTTP 协议的角度来看，就是点一下网页上的某个按钮，**前端发一次 HTTP 请求，网站返回一次 HTTP 响应**。这种由客户端主动请求，服务器响应的方式也满足大部分网页的功能场景。\n\n但有没有发现，这种情况下，服务器从来就「不会主动」给客户端发一次消息。就像你喜欢的女生从来不会主动找你一样。\n\n但如果现在，你在刷网页的时候「右下角」突然弹出一个小广告，提示你【一个人在家偷偷才能玩哦】。\n\n**求知，好学，勤奋**，这些刻在你 DNA 里的东西都动起来了。\n\n你点开后发现。\n\n长相平平无奇的古某提示你\"道士 9 条狗，全服横着走\"。\n\n影帝某辉老师跟你说\"系兄弟就来砍我\"。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/b8cca4b1291f25235bc8df3dddbb6da3.png)\n\n来都来了，你就选了个角色进到了游戏界面里。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/95e5b4cee384b182d0e604378c3ca00a.jpeg)\n\n这时候，上来就是一个小怪，从远处走来，然后疯狂拿木棒子抽你。\n\n**你全程没点任何一次鼠标**。服务器就自动将怪物的移动数据和攻击数据源源不断发给你了。\n\n这….太暖心了。\n\n感动之余，问题就来了：\n\n像这种**看起来服务器主动发消息给客户端的场景**，是怎么做到的？\n\n在真正回答这个问题之前，我们先来聊下一些相关的知识背景。\n\n## 使用 HTTP 不断轮询\n\n其实问题的痛点在于，**怎么样才能在用户不做任何操作的情况下，网页能收到消息并发生变更。**\n\n最常见的解决方案是，**网页的前端代码里不断定时发 HTTP 请求到服务器，服务器收到请求后给客户端响应消息。**\n\n这其实时一种「**伪**」服务器推的形式。\n\n它其实并不是服务器主动发消息到客户端，而是客户端自己不断偷偷请求服务器，只是用户无感知而已。\n\n用这种方式的场景也有很多，最常见的就是**扫码登录**。\n\n比如，某信公众号平台，登录页面二维码出现之后，**前端**网页根本不知道用户扫没扫，于是不断去向**后端**服务器询问，看有没有人扫过这个码。而且是以大概 1 到 2 秒的间隔去不断发出请求，这样可以保证用户在扫码后能在 1 到 2 秒内得到及时的反馈，不至于**等太久**。\n\n## 使用 HTTP 定时轮询\n\n但这样，会有两个比较明显的问题：\n\n- 当你打开 F12 页面时，你会发现满屏的 HTTP 请求。虽然很小，但这其实也消耗带宽，同时也会增加下游服务器的负担。\n- 最坏情况下，用户在扫码后，需要等个 1~2 秒，正好才触发下一次 HTTP 请求，然后才跳转页面，用户会感到**明显的卡顿**。\n\n使用起来的体验就是，二维码出现后，手机扫一扫，然后在手机上点个确认，这时候**卡顿等个 1~2 秒**，页面才跳转。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/5e0e0e25e8aca80812c9a2892032111c.png)\n\n那么问题又来了，**有没有更好的解决方案？**\n\n有，而且实现起来成本还非常低。\n\n## 长轮询\n\n我们知道，HTTP 请求发出后，一般会给服务器留一定的时间做响应，比如 3 秒，规定时间内没返回，就认为是超时。\n\n如果我们的 HTTP 请求**将超时设置的很大**，比如 30 秒，**在这 30 秒内只要服务器收到了扫码请求，就立马返回给客户端网页。如果超时，那就立马发起下一次请求。**\n\n这样就减少了 HTTP 请求的个数，并且由于大部分情况下，用户都会在某个 30 秒的区间内做扫码操作，所以响应也是及时的。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/1058a96ba35215c0f30accc3ff5bb824.png)\n\n比如，某度云网盘就是这么干的。所以你会发现一扫码，手机上点个确认，电脑端网页就**秒跳转**，体验很好。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/a3a8c95b97d2bac26cfab123a4da68b2.png)\n\n像这种发起一个请求，在较长时间内等待服务器响应的机制，就是所谓的**长轮询机制**。我们常用的消息队列 RocketMQ 中，消费者去取数据时，也用到了这种方式。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/6173c1d25abc914ef17ee9e534ed6a5f.png)\n\n像这种，在用户不感知的情况下，服务器将数据推送给浏览器的技术，就是所谓的**服务器推送**技术，它还有个毫不沾边的英文名，**comet** 技术，大家听过就好。\n\n上面提到的两种解决方案（不断轮询和长轮询），本质上，其实还是客户端主动去取数据。\n\n对于像扫码登录这样的**简单场景**还能用用。但如果是网页游戏呢，游戏一般会有大量的数据需要从服务器主动推送到客户端。\n\n这就得说下 **WebSocket** 了。\n\n## WebSocket 是什么\n\n我们知道 TCP 连接的两端，**同一时间里**，**双方**都可以**主动**向对方发送数据。这就是所谓的**全双工**。\n\n而现在使用最广泛的 `HTTP/1.1`，也是基于 TCP 协议的，**同一时间里**，客户端和服务器**只能有一方主动**发数据，这就是所谓的**半双工**。\n\n也就是说，好好的全双工 TCP，被 HTTP/1.1 用成了半双工。\n\n为什么？\n\n这是由于 HTTP 协议设计之初，考虑的是看看网页文本的场景，能做到**客户端发起请求再由服务器响应**，就够了，根本就没考虑网页游戏这种，客户端和服务器之间都要互相主动发大量数据的场景。\n\n所以，为了更好的支持这样的场景，我们需要另外一个**基于 TCP 的新协议**。\n\n于是新的应用层协议 **WebSocket** 就被设计出来了。\n\n大家别被这个名字给带偏了。虽然名字带了个 socket，但其实 **socket 和 WebSocket 之间，就跟雷峰和雷峰塔一样，二者接近毫无关系**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/3bbe4c5db972513f912d30ba8cbddd65.png)\n\n### 怎么建立 WebSocket 连接\n\n我们平时刷网页，一般都是在浏览器上刷的，一会刷刷图文，这时候用的是 **HTTP 协议**，一会打开网页游戏，这时候就得切换成我们新介绍的 **WebSocket 协议**。\n\n为了兼容这些使用场景。浏览器在 **TCP 三次握手**建立连接之后，都**统一使用 HTTP 协议**先进行一次通信。\n\n- 如果此时是**普通的 HTTP 请求**，那后续双方就还是老样子继续用普通 HTTP 协议进行交互，这点没啥疑问。\n- 如果这时候是**想建立 WebSocket 连接**，就会在 HTTP 请求里带上一些**特殊的 header 头**，如下：\n\n```http\nConnection: Upgrade\nUpgrade: WebSocket\nSec-WebSocket-Key: T2a6wZlAwhgQNqruZ2YUyg==\\r\\n\n```\n\n这些 header 头的意思是，浏览器想**升级协议（Connection: Upgrade）**，并且**想升级成 WebSocket 协议（Upgrade: WebSocket）**。同时带上一段**随机生成的 Base64 码（Sec-WebSocket-Key）**，发给服务器。\n\n如果服务器正好支持升级成 WebSocket 协议。就会走 WebSocket 握手流程，同时根据客户端生成的 Base64 码，用某个**公开的**算法变成另一段字符串，放在 HTTP 响应的 `Sec-WebSocket-Accept` 头里，同时带上 `101 状态码`，发回给浏览器。HTTP 的响应如下：\n\n```http\nHTTP/1.1 101 Switching Protocols\\r\\n\nSec-WebSocket-Accept: iBJKv/ALIW2DobfoA4dmr3JHBCY=\\r\\n\nUpgrade: WebSocket\\r\\n\nConnection: Upgrade\\r\\n\n```\n\nHTTP 状态码=200（正常响应）的情况，大家见得多了。101 确实不常见，它其实是指**协议切换**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/dea71991b336c876cae2e1ebdf03b62d.png)\n\n之后，浏览器也用同样的**公开算法**将 `Base64 码` 转成另一段字符串，如果这段字符串跟服务器传回来的**字符串一致**，那验证通过。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/117eebe06cc6b35ded3216a95706f080.png)\n\n就这样经历了一来一回两次 HTTP 握手，WebSocket 就建立完成了，后续双方就可以使用 WebSocket 的数据格式进行通信了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f4edd3018914fe6eb38fad6aa3fd2d65.png)\n\n### WebSocket 抓包\n\n我们可以用 WireShark 抓个包，实际看下数据包的情况。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/f756ca625523f0f9d40a402465179bbe.png)\n\n上面这张图，注意画了红框的第 `2445` 行报文，是 WebSocket 的**第一次握手**，意思是发起了一次带有 `特殊 Header` 的 HTTP 请求。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/82d65f08dad05e6b537ea06b94224a5f.png)\n\n上面这个图里画了红框的 `4714` 行报文，就是服务器在得到第一次握手后，响应的**第二次握手**，可以看到这也是个 HTTP 类型的报文，返回的状态码是 101。同时可以看到返回的报文 Header 中也带有各种 `WebSocket` 相关的信息，比如 `Sec-WebSocket-Accept`。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/382c7699530ea7e7b22f60bb68af21bd.png)\n\n上面这张图就是全貌了，从截图上的注释可以看出，WebSocket 和 HTTP 一样都是基于 TCP 的协议。**经历了三次 TCP 握手之后，利用 HTTP 协议升级为 WebSocket 协议**。\n\n你在网上可能会看到一种说法：“WebSocket 是基于 HTTP 的新协议”，**其实这并不对**，因为 WebSocket 只有在建立连接时才用到了 HTTP，**升级完成之后就跟 HTTP 没有任何关系了**。\n\n这就好像你喜欢的女生通过你要到了你大学室友的微信，然后他们自己就聊起来了。你能说这个女生是通过你去跟你室友沟通的吗？不能。你跟 HTTP 一样，都只是个**工具人**。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/2e9d4b1652bdfa1e3ae4bb24f70a1b5a.png)\n\n这就有点\"**借壳生蛋**\"的那意思。\n\n## HTTP 和 WebSocket 的关系\n\n### WebSocket 的消息格式\n\n上面提到在完成协议升级之后，两端就会用 WebSocket 的数据格式进行通信。\n\n数据包在 WebSocket 中被叫做**帧**，我们来看下它的数据格式长什么样子。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/3a63a86e5d7e72a37b9828fc6e65c21f.png)\n\n这里面字段很多，但我们只需要关注下面这几个。\n\n**opcode 字段**：这个是用来标志这是个**什么类型**的数据帧。比如。\n\n- 等于 1 ，是指text类型（`string`）的数据包\n- 等于 2 ，是二进制数据类型（`[]byte`）的数据包\n- 等于 8 ，是关闭连接的信号\n\n**payload 字段**：存放的是我们**真正想要传输的数据的长度**，单位是**字节**。比如你要发送的数据是 `字符串\"111\"`，那它的长度就是 `3`。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/437a076935f82be1d36960c9a4785fbd.png)\n\n另外，可以看到，我们存放**payload 长度的字段有好几个**，我们既可以用最前面的 `7 bit`, 也可以用后面的 `7+16 bit` 或 `7+64 bit`。\n\n那么问题就来了。\n\n我们知道，在数据层面，大家都是 01 二进制流。我怎么知道**什么情况下应该读 7 bit，什么情况下应该读 7+16 bit 呢？**\n\nWebSocket 会用最开始的 7 bit 做标志位。不管接下来的数据有多大，都**先读最先的 7 个 bit**，根据它的取值决定还要不要再读个 16 bit 或 64 bit。\n\n- 如果 `最开始的 7 bit` 的值是 0~125，那么它就表示了 **payload 全部长度**，只读最开始的 `7 个 bit` 就完事了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/690f5a4deda2de50f3a35eddf0be4d75.png)\n\n- 如果是 `126（0x7E）`。那它表示payload的长度范围在 `126~65535` 之间，接下来还需要**再读 16 bit**。这 16 bit 会包含 payload 的真实长度。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/c815c9dabc02fceb42a98c762705af33.png)\n\n- 如果是 `127（0x7F）`。那它表示 payload 的长度范围 `\u003e=65536`，接下来还需要**再读 64 bit**。这 64 bit 会包含 payload 的长度。这能放 2 的 64 次方 byte 的数据，换算一下好多个 TB，肯定够用了。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/192b22b4fe46e8dfb7b17549306d5998.png)\n\n**payload data 字段**：这里存放的就是真正要传输的数据，在知道了上面的 payload 长度后，就可以根据这个值去截取对应的数据。\n\n大家有没有发现一个小细节，WebSocket 的数据格式也是**数据头（内含 payload 长度） + payload data** 的形式。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/d449242f1bf41c6f95a5314ec8311d0d.jpeg)\n\n这是因为 TCP 协议本身就是全双工，但直接使用**纯裸 TCP** 去传输数据，会有**粘包**的\"问题\"。为了解决这个问题，上层协议一般会用**消息头+消息体**的格式去重新包装要发的数据。\n\n而**消息头**里一般含有**消息体的长度**，通过这个长度可以去截取真正的消息体。\n\nHTTP 协议和大部分 RPC 协议，以及我们今天介绍的 WebSocket 协议，都是这样设计的。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/b91fedb1856897c231b8fb5932b7b2d2.png)\n\n### WebSocket 的使用场景\n\nWebSocket 完美继承了 TCP 协议的**全双工**能力，并且还贴心的提供了解决粘包的方案。\n\n它适用于**需要服务器和客户端（浏览器）频繁交互**的大部分场景，比如网页/小程序游戏，网页聊天室，以及一些类似飞书这样的网页协同办公软件。\n\n回到文章开头的问题，在使用 WebSocket 协议的网页游戏里，怪物移动以及攻击玩家的行为是**服务器逻辑**产生的，对玩家产生的伤害等数据，都需要由**服务器主动发送给客户端**，客户端获得数据后展示对应的效果。\n\n![图片](https://img-blog.csdnimg.cn/img_convert/31410d2e885aab55c2c588aad754bb5c.png)\n\n## 总结\n\n- TCP 协议本身是**全双工**的，但我们最常用的 HTTP/1.1，虽然是基于 TCP 的协议，但它是**半双工**的，对于大部分需要服务器主动推送数据到客户端的场景，都不太友好，因此我们需要使用支持全双工的 WebSocket 协议。\n- 在 HTTP/1.1 里，只要客户端不问，服务端就不答。基于这样的特点，对于登录页面这样的简单场景，可以使用**定时轮询或者长轮询**的方式实现**服务器推送**(comet)的效果。\n- 对于客户端和服务端之间需要频繁交互的复杂场景，比如网页游戏，都可以考虑使用 WebSocket 协议。\n- WebSocket 和 socket 几乎没有任何关系，只是叫法相似。\n- 正因为各个浏览器都支持 HTTP 协议，所以 WebSocket 会先利用 HTTP 协议加上一些特殊的 Header 头进行握手升级操作，升级成功后就跟 HTTP 没有任何关系了，之后就用 WebSocket 的数据格式进行收发数据。\n\n------\n\n***哈喽，我是小林，就爱图解计算机基础，如果觉得文章对你有帮助，欢迎微信搜索「小林coding」关注我***\n\n![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)","lastmodified":"2023-08-01T17:34:29.63226112Z","tags":[]},"/Dobbo/Dubbo%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93":{"title":"Dubbo常见问题总结","content":"这篇文章是我根据官方文档以及自己平时的使用情况，对 Dubbo 所做的一个总结。欢迎补充！\n\n## [#](#dubbo-基础) Dubbo 基础\n\n### [#](#什么是-dubbo) 什么是 Dubbo?\n\n![Dubbo 官网](statistic/Dubbo_官网.png)\n\nDubbo 官网\n\n[Apache Dubboopen in new window](https://github.com/apache/dubbo) |ˈdʌbəʊ| 是一款高性能、轻量级的开源 WEB 和 RPC 框架。\n\n根据 [Dubbo 官方文档open in new window](https://dubbo.apache.org/zh/)的介绍，Dubbo 提供了六大核心能力\n\n1. 面向接口代理的高性能 RPC 调用。\n2. 智能容错和负载均衡。\n3. 服务自动注册和发现。\n4. 高度可扩展能力。\n5. 运行期流量调度。\n6. 可视化的服务治理与运维。\n\n![Dubbo提供的六大核心能力](statistic/Dubbo提供的六大核心能力.png)\n\nDubbo提供的六大核心能力\n\n简单来说就是：**Dubbo 不光可以帮助我们调用远程服务，还提供了一些其他开箱即用的功能比如智能负载均衡。**\n\nDubbo 目前已经有接近 34.4 k 的 Star 。\n\n在 **2020 年度 OSC 中国开源项目** 评选活动中，Dubbo 位列开发框架和基础组件类项目的第 7 名。相比几年前来说，热度和排名有所下降。\n\n![](statistic/image-20210107153159545.png)\n\nDubbo 是由阿里开源，后来加入了 Apache 。正是由于 Dubbo 的出现，才使得越来越多的公司开始使用以及接受分布式架构。\n\n### [#](#为什么要用-dubbo) 为什么要用 Dubbo?\n\n随着互联网的发展，网站的规模越来越大，用户数量越来越多。单一应用架构、垂直应用架构无法满足我们的需求，这个时候分布式服务架构就诞生了。\n\n分布式服务架构下，系统被拆分成不同的服务比如短信服务、安全服务，每个服务独立提供系统的某个核心服务。\n\n我们可以使用 Java RMI（Java Remote Method Invocation）、Hessian 这种支持远程调用的框架来简单地暴露和引用远程服务。但是！当服务越来越多之后，服务调用关系越来越复杂。当应用访问压力越来越大后，负载均衡以及服务监控的需求也迫在眉睫。我们可以用 F5 这类硬件来做负载均衡，但这样增加了成本，并且存在单点故障的风险。\n\n不过，Dubbo 的出现让上述问题得到了解决。**Dubbo 帮助我们解决了什么问题呢？**\n\n1. **负载均衡**：同一个服务部署在不同的机器时该调用哪一台机器上的服务。\n2. **服务调用链路生成**：随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo 可以为我们解决服务之间互相是如何调用的。\n3. **服务访问压力以及时长统计、资源调度和治理**：基于访问压力实时管理集群容量，提高集群利用率。\n4. ......\n\n![Dubbo 能力概览](statistic/Dubbo_能力概览.jpg)\n\nDubbo 能力概览\n\n另外，Dubbo 除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于 Spring Cloud 在微服务中应用更加广泛，所以，我觉得一般我们提 Dubbo 的话，大部分是分布式系统的情况。\n\n**我们刚刚提到了分布式这个概念，下面再给大家介绍一下什么是分布式？为什么要分布式？**\n\n## [#](#分布式基础) 分布式基础\n\n### [#](#什么是分布式) 什么是分布式?\n\n分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。\n\n![分布式事务示意图](statistic/分布式事务示意图.png)\n\n分布式事务示意图\n\n### [#](#为什么要分布式) 为什么要分布式?\n\n从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。\n\n另外，我觉得将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。你想一想嘛？把整个系统拆分成不同的服务/系统，然后每个服务/系统 单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？\n\n## [#](#dubbo-架构) Dubbo 架构\n\n### [#](#dubbo-架构中的核心角色有哪些) Dubbo 架构中的核心角色有哪些？\n\n[官方文档中的框架设计章节open in new window](https://dubbo.apache.org/zh/docs/v2.7/dev/design/) 已经介绍的非常详细了，我这里把一些比较重要的点再提一下。\n\n![dubbo-relation](statistic/dubbo-relation.jpg)\n\ndubbo-relation\n\n上述节点简单介绍以及他们之间的关系：\n\n- **Container：** 服务运行容器，负责加载、运行服务提供者。必须。\n- **Provider：** 暴露服务的服务提供方，会向注册中心注册自己提供的服务。必须。\n- **Consumer：** 调用远程服务的服务消费方，会向注册中心订阅自己所需的服务。必须。\n- **Registry：** 服务注册与发现的注册中心。注册中心会返回服务提供者地址列表给消费者。非必须。\n- **Monitor：** 统计服务的调用次数和调用时间的监控中心。服务消费者和提供者会定时发送统计数据到监控中心。 非必须。\n\n### [#](#dubbo-中的-invoker-概念了解么) Dubbo 中的 Invoker 概念了解么？\n\n`Invoker` 是 Dubbo 领域模型中非常重要的一个概念，你如果阅读过 Dubbo 源码的话，你会无数次看到这玩意。就比如下面我要说的负载均衡这块的源码中就有大量 `Invoker` 的身影。\n\n简单来说，`Invoker` 就是 Dubbo 对远程调用的抽象。\n\n![dubbo_rpc_invoke.jpg](statistic/dubbo_rpc_invoke.jpg)\n\ndubbo_rpc_invoke.jpg\n\n按照 Dubbo 官方的话来说，`Invoker` 分为\n\n- 服务提供 `Invoker`\n- 服务消费 `Invoker`\n\n假如我们需要调用一个远程方法，我们需要动态代理来屏蔽远程调用的细节吧！我们屏蔽掉的这些细节就依赖对应的 `Invoker` 实现， `Invoker` 实现了真正的远程服务调用。\n\n### [#](#dubbo-的工作原理了解么) Dubbo 的工作原理了解么？\n\n下图是 Dubbo 的整体设计，从下至上分为十层，各层均为单向依赖。\n\n\u003e 左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。\n\n![dubbo-framework](statistic/dubbo-framework.jpg)\n\ndubbo-framework\n\n- **config 配置层**：Dubbo 相关的配置。支持代码配置，同时也支持基于 Spring 来做配置，以 `ServiceConfig`, `ReferenceConfig` 为中心\n- **proxy 服务代理层**：调用远程方法像调用本地的方法一样简单的一个关键，真实调用过程依赖代理类，以 `ServiceProxy` 为中心。\n- **registry 注册中心层**：封装服务地址的注册与发现。\n- **cluster 路由层**：封装多个提供者的路由及负载均衡，并桥接注册中心，以 `Invoker` 为中心。\n- **monitor 监控层**：RPC 调用次数和调用时间监控，以 `Statistics` 为中心。\n- **protocol 远程调用层**：封装 RPC 调用，以 `Invocation`, `Result` 为中心。\n- **exchange 信息交换层**：封装请求响应模式，同步转异步，以 `Request`, `Response` 为中心。\n- **transport 网络传输层**：抽象 mina 和 netty 为统一接口，以 `Message` 为中心。\n- **serialize 数据序列化层**：对需要在网络传输的数据进行序列化。\n\n### [#](#dubbo-的-spi-机制了解么-如何扩展-dubbo-中的默认实现) Dubbo 的 SPI 机制了解么？ 如何扩展 Dubbo 中的默认实现？\n\nSPI（Service Provider Interface） 机制被大量用在开源项目中，它可以帮助我们动态寻找服务/功能（比如负载均衡策略）的实现。\n\nSPI 的具体原理是这样的：我们将接口的实现类放在配置文件中，我们在程序运行过程中读取配置文件，通过反射加载实现类。这样，我们可以在运行的时候，动态替换接口的实现类。和 IoC 的解耦思想是类似的。\n\nJava 本身就提供了 SPI 机制的实现。不过，Dubbo 没有直接用，而是对 Java 原生的 SPI 机制进行了增强，以便更好满足自己的需求。\n\n**那我们如何扩展 Dubbo 中的默认实现呢？**\n\n比如说我们想要实现自己的负载均衡策略，我们创建对应的实现类 `XxxLoadBalance` 实现 `LoadBalance` 接口或者 `AbstractLoadBalance` 类。\n\n```\npackage com.xxx;\n\nimport org.apache.dubbo.rpc.cluster.LoadBalance;\nimport org.apache.dubbo.rpc.Invoker;\nimport org.apache.dubbo.rpc.Invocation;\nimport org.apache.dubbo.rpc.RpcException;\n\npublic class XxxLoadBalance implements LoadBalance {\n    public \u003cT\u003e Invoker\u003cT\u003e select(List\u003cInvoker\u003cT\u003e\u003e invokers, Invocation invocation) throws RpcException {\n        // ...\n    }\n}\n```\n\n我们将这个实现类的路径写入到`resources` 目录下的 `META-INF/dubbo/org.apache.dubbo.rpc.cluster.LoadBalance`文件中即可。\n\n```\nsrc\n |-main\n    |-java\n        |-com\n            |-xxx\n                |-XxxLoadBalance.java (实现LoadBalance接口)\n    |-resources\n        |-META-INF\n            |-dubbo\n                |-org.apache.dubbo.rpc.cluster.LoadBalance (纯文本文件，内容为：xxx=com.xxx.XxxLoadBalance)\n```\n\n`org.apache.dubbo.rpc.cluster.LoadBalance`\n\n```\nxxx=com.xxx.XxxLoadBalance\n```\n\n其他还有很多可供扩展的选择，你可以在[官方文档open in new window](https://cn.dubbo.apache.org/zh-cn/overview/home/)中找到。\n\n### [#](#dubbo-的微内核架构了解吗) Dubbo 的微内核架构了解吗？\n\nDubbo 采用 微内核（Microkernel） + 插件（Plugin） 模式，简单来说就是微内核架构。微内核只负责组装插件。\n\n**何为微内核架构呢？** 《软件架构模式》 这本书是这样介绍的：\n\n\u003e 微内核架构模式（有时被称为插件架构模式）是实现基于产品应用程序的一种自然模式。基于产品的应用程序是已经打包好并且拥有不同版本，可作为第三方插件下载的。然后，很多公司也在开发、发布自己内部商业应用像有版本号、说明及可加载插件式的应用软件（这也是这种模式的特征）。微内核系统可让用户添加额外的应用如插件，到核心应用，继而提供了可扩展性和功能分离的用法。\n\n微内核架构包含两类组件：**核心系统（core system）** 和 **插件模块（plug-in modules）**。\n\n![](statistic/%E5%BE%AE%E5%86%85%E6%A0%B8%E6%9E%B6%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png)\n\n核心系统提供系统所需核心能力，插件模块可以扩展系统的功能。因此， 基于微内核架构的系统，非常易于扩展功能。\n\n我们常见的一些 IDE，都可以看作是基于微内核架构设计的。绝大多数 IDE 比如 IDEA、VSCode 都提供了插件来丰富自己的功能。\n\n正是因为 Dubbo 基于微内核架构，才使得我们可以随心所欲替换 Dubbo 的功能点。比如你觉得 Dubbo 的序列化模块实现的不满足自己要求，没关系啊！你自己实现一个序列化模块就好了啊！\n\n通常情况下，微核心都会采用 Factory、IoC、OSGi 等方式管理插件生命周期。Dubbo 不想依赖 Spring 等 IoC 容器，也不想自己造一个小的 IoC 容器（过度设计），因此采用了一种最简单的 Factory 方式管理插件：**JDK 标准的 SPI 扩展机制** （`java.util.ServiceLoader`）。\n\n### [#](#关于-dubbo-架构的一些自测小问题) 关于 Dubbo 架构的一些自测小问题\n\n#### [#](#注册中心的作用了解么) 注册中心的作用了解么？\n\n注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互。\n\n#### [#](#服务提供者宕机后-注册中心会做什么) 服务提供者宕机后，注册中心会做什么？\n\n注册中心会立即推送事件通知消费者。\n\n#### [#](#监控中心的作用呢) 监控中心的作用呢？\n\n监控中心负责统计各服务调用次数，调用时间等。\n\n#### [#](#注册中心和监控中心都宕机的话-服务都会挂掉吗) 注册中心和监控中心都宕机的话，服务都会挂掉吗？\n\n不会。两者都宕机也不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表。注册中心和监控中心都是可选的，服务消费者可以直连服务提供者。\n\n## [#](#dubbo-的负载均衡策略) Dubbo 的负载均衡策略\n\n### [#](#什么是负载均衡) 什么是负载均衡？\n\n先来看一下稍微官方点的解释。下面这段话摘自维基百科对负载均衡的定义：\n\n\u003e 负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动）的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件。\n\n**上面讲的大家可能不太好理解，再用通俗的话给大家说一下。**\n\n我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。\n\n### [#](#dubbo-提供的负载均衡策略有哪些) Dubbo 提供的负载均衡策略有哪些？\n\n在集群负载均衡时，Dubbo 提供了多种均衡策略，默认为 `random` 随机调用。我们还可以自行扩展负载均衡策略（参考 Dubbo SPI 机制）。\n\n在 Dubbo 中，所有负载均衡实现类均继承自 `AbstractLoadBalance`，该类实现了 `LoadBalance` 接口，并封装了一些公共的逻辑。\n\n```\npublic abstract class AbstractLoadBalance implements LoadBalance {\n\n    static int calculateWarmupWeight(int uptime, int warmup, int weight) {\n    }\n\n    @Override\n    public \u003cT\u003e Invoker\u003cT\u003e select(List\u003cInvoker\u003cT\u003e\u003e invokers, URL url, Invocation invocation) {\n    }\n\n    protected abstract \u003cT\u003e Invoker\u003cT\u003e doSelect(List\u003cInvoker\u003cT\u003e\u003e invokers, URL url, Invocation invocation);\n\n\n    int getWeight(Invoker\u003c?\u003e invoker, Invocation invocation) {\n\n    }\n}\n```\n\n`AbstractLoadBalance` 的实现类有下面这些：\n\n![](statistic/image-20210326105257812.png)\n\n官方文档对负载均衡这部分的介绍非常详细，推荐小伙伴们看看，地址：[https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#m-zhdocsv27devsourceloadbalanceopen in new window](https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#m-zhdocsv27devsourceloadbalance) 。\n\n#### [#](#randomloadbalance) RandomLoadBalance\n\n根据权重随机选择（对加权随机算法的实现）。这是 Dubbo 默认采用的一种负载均衡策略。\n\n`RandomLoadBalance` 具体的实现原理非常简单，假如有两个提供相同服务的服务器 S1,S2，S1 的权重为 7，S2 的权重为 3。\n\n我们把这些权重值分布在坐标区间会得到：S1-\u003e[0, 7) ，S2-\u003e[7, 10)。我们生成[0, 10) 之间的随机数，随机数落到对应的区间，我们就选择对应的服务器来处理请求。\n\n![RandomLoadBalance](statistic/RandomLoadBalance.png)\n\nRandomLoadBalance\n\n`RandomLoadBalance` 的源码非常简单，简单花几分钟时间看一下。\n\n\u003e 以下源码来自 Dubbo master 分支上的最新的版本 2.7.9。\n\n```\npublic class RandomLoadBalance extends AbstractLoadBalance {\n\n    public static final String NAME = \"random\";\n\n    @Override\n    protected \u003cT\u003e Invoker\u003cT\u003e doSelect(List\u003cInvoker\u003cT\u003e\u003e invokers, URL url, Invocation invocation) {\n\n        int length = invokers.size();\n        boolean sameWeight = true;\n        int[] weights = new int[length];\n        int totalWeight = 0;\n        // 下面这个for循环的主要作用就是计算所有该服务的提供者的权重之和 totalWeight（），\n        // 除此之外，还会检测每个服务提供者的权重是否相同\n        for (int i = 0; i \u003c length; i++) {\n            int weight = getWeight(invokers.get(i), invocation);\n            totalWeight += weight;\n            weights[i] = totalWeight;\n            if (sameWeight \u0026\u0026 totalWeight != weight * (i + 1)) {\n                sameWeight = false;\n            }\n        }\n        if (totalWeight \u003e 0 \u0026\u0026 !sameWeight) {\n            // 随机生成一个 [0, totalWeight) 区间内的数字\n            int offset = ThreadLocalRandom.current().nextInt(totalWeight);\n            // 判断会落在哪个服务提供者的区间\n            for (int i = 0; i \u003c length; i++) {\n                if (offset \u003c weights[i]) {\n                    return invokers.get(i);\n                }\n            }\n\n        return invokers.get(ThreadLocalRandom.current().nextInt(length));\n    }\n\n}\n\n```\n\n#### [#](#leastactiveloadbalance) LeastActiveLoadBalance\n\n`LeastActiveLoadBalance` 直译过来就是**最小活跃数负载均衡**。\n\n这个名字起得有点不直观，不仔细看官方对活跃数的定义，你压根不知道这玩意是干嘛的。\n\n我这么说吧！初始状态下所有服务提供者的活跃数均为 0（每个服务提供者的中特定方法都对应一个活跃数，我在后面的源码中会提到），每收到一个请求后，对应的服务提供者的活跃数 +1，当这个请求处理完之后，活跃数 -1。\n\n因此，**Dubbo 就认为谁的活跃数越少，谁的处理速度就越快，性能也越好，这样的话，我就优先把请求给活跃数少的服务提供者处理。**\n\n**如果有多个服务提供者的活跃数相等怎么办？**\n\n很简单，那就再走一遍 `RandomLoadBalance` 。\n\n```\npublic class LeastActiveLoadBalance extends AbstractLoadBalance {\n\n    public static final String NAME = \"leastactive\";\n\n    @Override\n    protected \u003cT\u003e Invoker\u003cT\u003e doSelect(List\u003cInvoker\u003cT\u003e\u003e invokers, URL url, Invocation invocation) {\n        int length = invokers.size();\n        int leastActive = -1;\n        int leastCount = 0;\n        int[] leastIndexes = new int[length];\n        int[] weights = new int[length];\n        int totalWeight = 0;\n        int firstWeight = 0;\n        boolean sameWeight = true;\n        // 这个 for 循环的主要作用是遍历 invokers 列表，找出活跃数最小的 Invoker\n        // 如果有多个 Invoker 具有相同的最小活跃数，还会记录下这些 Invoker 在 invokers 集合中的下标，并累加它们的权重，比较它们的权重值是否相等\n        for (int i = 0; i \u003c length; i++) {\n            Invoker\u003cT\u003e invoker = invokers.get(i);\n            // 获取 invoker 对应的活跃(active)数\n            int active = RpcStatus.getStatus(invoker.getUrl(), invocation.getMethodName()).getActive();\n            int afterWarmup = getWeight(invoker, invocation);\n            weights[i] = afterWarmup;\n            if (leastActive == -1 || active \u003c leastActive) {\n                leastActive = active;\n                leastCount = 1;\n                leastIndexes[0] = i;\n                totalWeight = afterWarmup;\n                firstWeight = afterWarmup;\n                sameWeight = true;\n            } else if (active == leastActive) {\n                leastIndexes[leastCount++] = i;\n                totalWeight += afterWarmup;\n                if (sameWeight \u0026\u0026 afterWarmup != firstWeight) {\n                    sameWeight = false;\n                }\n            }\n        }\n       // 如果只有一个 Invoker 具有最小的活跃数，此时直接返回该 Invoker 即可\n        if (leastCount == 1) {\n            return invokers.get(leastIndexes[0]);\n        }\n        // 如果有多个 Invoker 具有相同的最小活跃数，但它们之间的权重不同\n        // 这里的处理方式就和  RandomLoadBalance 一致了\n        if (!sameWeight \u0026\u0026 totalWeight \u003e 0) {\n            int offsetWeight = ThreadLocalRandom.current().nextInt(totalWeight);\n            for (int i = 0; i \u003c leastCount; i++) {\n                int leastIndex = leastIndexes[i];\n                offsetWeight -= weights[leastIndex];\n                if (offsetWeight \u003c 0) {\n                    return invokers.get(leastIndex);\n                }\n            }\n        }\n        return invokers.get(leastIndexes[ThreadLocalRandom.current().nextInt(leastCount)]);\n    }\n}\n\n```\n\n活跃数是通过 `RpcStatus` 中的一个 `ConcurrentMap` 保存的，根据 URL 以及服务提供者被调用的方法的名称，我们便可以获取到对应的活跃数。也就是说服务提供者中的每一个方法的活跃数都是互相独立的。\n\n```\npublic class RpcStatus {\n\n    private static final ConcurrentMap\u003cString, ConcurrentMap\u003cString, RpcStatus\u003e\u003e METHOD_STATISTICS =\n            new ConcurrentHashMap\u003cString, ConcurrentMap\u003cString, RpcStatus\u003e\u003e();\n\n   public static RpcStatus getStatus(URL url, String methodName) {\n        String uri = url.toIdentityString();\n        ConcurrentMap\u003cString, RpcStatus\u003e map = METHOD_STATISTICS.computeIfAbsent(uri, k -\u003e new ConcurrentHashMap\u003c\u003e());\n        return map.computeIfAbsent(methodName, k -\u003e new RpcStatus());\n    }\n    public int getActive() {\n        return active.get();\n    }\n\n}\n```\n\n#### [#](#consistenthashloadbalance) ConsistentHashLoadBalance\n\n`ConsistentHashLoadBalance` 小伙伴们应该也不会陌生，在分库分表、各种集群中就经常使用这个负载均衡策略。\n\n`ConsistentHashLoadBalance` 即**一致性 Hash 负载均衡策略**。 `ConsistentHashLoadBalance` 中没有权重的概念，具体是哪个服务提供者处理请求是由你的请求的参数决定的，也就是说相同参数的请求总是发到同一个服务提供者。\n\n![](statistic/consistent-hash-data-incline.jpg)\n\n另外，Dubbo 为了避免数据倾斜问题（节点不够分散，大量请求落到同一节点），还引入了虚拟节点的概念。通过虚拟节点可以让节点更加分散，有效均衡各个节点的请求量。\n\n![](statistic/consistent-hash-invoker.jpg)\n\n官方有详细的源码分析：[https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#23-consistenthashloadbalanceopen in new window](https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#23-consistenthashloadbalance) 。这里还有一个相关的 [PR#5440open in new window](https://github.com/apache/dubbo/pull/5440) 来修复老版本中 ConsistentHashLoadBalance 存在的一些 Bug。感兴趣的小伙伴，可以多花点时间研究一下。我这里不多分析了，这个作业留给你们！\n\n#### [#](#roundrobinloadbalance) RoundRobinLoadBalance\n\n加权轮询负载均衡。\n\n轮询就是把请求依次分配给每个服务提供者。加权轮询就是在轮询的基础上，让更多的请求落到权重更大的服务提供者上。比如假如有两个提供相同服务的服务器 S1,S2，S1 的权重为 7，S2 的权重为 3。\n\n如果我们有 10 次请求，那么 7 次会被 S1 处理，3 次被 S2 处理。\n\n但是，如果是 `RandomLoadBalance` 的话，很可能存在 10 次请求有 9 次都被 S1 处理的情况（概率性问题）。\n\nDubbo 中的 `RoundRobinLoadBalance` 的代码实现被修改重建了好几次，Dubbo-2.6.5 版本的 `RoundRobinLoadBalance` 为平滑加权轮询算法。\n\n## [#](#dubbo-序列化协议) Dubbo 序列化协议\n\n### [#](#dubbo-支持哪些序列化方式呢) Dubbo 支持哪些序列化方式呢？\n\n![Dubbo 支持的序列化协议](statistic/Dubbo_支持的序列化协议.png)\n\nDubbo 支持的序列化协议\n\nDubbo 支持多种序列化方式：JDK 自带的序列化、hessian2、JSON、Kryo、FST、Protostuff，ProtoBuf 等等。\n\nDubbo 默认使用的序列化方式是 hessian2。\n\n### [#](#谈谈你对这些序列化协议了解) 谈谈你对这些序列化协议了解？\n\n一般我们不会直接使用 JDK 自带的序列化方式。主要原因有两个：\n\n1. **不支持跨语言调用** : 如果调用的是其他语言开发的服务的时候就不支持了。\n2. **性能差**：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。\n\nJSON 序列化由于性能问题，我们一般也不会考虑使用。\n\n像 Protostuff，ProtoBuf、hessian2 这些都是跨语言的序列化方式，如果有跨语言需求的话可以考虑使用。\n\nKryo 和 FST 这两种序列化方式是 Dubbo 后来才引入的，性能非常好。不过，这两者都是专门针对 Java 语言的。Dubbo 官网的一篇文章中提到说推荐使用 Kryo 作为生产环境的序列化方式。\n\nDubbo 官方文档中还有一个关于这些[序列化协议的性能对比图open in new window](https://dubbo.apache.org/zh/docs/v2.7/user/serialization/#m-zhdocsv27userserialization)可供参考。\n\n![序列化协议的性能对比](statistic/序列化协议的性能对比.png)\n\n序列化协议的性能对比\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/distributed-system/rpc/dubbo.html#%E8%B0%88%E8%B0%88%E4%BD%A0%E5%AF%B9%E8%BF%99%E4%BA%9B%E5%BA%8F%E5%88%97%E5%8C%96%E5%8D%8F%E8%AE%AE%E4%BA%86%E8%A7%A3","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":[]},"/Obsidian/Front-Matter":{"title":"Front Matter","content":"\n\n使用 Front Matter 可以保存 note 待元数据，推荐使用 Hugo 的配置 [Front matter | Hugo (gohugo.io)](https://gohugo.io/content-management/front-matter/)\n\n","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/Obsidian/Obsidian-plugin":{"title":"Obsidian-plugin","content":"\n* [advanced-table](https://github.com/tgrosinger/advanced-tables-obsidian)\n* [banners](https://github.com/noatpad/obsidian-banners)\n* [calendar](https://github.com/liamcain/obsidian-calendar-plugin)\n* [commander](https://github.com/phibr0/obsidian-commander)\n* [dataview](https://github.com/blacksmithgu/obsidian-dataview)\n* [emoji-shortcodes](https://github.com/phibr0/obsidian-emoji-shortcodes)\n* [emoji-toolbar](https://github.com/oliveryh/obsidian-emoji-toolbar)\n* [excel-to-markdown-table](https://github.com/ganesshkumar/obsidian-excel-to-markdown-table)\n* [homepage](https://github.com/mirnovov/obsidian-homepage)\n* [hover-editor](https://github.com/nothingislost/obsidian-hover-editor)\n* [icon-folder)](https://github.com/FlorianWoelki/obsidian-icon-folder)\n* [icons](https://github.com/visini/obsidian-icons-plugin)\n* [image-toolkit](https://github.com/sissilab/obsidian-image-toolkit)\n* [minimal-settings](https://github.com/kepano/obsidian-minimal-settings)\n* [obsidian-git](https://github.com/denolehov/obsidian-git)\n* [recent-files](https://github.com/tgrosinger/recent-files-obsidian)\n* [settings-search](https://github.com/javalent/settings-search)\n* [style-settings](https://github.com/mgmeyers/obsidian-style-settings)\n* [tag-wrangler](https://github.com/pjeby/tag-wrangler)\n* [excalidraw](https://github.com/zsviczian/obsidian-excalidraw-plugin)","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/Obsidian/dataview":{"title":"dataview","content":"\n\n# 官方地址\n\n* [代码仓库](https://github.com/blacksmithgu/obsidian-dataview)\n* [文档地址](https://blacksmithgu.github.io/obsidian-dataview/)\n\n# 其他教程\n*  [Obsidian DataView 入门保姆级引导手册](https://zhuanlan.zhihu.com/p/614881764)\n\n# 元数据\n\n元数据是一系列的键值对,可以给笔记，可以给note,list item ,task 添加元数据\n\n## 如何添加元数据\n\n### Frontmatter\n\n* frontmatter 是markdown的一种扩展，可以使用yaml 来添加元数据\n\n```\n --- \n alias: \"document\" \n last-reviewed: 2021-08-17 \n thoughts: \n\t rating: 8 \n\t reviewable: false \n ---\n```\n\n###  inline fields\n* 使用方法为在文件的任意位置添加\n```text\n\nBasic Field:: Some random Value \n**Bold Field**:: Nice!\n  \n```\n\n* 如果你需要标注list itme 或者 task 需要使用中括号\n```\n- [ ] Send an mail to David about the deadline [due:: 2022-04-05].\n```\n\n\n# 另外还有隐含的元数据\n\n## page 中的元数据\n\n\n[# Metadata on Pages](https://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-pages/)\n\n\n| Field Name       | Data Type      | Description                                                                                                                                                                   |\n|------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| file.name        | Text           | The file name as seen in Obsidians sidebar.                                                                                                                                   |\n| file.folder      | Text           | The path of the folder this file belongs to.                                                                                                                                  |\n| file.path        | Text           | The full file path, including the files name.                                                                                                                                 |\n| file.ext         | Text           | The extension of the file type; generally md.                                                                                                                                 |\n| file.link        | Link           | A link to the file.                                                                                                                                                           |\n| file.size        | Number         | The size (in bytes) of the file.                                                                                                                                              |\n| file.ctime       | Date with Time | The date that the file was created.                                                                                                                                           |\n| file.cday        | Date           | The date that the file was created.                                                                                                                                           |\n| file.mtime       | Date with Time | The date that the file was last modified.                                                                                                                                     |\n| file.mday        | Date           | The date that the file was last modified.                                                                                                                                     |\n| file.tags        | List           | A list of all unique tags in the note. Subtags are broken down by each level, so #Tag/1/A will be stored in the list as [#Tag, #Tag/1, #Tag/1/A].                             |\n| file.etags       | List           | A list of all explicit tags in the note; unlike file.tags, does not break subtags down, i.e. [#Tag/1/A]                                                                       |\n| file.inlinks     | List           | A list of all incoming links to this file, meaning all files that contain a link to this file.                                                                                |\n| file.outlinks    | List           | A list of all outgoing links from this file, meaning all links the file contains.                                                                                             |\n| file.aliases     | List           | A list of all aliases for the note as defined via the YAML frontmatter.                                                                                                       |\n| file.tasks       | List           | A list of all tasks (I.e., \\| [ ] some task) in this file.                                                                                                                    |\n| file.lists       | List           | A list of all list elements in the file (including tasks); these elements are effectively tasks and can be rendered in task views.                                            |\n| file.frontmatter | List           | Contains the raw values of all frontmatter in form of key \\| value text values; mainly useful for checking raw frontmatter values or for dynamically listing frontmatter keys. |\n| file.day         | Date           | Only available if the file has a date inside its file name (of form yyyy-mm-dd or yyyymmdd), or has a Date field/inline field.                                                |\n| file.starred     | Boolean        | if this file has been starred via the Obsidian Core Plugin \"Starred Files\".                                                                                                   |\n\n\n## 列表和任务中的元数据\n\n[# Metadata on Tasks and Lists](https://blacksmithgu.github.io/obsidian-dataview/annotation/metadata-tasks/)\n\n| Field name     | Data Type | Description                                                                                                                                                                                                                                                                                               |\n|----------------|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| status         | Text      | The completion status of this task, as determined by the character inside the [ ] brackets. Generally a space \" \" for incomplete tasks and a \"x\" for complete tasks, but allows for plugins which support alternative task statuses.                                                                      |\n| checked        | Boolean   | Whether or not this task status is empty, meaning it has a space in its [ ] brackets                                                                                                                                                                                                                      |\n| completed      | Boolean   | Whether or not this specific task has been completed; this does not consider the completionnon-completion of any child tasks. A task is explicitly considered \"completed\" if it has been marked with an 'x'. If you use a custom status, i.e. [-], checked will be true, whereas completed will be false. |\n| fullyCompleted | Boolean   | Whether or not this task and all of its subtasks are completed.                                                                                                                                                                                                                                           |\n| text           | Text      | The plain text of this task, including any metadata field annotations.                                                                                                                                                                                                                                    |\n| visual         | Text      | The text of this task, which is rendered by Dataview. It can be modified to render arbitary text.                                                                                                                                                                                                         |\n| line           | Number    | The line of the file this task shows up on.                                                                                                                                                                                                                                                               |\n| lineCount      | Number    | The number of Markdown lines that this task takes up.                                                                                                                                                                                                                                                     |\n| path           | Text      | The full path of the file this task is in. Equals to file.path for pages                                                                                                                                                                                                                                  |\n| section        | Link      | link to the section this task is contained in.                                                                                                                                                                                                                                                            |\n| tags           | List      | Any tags inside of the text task.                                                                                                                                                                                                                                                                         |\n| outlinks       | List      | Any links defined in this task.                                                                                                                                                                                                                                                                           |\n| link           | Link      | link to the closest linkable block near this task; useful for making links which go to the task.                                                                                                                                                                                                          |\n| children       | List      | ny subtasks or sublists of this task.                                                                                                                                                                                                                                                                     |\n| task           | Boolean   | If true, this is a task; otherwise, it is a regular list element.                                                                                                                                                                                                                                         |\n| annotated      | Boolean   | True if the task text contains any metadata fields, false otherwise.                                                                                                                                                                                                                                      |\n| parent         | Number    | The line number of the task above this task, if present; will be null if this is a root-level task.                                                                                                                                                                                                       |\n| blockId        | Text      | The block ID of this task / list element, if one has been defined with the ^blockId syntax; otherwise null.                                                                                                                                                                                               |\n\n\n# DQL\n\n比较类似于sql, 可是实现以下的功能\n\n- Choosing an **output format** of your output (the [Query Type](https://blacksmithgu.github.io/obsidian-dataview/queries/query-types/))\n- Fetch pages **from a certain [source](https://blacksmithgu.github.io/obsidian-dataview/reference/sources/)**, i.e. a tag, folder or link\n- **Filtering pages/data** by simple operations on fields, like comparison, existence checks, and so on\n- **Transforming fields** for displaying, i.e. with calculations or splitting up multi-value fields\n- **Sorting** results based on fields\n- **Grouping** results based on fields\n- **Limiting** your result count\n\n## 查询语法\n\n```text\n```dataview \n\t\u003cQUERY-TYPE\u003e \u003cfields\u003e \n\tFROM \u003csource\u003e \n\t\u003cDATA-COMMAND\u003e \u003cexpression\u003e \n\t\u003cDATA-COMMAND\u003e \u003cexpression\u003e \n\t...\n```\n```\t\n```\n\n\n## 输出类型\n\n* **TABLE**: A table of results with one row per result and one or many columns of **field data**.\n* **LIST**: A bullet point list of **pages** which match the query. You can output one field for each page alongside their file links.\n* **TASK**: An interactive task list of **tasks** that match the given query.\n* **CALENDAR**: A calendar view displaying each hit via a dot on its referred date.\n\n\n```text\nLists all pages in your vault as a bullet point list\n\t```dataview \n\tLIST \n\t```\n\t\nLists all tasks (completed or not) in your vault \n\t```dataview \n\tTASK \n\t```\n\t\nRenders a Calendar view where each page is represented as a dot on its creation date. \n\t```dataview \n\tCALENDAR file.cday \n\t```\n\t\nShows a table with all pages of your vault, their field value of due, the files' tags and an average of the values of multi-value field working-hours \n\t```dataview \n\tTABLE due, file.tags AS \"tags\", average(working-hours)\n\t ```\n\n```\n\n\n## 数据来源\n\n* tags\n* folders\n* note\n* lint\n\n```\nLists all pages inside the folder Books and its sub folders \n\t```dataview \n\tLIST FROM \"Books\" \n\t``` \n\t\nLists all pages that include the tag #status/open or #status/wip \n\t```dataview \n\tLIST FROM #status/open OR #status/wip \n\t``` \n\t\nLists all pages that have either the tag #assignment and are inside folder \"30 School\" (or its sub folders), or are inside folder \"30 School/32 Homeworks\" and are linked on the page School Dashboard Current To Dos \n\n\t```dataview \n\tLIST FROM (#assignment AND \"30 School\") OR (\"30 School/32 Homeworks\" AND outgoing([[School Dashboard Current To Dos]])) \n\t```\n\n```\n\n\n## Filter, sort, group or limit results\n\n* ***FROM** like explained [above](https://blacksmithgu.github.io/obsidian-dataview/queries/structure/#choose-your-source).\n*  **WHERE**: Filter notes based on information **inside** notes, the meta data fields.\n*  **SORT**: Sorts your results depending on a field and a direction.\n*  **GROUP BY**: Bundles up several results into one result row per group.\n*  **LIMIT**: Limits the result count of your query to the given number.\n*  **FLATTEN**: Splits up one result into multiple results based on a field or calculation.\n\n\n```\nLists all pages that have a metadata field `due` and where `due` is before today \n\n\t```dataview \n\tLIST WHERE due AND due \u003c date(today) \n\t``` \nLists the 10 most recently created pages in your vault that have the tag #status/open \n\t```dataview \n\tLIST FROM #status/open SORT file.ctime DESC LIMIT 10 \n\t``` \nLists the 10 oldest and incompleted tasks of your vault as an interactive task list, grouped by their containing file and sorted from oldest to newest file. \n\t```dataview \n\tTASK WHERE !completed SORT created ASC LIMIT 10 GROUP BY file.link SORT rows.file.ctime ASC \n\t```\n\n\n```","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/Obsidian/excalidraw":{"title":"excalidraw","content":"\n*  [代码仓库](https://github.com/zsviczian/obsidian-excalidraw-plugin)\n* note 中插入excalidraw 语法\n\n```\n![[excalidraw]]\n```","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/Obsidian/obsidian-overview":{"title":"obsidian overview","content":"\n# 主页内容\n\nobsidian 相关内容，包括插件\n\n\n# 结构\n\n","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/Obsidian/publish":{"title":"publish","content":"\n\n\n[obsidian 目前最完美的免费发布方案 渐进式教程 by oldwinter](https://publish.obsidian.md/chinesehelp/01+2021%E6%96%B0%E6%95%99%E7%A8%8B/obsidian+%E7%9B%AE%E5%89%8D%E6%9C%80%E5%AE%8C%E7%BE%8E%E7%9A%84%E5%85%8D%E8%B4%B9%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88+%E6%B8%90%E8%BF%9B%E5%BC%8F%E6%95%99%E7%A8%8B+by+oldwinter#%E5%87%A0%E4%B8%AA%E5%8F%91%E5%B8%83%E6%96%B9%E6%A1%88%E5%AF%B9%E6%AF%94)","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/Obsidian/template":{"title":"template","content":"\n* *[模板的使用方法](https://publish.obsidian.md/help-zh/%E6%8F%92%E4%BB%B6/%E6%A8%A1%E6%9D%BF)\n* 默认存放的文件夹 `/template`\n\n","lastmodified":"2023-08-01T17:34:25.780139793Z","tags":["Obsidian"]},"/awesome/JavaGuide-%E7%9F%A5%E8%AF%86%E6%98%9F%E7%90%83%E4%BC%98%E8%B4%A8%E4%B8%BB%E9%A2%98%E6%B1%87%E6%80%BB":{"title":"JavaGuide 知识星球优质主题汇总","content":"为了避免这里成为知识杂货铺，我会对严格筛选入选的优质主题。  \n  \n更新日期：2023-06-11  \n  \n\n面试  \n  \n●[Java面试常见问题总结（2023最新版）](https://t.zsxq.com/0eRq7EJPy)  \n●[一位 HR 分享的求职建议](https://t.zsxq.com/0dSKX0jkK)  \n●[面试和简历上的一些大忌](https://t.zsxq.com/0eOgYt3qU)  \n●项目：  \n○[如何回答项目遇到什么困难，如何解决这类问题](https://t.zsxq.com/0dduy9CeQ)  \n○[项目太简单怎么办?](https://t.zsxq.com/0eV4BksDb)  \n○[商城项目到底能不能做？](https://t.zsxq.com/0eBcCNhbB)  \n  \n\n安抚心态  \n  \n如果你陷入精神内耗或者自我怀疑，不然看看下面这些内容：  \n  \n●[编程找工作现状 - 哔哩哔哩](https://t.zsxq.com/0edKnfcZW)  \n  \n\n技术资源  \n  \n学习路线：[Java 系统学习路线](https://t.zsxq.com/0dupYAEaq)  \n  \n总结 ：  \n  \n●[Java 后端开发常用的技术书籍+原创面试资料 PDF 版本](https://t.zsxq.com/0bWeUrBVq)  \n●[JavaGuide 网站总结的八股文合集  - 念神](https://t.zsxq.com/0biGG9UlX)  \n●[Java 后端常见知识点思维导图分享 - 吴不卷](https://t.zsxq.com/0bHk3wEDs)  \n  \n常用技术：  \n  \n●[SpringBoot 学习资源推荐](https://t.zsxq.com/0eEGBV1Md)  \n●[单测技术选型+学习资源推荐](https://t.zsxq.com/0d7jOz9Vm)  \n●[Redis 学习资源推荐](https://t.zsxq.com/0dwd4ONZ9)  \n●[Elasticsearch 学习资源推荐](https://t.zsxq.com/0dEWEThKR)  \n●[Kafka、RocketMQ、RabbitMQ 学习资源推荐](https://t.zsxq.com/0bEDFwgon)  \n●[分布式学习资源推荐（偏理论方向）](https://t.zsxq.com/0euwZ8uiP)  \n●[Git 学习资源推荐](https://t.zsxq.com/0bTheL01q)  \n●[《阿里开发者手册 - Redis 专题》PDF 文档](https://t.zsxq.com/0bEiLJIVW)  \n  \n\n代码质量  \n  \n●[24 个写出漂亮代码的小技巧](https://t.zsxq.com/0foGrZIc7)  \n●[程序员“起名”头痛根治指南](https://t.zsxq.com/0d8ODCefj)  \n●[5天带你读完《Effective Java》](https://t.zsxq.com/0dIkpk8AV)  \n●[提高代码质量的书籍和文章推荐](https://t.zsxq.com/0dWEHSiBW)  \n●[一个练习重构的开源教程](https://t.zsxq.com/0d221QNao)  \n●[分享3本对于提高代码质量有实际帮助的书籍](https://t.zsxq.com/0dFLaE2Lp)（《编写可读代码的艺术》、《Clean Code》、《The Clean Coder》）  \n  \n\n进阶攻略  \n  \n●[如何撰写一份令人赞叹的软件专利技术交底书？](https://articles.zsxq.com/id_2kdw0o0ovc44.html)  \n●[校招生如何参与开源项目？如何获得开源经历？](https://articles.zsxq.com/id_q0g14e71eqc3.html)  \n●[一些读书心得和看书做笔记的经验](https://t.zsxq.com/0cpx9pkIE)  \n●[一个关于提升学习能力和效率的视频](https://t.zsxq.com/0c2OboF1Q)（收益匪浅）  \n●[如何有效提升专注力？](https://t.zsxq.com/0eIuSBARU)  \n●[快速熟悉业务逻辑并付诸落地的建议 - 念神](https://t.zsxq.com/0cGu9HjPQ)  \n●[给初级 Java 工程师的一些学习建议 - 念神](https://t.zsxq.com/0ckNvT31a)  \n●[项目技术选型的建议](https://t.zsxq.com/0ciPGdBoZ)（听了一个技术选型分享之后的一些心得体会）  \n●[使用 Google 搜索的实用建议](https://t.zsxq.com/0c0K3zPRk)  \n●[不要把自己局限在技术上!](https://t.zsxq.com/0cdHCFWNw) （重视技术能力，但你的世界不能仅仅只有技术）  \n●[给一些想要换职业方向的朋友一些客观的建议](https://t.zsxq.com/0crTLD2hY)  \n●[如何做编程知识投资及减少知识失效的影响](https://t.zsxq.com/0ccagnzao) （感触很深的一篇文章，强烈推荐阅读）  \n●[碎片化知识可能会带来的坏处](https://t.zsxq.com/0cdfq7iR4)（碎片化知识泛滥的时代，应该注意其对自身的影响）  \n  \n\n开源项目  \n  \n●[一个简易版的IoC的轮子（球友自制，附笔记）](https://t.zsxq.com/0eCVtaUND)  \n●[Java 语言手写的一款简易版 Git（球友自制）](https://t.zsxq.com/0ekLd7XaX)  \n●[基于 SpringBoot 的国密前后端分离快速开发平台](https://t.zsxq.com/0b3wlSfjS)  \n●[《高并发的哲学原理》开源图书](https://t.zsxq.com/0bmQXO4bf)  \n●[zyplayer-doc：适合团队和个人使用的WIKI文档管理工具，同时还包含数据库文档、Api接口文档。](https://t.zsxq.com/0bcfjG15v)  \n●[《深入理解 Java 虚拟机》阅读笔记，基于第二版（目前最新版是第三版）](https://t.zsxq.com/0b1CDmh5H)  \n●[think：一款开源知识管理工具，支持创建知识库、多人协作、分享知识库、绘制思维导图、添加附加附件等功能](https://wx.zsxq.com/dweb2/index/group/48418884588288)  \n●[ip2region：高性能离线IP地址定位库，10微秒级别的查询效率，开箱即用，提供了多种主流编程语言（如 Go，Java，Python）的 xdb 数据生成和查询客户端 API。](https://t.zsxq.com/0b6BYX1rp)  \n●[novel：一套基于时下最新 Java 技术栈 Spring Boot 3 + Vue 3 开发的前后端分离学习型小说项目](https://t.zsxq.com/0b71m4luD)  \n●[lu-raft-kv：分布式 KV 存储轮子](https://t.zsxq.com/0baOCvT01)  \n●[MYDB：简易版数据库](https://t.zsxq.com/0b0d5pFHt)  \n●[mini-spring-cloud：手写的简化版的 Spring Cloud](https://t.zsxq.com/0bai0TuJX)  \n●[SurveyKing：号称功能最强大的调查问卷系统和考试系统](https://t.zsxq.com/0bJaH3GSP)  \n●[cs-self-learning：计算机自学指南](https://t.zsxq.com/0biDtoOF7)  \n●[upupor：小众但是功能强大的开源社区](https://t.zsxq.com/0bZrBeqMg)  \n●[Easy-Es： Elasticsearch 工具库](https://t.zsxq.com/0bWdejp6D)  \n  \n\n工具网站  \n  \n●[两个巨好用的 Linux 命令网站](https://t.zsxq.com/0eKkOJPDA)  \n●[GitHub Web IDE：直接通过多种在线 IDE 打开Github项目](https://t.zsxq.com/0eDXSZsBw)","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":[]},"/elastic/KQL":{"title":"KQL","content":"\nKibana  Query Language\n\n\nhttps://juejin.cn/post/7003201901382598686\n\n\nhttps://www.elastic.co/guide/en/kibana/7.14/kuery-query.html#kuery-query\n\n\n","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":[]},"/lua/Lua%E9%AB%98%E7%BA%A7":{"title":"Lua高级","content":"\n# 元表\n\n元表 _(metatable)_ 的**表现行为类似于 C++ 语言中的操作符重载**，例如我们可以重载 \"__add\" 元方法 _(metamethod)_，来计算两个 Lua 数组的并集；或者重载 \"__index\" 方法，来定义我们自己的 Hash 函数。Lua 提供了两个十分重要的用来处理元表的方法\n\n- setmetatable(table, metatable)：此方法用于为一个表设置元表。\n    \n- getmetatable(table)：此方法用于获取表的元表对象\n    \n\n设置元表\n\n```Lua\nlocal mytable = {}\nlocal mymetatable = {}\nsetmetatable(mytable, mymetatable)\n```\n\n  \n\n## **修改表的操作符行为**\n\n  \n\n通过重载 \"__add\" 元方法来计算集合的并集实例\n\n```Lua\nlocal set1 = {10, 20, 30}   -- 集合\nlocal set2 = {20, 40, 50}   -- 集合\n\n-- 将用于重载__add的函数，注意第一个参数是self\nlocal union = function (self, another)\n    local set = {}\n    local result = {}\n\n    -- 利用数组来确保集合的互异性\n    for i, j in pairs(self) do set[j] = true end\n    for i, j in pairs(another) do set[j] = true end\n\n    -- 加入结果集合\n    for i, j in pairs(set) do table.insert(result, i) end\n    return result\nend\nsetmetatable(set1, {__add = union}) -- 重载 set1 表的 __add 元方法\n\nlocal set3 = set1 + set2\nfor _, j in pairs(set3) do\n    io.write(j..\" \")               --\u003eoutput：30 50 20 40 10\nend\n```\n\n除了加法可以被重载之外，Lua 提供的所有操作符都可以被重载：\n| 元方法        | 含义                                                                         |\n|------------|----------------------------------------------------------------------------|\n| \"__add    | #NAME?                                                                     |\n| \"__sub   | - 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__mul    | * 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__div   | / 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__mod   | % 操作 其行为类似于 \"add\" 操作                                                       |\n| \"__pow    | ^ （幂）操作 其行为类似于 \"add\" 操作                                                    |\n| \"__unm\"   | 一元 - 操作                                                                    |\n| \"__concat\" | .. （字符串连接）操作                                                               |\n| \"__len\"    | # 操作                                                                       |\n| \"__eq\"     | == 操作 函数 getcomphandler 定义了 Lua 怎样选择一个处理器来作比较操作 仅在两个对象类型相同且有对应操作相同的元方法时才起效 |\n| \"__lt\"     | \u003c 操作                                                                       |\n| \"__le\"     | \u003c= 操作                                                                      |\n\n\n除了操作符之外，如下元方法也可以被重载，下面会依次解释使用方法：\n\n|   |   |\n|---|---|\n|元方法|含义|\n|\"__index\"|取下标操作用于访问 table[key]|\n|\"__newindex\"|赋值给指定下标 table[key] = value|\n|\"__tostring\"|转换成字符串|\n|\"__call\"|当 Lua 调用一个值时调用|\n|\"__mode\"|用于弱表(week table)|\n|\"__metatable\"|用于保护metatable不被访问|\n\n## **__index 元方法**\n\n```Lua\nmytable = setmetatable({key1 = \"value1\"},   --原始表\n{__index = function(self, key)            --重载函数\n    if key == \"key2\" then\n        return \"metatablevalue\"\n    end\nend\n})\n\nprint(mytable.key1,mytable.key2)  --\u003e output：value1 metatablevalue\n```\n\n关于 __index 元方法，有很多比较高阶的技巧，例如：__index 的元方法不需要非是一个函数，他也可以是一个表。\n\n```Lua\nt = setmetatable({[1] = \"hello\"}, {__index = {[2] = \"world\"}})\nprint(t[1], t[2])   --\u003ehello wor\n```\n\n## **__tostring 元方法**\n\n  \n\n与 Java 中的 toString() 函数类似，可以实现自定义的字符串转换。\n\n```Lua\narr = {1, 2, 3, 4}\narr = setmetatable(arr, {__tostring = function (self)\n    local result = '{'\n    local sep = ''\n    for _, i in pairs(self) do\n        result = result ..sep .. i\n        sep = ', '\n    end\n    result = result .. '}'\n    return result\nend})\nprint(arr)  --\u003e {1, 2, 3, 4}\n```\n\n## **__call 元方法**\n\n__call 元方法的功能类似于 C++ 中的仿函数，使得普通的表也可以被调用。\n\n  \n\n```Lua\nfunctor = {}\nfunction func1(self, arg)\n    print (\"called from\", arg)\nend\nsetmetatable(functor, {__call = func1})\n\nfunctor(\"functor\")  --\u003e called from functor\nprint(functor)      --\u003e output：0x00076fc8 （后面这串数字可能不一样）\n```\n\n## **__metatable 元方法**\n\n假如我们想保护我们的对象使其使用者既看不到也不能修改 metatables。我**们可以对 metatable 设置了 __metatable 的值，getmetatable 将返回这个域的值，而调用 setmetatable 将会出错**：\n\n```Lua\nbject = setmetatable({}, {__metatable = \"You cannot access here\"})\n\nprint(getmetatable(Object)) --\u003e You cannot access heresetmetatable(Object, {})    --\u003e 引发编译器报错\n```\n\n  \n\n# 面向对象\n\n## 类\n\n在 Lua 中，我们可以使用表和函数实现面向对象。**将函数和相关的数据放置于同一个表中就形成了一个对象。**\n\n```Plaintext\nlocal _M = {}\n\nlocal mt = { __index = _M }\n\nfunction _M.deposit (self, v)\n    self.balance = self.balance + v\nend\n\nfunction _M.withdraw (self, v)\n    if self.balance \u003e v then\n        self.balance = self.balance - v\n    else\n        error(\"insufficient funds\")\n    end\nend\n\nfunction _M.new (self, balance)\n    balance = balance or 0\n    return setmetatable({balance = balance}, mt)\nend\n\nreturn _M\n```\n\n引用\n\n```Lua\nlocal account = require(\"account\")\n\nlocal a = account:new()\na:deposit(100)\n\nlocal b = account:new()\nb:deposit(50)\n\nprint(a.balance)  --\u003e output: 100\nprint(b.balance)  --\u003e output: 50\n```\n\n上面这段代码 \"setmetatable({balance = balance}, mt)\"，其中 mt 代表 `{ __index = _M }` ，这句话值得注意。根据我们在元表这一章学到的知识，我们明白，setmetatable 将 `_M` 作为新建表的原型，所以在自己的表内找不到 'deposit'、'withdraw' 这些方法和变量的时候，便会到 __index 所指定的 _M 类型中去寻找。\n\n  \n\n## 继承\n\n继承可以用元表实现，它提供了在父类中查找存在的方法和变量的机制。在 Lua 中是不推荐使用继承方式完成构造的，这样做引入的问题可能比解决的问题要多，下面一个是字符串操作类库，给大家演示一下。\n\n```Lua\n---------- s_base.lualocal _M = {}\n\nlocal mt = { __index = _M }\n\nfunction _M.upper (s)return string.upper(s)\nendreturn _M\n\n---------- s_more.lualocal s_base = require(\"s_base\")\n\nlocal _M = {}\n_M = setmetatable(_M, { __index = s_base })\n\n\nfunction _M.lower (s)return string.lower(s)\nendreturn _M\n\n---------- test.lualocal s_more = require(\"s_more\")\n\nprint(s_more.upper(\"Hello\"))   -- output: HELLOprint(s_more.lower(\"Hello\"))   -- output: hello\n```\n\n  \n\n## 成员私有性\n\n在动态语言中引入成员私有性并没有太大的必要，反而会显著增加运行时的开销，毕竟这种检查无法像许多静态语言那样在编译期完成。下面的技巧把对象作为各方法的 upvalue，本身是很巧妙的，但会让子类继承变得困难，同时构造函数动态创建了函数，会导致构造函数无法被 JIT 编译。\n\n在 Lua 中，成员的私有性，使用类似于函数闭包的形式来实现。在我们之前的银行账户的例子中，我们使用一个工厂方法来创建新的账户实例，通过工厂方法对外提供的闭包来暴露对外接口。而不想暴露在外的例如 balance 成员变量，则被很好的隐藏起来。\n\n```Lua\nfunction newAccount (initialBalance)\n    local self = {balance = initialBalance}\n    local withdraw = function (v)\n        self.balance = self.balance - v\n    end\n    local deposit = function (v)\n        self.balance = self.balance + v\n    end\n    local getBalance = function () \n        return self.balance \n    end\n    \n    return {\n        withdraw = withdraw,\n        deposit = deposit,\n        getBalance = getBalance\n    }\nend\n\na = newAccount(100)\na.deposit(100)\nprint(a.getBalance()) --\u003e 200print(a.balance)      --\u003e nil\n```\n\n  \n\n# 局部变量\n\nLua 的设计有一点很奇怪，**在一个 block 中的变量，如果之前没有定义过，那么认为它是一个全局变量**，**而不是这个 block 的局部变量**。这一点和别的语言不同。**容易造成不小心覆盖了全局同名变量的错误**。\n\n## **定义**\n\nLua 中的局部变量要用 local 关键字来显式定义，不使用 local 显式定义的变量就是全局变量\n\n```Lua\ng_var = 1         -- global var\nlocal l_var = 2   -- local var\n```\n\n## **作用域**\n\n**局部变量的生命周期是有限的，它的作用域仅限于声明它的块（block）**。一个块是一个控制结构的执行体、或者是一个函数的执行体再或者是一个程序块（chunk）。\n\n```Lua\nx = 10\nlocal i = 1         -- 程序块中的局部变量 i\n\nwhile i \u003c=x do\n    local x = i * 2   -- while 循环体中的局部变量 x\n    print(x)          -- output： 2, 4, 6, 8, ...\n    i = i + 1\nend\n\nif i \u003e 20 then\n    local x           -- then 中的局部变量 x\n    x = 20\n    print(x + 2)      -- 如果i \u003e 20 将会打印 22，此处的 x 是局部变量\nelse\n    print(x)          -- 打印 10，这里 x 是全局变量\nend\n\nprint(x)            -- 打印 10\n```\n\n  \n\n## 使用局部变量的好处\n\n  \n\n1. 局部变量可以避免因为命名问题污染了全局环境\n    \n2. local 变量的访问比全局变量更快\n    \n3. 由于局部变量出了作用域之后生命周期结束，这样可以被垃圾回收器及时释放\n    \n\n  \n\n  \n\n## 检测模块的函数使用局部变量\n\nfoo.lua\n\n```Lua\nlocal _M = { _VERSION = '0.01' }\n\nfunction _M.add(a, b)     --两个number型变量相加\n    return a + b\nend\n\nfunction _M.update_A()    --更新变量值\n    A = 365               -- A 是全局变量\nend\n\nreturn _M\n```\n\nuse_foo.lua\n\n```Lua\nA = 360     --定义全局变量\n\nlocal foo = require(\"foo\")\n\nlocal b = foo.add(A, A)\nprint(\"b = \", b)\n\nfoo.update_A()\nprint(\"A = \", A)\n```\n\n因为A 是全局变量，改变了A的值\n\nLua 上下文中应当严格避免使用自己定义的全局变量。**可以使用一个 lj-releng 工具来扫描 Lua 代码，定位使用 Lua 全局变量的地方**。lj-releng 的相关链接：[https://github.com/openresty/openresty-devel-utils/blob/master/lj-releng](https://github.com/openresty/openresty-devel-utils/blob/master/lj-releng)\n\nWindows 用户把 lj-releng 文件所在的目录的绝对路径添加进 PATH 环境变量。然后进入你自己的 Lua 文件所在的工作目录，得到如下结果：\n\n```Lua\n#  lj-releng\nfoo.lua: 0.01 (0.01)\nChecking use of Lua global variables in file foo.lua...\nop no.  line  instruction args  ; code\n2  [8] SETGLOBAL 0 -1  ; A\nChecking line length exceeding 80...\nWARNING: No \"_VERSION\" or \"version\" field found in `use_foo.lua`.\nChecking use of Lua global variables in file use_foo.lua...\nop no.  line  instruction args  ; code\n2  [1] SETGLOBAL 0 -1  ; A\n7  [4] GETGLOBAL 2 -1  ; A\n8  [4] GETGLOBAL 3 -1  ; A\n18 [8] GETGLOBAL 4 -1  ; A\n```\n\n当然，更推荐采用 **luacheck 来检查项目中全局变量，之后的“代码静态分析”一节，我们还会讲到如何使用 luacheck**。\n\n  \n\n# 判断数组的大小\n\n- table.getn(t) 等价于 t 但**计算的是数组元素，不包括 hash 键值**。而且数组是以第一个 nil 元素来判断数组结束。\n    \n- `#` 只计算 array 的元素个数，它实际上调用了对象的 metatable 的 `__len` 函数。对于有 `__len` 方法的函数返回函数返回值，不然就返回数组成员数目\n    \n- _Lua_ 内部实际采用哈希表和数组分别保存键值对、普通值，所以不推荐混合使用这两种赋值方式。\n    \n- Lua 数组中允许 nil 值的存在，但是数组默认结束标志却是 nil。这类比于 C 语言中的字符串，字符串中允许 '\\0' 存在，但当读到 '\\0' 时，就认为字符串已经结束了。\n    \n- 初始化是例外，在 Lua 相关源码中，初始化数组时首先判断数组的长度，若长度大于 0 ，并且最后一个值不为 nil，返回包括 nil 的长度；若最后一个值为 nil，则返回截至第一个非 nil 值的长度。\n    \n- **如果你要删除一个数组中的元素，请使用 remove 函数，而不是用 nil 赋值**\n    \n\n```Lua\n-- test.lua\nlocal tblTest1 = { 1, a = 2, 3 }\nprint(\"Test1 \" .. #(tblTest1))\n\nlocal tblTest2 = { 1, nil }\nprint(\"Test2 \" .. #(tblTest2))\n\nlocal tblTest3 = { 1, nil, 2 }\nprint(\"Test3 \" .. #(tblTest3))\n\nlocal tblTest4 = { 1, nil, 2, nil }\nprint(\"Test4 \" .. #(tblTest4))\n\nlocal tblTest5 = { 1, nil, 2, nil, 3, nil }\nprint(\"Test5 \" .. #(tblTest5))\n\nlocal tblTest6 = { 1, nil, 2, nil, 3, nil, 4, nil }\nprint(\"Test6 \" .. #(tblTest6))\n```\n\n我们分别使用 Lua 和 LuaJIT 来执行一下：\n\n```Lua\n➜ luajit test.lua\nTest1 2\nTest2 1\nTest3 1\nTest4 1\nTest5 1\nTest6 1\n\n➜ lua test.lua\nTest1 2\nTest2 1\nTest3 3\nTest4 1\nTest5 3\nTest6 1\n```\n\n这一段的输出结果，就是这么 **匪夷所思**。不要在 Lua 的 table 中使用 nil 值，**如果一个元素要删除，直接 remove，不要用 nil 去代替**。\n\n  \n\n# 非空判断\n\n  \n\n有时候不小心引用了一个没有赋值的变量，这时它的值默认为 nil。如果对一个 nil 进行索引的话，会导致异常。\n\n```Plaintext\nlocal person = {name = \"Bob\", sex = \"M\"}\n\n-- do something\nperson = nil\n-- do something\nprint(person.name)\n```\n\n会报错\n\n```Lua\nstdin:1:attempt to index global 'person' (a nil value)\nstack traceback:\n   stdin:1: in main chunk\n   [C]: ?\n```\n\n在实际的工程代码中，我们很难这么轻易地发现我们引用了 nil 变量。因此，在很多情况下我们在访问一些 table 型变量时，需要先判断该变量是否为 nil，例如将上面的代码改成\n\n```Lua\nlocal person = {name = \"Bob\", sex = \"M\"}\n\n-- do something\nperson = nil\n-- do something\nif person ~= nil and person.name ~= nil then\n    print(person.name)\nelse\n-- do somethingend\n```\n\n对于简单类型的变量，我们可以用 _if (var == nil) then_ 这样的简单句子来判断。**但是对于 table 型的 Lua 对象，就不能这么简单判断它是否为空了。一个 table 型变量的值可能是** **`{}`**，这时它不等于 nil。我们来看下面这段代码：\n\n```Lua\nlocal next = next\nlocal a = {}\nlocal b = {name = \"Bob\", sex = \"Male\"}\nlocal c = {\"Male\", \"Female\"}\nlocal d = nil\n\nprint(#a)\nprint(#b)\nprint(#c)\n--print(#d)    -- error\n\nif a == nil then\n    print(\"a == nil\")\nend\n\nif b == nil then\n    print(\"b == nil\")\nend\n\nif c == nil then\n    print(\"c == nil\")\nend\n\nif d== nil then\n    print(\"d == nil\")\nend\n\nif next(a) == nil then\n    print(\"next(a) == nil\")\nend\n\nif next(b) == nil then\n    print(\"next(b) == nil\")\nend\n\nif next(c) == nil then\n    print(\"next(c) == nil\")\nend\n```\n\n输出\n\n```Lua\n0\n0\n2\nd == nil\nnext(a) == nil\n```\n\n因此，我们要判断一个 table 是否为 `{}`，不能采用 `#table == 0` 的方式来判断。可以用下面这样的方法来判断：\n\n```Plaintext\nfunction isTableEmpty(t)\n    return t == nil or next(t) == nil\nend\n```\n\n注意：**`next`** **指令是不能被 LuaJIT 的 JIT 编译优化，并且 LuaJIT 貌似没有明确计划支持这个指令优化，在不是必须的情况下，尽量少用。**\n\n  \n\n# 正则表达式\n\n同时存在两套正则表达式规范：_Lua_ 语言的规范和 `ngx.re.*` 的规范，即使您对 _Lua_ 语言中的规范非常熟悉，我们仍不建议使用 _Lua_ 中的正则表达式。\n\n- 一是因为 _Lua_ 中正则表达式的性能并不如 `ngx.re.*` 中的正则表达式优秀；\n    \n- 二是 _Lua_ 中的正则表达式并不符合 _POSIX_ 规范，而 `ngx.re.*` 中实现的是标准的 _POSIX_ 规范，后者明显更具备通用性。\n    \n\n`ngx.re.*` 中的 `o` 选项，指明该参数，被编译的 Pattern 将会在工作进程中缓存，并且被当前工作进程的每次请求所共享。Pattern 缓存的上限值通过 `lua_regex_cache_max_entries` 来修改，它的默认值为1024。\n\n`ngx.re.*` 中的 `j` 选项，指明该参数，如果使用的 PCRE 库支持 JIT，OpenResty 会在编译 Pattern 时启用 JIT。启用 JIT 后正则匹配会有明显的性能提升。较新的平台，自带的 PCRE 库均支持 JIT。如果系统自带的 PCRE 库不支持 JIT，出于性能考虑，最好自己编译一份 libpcre.so，然后在编译 OpenResty 时链接过去。要想验证当前 PCRE 库是否支持 JIT，可以这么做\n\n1. 编译 OpenResty 时在 `./configure` 中指定 `--with-debug` 选项\n    \n2. 在 `error_log` 指令中指定日志级别为 `debug`\n    \n3. 运行正则匹配代码，查看日志中是否有 `pcre JIT compiling result: 1`\n    \n\n即使运行在不支持 JIT 的 OpenResty 上，加上 `j` 选项也不会带来坏的影响。在 OpenResty 官方的 Lua 库中，正则匹配至少都会带上 `jo` 这两个选项。\n\n```Lua\nlocation /test {\n    content_by_lua_block {\n        local regex = [[\\d+]]\n\n        -- 参数 \"j\" 启用 JIT 编译，参数 \"o\" 是开启缓存必须的\n        local m = ngx.re.match(\"hello, 1234\", regex, \"jo\")\n        if m then\n            ngx.say(m[0])\n        else\n            ngx.say(\"not matched!\")\n        end\n    }\n}\n```\n\n#### **Lua 正则简单汇总**\n\n_Lua_ 中正则表达式语法上最大的区别，_Lua_ 使用 _'%'_ 来进行转义，而其他语言的正则表达式使用 _'\\'_ 符号来进行转义。其次，_Lua_ 中并不使用 _'?'_ 来表示非贪婪匹配，而是定义了不同的字符来表示是否是贪婪匹配。定义如下：\n\n|符号|匹配次数|匹配模式|\n|---|---|---|\n|+|匹配前一字符 1 次或多次|非贪婪|\n|`*`|匹配前一字符 0 次或多次|贪婪|\n|-|匹配前一字符 0 次或多次|非贪婪|\n|?|匹配前一字符 0 次或1次|仅用于此，不用于标识是否贪婪|\n\n|符号|匹配模式|\n|---|---|\n|.|任意字符|\n|%a|字母|\n|%c|控制字符|\n|%d|数字|\n|%l|小写字母|\n|%p|标点字符|\n|%s|空白符|\n|%u|大写字母|\n|%w|字母和数字|\n|%x|十六进制数字|\n|%z|代表 0 的字符|\n\n  \n\n# 虚变量\n\n当一个方法返回多个值时，有些返回值有时候用不到，要是声明很多变量来一一接收，显然不太合适（不是不能）。**Lua 提供了一个虚变量(dummy variable)的概念， 按照****[惯例](https://www.lua.org/pil/1.3.html)****以一个下划线（“_”）来命名，用它来表示丢弃不需要的数值，仅仅起到占位的作用。**\n\n  \n\n## 返回值\n\n```Lua\n-- string.find (s,p) 从string 变量s的开头向后匹配 string\n-- p，若匹配不成功，返回nil，若匹配成功，返回第一次匹配成功\n-- 的起止下标。\n\nlocal start, finish = string.find(\"hello\", \"he\") --start 值为起始下标，finish\n--值为结束下标\nprint ( start, finish )                          --输出 1   2\n\nlocal start = string.find(\"hello\", \"he\")      -- start值为起始下标\nprint ( start )                               -- 输出 1\n\n\nlocal _,finish = string.find(\"hello\", \"he\")   --采用虚变量（即下划线），接收起\n--始下标值，然后丢弃，finish接收\n--结束下标值\nprint ( finish )                              --输出 2\nprint ( _ )    \n```\n\n  \n\n## 迭代\n\n```Lua\n-- test.lua 文件\nlocal t = {1, 3, 5}\n\nprint(\"all  data:\")\nfor i,v in ipairs(t) do\n    print(i,v)\nend\n\nprint(\"\")\nprint(\"part data:\")\nfor _,v in ipairs(t) do\n    print(v)\nend\n```\n\n输出\n\n```Lua\n# luajit test.lua\nall  data:\n1   1\n2   3\n3   5\n\npart data:\n1\n3\n5\n```\n\n# **抵制使用 module() 定义模块**\n\n旧式的模块定义方式是通过 `module(\"filename\"[,package.seeall])*` 来显式声明一个包，现在官方不推荐再使用这种方式\n\n这种方式将会返回一个由 `filename` 模块函数组成的 `table`，并且还会定义一个包含该 `table` 的全局变量。\n\n  \n\n1. `package.seeall` 这种方式破坏了模块的高内聚，原本引入 \"filename\" 模块只想调用它的 _foobar()_ 函数，但是它却可以读写全局属性，例如 `\"filename.os\"`。\n    \n2. `module` 函数压栈操作引发的副作用，污染了全局环境变量。例如 `module(\"filename\")` 会创建一个 `filename` 的 `table`，并将这个 `table` 注入全局环境变量中，这样使得没有引用它的文件也能调用 `filename` 模块的方法。\n    \n\n  \n\n推荐的模块定义\n\n```Lua\n-- square.lua 长方形模块\nlocal _M = {}           -- 局部的变量\n_M._VERSION = '1.0'     -- 模块版本\n\nlocal mt = { __index = _M }\n\nfunction _M.new(self, width, height)\n    return setmetatable({ width=width, height=height }, mt)\nend\n\nfunction _M.get_square(self)\n    return self.width * self.height\nend\n\nfunction _M.get_circumference(self)\n    return (self.width + self.height) * 2\nend\n\nreturn _M\n```\n\n使用\n\n```Lua\nlocal square = require \"square\"\nlocal s1 = square:new(1, 2)\nprint(s1:get_square())          --output: 2\nprint(s1:get_circumference())   --output: 6\n```\n\n另一个跟 Lua 的 module 模块相关需要注意的点是，当 lua_code_cache on 开启时，require 加载的模块是会被缓存下来的，这样我们的模块就会以最高效的方式运行，直到被显式地调用如下语句（这里有点像模块卸载）：\n\n```Plaintext\npackage.loaded[\"square\"] = nil\n```\n\n  \n\n## 调用函数前先定义函数\n\nLua 里面的函数必须放在调用的代码之前，下面的代码是一个常见的错误：\n\n```Lua\n-- test.lua 文件local i = 100\ni = add_one(i)\n\nfunction add_one(i)\n    return i + 1\nend\n```\n\n因此在函数定义之前使用函数相当于在变量赋值之前使用变量，Lua 世界对于没有赋值的变量，默认都是 nil，所以这里也就产生了一个 nil 的错误。\n\n  \n\n# 点号操作符和冒号操作符的区别\n\n```Plaintext\nlocal str = \"abcde\"\n\nprint(\"case 1:\", str:sub(1, 2))\nprint(\"case 2:\", str.sub(str, 1, 2))\n```\n\n输出\n\n```Lua\ncase 1: ab\ncase 2: ab\n```\n\n- **冒号操作会带入一个** **`self`** **参数，用来代表** **`自己`****。**\n    \n- 而点号操作，只是 `内容` 的展开。\n    \n\n在函数定义时，使用冒号将默认接收一个 `self` 参数，而使用点号则需要显式传入 `self` 参数\n\n示例代码：\n\n```Plaintext\nobj = { x = 20 }\n\nfunction obj:fun1()\n    print(self.x)\nend\n```\n\n等价于\n\n```Plaintext\nobj = { x = 20 }\n\nfunction obj.fun1(self)\n    print(self.x)\nend\n```\n\n# module的缺点\n\n由于 `lua_code_cache off` 情况下，缓存的代码会伴随请求完结而释放。module 的最大好处缓存这时候是无法发挥的，所以本章的内容都是基于 `lua_code_cache on` 的情况下。\n\n先看看下面代码：\n\n```Plaintext\nlocal ngx_socket_tcp = ngx.socket.tcp           -- ①\n\nlocal _M = { _VERSION = '0.06' }                -- ②\nlocal mt = { __index = _M }                     -- ③\n\nfunction _M.new(self)\n    local sock, err = ngx_socket_tcp()          -- ④\n    if not sock then\n        return nil, err\n    end\n    return setmetatable({ sock = sock }, mt)    -- ⑤\nend\n\nfunction _M.set_timeout(self, timeout)\n    local sock = self.sock\n    if not sock then\n        return nil, \"not initialized\"\n    end\n\n    return sock:settimeout(timeout)\nend\n\n-- ... 其他功能代码，这里简略\n\nreturn _M\n```\n\n1. 对于比较底层的模块，内部使用到的非本地函数，都需要 local 本地化，这样做的好处：\n    \n    1. 避免命名冲突：防止外部是 `require(...)` 的方法调用造成全局变量污染\n        \n    2. 访问局部变量的速度比全局变量更快、更快、更快（重要事情说三遍）\n        \n\n  \n\n2. 每个基础模块最好有自己 `_VERSION` 标识，方便后期利用 `_VERSION` 完成热代码部署等高级特性，也便于使用者对版本有整体意识。\n    \n3. 其实 `_M` 和 `mt` 对于不同的请求实例（require 方法得到的对象）是相同的，因为 module 会被缓存到全局环境中。所以在这个位置千万不要放单请求内个性信息，例如 ngx.ctx 等变量。\n    \n4. **这里需要实现的是给每个实例绑定不同的 tcp 对象**，后**面 setmetatable 确保了每个实例拥有自己的 socket 对象，所以必须放在 new 函数中**。如果放在 ③ 的下面，那么这时候所有的不同实例内部将绑定了同一个 socket 对象。\n    \n\n```Plaintext\nlocal mt = { __index = _M }                     -- ③\nlocal sock = ngx_socket_tcp()                   -- ④ 错误的\n\nfunction _M.new(self)\n    return setmetatable({ sock = sock }, mt)    -- ⑤\nend\n```\n\n5. Lua 的 module 有两种类型：\n    \n    1. 支持面向对象痕迹可以保留私有属性；静态方法提供者，没有任何私有属性。\n        \n    2. 真正起到区别作用的就是 setmetatable 函数，是否有自己的个性元表，最终导致两种不同的形态。\n        \n\n# FFI\n\nhttps://moonbingbing.gitbooks.io/openresty-best-practices/content/lua/FFI.html\n\nFFI 库，是 LuaJIT 中最重要的一个扩展库。它允许从纯 Lua 代码调用外部 C 函数，使用 C 数据结构。\n\n  \n\nFFI 库最大限度的省去了使用 C 手工编写繁重的 `Lua/C` 绑定的需要。不需要学习一门独立/额外的绑定语言——它解析普通 C 声明。这样可以从 C 头文件或参考手册中，直接剪切，粘贴。它的任务就是绑定很大的库，但不需要捣鼓脆弱的绑定生成器。\n\nFFI 紧紧的整合进了 LuaJIT（几乎不可能作为一个独立的模块）。`JIT` 编译器在 C 数据结构上所产生的代码，等同于一个 C 编译器应该生产的代码。在 `JIT` 编译过的代码中，调用 C 函数，可以被内连处理，不同于基于 `Lua/C API` 函数调用。\n\n  \n\n## **ffi 库 词汇**\n\n|   |   |\n|---|---|\n|noun|Explanation|\n|cdecl|A definition of an abstract C type(actually, is a lua string)|\n|ctype|C type object|\n|cdata|C data object|\n|ct|C type format, is a template object, may be cdecl, cdata, ctype|\n|cb|callback object|\n|VLA|An array of variable length|\n|VLS|A structure of variable length|\n\n## **ffi.* API**\n\n**功能：** _Lua ffi 库的 API，与 LuaJIT 不可分割。_\n\n毫无疑问，在 `lua` 文件中使用 `ffi` 库的时候，必须要有下面的一行。\n\n```Plaintext\nlocal ffi = require \"ffi\"\n```\n\n# JIT\n\n看一下 LuaJIT 官方的解释：LuaJIT is a Just-In-Time Compilerfor the Lua programming language。\n\n**LuaJIT 的运行时环境包括一个用手写汇编实现的 Lua 解释器和一个可以直接生成机器代码的 JIT 编译器**\n\n- 一开始的时候，Lua 字节码总是被 LuaJIT 的解释器解释执行。LuaJIT 的解释器会在执行字节码时同时记录一些运行时的统计信息，比如每个 Lua 函数调用入口的实际运行次数，还有每个 Lua 循环的实际执行次数。\n    \n- 当这些次数超过某个预设的阈值时，便认为对应的 Lua 函数入口或者对应的 Lua 循环足够的“热”，这时便会触发 JIT 编译器开始工作。\n    \n- JIT 编译器会从热函数的入口或者热循环的某个位置开始尝试编译对应的 Lua 代码路径。编译的过程是把 LuaJIT 字节码先转换成 LuaJIT 自己定义的中间码（IR），然后再生成针对目标体系结构的机器码（比如 x86_64 指令组成的机器码）\n    \n- 如果当前 Lua 代码路径上的所有的操作都可以被 JIT 编译器顺利编译，则这条编译过的代码路径便被称为一个“trace”，在物理上对应一个 `trace` 类型的 GC 对象（即参与 Lua GC 的对象）。\n    \n\n  \n\nJIT 编译器不支持的原语被称为 **NYI（Not Yet Implemented）原语**。比较完整的 NYI 列表在这篇文档里面：\n\n```Plaintext\nhttp://wiki.luajit.org/NYI\n```\n\n所谓“让更多的 Lua 代码被 JIT 编译”，其实就是帮助更多的 Lua 代码路径能为 JIT 编译器所接受。这一般通过两种途径来实现：\n\n1. 调整对应的 Lua 代码，**避免使用 NYI 原语**。\n    \n2. 增强 JIT 编译器，让越来越多的 NYI 原语能够被编译。\n    \n\n## **可以被 JIT 编译的元操作**\n\n下面给大家列一下截止到目前已经可以被 JIT 编译的元操作。 其他还有 IO、Bit、FFI、Coroutine、OS、Package、Debug、JIT 等分类，使用频率相对较低，这里就不罗列了，可以参考官网：[http://wiki.luajit.org/NYI](http://wiki.luajit.org/NYI)。\n\n### **基础库的支持情况**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|assert|yes||\n|collectgarbage|no||\n|dofile|never||\n|error|never||\n|getfenv|2.1 partial|只有 getfenv(0) 能编译|\n|getmetatable|yes||\n|ipairs|yes||\n|load|never||\n|loadfile|never||\n|loadstring|never||\n|next|no||\n|pairs|no||\n|pcall|yes||\n|print|no||\n|rawequal|yes||\n|rawget|yes||\n|rawlen (5.2)|yes||\n|rawset|yes||\n|select|partial|第一个参数是静态变量的时候可以编译|\n|setfenv|no||\n|setmetatable|yes||\n|tonumber|partial|不能编译非10进制，非预期的异常输入|\n|tostring|partial|只能编译：字符串、数字、布尔、nil 以及支持 __tostring元方法的类型|\n|type|yes||\n|unpack|no||\n|xpcall|yes||\n\n### **字符串库**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|string.byte|yes||\n|string.char|2.1||\n|string.dump|never||\n|string.find|2.1 partial|只有字符串样式查找（没有样式）|\n|string.format|2.1 partial|不支持 %p 或 非字符串参数的 %s|\n|string.gmatch|no||\n|string.gsub|no||\n|string.len|yes||\n|string.lower|2.1||\n|string.match|no||\n|string.rep|2.1||\n|string.reverse|2.1||\n|string.sub|yes||\n|string.upper|2.1||\n\n### **表**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|table.concat|2.1||\n|table.foreach|no|2.1: 内部编译，但还没有外放|\n|table.foreachi|2.1||\n|table.getn|yes||\n|table.insert|partial|只有 push 操作|\n|table.maxn|no||\n|table.pack (5.2)|no||\n|table.remove|2.1|部分，只有 pop 操作|\n|table.sort|no||\n|table.unpack (5.2)|no||\n\n### **math 库**\n\n|   |   |   |\n|---|---|---|\n|函数|编译?|备注|\n|math.abs|yes||\n|math.acos|yes||\n|math.asin|yes||\n|math.atan|yes||\n|math.atan2|yes||\n|math.ceil|yes||\n|math.cos|yes||\n|math.cosh|yes||\n|math.deg|yes||\n|math.exp|yes||\n|math.floor|yes||\n|math.fmod|no||\n|math.frexp|no||\n|math.ldexp|yes||\n|math.log|yes||\n|math.log10|yes||\n|math.max|yes||\n|math.min|yes||\n|math.modf|yes||\n|math.pow|yes||\n|math.rad|yes||\n|math.random|yes||\n|math.randomseed|no||\n|math.sin|yes||\n|math.sinh|yes||\n|math.sqrt|yes||\n|math.tan|yes||\n|math.tanh|yes||","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":["lua"]},"/lua/lua%E5%9F%BA%E7%A1%80":{"title":"lua基础","content":"\n\n# Lua 简介\n\nLua 是一个小巧的脚本语言。是巴西里约热内卢天主教大学（Pontifical Catholic University of Rio de Janeiro）里的一个研究小组并于 1993 年开发。**其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能**。Lua 由标准 C 编写而成，几乎在所有操作系统和平台上都可以编译、运行。Lua 并没有提供强大的库，这是由它的定位决定的。所以 Lua 不适合作为开发独立应用程序的语言。**Lua 有一个同时进行的 JIT 项目，提供在特定平台上的即时编译功能**。\n\n- **Lua 脚本可以很容易的被 C/C++ 代码调用，也可以反过来调用 C/C++ 的函数，这使得 Lua 在应用程序中可以被广泛应用**。\n    \n- **不仅仅作为扩展脚本，也可以作为普通的配置文件，代替 XML、ini 等文件格式，并且更容易理解和维护**。\n    \n- 标准 Lua 5.1 解释器由标准 C 编写而成，代码简洁优美，几乎在所有操作系统和平台上都可以编译和运行；\n    \n- 一个完整的标准 Lua 5.1 解释器不足 200 KB。而本书推荐使用的 LuaJIT 2 的代码大小也只有不足 500 KB\n    \n- 同时也支持大部分常见的体系结构。在目前所有脚本语言引擎中，LuaJIT 2 实现的速度应该算是最快的之一。这一切都决定了 Lua 是作为嵌入式脚本的最佳选择。\n    \n\nLua 语言的各个版本是不相兼容的。因此本书只介绍 Lua 5.1 语言，这是为标准 Lua 5.1 解释器和 LuaJIT 2 所共同支持的。LuaJIT 支持的对 Lua 5.1 向后兼容的 Lua 5.2 和 Lua 5.3 的特性，我们也会在方便的时候予以介绍。\n\n  \n\n# Lua 环境搭建\n\n[http://openresty.org](http://openresty.org/)\n\n  \n\n## Helloworld\n\n```Go\n# cat hello.lua\nprint(\"hello world\")\n# luajit hello.lua\nhello world\n```\n\n  \n\n# 基本数据类型\n\n  \n\n```Go\nprint(type(\"helloworld\"))\nprint(type('helloworld'))\nprint(type('true'))\nprint(type(1))\nprint(type(2.1))\nprint(type(nil))\nfunction hello()\n    print(\"hello\")\nend\nprint(type(hello))\n```\n\n输出\n\n```Go\nstring\nstring\nstring\nnumber\nnumber\nnil\nfunction\n```\n\n## Nil\n\nNil 是一种类型，Lua 将 nil 用于表示“无效值”。\n\n- 一个变量在第一次赋值前的默认值是 nil，\n    \n- 将 nil 赋予给一个全局变量就等同于删除它。\n    \n\n```Go\nlocal num\nprint(num)        --\u003eoutput:nil\n\nnum = 100\nprint(num)        --\u003eoutput:100\n```\n\n## Boolean (布尔)\n\n布尔类型，可选值 true/false；\n\n- Lua 中 nil 和 false 为“假”\n    \n- 其它所有值均为“真”。比如 0 和空字符串就是“真”；\n    \n\n```Go\nlocal a = true\nlocal b = 0\nlocal c = nil\nif a then\n    print(\"a\")        --\u003eoutput:a\nelse\n    print(\"not a\")    --这个没有执行\nend\n\nif b then\n    print(\"b\")        --\u003eoutput:b\nelse\n    print(\"not b\")    --这个没有执行\nend\n\nif c then\n    print(\"c\")        --这个没有执行\nelse\n    print(\"not c\")    --\u003eoutput:not c\nend\n```\n\n## **number（数字）**\n\nNumber 类型用于表示实数，和 C/C++ 里面的 double 类型很类似。可以使用数学函数 math. Floor（向下取整）和 math. Ceil（向上取整）进行取整操作。\n\n一般地，Lua 的 number 类型就是用双精度浮点数来实现的。值得一提的是，LuaJIT 支持所谓的“dual-number”（双数）模式，\n\n- 即 **LuaJIT 会根据上下文用整型来存储整数，而用双精度浮点数来存放浮点数。**\n    \n\n```Go\nlocal order = 3.99\nlocal score = 98.01\nprint(math.floor(order))   --\u003eoutput:3\nprint(math.ceil(score))    --\u003eoutput:99\nprint(9223372036854775807LL - 1)  --\u003eoutput:9223372036854775806LL\n```\n\n## String（字符串）\n\nLua 中有三种方式表示字符串:\n\n1. 使用一对匹配的单引号。例：'hello'。\n    \n2. 使用一对匹配的双引号。例：\"abclua\"。\n    \n3. 字符串还可以用一种长括号（即 [[ ]]）括起来的方式定义\n    \n    1. 我们把两个正的方括号（即[[）间插入 n 个等号定义为第 n 级正长括号。\n        \n    2. 0 级正的长括号写作 [[ ，一级正的长括号写作 [=[\n        \n    3. 反的长括号也作类似定义；举个例子，4 级反的长括号写作 ]====]\n        \n    4. **一个长字符串可以由任何一级的正的长括号开始，而由第一个碰到的同级反的长括号结束**。整个词法分析过程将**不受分行限制，不处理任何转义符，并且忽略掉任何不同级别的长括号**\n        \n\n  \n\n```Plaintext\nlocal str1 = 'hello world'\nlocal str2 = \"hello lua\"\nlocal str3 = [[\"add\\name\",'hello']]\nlocal str4 = [=[string have a [[]].]=]\nlocal str5 = [=[asdfasd]=]\n\nprint(str1)    --\u003eoutput:hello world\nprint(str2)    --\u003eoutput:hello lua\nprint(str3)    --\u003eoutput:\"add\\name\",'hello'\nprint(str4)    --\u003eoutput:string have a [[]].\nprint(str5)    --\u003eoutput:asdfasd\n```\n\n在 Lua 实现中，Lua 字符串一般都会经历一个“内化”（intern）的过程，**即两个完全一样的 Lua 字符串在 Lua 虚拟机中只会存储一份**。每一个 Lua 字符串在创建时都会**插入到 Lua 虚拟机内部的一个全局的哈希表**中\n\n1. 创建相同的 Lua 字符串并不会引入新的动态内存分配操作，所以相对便宜（但仍有全局哈希表查询的开销），\n    \n2. 内容相同的 Lua 字符串不会占用多份存储空间，\n    \n3. 已经创建好的 Lua 字符串之间进行相等性比较时是 `O(1)` 时间度的开销，而不是通常见到的 `O(n)`.\n    \n\n## Table (表)\n\nTable 类型实现了一种抽象的“关联数组”。“关联数组”是一种具有特殊索引方式的数组，\n\n- 索引通常是**字符串（string）或者 number 类型，但也可以是除** **`nil`** **以外的任意类型的值**\n    \n\n```Go\n\nlocal corp = {\n    web = \"www.google.com\",   --索引为字符串，key = \"web\",\n    --            value = \"www.google.com\"\n    telephone = \"12345678\",   --索引为字符串\n    staff = {\"Jack\", \"Scott\", \"Gary\"}, --索引为字符串，值也是一个表\n    100876,              --相当于 [1] = 100876，此时索引为数字\n    --      key = 1, value = 100876\n    100191,              --相当于 [2] = 100191，此时索引为数字\n    [10] = 360,          --直接把数字索引给出\n    [\"city\"] = \"Beijing\" --索引为字符串\n}\n\nprint(corp.web)               --\u003eoutput:www.google.com\nprint(corp[\"web\"])               --\u003eoutput:www.google.com\nprint(corp[\"telephone\"])      --\u003eoutput:12345678\nprint(corp[2])                --\u003eoutput:100191\nprint(corp[\"city\"])           --\u003eoutput:\"Beijing\"\nprint(corp.staff[1])          --\u003eoutput:Jack\nprint(corp[\"staff\"][1])          --\u003eoutput:Jack\nprint(corp[10])               --\u003eoutput:360\n```\n\n在内部实现上，table 通常实现为一个哈希表、一个数组、或者两者的混合。具体的实现为何种形式，动态依赖于具体的 table 的键分布特点。\n\n## Function (函数)\n\n在 Lua 中，**函数** 也是一种数据类型，函数可以存储在变量中，可以通过参数传递给其他函数，还可以作为其他函数的返回值\n\n```Go\nlocal function foo()\n    print(\"in the function\")\n    --dosomething()\n    local x = 10\n    local y = 20\n    return x + y\nend\n\nlocal a = foo    --把函数赋给变量\n\nprint(a())\n\n--output:\n--in the function\n--30\n\nfunction foo()\nend\n--等价于\n\nfoo = function ()\nend\n\nlocal function foo()\nend\n-- 等价于\n\nlocal foo = function ()\nend\n```\n\n  \n\n# 表达式\n\n## 算术运算符\n\n|            |      |\n| ---------- | ---- |\n| 算术运算符 | 说明 |\n| +          | 加法 |\n| -          | 减法 |\n| *          | 乘法 |\n| /          | 除法 |\n| ^          | 指数 |\n| %          | 取模 |\n\n```Go\nprint(1 + 2)       --\u003e打印 3\nprint(5 / 10)      --\u003e打印 0.5。 这是Lua不同于c语言的\nprint(5.0 / 10)    --\u003e打印 0.5。 浮点数相除的结果是浮点数\n-- print(10 / 0)   --\u003e注意除数不能为0，计算的结果会出错\nprint(2 ^ 10)      --\u003e打印 1024。 求2的10次方\n\nlocal num = 1357\nprint(num % 2)       --\u003e打印 1\nprint((num % 2) == 1) --\u003e打印 true。 判断num是否为奇数\n```\n\n## 关系运算符\n\n  \n\n|            |          |\n| ---------- | -------- |\n| 关系运算符 | 说明     |\n| \u003c          | 小于     |\n| \u003e          | 大于     |\n| \u003c=         | 小于等于 |\n| \u003e=         | 大于等于 |\n| ==         | 等于     |\n| ~=         | 不等于   |\n\n  \n\n```Go\nprint(1 \u003c 2)    --\u003e打印 true\nprint(1 == 2)   --\u003e打印 false\nprint(1 ~= 2)   --\u003e打印 true\nlocal a, b = true, false\nprint(a == b)  --\u003e打印 false\n```\n\n- 在使用“==”做等于判断时，要注意对于 table, userdate 和函数， Lua 是作引用比较的。也就是说，只有当两个变量引用同一个对象时，才认为它们相等\n    \n\n```Go\nlocal a = { x = 1, y = 0}\nlocal b = { x = 1, y = 0}\nif a == b then\n    print(\"a==b\")\nelse\n    print(\"a~=b\")\nend\n---output:\na~=b\n```\n\n- Lua 字符串总是会被“内化”，即相同内容的字符串只会被保存一份，因此 Lua 字符串之间的相等性比较可以简化为其内部存储地址的比较。\n    \n- 这意味着 Lua 字符串的相等性比较总是为 O (1)\n    \n\n## 逻辑运算符\n\n|            |        |\n| ---------- | ------ |\n| 逻辑运算符 | 说明   |\n| and        | 逻辑与 |\n| or         | 逻辑或 |\n| not        | 逻辑非 |\n\n在 c 语言中，and 和 or 只得到两个值 1 和 0，其中 1 表示真，0 表示假。而 Lua 中 and 的执行过程是这样的：\n\n- `a and b` 如果 a 为 nil，则返回 a，否则返回 b;\n    \n- `a or b` 如果 a 为 nil，则返回 b，否则返回 a。\n    \n- **所有逻辑操作符将 false 和 nil 视作假，其他任何值视作真，对于 and 和 or，“短路求值”，对于 not，永远只返回 true 或者 false。**\n    \n\n```Go\nlocal c = nil\nlocal d = 0\nlocal e = 100\nprint(c and d)  --\u003e打印 nil\nprint(c and e)  --\u003e打印 nil\nprint(d and e)  --\u003e打印 100\nprint(c or d)   --\u003e打印 0\nprint(c or e)   --\u003e打印 100\nprint(not c)    --\u003e打印 true\nprint(not d)    --\u003e打印 false\n```\n\n## 字符串连接\n\nLua 中连接两个字符串，可以使用操作符“..”（两个点）\n\n- 如果其任意一个操作数是数字的话，Lua 会将这个数字转换成字符串。\n    \n- 注意，连接操作符只会创建一个新字符串，而不会改变原操作数\n    \n- 也可以使用 string 库函数 `string.format` 连接字符串\n    \n\n```Go\nprint(\"Hello \" .. \"World\")    --\u003e打印 Hello Worldprint(0 .. 1)                 --\u003e打印 01\n\nstr1 = string.format(\"%s-%s\",\"hello\",\"world\")\nprint(str1)              --\u003e打印 hello-world\n\nstr2 = string.format(\"%d-%s-%.2f\",123,\"world\",1.21)\nprint(str2)              --\u003e打印 123-world-1.21\n```\n\n于 Lua 字符串本质上是只读的，**因此字符串连接运算符几乎总会创建一个新的（更大的）字符串**。这意味着如果有很多这样的连接操作（比如在循环中使用 .. 来拼接最终结果），则性能损耗会非常大。在这种情况下，推荐使用 table 和 `table.concat()` 来进行很多字符串的拼接\n\n```Go\nlocal pieces = {}\nfor i, elem in ipairs(my_list) do\n    pieces[i] = my_process(elem)\nend\nlocal res = table.concat(pieces)\n```\n\n上面的例子还可以使用 LuaJIT 独有的 `table.new` 来恰当地初始化 `pieces` 表的空间，以避免该表的动态生长。\n\n## 优先级\n\n| f               |     |\n| --------------- | --- |\n| ^               |     |\n| not # -         |     |\n| * / %           |     |\n| + -             |     |\n| ..              |     |\n| \u003c \u003e \u003c= \u003e= == ~= |     |\n| and             |     |\n| or              |     |\n  \n\n```Go\nlocal a, b = 1, 2\nlocal x, y = 3, 4\nlocal i = 10\nlocal res = 0\nres = a + i \u003c b/2 + 1  --\u003e等价于res =  (a + i) \u003c ((b/2) + 1)\nres = 5 + x^2*8        --\u003e等价于res =  5 + ((x^2) * 8)\nres = a \u003c y and y \u003c=x  --\u003e等价于res =  (a \u003c y) and (y \u003c= x)\n```\n\n  \n\n# 控制结构\n\n## If-else\n\n### **单个 if 分支型**\n\n```Go\nx = 10\nif x \u003e 0 then\n    print(\"x is a positive number\")\nend\n```\n\n### **两个分支 if-else 型**\n\n```Go\nx = 10\nif x \u003e 0 then\n    print(\"x is a positive number\")\nelse\n    print(\"x is a non-positive number\")\nend\n```\n\n### 多个分支的 if-elseif-else\n\n```Go\n\nscore = 90\nif score == 100 then\n    print(\"Very good!Your score is 100\")\nelseif score \u003e= 60 then\n    print(\"Congratulations, you have passed it,your score greater or equal to 60\")\n    --此处可以添加多个elseif\nelse\n    print(\"Sorry, you do not pass the exam! \")\nend\n```\n\n与 C 语言的不同之处是 else 与 if 是连在一起的，若将 else 与 if 写成 \"else if\" 则相当于在 else 里嵌套另一个 if 语句，如下代码：\n\n```Go\nscore = 0\nif score == 100 then\n    print(\"Very good!Your score is 100\")\nelseif score \u003e= 60 then\n    print(\"Congratulations, you have passed it,your score greater or equal to 60\")\nelse\n    if score \u003e 0 then\n        print(\"Your score is better than 0\")\n    else\n        print(\"My God, your score turned out to be 0\")\n    end --与上一示例代码不同的是，此处要添加一个end\nend\n```\n\n## While\n\n```Go\nwhile 表达式 do\n    --body\nend\n```\n\n  \n\n## Repeat\n\nLua 中的 repeat 控制结构类似于其他语言（如：C++ 语言）中的 do-while，但是控制方式是刚好相反的。简单点说，**执行 repeat 循环体后，直到 until 的条件为真时才结束**\n\n```Lua\n-- 以下代码会死循环\nx = 10\nrepeat\n    print(x)\nuntil false\n```\n\n  \n\n## For\n\n### **for 数字型**\n\n```Lua\nfor var = begin, finish, step do\n    --body\nend\n```\n\n1. Var 从 begin 变化到 finish，每次变化都以 step 作为步长递增 var\n    \n2. Begin、finish、step 三个表达式只会在循环开始时执行一次\n    \n3. 第三个表达式 step 是可选的，默认为 1\n    \n4. 控制变量 var 的作用域仅在 for 循环内，需要在外面控制，则需将值赋给一个新的变量\n    \n5. 循环过程中不要改变控制变量的值，那样会带来不可预知的影响\n    \n\n```Lua\nfor i = 1, 5 do\n    print(i)\nend\n-- output:\n1\n2\n3\n4\n5\n\nfor i = 1, 10, 2 do\n    print(i)\nend\n-- output:\n1\n3\n5\n7\n9\n```\n\n## For 泛型\n\n泛型 for 循环通过一个迭代器（iterator）函数来遍历所有值：\n\n```Lua\n-- 打印数组a的所有值local a = {\"a\", \"b\", \"c\", \"d\"}\nfor i, v in ipairs(a) do\n    print(\"index:\", i, \" value:\", v)\nend\n-- output:\nindex:  1  value: a\nindex:  2  value: b\nindex:  3  value: c\nindex:  4  value: d\n```\n\nLua 的基础库提供了 **ipairs，这是一个用于遍历数组的迭代器函数**。在每次循环中，i 会被赋予一个索引值，同时 v 被赋予一个对应于该索引的数组元素值。\n\n```Lua\n-- 打印table t中所有的\nkeyfor k in pairs(t) do\n    print(k)\nend\n```\n\n通过不同的迭代器，几乎可以遍历所有的东西，而且写出的代码极具可读性。标准库提供了几种迭代器，包括用于迭代文件中每行的（io. Lines）、迭代 table 元素的（pairs）、迭代数组元素的（ipairs）、迭代字符串中单词的（string. Gmatch）\n\n泛型 for 循环与数字型 for 循环有两个相同点：\n\n1. 循环变量是循环体的局部变量；\n    \n2. 决不应该对循环变量作任何赋值。\n    \n\n在 LuaJIT 2.1 中，**`ipairs()`** **内建函数是可以被 JIT 编译的，而** **`pairs()`** **则只能被解释执行。因此在性能敏感的场景，应当合理安排数据结构，避免对哈希表进行遍历**\n\n  \n\n## Break\n\n语句 `break` 用来终止 `while`、`repeat` 和 `for` 三种循环的执行，并跳出当前循环体，继续执行当前循环之后的语句\n\n```Lua\n-- 计算最小的x,使从1到x的所有数相加和大于100\nsum = 0\ni = 1while true do\n    sum = sum + i\n    if sum \u003e 100 then\n        break\n    end\n    i = i + 1\nend\nprint(\"The result is \" .. i)  \n--\u003eoutput:The result is 14\n```\n\n## Return\n\n  \n\n`return` 主要用于从函数中返回结果，或者用于简单的结束一个函数的执行。\n\n```Lua\nlocal function add(x, y)\n    return x + y\n    --print(\"add: I will return the result \" .. (x + y))\n    --因为前面有个return，若不注释该语句，则会报错\nend\n\nlocal function is_positive(x)\n    if x \u003e 0 then\n        return x .. \" is positive\"\n    else\n        return x .. \" is non-positive\"\n    end\n\n    --由于return只出现在前面显式的语句块，所以此语句不注释也不会报错\n    --，但是不会被执行，此处不会产生输出\n    print(\"function end!\")\nend\n\nlocal sum = add(10, 20)\nprint(\"The sum is \" .. sum)  --\u003eoutput:The sum is 30\nlocal answer = is_positive(-10)\nprint(answer)                --\u003eoutput:-10 is non-positive\n```\n\n  \n\n## Goto\n\n有了 `goto`，我们可以实现 `continue` 的功能：\n\n```Lua\nfor i=1, 3 do\n    if i \u003c= 2 then\n        print(i, \"yes continue\")\n        goto continue\n    end\n    print(i, \" no continue\")\n\n    ::continue::\n    print([[i'm end]])\nend\n```\n\n输出结果\n\n```Lua\n$ luajit test.lua\n1   yes continue\ni'm end\n2   yes continue\ni'm end\n3    no continue\ni'm end\n```\n\n# 函数\n\n## 定义\n\n```Lua\nfunction function_name (arc)  -- arc 表示参数列表，函数的参数列表可以为空\n    -- body\nend\n```\n\n上面的语法定义了一个全局函数，名为 `function_name`. 全局函数本质上就是函数类型的值赋给了一个全局变量，即上面的语法等价于\n\n```Lua\nfunction_name = function (arc)\n     -- body\nend\n```\n\n由于全局变量一般会污染全局名字空间，同时也有性能损耗（即查询全局环境表的开销），因此我们应当尽量使用“局部函数”，其记法是类似的，只是开头加上 `local` 修饰符：\n\n```Lua\nlocal function function_name (arc)\n    -- body\nend\n```\n\n定义函数\n\n1. 利用名字来解释函数、变量的目的，使人通过名字就能看出来函数、变量的作用。\n    \n2. 每个函数的长度要尽量控制在一个屏幕内，一眼可以看明白。\n    \n3. 让代码自己说话，不需要注释最好。\n    \n\n  \n\n由于函数定义等价于变量赋值，我们也可以把函数名替换为某个 Lua 表的某个字段，例如\n\n```Lua\nlocal foo = {}\nfunction foo.pr()\n    print(\"ssss\")\nend\n\nfoo.pr()\n```\n\n  \n\n## 参数\n\n### 按值传递\n\n**Lua 函数的参数大部分是按值传递的**。**当函数参数是 table 类型时，传递进来的是实际参数的引用**\n\n值传递就是调用函数时，实参把它的值通过赋值运算传递给形参，然后形参的改变和实参就没有关系了。在这个过程中，实参是通过它在参数表中的位置与形参匹配起来的。\n\n```Lua\nlocal function swap(a, b) --定义函数swap,函数内部进行交换两个变量的值\n    local temp = a\n    a = b\n    b = temp\n    print(a, b)\nend\n\nlocal x = \"hello\"\nlocal y = 20\nprint(x, y)\nswap(x, y)    --调用swap函数\nprint(x, y)   --调用swap函数后，x和y的值并没有交换\n\n--\u003eoutput\nhello 20\n20  hello\nhello 20\n```\n\n在调用函数的时候，**若形参个数和实参个数不同时，Lua 会自动调整实参个数**。调整规则：\n\n- 若实参个数大于形参个数，从左向右，多余的实参被忽略；\n    \n- 若实参个数小于形参个数，从左向右，**没有被实参初始化的形参会被初始化为 nil**\n    \n\n```Lua\nlocal function fun1(a, b)       --两个形参，多余的实参被忽略掉\n    print(a, b)\nend\n\nlocal function fun2(a, b, c, d) --四个形参，没有被实参初始化的形参，用nil初始化\n    print(a, b, c, d)\nend\n\nlocal x = 1\nlocal y = 2\nlocal z = 3\n\nfun1(x, y, z)         -- z被函数fun1忽略掉了，参数变成 x, y\nfun2(x, y, z)         -- 后面自动加上一个nil，参数变成 x, y, z, nil\n\n--\u003eoutput\n1   2\n1   2   3   nil\n```\n\n### 变长参数\n\n其实 Lua 还支持变长参数。若形参为 `...`，表示该函数可以接收不同长度的参数。访问参数的时候也要使用 `...`\n\n```Lua\n\nlocal function func( ... )                -- 形参为 ... ,表示函数采用变长参数\n\n    local temp = {...}                     -- 访问的时候也要使用 ...\n    local ans = table.concat(temp, \" \")    -- 使用 table.concat 库函数对数\n    -- 组内容使用 \" \" 拼接成字符串。\n    print(ans)\nend\n\nfunc(1, 2)        -- 传递了两个参数\nfunc(1, 2, 3, 4)  -- 传递了四个参数\n\n--\u003eoutput\n1 2\n\n1 2 3 4\n```\n\n### **具名参数**\n\nLua 还支持通过名称来指定实参，这时候要把所有的实参组织到一个 table 中，并将这个 table 作为唯一的实参传给函数。\n\n```Lua\nlocal function change(arg) -- change 函数，改变长方形的长和宽，使其各增长一倍\n  arg.width = arg.width * 2\n  arg.height = arg.height * 2return arg\nendlocal rectangle = { width = 20, height = 15 }\nprint(\"before change:\", \"width  =\", rectangle.width,\n                        \"height =\", rectangle.height)\nrectangle = change(rectangle)\nprint(\"after  change:\", \"width  =\", rectangle.width,\n                        \"height =\", rectangle.height)\n\n--\u003eoutput\nbefore change: width = 20  height =  15\nafter  change: width = 40  height =  30\n```\n\n  \n\n### 按引用传递\n\n**当函数参数是 table 类型时，传递进来的是实际参数的引用**，此时在函数内部对该 table 所做的修改，会直接对调用者所传递的实际参数生效，而无需自己返回结果和让调用者进行赋值\n\n```Plaintext\nfunction change(arg) --change函数，改变长方形的长和宽，使其各增长一倍\n  arg.width = arg.width * 2  --表arg不是表rectangle的拷贝，他们是同一个表\n  arg.height = arg.height * 2end                  -- 没有return语句了local rectangle = { width = 20, height = 15 }\nprint(\"before change:\", \"width = \", rectangle.width,\n                        \" height = \", rectangle.height)\nchange(rectangle)\nprint(\"after change:\", \"width = \", rectangle.width,\n                       \" height =\", rectangle.height)\n\n--\u003e output\nbefore change: width = 20  height = 15\nafter  change: width = 40  height = 30\n```\n\n## 函数返回值\n\nLua 具有一项与众不同的特性，允许函数返回多个值。\n\n```Lua\nlocal function swap(a, b)   \n    -- 定义函数 swap，实现两个变量交换值\n    return b, a              \n    -- 按相反顺序返回变量的值\nend\n\nlocal x = 1\nlocal y = 20\nx, y = swap(x, y)           -- 调用 swap 函数\nprint(x, y)                 --\u003e output   20     1\n```\n\n  \n\n当函数返回值的个数和接收返回值的变量的个数不一致时，Lua 也会自动调整参数个数调整规则：\n\n- 若返回值个数大于接收变量的个数，多余的返回值会被忽略掉；\n    \n- 若返回值个数小于参数个数，从左向右，没有被返回值初始化的变量会被初始化为 nil。\n    \n\n```Lua\nfunction init()             \n    --init 函数 返回两个值 1 和 \"lua\"\n    return 1, \"lua\"\nend\n\nx = init()\nprint(x)\n\nx, y, z = init()\nprint(x, y, z)\n\n--output\n1\n1 lua nil\n```\n\n  \n\n当一个函数有一个以上返回值，且函数调用不是一个列表表达式的最后一个元素，那么函数调用只会产生一个返回值, 也就是第一个返回值。\n\n```Lua\nlocal function init()       -- init 函数 返回两个值 1 和 \"lua\"\n    return 1, \"lua\"\nend\n\nlocal x, y, z = init(), 2   -- init 函数的位置不在最后，此时只返回 1\nprint(x, y, z)              --\u003eoutput  1  2  nil\n\nlocal a, b, c = 2, init()   -- init 函数的位置在最后，此时返回 1 和 \"lua\"\nprint(a, b, c)              --\u003eoutput  2  1  lua\n```\n\n函数调用的实参列表也是一个列表表达式。考虑下面的例子：\n\n```Lua\nlocal function init()\n    return 1, \"lua\"\nend\n\nprint(init(), 2)   --\u003eoutput  1  2\nprint(2, init())   --\u003eoutput  2  1  lua\n```\n\n如果你确保只取函数返回值的第一个值，可以使用括号运算符\n\n```Lua\nlocal function init()\n    return 1, \"lua\"\nend\nprint((init()), 2)   --\u003eoutput  1  2\nprint(2, (init()))   --\u003eoutput  2  1\n```\n\n**值得一提的是，如果实参列表中某个函数会返回多个值，同时调用者又没有显式地使用括号运算符来筛选和过滤，则这样的表达式是不能被 LuaJIT 2 所 JIT 编译的，而只能被解释执行。**\n\n  \n\n  \n\n# 全动态函数调用\n\n调用回调函数，并把一个数组参数作为回调函数的参数。\n\n```Lua\nlocal args = {...} or {}\nmethod_name(unpack(args, 1, table.maxn(args)))\n```\n\n```Lua\nlocal function run(x, y)\n    print('run', x, y)\nend\n\nlocal function attack(targetId)\n    print('targetId', targetId)\nend\n\nlocal function do_action(method, ...)\n    local args = {...} or {}\n    method(unpack(args, 1, table.maxn(args)))\nend\n\ndo_action(run, 1, 2)         -- output: run 1 2\ndo_action(attack, 1111)      -- output: targetId    1111\n```\n\n  \n\n# 模块\n\n从 Lua 5.1 语言添加了对模块和包的支持。一**个 Lua 模块的数据结构是用一个 Lua 值（通常是一个 Lua 表或者 Lua 函数）**。**一个 Lua 模块代码就是一个会返回这个 Lua 值的代码块**\n\n- 可以使用内建函数 `require()` 来加载和缓存模块。\n    \n- 简单的说，一个代码模块就是一个程序库，可以通过 `require` 来加载。**模块加载后的结果通过是一个 Lua table**\n    \n- **这个表就像是一个命名空间**，其内容就是模块中导出的所有东西，**比如函数和变量**。`require` 函数会返回 Lua 模块加载后的结果，即用于表示该 Lua 模块的 Lua 值。\n    \n\n  \n\n  \n\nLua 提供了一个名为 `require` 的函数用来加载模块。**要加载一个模块，只需要简单地调用** **`require`** **\"file\" 就可以了，file 指模块所在的文件名**。这个调用会返回一个由模块函数组成的 table，并且还会定义一个包含该 table 的全局变量。\n\n在 Lua 中创建一个模块最简单的方法是：**创建一个 table，并将所有需要导出的函数放入其中，最后返回这个 table 就可以了。相当于将导出的函数作为 table 的一个字段，在 Lua 中函数是第一类值，提供了天然的优势。**\n\n- 创建 my. Lua\n    \n\n```Lua\nlocal _M = {}\n\nlocal function get_name()\n    return \"Lucy\"\n    end\nfunction _M.greeting()\n    print(\"hello \" .. get_name())\nend\n\nreturn _M\n```\n\n- 把下面代码保存在文件 main. Lua 中，然后执行 main. Lua，调用上述模块。\n    \n\n```Lua\nlocal my_module = require(\"my\")\nmy_module.greeting()     --\u003eoutput: hello Lucy\n```\n\n  \n\n\u003e - 对于需要导出给外部使用的公共模块，处于安全考虑，**是要避免全局变量的出现**。我们可以使用 lj-releng 或 luacheck 工具完成全局变量的检测。至于如何做，到后面再讲。\n\u003e     \n\u003e - 另一个要注意的是，由于在 LuaJIT 中，**require 函数内不能进行上下文切换**，**所以不能够在模块的顶级上下文中调用 cosocket 一类的 API**。否则会报 `attempt to yield across C-call boundary` 错误。\n\u003e     \n\n  \n\n# String\n\nLua 字符串总是由字节构成的。Lua 核心并不尝试理解具体的字符集编码（比如 GBK 和 UTF-8 这样的多字节字符编码）\n\nLua 字符串内部用来标识各个组成字节的下标是从 1 开始的，这不同于像 C 和 Perl 这样的编程语言。这样数字符串位置的时候再也不用调整，对于非专业的开发者来说可能也是一个好事情，**string.Sub (str, 3, 7) 直接表示从第三个字符开始到第七个字符（含）为止的子串。**\n\n## **string.Byte (s [, i [, j ]])**\n\n返回字符 s[i]、s[i + 1]、s[i + 2]、······、s[j] 所对应的 ASCII 码\n\n```Lua\nprint(string.byte(\"abc\", 1, 3))\nprint(string.byte(\"abc\", 3)) -- 缺少第三个参数，第三个参数默认与第二个相同，此时为 3\nprint(string.byte(\"abc\"))    -- 缺少第二个和第三个参数，此时这两个参数都默认为 1\n\n--\u003eoutput\n97    98    99\n99\n97\n```\n\n## **string. Char (...)**\n\n接收 0 个或更多的整数（整数范围：0~255），返回这些整数所对应的 ASCII 码字符组成的字符串。当参数为空时，默认是一个 0。\n\n```Lua\nprint(string.char(96, 97, 98))\nprint(string.char())        -- 参数为空，默认是一个0，-- 你可以用string.byte(string.char())测试一下print(string.char(65, 66))\n\n--\u003e output\n`ab\n\nAB\n```\n\n## **string.Upper (s)**\n\n接收一个字符串 s，返回一个把所有小写字母变成大写字母的字符串。\n\n```Lua\nprint(string.upper(\"Hello Lua\"))  --\u003eoutput  HELLO LUA\n```\n\n## **string.Lower (s)**\n\n接收一个字符串 s，返回一个把所有大写字母变成小写字母的字符串。\n\n```Lua\nprint(string.lower(\"Hello Lua\"))  --\u003eoutput   hello lua\n```\n\n## **string.Len (s)**\n\n接收一个字符串，返回它的长度。\n\n```Lua\nprint(string.len(\"hello lua\")) --\u003eoutput  9\n```\n\n使用此函数是不推荐的。应当总是使用 `#` 运算符来获取 Lua 字符串的长度\n\n## **string.Find (s, p [, init [, plain]])**\n\n在 s 字符串中第一次匹配 p 字符串。若匹配成功，则返回 p 字符串在 s 字符串中出现的开始位置和结束位置；若匹配失败，则返回 nil,\n\n第三个参数第三个参数 init 默认为 1，并且可以为负整数，\n\n当 init 为负数时，表示从 s 字符串的 string.Len (s) + init + 1 索引处开始向后匹配字符串 p 。\n\n第四个参数默认为 false，当其为 true 时，只会把 p 看成一个字符串对待。\n\n```Lua\nlocal find = string.find\nprint(find(\"abc cba\", \"ab\"))\nprint(find(\"abc cba\", \"ab\", 2))     -- 从索引为2的位置开始匹配字符串：ab\nprint(find(\"abc cba\", \"ba\", -1))    -- 从索引为7的位置开始匹配字符串：ba\nprint(find(\"abc cba\", \"ba\", -3))    -- 从索引为5的位置开始匹配字符串：ba\nprint(find(\"abc cba\", \"(%a+)\", 1))  -- 从索引为1处匹配最长连续且只含字母的字符串\nprint(find(\"abc cba\", \"(%a+)\", 1, true)) --从索引为1的位置开始匹配字符串：(%a+)\n\n--\u003eoutput\n1   2\nnil\nnil\n6   7\n1   3   abc\nnil\n```\n\n## **string.Format (formatstring, ...)**\n\n按照格式化参数 formatstring，返回后面 `...` 内容的格式化版本\n\n```Plaintext\nprint(string.format(\"%.4f\", 3.1415926))     -- 保留4位小数\nprint(string.format(\"%d %x %o\", 31, 31, 31))-- 十进制数31转换成不同进制\nd = 29; m = 7; y = 2015                     -- 一行包含几个语句，用；分开\nprint(string.format(\"%s %02d/%02d/%d\", \"today is:\", d, m, y))\n\n--\u003eoutput\n3.1416\n31 1f 37\ntoday is: 29/07/2015\n```\n\n## **string.Match (s, p [, init])**\n\n在字符串 s 中匹配（模式）字符串 p，若匹配成功，则返回目标字符串中与模式匹配的子串；否则返回 nil。第三个参数 init 默认为 1，并且可以为负整数，当 init 为负数时，表示从 s 字符串的 string.Len (s) + init + 1 索引处开始向后匹配字符串 p。\n\n```Lua\nprint(string.match(\"hello lua\", \"lua\"))\nprint(string.match(\"lua lua\", \"lua\", 2))  --匹配后面那个luaprint(string.match(\"lua lua\", \"hello\"))\nprint(string.match(\"today is 27/7/2015\", \"%d+/%d+/%d+\"))\n\n--\u003eoutput\nlua\nlua\nnil27/7/2015\n```\n\n## **string.Gmatch (s, p)**\n\n返回一个迭代器函数，通过这个迭代器函数可以遍历到在字符串 s 中出现模式串 p 的所有地方。\n\n```Lua\ns = \"hello world from Lua\"\nfor w in string.gmatch(s, \"%a+\") do  --匹配最长连续且只含字母的字符串\n    print(w)\nend\n\n--\u003eoutput\nhello\nworld\nfrom\nLua\n\n\nt = {}\ns = \"from=world, to=Lua\"\nfor k, v in string.gmatch(s, \"(%a+)=(%a+)\") do  --匹配两个最长连续且只含字母的\n    t[k] = v                                    --字符串，它们之间用等号连接\nend\nfor k, v in pairs(t) do\n    print (k,v)\nend\n\n--\u003eoutput\nto      Lua\nfrom    worl\n```\n\n## **string.Rep (s, n)**\n\n返回字符串 s 的 n 次拷贝。\n\n```Lua\nprint(string.rep(\"abc\", 3)) \n\n--拷贝3次\"abc\"--\u003eoutput  abcabcabc\n```\n\n## **string.Sub (s, i [, j])**\n\n返回字符串 s 中，索引 i 到索引 j 之间的子字符串。当 j 缺省时，默认为 -1，也就是字符串 s 的最后位置。I 可以为负数。当索引 i 在字符串 s 的位置在索引 j 的后面时，将返回一个空字符串。\n\n```Lua\nprint(string.sub(\"Hello Lua\", 4, 7))\nprint(string.sub(\"Hello Lua\", 2))\nprint(string.sub(\"Hello Lua\", 2, 1))    --看到返回什么了吗print(string.sub(\"Hello Lua\", -3, -1))\n\n--\u003eoutput\nlo L\nello Lua\n\nLua\n```\n\n## **string.Gsub (s, p, r [, n])**\n\n将目标字符串 s 中所有的子串 p 替换成字符串 r。可选参数 n，表示限制替换次数。返回值有两个，第一个是被替换后的字符串，第二个是替换了多少次。\n\n```Plaintext\nprint(string.gsub(\"Lua Lua Lua\", \"Lua\", \"hello\"))\nprint(string.gsub(\"Lua Lua Lua\", \"Lua\", \"hello\", 2)) --指明第四个参数--\u003eoutput\nhello hello hello   3\nhello hello Lua     2\n```\n\n## **string. Reverse (s)**\n\n接收一个字符串 s，返回这个字符串的反转\n\n```Lua\nprint(string.reverse(\"Hello Lua\"))  --\u003e output: auL olleH\n```\n\n  \n\n# Table\n\n## **下标从 1 开始**\n\n数组下标从 1 开始计数。\n\n而 Lua 最初设计是一种类似 XML 的数据描述语言，所以索引（index）反应的是数据在里面的位置，而不是偏移量。\n\n  \n\n在初始化一个数组的时候，**若不显式地用键值对方式赋值，则会默认用数字作为下标**，从 1 开始。由于在 _Lua_ 内部实际采用哈希表和数组分别保存键值对、普通值，所以不推荐混合使用这两种赋值方式\n\n```Lua\nlocal color={first=\"red\", \"blue\", third=\"green\", \"yellow\"}\nprint(color[\"first\"])                 --\u003e output: red\nprint(color[1])                       --\u003e output: blue\nprint(color[\"third\"])                 --\u003e output: green\nprint(color[2])                       --\u003e output: yellow\nprint(color[3])                       --\u003e output: nil\n```\n\n- **当我们把 table 当作栈或者队列使用的时候，容易犯错，追加到 table 的末尾用的是** **`s[#s+1] = something`****, 而不是** **`s[#s] = something`**\n    \n- 而且如果这个 something 是一个 nil 的话**，会导致这一次压栈（或者入队列）没有存入任何东西**， s 的值没有变\n    \n- 如果 `s = { 1, 2, 3, 4, 5, 6 }`，你令 `s[4] = nil`， s 会令你“匪夷所思”地变成 3。\n    \n\n## **table. Getn 获取长度**\n\n取长度操作符写作一元操作 。字符串的长度是它的字节数（就是以一个字符一个字节计算的字符串长度）\n\n- 对于常规的数组，里面从 1 到 n 放着一些非空的值的时候，它的长度就精确的为 n，即最后一个值的下标\n    \n- 如果数组有一个“空洞”（**就是说，nil 值被夹在非空值之间**），**那么 t 可能是指向任何一个是 nil 值的前一个位置的下标**\n    \n- 这也就说明对于有“空洞”的情况，table 的长度存在一定的 **不可确定性**\n    \n\n```Lua\nlocal tblTest1 = { 1, a = 2, 3 }\nprint(\"Test1 \" .. table.getn(tblTest1))\n\nlocal tblTest2 = { 1, nil }\nprint(\"Test2 \" .. table.getn(tblTest2))\n\nlocal tblTest3 = { 1, nil, 2 }\nprint(\"Test3 \" .. table.getn(tblTest3))\n\nlocal tblTest4 = { 1, nil, 2, nil }\nprint(\"Test4 \" .. table.getn(tblTest4))\n\nlocal tblTest5 = { 1, nil, 2, nil, 3, nil }\nprint(\"Test5 \" .. table.getn(tblTest5))\n\nlocal tblTest6 = { 1, nil, 2, nil, 3, nil, 4, nil }\nprint(\"Test6 \" .. table.getn(tblTest6))\n```\n\n我们使用 Lua 5.1 和 LuaJIT 2.1 分别执行这个用例，结果如下：\n\n```Lua\n# lua test.lua\nTest1 2\nTest2 1\nTest3 3\nTest4 1\nTest5 3\nTest6 1\n# luajit test.lua\nTest1 2\nTest2 1\nTest3 1\nTest4 1\nTest5 1\nTest6 1\n```\n\n不要在 Lua 的 table 中使用 nil 值，**如果一个元素要删除，直接 remove，不要用 nil 去代替**。\n\n## **table. Concat (table [, sep [, i [, j ] ] ])**\n\n对于元素是 string 或者 number 类型的表 table，返回 `table[i]..sep..table[i+1] ··· sep..table[j]` 连接成的字符串。填充字符串 sep 默认为空白字符串。起始索引位置 i 默认为 1，结束索引位置 j 默认是 table 的长度。\n\n```Lua\nlocal a = {1, 3, 5, \"hello\" }\nprint(table.concat(a))              -- output: 135hello\nprint(table.concat(a, \"|\"))         -- output: 1|3|5|hello\nprint(table.concat(a, \" \", 4, 2))   -- output:\nprint(table.concat(a, \" \", 2, 4))   -- output: 3 5 hello\n```\n\n## **table. Insert (table, [pos ,] value)**\n\n在（数组型）表 table 的 pos 索引位置插入 value，其它元素向后移动到空的地方。Pos 的默认值是表的长度加一，即默认是插在表的最后\n\n```Lua\nlocal a = {1, 8}             --a[1] = 1,a[2] = 8\ntable.insert(a, 1, 3)   --在表索引为1处插入3\nprint(a[1], a[2], a[3])\ntable.insert(a, 10)    --在表的最后插入10\nprint(a[1], a[2], a[3], a[4])\n\n--\u003eoutput\n3    1    8\n3    1    8    10\n```\n\n## **table. Maxn (table)**\n\n返回（数组型）表 table 的最大索引编号；如果此表没有正的索引编号，返回 0。\n\n```Lua\nlocal a = {}\na[-1] = 10\nprint(table.maxn(a))\na[5] = 10\nprint(table.maxn(a))\n\n--\u003eoutput05\n```\n\n## **table. Remove (table [, pos])**\n\n在表 table 中删除索引为 pos（pos 只能是 number 型）的元素，并返回这个被删除的元素，它后面所有元素的索引值都会减一。Pos 的默认值是表的长度，即默认是删除表的最后一个元素。\n\n```Lua\nlocal a = { 1, 2, 3, 4}\nprint(table.remove(a, 1)) --删除速索引为1的元素print(a[1], a[2], a[3], a[4])\n\nprint(table.remove(a))   --删除最后一个元素print(a[1], a[2], a[3], a[4])\n\n--\u003eoutput12    3    4    nil42    3    nil    nil\n```\n\n## **table. Sort (table [, comp])**\n\n按照给定的比较函数 comp 给表 table 排序，也就是从 table[1] 到 table[n]，这里 n 表示 table 的长度。比较函数有两个参数，如果希望第一个参数排在第二个的前面，就应该返回 true，否则返回 false。如果比较函数 comp 没有给出，默认从小到大排序。\n\n```Lua\n\nlocal function compare(x, y) --从大到小排序\n    return x \u003e y         --如果第一个参数大于第二个就返回true，否则返回false\nend\n\nlocal a = { 1, 7, 3, 4, 25}\ntable.sort(a)           --默认从小到大排序\nprint(a[1], a[2], a[3], a[4], a[5])\ntable.sort(a, compare) --使用比较函数进行排序\nprint(a[1], a[2], a[3], a[4], a[5])\n\n--\u003eoutput\n1    3    4    7    25\n25    7    4    3    1\n```\n\n## 其他\n\nLuaJIT 2.1 新增加的 `table.new` 和 `table.clear` 函数是非常有用的。前者主要用来预分配 Lua table 空间，后者主要用来高效的释放 table 空间，并且它们都是可以被 JIT 编译的\n\n  \n\n# 日期时间\n\n函数 time、date 和 difftime 提供了所有的日期和时间功能。\n\n在 OpenResty 的世界里，不推荐使用这里的标准时间函数，因为这些函数通常会引发不止一个昂贵的系统调用，同时无法为 LuaJIT JIT 编译，对性能造成较大影响。推荐使用 ngx_lua 模块提供的带缓存的时间接口，如 `ngx.today`, `ngx.time`, `ngx.utctime`, `ngx.localtime`, `ngx.now`, `ngx.http_time`，以及 `ngx.cookie_time` 等。\n\n  \n\n## **os. Time ([table])**\n\n如果不使用参数 table 调用 time 函数，\n\n- 它会返回当前的时间和日期（它表示从某一时刻到现在的秒数）。\n    \n- 如果用 table 参数，它会返回一个数字，表示该 table 中所描述的日期和时间（它表示从某一时刻到 table 中描述日期和时间的秒数）。Table 的字段如下：\n    \n\n|          |                            |\n| -------- | -------------------------- |\n| 字段名称 | 取值范围                   |\n| year     | 四位数字                   |\n| month    | 1--12                      |\n| day      | 1--31                      |\n| hour     | 0--23                      |\n| min      | 0--59                      |\n| sec      | 0--61                      |\n| isdst    | boolean（true 表示夏令时） |\n\n对于 time 函数，如果参数为 table，那么 table 中必须含有 year、month、day 字段。其他字缺省时段默认为中午（12:00:00）。\n\n\u003e 示例代码：（地点为北京）\n\n```Plaintext\nprint(os.time())    --\u003eoutput  1438243393\na = { year = 1970, month = 1, day = 1, hour = 8, min = 1 }\nprint(os.time(a))   --\u003eoutput  60\n```\n\n## **os. Difftime (t 2, t 1)**\n\n返回 t 1 到 t 2 的时间差，单位为秒。\n\n\u003e 示例代码:\n\n```Plaintext\nlocal day1 = { year = 2015, month = 7, day = 30 }\nlocal t1 = os.time(day1)\n\nlocal day2 = { year = 2015, month = 7, day = 31 }\nlocal t2 = os.time(day2)\nprint(os.difftime(t2, t1))   --\u003eoutput  86400\n```\n\n## **os. Date ([format [, time]])**\n\n把一个表示日期和时间的数值，转换成更高级的表现形式。\n\n- 其第一个参数 format 是一个格式化字符串，描述了要返回的时间形式。\n    \n- 第二个参数 time 就是日期和时间的数字表示，缺省时默认为当前的时间。\n    \n- 使用格式字符 \"*t\"，创建一个时间表。\n    \n\n\u003e 示例代码：\n\n```Plaintext\nlocal tab1 = os.date(\"*t\")  --返回一个描述当前日期和时间的表\nlocal ans1 = \"{\"\nfor k, v in pairs(tab1) do  --把tab1转换成一个字符串\n    ans1 = string.format(\"%s %s = %s,\", ans1, k, tostring(v))\nend\n\nans1 = ans1 .. \"}\"\nprint(\"tab1 = \", ans1)\n\n\nlocal tab2 = os.date(\"*t\", 360)  --返回一个描述日期和时间数为360秒的表\nlocal ans2 = \"{\"\nfor k, v in pairs(tab2) do      --把tab2转换成一个字符串\n    ans2 = string.format(\"%s %s = %s,\", ans2, k, tostring(v))\nend\n\nans2 = ans2 .. \"}\"\nprint(\"tab2 = \", ans2)\n\n--\u003eoutput\ntab1 = { hour = 17, min = 28, wday = 5, day = 30, month = 7, year = 2015, sec = 10, yday = 211, isdst = false,}\ntab2 = { hour = 8, min = 6, wday = 5, day = 1, month = 1, year = 1970, sec = 0, yday = 1, isdst = false,}\n```\n\n该表中除了使用到了 time 函数参数 table 的字段外，这还提供了星期（wday，星期天为 1）和一年中的第几天（yday，一月一日为 1）。除了使用 \"*t\" 格式字符串外，如果使用带标记（见下表）的特殊字符串，os. Date 函数会将相应的标记位以时间信息进行填充，得到一个包含时间的字符串。表如下：\n\n|          |                                           |\n| -------- | ----------------------------------------- |\n| 格式字符 | 含义                                      |\n| %a       | 一星期中天数的简写（例如：Wed）           |\n| %A       | 一星期中天数的全称（例如：Wednesday）     |\n| %b       | 月份的简写（例如：Sep）                   |\n| %B       | 月份的全称（例如：September）             |\n| %c       | 日期和时间（例如：07/30/15 16:57:24）     |\n| %d       | 一个月中的第几天[01 ~ 31]                 |\n| %H       | 24 小时制中的小时数[00 ~ 23]              |\n| %I       | 12 小时制中的小时数[01 ~ 12]              |\n| %j       | 一年中的第几天[001 ~ 366]                 |\n| %M       | 分钟数[00 ~ 59]                           |\n| %m       | 月份数[01 ~ 12]                           |\n| %p       | “上午（am）”或“下午（pm）”                |\n| %S       | 秒数[00 ~ 59]                             |\n| %w       | 一星期中的第几天[1 ~ 7 = 星期天 ~ 星期六] |\n| %x       | 日期（例如：07/30/15）                    |\n| %X       | 时间（例如：16:57:24）                    |\n| %y       | 两位数的年份[00 ~ 99]                     |\n| %Y       | 完整的年份（例如：2015）                  |\n| %%       | 字符'%'                                   |\n\n\u003e 示例代码：\n\n```Plaintext\nprint(os.date(\"today is %A, in %B\"))\nprint(os.date(\"now is %x %X\"))\n\n--\u003eoutput\ntoday is Thursday, in July\nnow is 07/30/15 17:39:22\n```\n\n  \n\n# 数学库\n\nUa 数学库由一组标准的数学函数构成。数学库的引入丰富了 Lua 编程语言的功能，同时也方便了程序的编写。常用数学函数见下表：\n\n|               asd           |                                                                                        sdfa                                                                                                      | \n| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| 函数名                   | 函数功能                                                                                                                                                                                     |\n| math.Rad (x)             | 角度 x 转换成弧度                                                                                                                                                                            |\n| math.Deg (x)             | 弧度 x 转换成角度                                                                                                                                                                            |\n| math.Max (x, ...)        | 返回参数中值最大的那个数，参数必须是 number 型                                                                                                                                               |\n| math.Min (x, ...)        | 返回参数中值最小的那个数，参数必须是 number 型                                                                                                                                               |\n| math. Random ([m [, n]]) | 不传入参数时，返回一个在区间[0,1)内均匀分布的伪随机实数；只使用一个整数参数 m 时，返回一个在区间[1, m]内均匀分布的伪随机整数；使用两个整数参数时，返回一个在区间[m, n]内均匀分布的伪随机整数 |\n| math. Randomseed (x)     | 为伪随机数生成器设置一个种子 x，相同的种子将会生成相同的数字序列                                                                                                                             |\n| math.Abs (x)             | 返回 x 的绝对值                                                                                                                                                                              |\n| math.Fmod (x, y)         | 返回 x 对 y 取余数                                                                                                                                                                           |\n| math.Pow (x, y)          | 返回 x 的 y 次方                                                                                                                                                                             |\n| math.Sqrt (x)            | 返回 x 的算术平方根                                                                                                                                                                          |\n| math.Exp (x)             | 返回自然数 e 的 x 次方                                                                                                                                                                       |\n| math.Log (x)             | 返回 x 的自然对数                                                                                                                                                                            |\n| math. Log 10 (x)         | 返回以 10 为底，x 的对数                                                                                                                                                                     |\n| math.Floor (x)           | 返回最大且不大于 x 的整数                                                                                                                                                                    |\n| math.Ceil (x)            | 返回最小且不小于 x 的整数                                                                                                                                                                    |\n| math. Pi                 | 圆周率                                                                                                                                                                                       |\n| math.Sin (x)             | 求弧度 x 的正弦值                                                                                                                                                                            |\n| math.Cos (x)             | 求弧度 x 的余弦值                                                                                                                                                                            |\n| math.Tan (x)             | 求弧度 x 的正切值                                                                                                                                                                            |\n| math.Asin (x)            | 求 x 的反正弦值                                                                                                                                                                              |\n| math.Acos (x)            | 求 x 的反余弦值                                                                                                                                                                              |\n| math.Atan (x)            | 求 x 的反正切值                                                                                                                                                                              |\n\n```Lua\nprint(math.pi)           --\u003eoutput  3.1415926535898\nprint(math.rad(180))     --\u003eoutput  3.1415926535898\nprint(math.deg(math.pi)) --\u003eoutput  180\n\nprint(math.sin(1))       --\u003eoutput  0.8414709848079\nprint(math.cos(math.pi)) --\u003eoutput  -1\nprint(math.tan(math.pi / 4))  --\u003eoutput  1\n\nprint(math.atan(1))      --\u003eoutput  0.78539816339745\nprint(math.asin(0))      --\u003eoutput  0\n\nprint(math.max(-1, 2, 0, 3.6, 9.1))     --\u003eoutput  9.1\nprint(math.min(-1, 2, 0, 3.6, 9.1))     --\u003eoutput  -1\n\nprint(math.fmod(10.1, 3))   --\u003eoutput  1.1\nprint(math.sqrt(360))      --\u003eoutput  18.97366596101\n\nprint(math.exp(1))         --\u003eoutput  2.718281828459\nprint(math.log(10))        --\u003eoutput  2.302585092994\nprint(math.log10(10))      --\u003eoutput  1\n\nprint(math.floor(3.1415))  --\u003eoutput  3\nprint(math.ceil(7.998))    --\u003eoutput  8\n```\n\n使用 `math.random()` 函数获得伪随机数时，如果不使用 `math.randomseed()` 设置伪随机数生成种子或者设置相同的伪随机数生成种子，那么得得到的伪随机数序列是一样的。\n\n```Lua\nmath.randomseed (100) --把种子设置为100\nprint(math.random())         --\u003eoutput  0.0012512588885159\nprint(math.random(100))      --\u003eoutput  57\nprint(math.random(100, 360)) --\u003eoutput  150\n```\n\n稍等片刻，再次运行上面的代码。\n\n```Lua\nmath.randomseed (100) --把种子设置为100\nprint(math.random())         --\u003eoutput  0.0012512588885159\nprint(math.random(100))      --\u003eoutput  57\nprint(math.random(100, 360)) --\u003eoutput  150\n```\n\n两次运行的结果一样。为了避免每次程序启动时得到的都是相同的伪随机数序列，通常是使用当前时间作为种子。\n\n\u003e 修改上例中的代码：\n\n```Lua\nmath.randomseed (os.time())   --把100换成os.time()\nprint(math.random())          --\u003eoutput 0.88369396038697\nprint(math.random(100))       --\u003eoutput 66\nprint(math.random(100, 360))  --\u003eoutput 228\n```\n\n稍等片刻，再次运行上面的代码。\n\n```Plaintext\nmath.randomseed (os.time())   --把100换成os.time()\nprint(math.random())          --\u003eoutput 0.88946195867794\nprint(math.random(100))       --\u003eoutput 68\nprint(math.random(100, 360))  --\u003eoutput 129\n```\n\n  \n\n# 文件\n\nLua I/O 库提供两种不同的方式处理文件：隐式文件描述，显式文件描述。\n\n这些文件 I/O 操作，**在 OpenResty 的上下文中对事件循环是会产生阻塞效应**。OpenResty 比较擅长的是高并发网络处理，在这个环境中，任何文件的操作，都将阻塞其他并行执行的请求。**实际中的应用，在 OpenResty 项目中应尽可能让网络处理部分、文件 I/0 操作部分相互独立，不要揉和在一起**。\n\n## **隐式文件描述**\n\n设置一个默认的输入或输出文件，然后在这个文件上进行所有的输入或输出操作。所有的操作函数由 io 表提供。\n\n\u003e 打开已经存在的 `test1.txt` 文件，并读取里面的内容\n\n```Plaintext\nfile = io.input(\"test1.txt\")    -- 使用 io.input() 函数打开文件repeat\n    line = io.read()            -- 逐行读取内容，文件结束时返回nil\n    if nil == line then\n        break\n    end\n    print(line)\nuntil (false)\n\nio.close(file)                  -- 关闭文件--\u003e output\nmy test file\nhello\nlua\n```\n\n\u003e 在 `test1.txt` 文件的最后添加一行 \"hello world\"\n\n```Plaintext\nfile = io.open(\"test1.txt\", \"a+\")   -- 使用 io.open() 函数，以添加模式打开文件\nio.output(file)                     -- 使用 io.output() 函数，设置默认输出文件\nio.write(\"\\nhello world\")           -- 使用 io.write() 函数，把内容写到文件\nio.close(file)\n```\n\n在相应目录下打开 `test1.txt` 文件，查看文件内容发生的变化。\n\n## **显式文件描述**\n\n使用 file: XXX () 函数方式进行操作, 其中 file 为 io.Open () 返回的文件句柄。\n\n\u003e 打开已经存在的 test 2. Txt 文件，并读取里面的内容\n\n```Plaintext\nfile = io.open(\"test2.txt\", \"r\")    -- 使用 io.open() 函数，以只读模式打开文件\n\nfor line in file:lines() do         -- 使用 file:lines() 函数逐行读取文件\n    print(line)\nend\n\nfile:close()\n\n--\u003eoutput\nmy test2\nhello lua\n```\n\n\u003e 在 test 2. Txt 文件的最后添加一行 \"hello world\"\n\n```Plaintext\nfile = io.open(\"test2.txt\", \"a\")  -- 使用 io.open() 函数，以添加模式打开文件\nfile:write(\"\\nhello world\")       -- 使用 file:write() 函数，在文件末尾追加内容\nfile:close()\n```\n\n在相应目录下打开 `test2.txt` 文件，查看文件内容发生的变化。\n\n## **文件操作函数**\n\n#### **io. Open (filename [, mode])**\n\n按指定的模式 mode，打开一个文件名为 `filename` 的文件，成功则返回文件句柄，失败则返回 nil 加错误信息。模式：\n\n|      |                                                |                     | \n| ---- | ---------------------------------------------- | ------------------- |\n| 模式 | 含义                                           | 文件不存在时        |\n| \"r\"  | 读模式 (默认)                                  | 返回 nil 加错误信息 |\n| \"w\"  | 写模式                                         | 创建文件            |\n| \"a\"  | 添加模式                                       | 创建文件            |\n| \"r+\" | 更新模式，保存之前的数据                       | 返回 nil 加错误信息 |\n| \"w+\" | 更新模式，清除之前的数据                       | 创建文件            |\n| \"a+\" | 添加更新模式，保存之前的数据, 在文件尾进行添加 | 创建文件            |\n\n模式字符串后面可以有一个 'b'，用于在某些系统中打开二进制文件。\n\n注意 \"w\" 和 \"wb\" 的区别\n\n- \"w\" 表示文本文件。某些文件系统 (如 Linux 的文件系统)认为 0 x 0 A 为文本文件的换行符，Windows 的文件系统认为 0 x 0 D 0 A 为文本文件的换行符。为了兼容其他文件系统（如从 Linux 拷贝来的文件），Windows 的文件系统在写文件时，会在文件中 0 x 0 A 的前面加上 0 x 0 D。使用 \"w\"，其属性要看所在的平台。\n    \n- \"wb\" 表示二进制文件。文件系统会按纯粹的二进制格式进行写操作，因此也就不存在格式转换的问题。（Linux 文件系统下 \"w\" 和 \"wb\" 没有区别）\n    \n\n#### **file: close ()**\n\n关闭文件。注意：当文件句柄被垃圾收集后，文件将自动关闭。句柄将变为一个不可预知的值。\n\n#### **io. Close ([file])**\n\n关闭文件，和 file: close () 的作用相同。没有参数 file 时，关闭默认输出文件。\n\n#### **file: flush ()**\n\n把写入缓冲区的所有数据写入到文件 file 中。\n\n#### **io. Flush ()**\n\n相当于 file: flush ()，把写入缓冲区的所有数据写入到默认输出文件。\n\n#### **io. Input ([file])**\n\n当使用一个文件名调用时，打开这个文件（以文本模式），并设置文件句柄为默认输入文件；当使用一个文件句柄调用时，设置此文件句柄为默认输入文件；当不使用参数调用时，返回默认输入文件句柄。\n\n#### **file: lines ()**\n\n返回一个迭代函数, 每次调用将获得文件中的一行内容, 当到文件尾时，将返回 nil，但不关闭文件。\n\n#### **io. Lines ([filename])**\n\n打开指定的文件 filename 为读模式并返回一个迭代函数, 每次调用将获得文件中的一行内容, 当到文件尾时，将返回 nil，并自动关闭文件。若不带参数时 io.Lines () 等价于 io.Input (): lines () 读取默认输入设备的内容，结束时不关闭文件。\n\n#### **io. Output ([file])**\n\n类似于 io. Input，但操作在默认输出文件上。\n\n#### **file: read (...)**\n\n按指定的格式读取一个文件。按每个格式将返回一个字符串或数字, 如果不能正确读取将返回 nil，若没有指定格式将指默认按行方式进行读取。格式：\n\n|        |                                                                                                        |\n| ------ | ------------------------------------------------------------------------------------------------------ |\n| 格式   | 含义                                                                                                   |\n| \"*n\"   | 读取一个数字                                                                                           |\n| \"*a\"   | 从当前位置读取整个文件。若当前位置为文件尾，则返回空字符串                                             |\n| \"*l\"   | 读取下一行的内容。若为文件尾，则返回 nil。(默认)                                                       |\n| number | 读取指定字节数的字符。若为文件尾，则返回 nil。如果 number 为 0, 则返回空字符串，若为文件尾, 则返回 nil |\n\n#### **io. Read (...)**\n\n相当于 io.Input ():read\n\n#### **io. Type (obj)**\n\n检测 obj 是否一个可用的文件句柄。如果 obj 是一个打开的文件句柄，则返回 \"file\" 如果 obj 是一个已关闭的文件句柄，则返回 \"closed file\" 如果 obj 不是一个文件句柄，则返回 nil。\n\n#### **file: write (...)**\n\n把每一个参数的值写入文件。参数必须为字符串或数字，若要输出其它值，则需通过 tostring 或 string. Format 进行转换。\n\n#### **io. Write (...)**\n\n相当于 io.Output (): write。\n\n#### **file: seek ([whence] [, offset])**\n\n设置和获取当前文件位置，成功则返回最终的文件位置 (按字节，相对于文件开头), 失败则返回 nil 加错误信息。缺省时，whence 默认为 \"cur\"，offset 默认为 0 。参数 whence：\n\n|        |                     |\n| ------ | ------------------- |\n| whence | 含义                |\n| \"set\"  | 文件开始            |\n| \"cur\"  | 文件当前位置 (默认) |\n| \"end\"  | 文件结束            |\n\n#### **file: setvbuf (mode [, size])**\n\n设置输出文件的缓冲模式。模式：\n\n|        |                                                              |\n| ------ | ------------------------------------------------------------ |\n| 模式   | 含义                                                         |\n| \"no\"   | 没有缓冲，即直接输出                                         |\n| \"full\" | 全缓冲，即当缓冲满后才进行输出操作 (也可调用 flush 马上输出) |\n| \"line\" | 以行为单位，进行输出                                         |\n\n最后两种模式，size 可以指定缓冲的大小（按字节），忽略 size 将自动调整为最佳的大小。","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":["lua"]},"/netty/%E4%BC%A0%E8%BE%93%E5%92%8CChannel":{"title":"传输和Channel","content":"在某些时候，你需要支撑比预期多很多的并发连接。如果你随后尝试从阻塞传输切换到非阻塞传输，那么你可能会因为这两种网络 API 的截然不同而遇到问题。\n\n**Netty 为它所有的传输实现提供了一个通用 API，这使得这种转换比你直接使用 JDK 所能够达到的简单得多。**\n\n# 传输方式\n\n- OIO\n    \n- NIO\n    \n- AIO\n    \n\n  \n\n# Channel\n\n传输 API 的核心是 interface Channel，它被用于所有的 I/O 操作\n\n![](statistic/asynccode-670.png)\n\n- 每个 Channel 都将会被分配一个 ChannelPipeline 和 ChannelConfig。\n    \n    - ChannelConfig 包含了该 Channel 的所有配置设置，并且支持热更新。由于特定的传输可能 具有独特的设置，所以它可能会实现一个 ChannelConfig 的子类型。\n        \n- 由于 Channel 是独一无二的，所以为了保证顺序将 Channel 声明为 java.lang.Comparable 的一个子接口\n    \n- ChannelPipeline **持有所有将应用于入站和出站数据以及事件的 ChannelHandler实例**，这些 ChannelHandler 实现了应用程序用于处理状态变化以及数据处理的逻辑\n    \n    - ChannelHandler 的典型用途包括：\n        \n        -  将数据从一种格式转换为另一种格式；\n            \n        -  提供异常的通知；\n            \n        -  提供 Channel 变为活动的或者非活动的通知；\n            \n        -  提供当 Channel 注册到 EventLoop 或者从 EventLoop 注销时的通知；\n            \n        -  提供有关用户自定义事件的通知。\n            \n- 也可以利用 Channel 的其他方法\n    \n\n![](statistic/asynccode-668.png)\n\nNetty 的 **Channel 实现是线程安全的.**\n\n# NIO\n\n  \n\nNIO 提供了一个所有 I/O 操作的全异步的实现。\n\n选择器背后的基本概念是充当一个注册表，在那里你将可以请求在 Channel 的状态发生变化时得到通知。可能的状态变化有：\n\n-  新的 Channel 已被接受并且就绪；\n    \n-  Channel 连接已经完成；\n    \n-  Channel 有已经就绪的可供读取的数据；\n    \n-  Channel 可用于写数据。\n    \n\n**选择器运行在一个检查状态变化并对其做出相应响应的线程上，在应用程序对状态的改变做出响应之后，选择器将会被重置**，并将重复这个过程\n\n的常量值代表了由class java.nio.channels.SelectionKey定义的位模式。这些位模式可以组合起来定义一组应用程序正在请求通知的状态变化集。\n\n![](statistic/asynccode-669.png)\n\n![](statistic/asynccode-667.png)\n\n# 零拷贝\n\n零拷贝（zero-copy）是一种目前只有在使用 NIO 和 Epoll 传输时才可使用的特性**。它使你可以快速 高效地将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间，**其在像 FTP 或者 HTTP 这样的协议中可以显著地提升性能。但是，并不是所有的操作系统都支持这一特性。特别地，它对于实现了数据加密或者压缩的文件系统是不可用的——只能传输文件的原始内容。","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/netty/%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84":{"title":"概念和体系结构","content":"# IO 模型\n\n[IO模型详解](https://jovbd87bon.feishu.cn/wiki/wikcnlmBPltNAH5jI0rrD6bpNWf)\n\n  \n\n  \n\n# Java 网络编程\n\n最早期的 Java API（java.net）只支持由本地系统套接字库提供的所谓的阻塞函数\n\n```Java\n//  创建一个新的 ServerSocket，用以 监听指定端口上的连接请求\nServerSocket serverSocket = new ServerSocket(portNumber); \n// 对accept 阻塞，知道创建一个\nSocket clientSocket = serverSocket.accept(); \nBufferedReader in = new BufferedReader(new InputStreamReader(clientSocket.getInputStream())); \nPrintWriter out = new PrintWriter(clientSocket.getOutputStream(), true); \nString request, response; \nwhile ((request = in.readLine()) != null) { \n    if (\"Done\".equals(request)) { \n        break; \n    } \n    response = processRequest(request); \n    out.println(response); \n} \n```\n\n  \n\n# Java Nio\n\n本地套接字库很早就提供了非阻塞调用， 其为网络资源的利用率提供了相当多的控制：\n\n- 可以使用 setsockopt()方法配置套接字，以便读/写调用在没有数据的时候立即返回， 也就是说，如果是一个阻塞调用应该已经被阻塞了① ；\n    \n- 可以使用操作系统的事件通知 API②注册一组非阻塞套接字，以确定它们中是否有任何的套接字已经有数据可供读写。\n    \n\nJava 对于非阻塞 I/O 的支持是在 2002 年引入的，位于 JDK 1.4 的 java.nio 包中\n\n  \n\n## 选择器\n\n![](statistic/asynccode-657.png)\n\nclass java.nio.channels.Selector 是 Java 的非阻塞 I/O 实现的关键。它使用了事件通知 API\n\n以确定在一组非阻塞套接字中有哪些已经就绪能够进 行 I/O 相关的操作。**因为可以在任何的时间检查任意**\n\n**的读操作或者写操作的完成状态，一个单一的线程便可以处理多个并发的连接。**\n\n与阻塞 I/O 模型相比，这种模型提供了更好的资源管理：\n\n- 使用较少的线程便可以处理许多连接，因此也减少了内存管理和上下文切换所带来开销；\n    \n- 当没有 I/O 操作需要处理的时候，线程也可以被用于其他任务。\n    \n\n  \n\n# Netty\n\n在网络编程领域，Netty是Java的卓越框架。 它驾驭了Java高级API的能力，并将其隐藏在一个易于使用的API之后。\n\nNetty 的关键特性\n\n![](statistic/asynccode-665.png)\n\n## Netty 的主要构件块\n\n- Channel；\n    \n- 回调；\n    \n- Future；\n    \n- 事件和 ChannelHandler。\n    \n\n这些构建块代表了不同类型的构造：资源、逻辑以及通知\n\n  \n\n### Channel\n\nChannel 是 Java NIO 的一个基本构造。\n\n\u003e 它代表一个到实体（如一个硬件设备、一个文件、一个网络套接字或者一个能够执行一个或者多个不同的I/O操作的程序组件）的开放连接，如读操作和写操作 ①\n\n**把 Channel 看作是传入（入站）或者传出（出站）数据的载体。因此，它可以被打开或者被关闭，连接或者断开连接。**\n\n### 回调\n\n一个回调其实就是一个方法，**一个指向已经被提供给另外一个方法的方法的引用**。\n\nNetty 在内部使用了回调来处理事件；当一个回调被触发时，相关的事件可以被一个 interface ChannelHandler 的实现处理。\n\n![](statistic/asynccode-663.png)\n\n当一个新的连接已经被建立时，ChannelHandler 的 channelActive()回调方法将会被调用，并将打印出一条信息。\n\n  \n\n### Future\n\nFuture 提供了另一种在操作完成时通知应用程序的方式。这个对象可以看作是一个异步操作的结果的占位符；它将在未来的某个时刻完成，并提供对其结果的访问\n\n- JDK 预置了 interface java.util.concurrent.Future，但是其所提供的实现，只 允许手动检查对应的操作是否已经完成，或者一直阻塞直到它完成。这是非常繁琐的，\n    \n- 所以 Netty 提供了它自己的实现——ChannelFuture，用于在执行异步操作的时候使用\n    \n\n  \n\n- ChannelFuture提供了几种额外的方法，这些方法使得我们能够注册一个或者多个 ChannelFutureListener实例。监听器的回调方法operationComplete()，将会在对应的操作完成时被调用\n    \n- 每个 Netty 的出站 I/O 操作都将返回一个 ChannelFuture；也就是说，它们都不会阻塞。正如我们前面所提到过的一样，Netty 完全是异步和事件驱动的\n    \n\n![](statistic/asynccode-656.png)\n\n  \n\nChannel 回调的用法\n\n![](statistic/asynccode-662.png)\n\n### 事件和ChannelHandler\n\nNetty 使用不同的事件来通知我们状态的改变或者是操作的状态。这使得我们能够基于已经 =发生的事件来触发适当的动作。\n\nNetty 是一个网络编程框架，所以事件是按照它们与入站或出站数据流的相关性进行分类的。\n\n  \n\n可能由**入站**数据或者相关的状态更改而触发的事件包括：\n\n-  连接已被激活或者连接失活；\n    \n-  数据读取；\n    \n-  用户事件；\n    \n-  错误事件。\n    \n\n**出站事件**是未来将会触发的某个动作的操作结果，这些动作包括：\n\n-  打开或者关闭到远程节点的连接；\n    \n-  将数据写到或者冲刷到套接字。\n    \n\n每个事件都可以被分发给 ChannelHandler 类中的某个用户实现的方法。这是一个很好的将事件驱动范式直接转换为应用程序构件块的例子。\n\n![](statistic/asynccode-664.png)\n\n  \n\n## Netty 的组件和设计\n\n- 首先，它的基于 Java NIO 的异步的和事件驱动的实现，保证了高负载下应用程序 性能的最大化和可伸缩性。\n    \n- 其次，Netty 也包含了一组设计模式，将应用程序逻辑从网络层解耦，简化了开发过程，同时也最大限度地提高了可测试性、模块化以及代码的可重用性。\n    \n\n### Channel\n\nNetty 的 Channel 接 口所提供的 API，大大地降低了直接使用 Socket 类的复杂性。其基本的构造是 class Socket。\n\nChannel 也是拥有许多 预定义的、专门化实现的广泛类层次结构的根，下面是一个简短的部分清单：\n\n- EmbeddedChannel；\n    \n- LocalServerChannel；\n    \n- NioDatagramChannel；\n    \n- NioSctpChannel；\n    \n- NioSocketChannel。\n    \n\n### EventLoop\n\nEventLoop 定义了 Netty 的核心抽象，用于处理连接的生命周期中所发生的事件\n\n![](statistic/asynccode-659.png)\n\nChannel、EventLoop、Thread 以及 EventLoopGroup 之间的关系\n\n- 一个 EventLoopGroup 包含一个或者多个 EventLoop；\n    \n- 一个 EventLoop 在它的生命周期内只和一个 Thread 绑定；\n    \n- 所有由 EventLoop 处理的 I/O 事件都将在它专有的 Thread 上被处理；\n    \n- 一个 Channel 在它的生命周期内只注册于一个 EventLoop；\n    \n- 一个 EventLoop 可能会被分配给一个或多个 Channel。\n    \n\n### ChannelFuture\n\n- Netty 中所有的 I/O 操作都是异步的。因为一个操作可能不会立即返回，所以我们需要一种用于在之后的某个时间点确定其结果的方法。\n    \n- Netty 提供了 ChannelFuture 接口，其 addListener()方法注册了一个 ChannelFutureListener，以便在某个操作完成时（无论是否成功）得到通知。\n    \n\n### ChannelHandler\n\n- 从应用程序开发人员的角度来看，Netty 的主要组件是 ChannelHandler，它充当了所有处理入站和出站数据的应用程序逻辑的容器\n    \n- 事实上，ChannelHandler 可专 门用于几乎任何类型的动作，例如将数据从一种格式转换为另外一种格式，或者处理转换过程中所抛出的异常。\n    \n\n### ChannaelPipeline\n\n- ChannelPipeline 提供了 ChannelHandler 链的容器，并**定义了用于在该链上传播入站和出站事件**流的 API\n    \n- 当 Channel 被创建时，它会被自动地分配到它专属的 ChannelPipeline。\n    \n- ChannelHandler 安装到 ChannelPipeline 中的过程如下所示\n    \n    - 一个ChannelInitializer的实现被注册到了ServerBootstrap中 ① ；\n        \n    - 当 ChannelInitializer.initChannel()方法被调用时，ChannelInitializer将在 ChannelPipeline 中安装一组自定义的 ChannelHandler；\n        \n    - ChannelInitializer **将它自己从 ChannelPipeline 中移除**。\n        \n\n  \n\n从 ChannelHandler 派生的 ChannelInboundHandler 和 ChannelOutboundHandler 接口\n\n![](statistic/asynccode-661.png)\n\n  \n\n- 上面的接口使得事件流经 ChannelPipeline 是 ChannelHandler 的工作，它们是在应用程序的**初始化或者引导阶段被安装的**。\n    \n- 这些对象接收事件、执行它们所实现的处理逻辑，并将数据传递给链中的下一个 ChannelHandler。\n    \n\n![](statistic/asynccode-658.png)\n\n- 它们的执行顺序是由它们被添加的顺序所决定的。实际上，被我们称为 ChannelPipeline 的是这些 ChannelHandler 的编排顺序\n    \n- 虽然 ChannelInboundHandle 和 ChannelOutboundHandle 都扩展自 ChannelHandler，但是 Netty 能区分 ChannelInboundHandler 实现和 ChannelOutboundHandler 实现，并确保数据只会在具有相同定向类型的两个 ChannelHandler 之间传递。\n    \n- 在 Netty 中，有**两种发送消息的方式。**\n    \n    - 你可以**直接写到 Channel** 中，会导致消息**从Channe-lPipeline 的尾端开始流动**\n        \n    - 也可以写到和 ChannelHandler相关联的**ChannelHandlerContext对象**中将导致消息从 **ChannelPipeline 中的下一个 ChannelHandler 开始流动。**\n        \n\n  \n\n#### 适配器\n\nNetty 以适配器类的形式提供了大量默认的 ChannelHandler 实现，\n\n- 其旨在简化应用程序处理逻辑的开发过程。\n    \n- 你已经看到了，ChannelPipeline中的每个ChannelHandler 将负责把事件转发到链中的下一个 ChannelHandler。这些适配器类（及它们的子类）将自动 执行这个操作，所以你可以只重写那些你想要特殊处理的方法和事件。\n    \n- 常用的适配器\n    \n    -  ChannelHandlerAdapter\n        \n    -  ChannelInboundHandlerAdapter\n        \n    -  ChannelOutboundHandlerAdapter\n        \n    -  ChannelDuplexHandler\n        \n\n  \n\n#### 编解码器\n\n- 通常来说，这些基类的名称将类似于 ByteToMessageDecoder 或 MessageToByteEncoder。\n    \n- 对于特殊的类型，你可能会发现类似于 ProtobufEncoder 和 ProtobufDecoder 这样的名称——预置的用来支持 Google 的 Protocol Buffers。\n    \n- 严格地说，其他的处理器也可以完成编码器和解码器的功能。但是，正如有用来简化ChannelHandler 的创建的适配器类一样，所有由 Netty 提供的编码器/解码器适配器类都实现 ChannelOutboundHandler 或者 ChannelInboundHandler 接口。\n    \n\n  \n\n#### SimpleChannelInboundHandler\n\n- 最常见的情况是，你的应用程序会利用一个 ChannelHandler 来接收解码消息，并对该数据应用业务逻辑。\n    \n- 要创建一个这样的 ChannelHandler,你只需要扩展基类 `SimpleChannelInboundHandler\u003cT\u003e`，其中 T 是你要处理的消息的 Java 类型 。\n    \n    - 在这种类型的 ChannelHandler 中，最重要的方法是 channelRead0(ChannelHandlerContext,T)**。除了要求不要阻塞当前的 I/O 线程之外**，其具体实现完全取决于你\n        \n\n### BootStrap\n\nNetty 的引导类为应用程序的网络层配置提供了容器，\n\n- 这涉及将一个进程绑定到某个指定的端口，ServerBootStrap\n    \n- 或者将一个进程连接到另一个运行在某个指定主机的指定端口上的进程,BootStrap\n    \n\n![](statistic/asynccode-660.png)\n\n引导一个客户端只需要一个 EventLoopGroup，但是一个ServerBootstrap 则需要两个（也可以是同一个实例）。为什么呢？\n\n因为服务器需要两组不同的 Channel。\n\n- 第一组将只包含一个 ServerChannel，代表服务器自身的已绑定到某个本地端口的正在监听的套接字。\n    \n- 而第二组将包含所有已创建的用来处理传 入客户端连接（**对于每个服务器已经接受的连接都有一个**）的 Channel。\n    \n\n![](statistic/asynccode-666.png)","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/netty/%E7%BC%96%E8%A7%A3%E7%A0%81%E5%99%A8":{"title":"编解码器","content":"每个网络应用程序都必须定义如何解析在\n\n- **两个节点之间来回传输的原始字节**\n    \n- **其和 目标应用程序的数据格式做相互转换**\n    \n\n  \n\n- 编码器是将消息转换为适合于传输的格式（最有可能的就是字节流）；\n    \n- 而对应的解码器则是将网络字节流转换回应用程序的消息格式\n    \n\n  \n\n# 解码器\n\n因为解码器是负责将入站数据从一种格式转换到另一种格式的，所以知道 Netty 的解码器实现了 **ChannelInboundHandler** 也不会让你感到意外。\n\n我们将研究 Netty 所提供的解码器类，并提供关于何时以及如何使用它们的具 体示例。这些类覆盖了两个不同的用例：\n\n- 将字节解码为消息——**ByteToMessageDecoder 和 ReplayingDecoder；**\n    \n- 将一种消息类型解码为另一种——**MessageToMessageDecoder**\n    \n\n  \n\n## ByteToMessageDecoder\n\n将字节解码为消息（或者另一个字节序列）是一项如此常见的任务，以至于 Netty 为它提供了一个抽象的基类：ByteToMessageDecoder\n\n![](statistic/asynccode-750.png)\n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-748.png)\n\n虽然 ByteToMessageDecoder 使得可以很简单地实现这种模式，但是你可能会发现，**在调用 readInt()方法前不得不验证所输入的 ByteBuf 是否具有足够的数据有点繁琐**。在下一节中，\n\n我们将讨论 ReplayingDecoder，它是一个特殊的解码器，以少量的开销消除了这个步骤\n\n  \n\n## ReplayingDecoder\n\nReplayingDecoder扩展了ByteToMessageDecoder类（如代码清单 10-1 所示），使得我们不必调用 readableBytes()方法，它通过使用一个自定义的ByteBuf实现 ，\n\nReplayingDecoderByteBuf，包装传入的ByteBuf实现了这一点，其将在内部执行该调用\n\n![](statistic/asynccode-750.png)\n\n- ByteBuf中提取的int将会被添加到List中。**如果没有足够的字节可用，这个readInt()方法的实现将会抛出一个Error**\n    \n- 并不是所有的 ByteBuf 操作都被支持，如果调用了一个不被支持的方法，将会抛出一个 UnsupportedOperationException；\n    \n- ReplayingDecoder 稍慢于 ByteToMessageDecoder。\n    \n\n  \n\n## MessageToMessageDecoder\n\n```Go\npublic abstract class MessageToMessageDecoder\u003cI\u003e extends ChannelInboundHandlerAdapter\n```\n\n![](statistic/asynccode-750.png)\n\n  \n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-745.png)\n\n## TooLongFrameException 类\n\n由于 Netty 是一个异步框架，所以需要在字节可以解码之前在内存中缓冲它们。因此，不能让解码器缓冲大量的数据以至于耗尽可用的内存.Netty 提供了TooLongFrameException 类，其将由解码器在帧超出指定的大小限制时抛出。\n\nByteToMessageDecoder 是如何使用 TooLongFrameException 来通知 ChannelPipeline 中的其他 ChannelHandler 发生了帧大小溢出的。\n\n![](statistic/asynccode-745.png)\n\n  \n\n# 编码器\n\n编码器实现了 ChannelOutboundHandler，并将出站数据从一种格式转换为另一种格式，\n\n-  将消息编码为字节；\n    \n-  将消息编码为消息 ①\n    \n\n我们将首先从抽象基类 MessageToByteEncoder 开始来对这些类进行考察。\n\n## MessageToByteEncoder\n\n使 用 MessageToByteEncoder 将消息转化为字节\n\n![](statistic/asynccode-744.png)\n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-743.png)\n\n## MessageToMessageEncoder\n\n数据将如何从一种消息编码为另一种\n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-726.png)\n\n  \n\n  \n\n# 抽象的编解码\n\n们一直将解码器和编码器作为单独的实体讨论，但是你有时将会发现在同一个类中管理入站和出站数据和消息的转换是很有用的。\n\n这些类同时实现了 ChannelInboundHandler 和 ChannelOutboundHandler 接口。\n\n  \n\n## ByteToMessageCodec\n\n![](statistic/asynccode-728.png)\n\n## MessageToMessageCodec\n\n![](statistic/asynccode-727.png)\n\n## CombinedChannelDuplexHandler\n\n结合一个解码器和编码器可能会对可重用性造成影响。但是，有一 种方法既能够避免这种惩罚，又不会牺牲将一个解码器和一个编码器作为一个单独的单元部署所带来的便利性。CombinedChannelDuplexHandler 提供了这个解决方案\n\n```Go\npublic class CombinedChannelDuplexHandler \u003cI extends ChannelInboundHandler, O extends ChannelOutboundHandler\u003e\n```\n\n这个类充当了 ChannelInboundHandler 和 ChannelOutboundHandler（该类的类型 参数 I 和 O）的容器。\n\n通过提供分别继承了解码器类和编码器类的类型，我们可以实现一个编解码器，**而又不必直接扩展抽象的编解码器类**\n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-745.png)\n\n![](statistic/asynccode-745.png)","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/netty/BootStrap":{"title":"BootStrap","content":"\n简单来说，**引导一个应用程序是指对它进行配置，并使它运行起来的过程**—尽管该过程的具体细节可能并不如它的定义那样简单，尤其是对于一个网络应用程序来说\n\n  \n\nNetty处理引导的方式**使你的应用程序和网络层相隔离**，无论它是客户端还是服务器\n\n  \n\n# BootStrap类\n\n引导类的层次结构包括一个抽象的父类和两个具体的引导子类\n\n![](statistic/asynccode-699.png)\n\n相对于将具体的引导类分别看作用于服务器和客户端的引导来说，记住它们的本意是用来支撑不同的应用程序的功能的将有所裨益。\n\n- 服务器致力于使用一个父 Channel 来接受来自客户端的连接，并创建子 Channel 以用于它们之间的通信\n    \n- 而客户端将最可能只需要一个单独的、没有父 Channel 的 Channel 来用于所有的网络交互\n    \n\n  \n\n两种应用程序类型之间通用的引导步骤由 AbstractBootstrap 处理，而特定于客户端或者服务器的引导步骤则分别由 Bootstrap 或 ServerBootstrap 处理\n\n  \n\nAbstractBootstrap 类的完整声明是：\n\n```Go\npublic abstract class AbstractBootstrap \u003cB extends AbstractBootstrap\u003cB,C\u003e,C extends Channel\u003e\n```\n\n在这个签名中，子类型 B 是其父类型的一个类型参数，**因此可以返回到运行时实例的引用以支持方法的链式调用**\n\n其子类的声明如下：\n\n```Go\npublic class Bootstrap extends AbstractBootstrap\u003cBootstrap,Channel\u003e\n\npublic class ServerBootstrap extends AbstractBootstrap\u003cServerBootstrap,ServerChannel\u003e\n```\n\n  \n\n# 引导客户端和无连接协议\n\n**Bootstrap 类被用于客户端或者使用了无连接协议的应用程序中**\n\n![](statistic/asynccode-732.png)\n\n![](statistic/asynccode-708.png)\n\n## BootStrap\n\nBootstrap 类负责为客户端和使用无连接协议的应用程序创建 Channel\n\n![](statistic/asynccode-709.png)\n\n![](statistic/asynccode-734.png)\n\n![](statistic/asynccode-703.png)\n\n## Channel 和 EventLoopGroup 的兼容性\n\n你可以从包名以及与其相对应 的类名的前缀看到，对于 NIO 以及 OIO 传输两者来说，都有相关的 EventLoopGroup 和Channel 实现。\n\n![](statistic/asynccode-733.png)\n\n不能混用具有不同前缀的组件，如 NioEventLoopGroup 和 OioSocketChannel，会导致 IllegalStateException，\n\n![](statistic/asynccode-710.png)\n\n# 引导服务端\n\n  \n\n## ServerBootStrap\n\n  \n\n![](statistic/asynccode-702.png)\n\n列出了一些bootStrap不存在的方法：childHandler()、 childAttr()和 childOption()。这些调用支持特别用于服务器应用程序的操作。具体来说， **ServerChannel 的实现负责创建子 Channel，这些子 Channel 代表了已被接受的连接。**负责引导 ServerChannel 的 ServerBootstrap 提供了这些方法，以简化将设置应用到已被接受的子 Channel 的 ChannelConfig 的任务\n\n- ServerBootstrap 在 bind()方法被调用时创建了一个 ServerChannel， 且该 ServerChannel 管理了多个子 Channel。\n    \n\n![](statistic/asynccode-704.png)\n\n![](statistic/asynccode-700.png)\n\n  \n\n# 从Channel引导客户端\n\n我们都在引导的过程中调用了 handler()或者 childHandler()方法来添加单个的 ChannelHandler。\n\n通过在 ChannelPipeline 中将它们链接在一起来部署尽可能多的 ChannelHandler。\n\n![](statistic/asynccode-707.png)\n\n![](statistic/asynccode-706.png)\n\n  \n\n# 使用Netty的ChannelOption属性\n\n可以使用 option()方法来将 ChannelOption 应用到引\n\n在某些常用的属性和数据不可用时，Netty 提供了 AttributeMap 抽象（一个由 Channel 和引导类提供的集合）以及 `AttributeKey\u003cT\u003e`（一 个用于插入和获取属性值的泛型类）。\n\n使用这些工具，便可以安全地将任何类型的数据项与客户端和服务器 Channel（包含 ServerChannel 的子 Channel）相关联了。\n\n![](statistic/asynccode-705.png)\n\n# 引导DataGramChannel\n\nNetty 提供了各种 DatagramChannel 的实现。唯一区别就是，**不再调用 connect()方法，而是只调用 bind()方法**\n\n![](statistic/asynccode-701.png)\n\n# 关闭\n\n- 引导使你的应用程序启动并且运行起来，但是迟早你都需要优雅地将它关闭\n    \n- 最重要的是，你需要关闭 EventLoopGroup，它将处理任何挂起的事件和任务，并且随后释放所有活动的线程。\n    \n- 这就是调用 EventLoopGroup.shutdownGracefully()方法的作用。\n    \n\n![](statistic/asynccode-729.png)","lastmodified":"2023-08-01T17:34:29.084243847Z","tags":[]},"/netty/ByteBuf":{"title":"ByteBuf","content":"网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。\n\nNetty 的 ByteBuffer 替代品是 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。\n\n# ByteBuf 的优点\n\nNetty 的数据处理 API 通过两个组件暴露——abstract class ByteBuf 和 interface ByteBufHolder。\n\n下面是一些 ByteBuf API 的优点：\n\n 它可以被用户自定义的缓冲区类型扩展；\n\n 通过内置的复合缓冲区类型实现了透明的零拷贝；\n\n 容量可以按需增长（类似于 JDK 的 StringBuilder）；\n\n 在读和写这两种模式之间切换不需要调用 ByteBuffer 的 flip()方法；\n\n 读和写使用了不同的索引；\n\n 支持方法的链式调用；\n\n# ByteBuf 如何工作\n\n**ByteBuf 维护了两个不同的索引：一个用于读取，一个用于写入**。当你从 ByteBuf 读取时， 它的 readerIndex 将会被递增已经被读取的字节数。同样地，当你写入 ByteBuf 时，它的 writerIndex 也会被递增。\n\n![](statistic/asynccode-671.png)\n\nreaderIndex 达到 和 writerIndex 同样的值时, 试图读取超出该点的数据将会触发一个 IndexOutOfBoundsException。\n\n  \n\n# ByteBuf 的使用模式\n\n## 堆缓冲区\n\n最常用的 ByteBuf 模式是将数据存储在 **JVM 的堆空间**中，这种模式被称为**支撑数组** （backing array），它能在**没有使用池化的情况下提供快速的分配和释放**\n\n![](statistic/asynccode-673.png)\n\n## 直接缓冲区\n\nNIO 在 JDK 1.4 中引入的 ByteBuffer 类允许 JVM 实现通过本地调 用来分配内存。这主要是为了避**免在每次调用本地 I/O 操作之前（或者之后）将缓冲区的内容复 制到一个中间缓冲区（或者从中间缓冲区把内容复制到缓冲区）。**\n\n直接缓冲区的主要缺点是，相对于基于堆的缓冲区，**它们的分配和释放都较为昂贵。**\n\n![](statistic/asynccode-679.png)\n\n## 符合缓冲区\n\n它为**多个 ByteBuf 提供一个聚合视图。**在这里你可以根据需要添加或者删除 ByteBuf 实例，这是一个 JDK 的 ByteBuffer 实现完全缺失的特性。\n\nNetty 通过一个 ByteBuf 子类——**CompositeByteBuf——实现了这个模式**，它提供了一个将多个**缓冲区表示为单个合并缓冲区的虚拟表示**。\n\n为了举例说明，让我们考虑一下一个由两部分——头部和主体——组成的将通过 HTTP 协议 传输的消息。**这两部分由应用程序的不同模块产生，将会在消息被发送的时候组装**。该应用程序 可以选择为多个消息重用相同的消息主体。当这种情况发生时，对于每个消息都将会创建一个新的头部。\n\n![](statistic/asynccode-683.png)\n\n- 使用ByteBuffer 的符合缓冲区模式\n    \n\n![](statistic/asynccode-684.png)\n\n- 使用 **CompositeByteBuf** 的复合缓冲区模式\n    \n\n![](statistic/asynccode-684.png)\n\n- 访问 **CompositeByteBuf** 中的数据\n    \n\n![](statistic/asynccode-687.png)\n\n  \n\n# 字节级操作\n\n## 随机访问索引\n\n![](statistic/asynccode-672.png)\n\n那些需要一个索引值参数的方法（的其中）之一来访问数据既不会改变readerIndex 也不会改变 writerIndex。\n\n  \n\n## 顺序访问索引\n\n虽然 ByteBuf 同时具有读索引和写索引，\n\n![](statistic/asynccode-677.png)\n\n## 可丢弃字节\n\n记为可丢弃字节的分段包含了已经被读过的字节。通过调用 discardReadBytes()方法，可以丢弃它们并回收空间\n\n  \n\n## 可读字节\n\nByteBuf 的可读字节分段存储了实际数据。新分配的、包装的或者复制的缓冲区的默认的\n\nreaderIndex 值为 0。任何名称以 read 或者 skip 开头的操作都将检索或者跳过位于当前\n\nreaderIndex 的数据，并且将它增加已读字节数。\n\n  \n\n## 可写字节\n\n可写字节分段是指一个拥有未定义内容的、写入就绪的内存区域。新分配的缓冲区的 writerIndex 的默认值为 0。任何名称以 write 开头的操作都将从当前的 writerIndex 处 开始写数据，并将它增加已经写入的字节数。如果写操作的目标也是 ByteBuf，并且没有指定源索引的值，则源缓冲区的 readerIndex 也同样会被增加相同的大小。这个调用如下所示：\n\nwriteBytes(ByteBuf dest);\n\n  \n\n## 读写操作\n\n正如我们所提到过的，有两种类别的读/写操作：\n\n get()和 set()操作，从给定的索引开始，并且保持索引不变；\n\n read()和 write()操作，从给定的索引开始，并且会根据已经访问过的字节数对索\n\n引进行调整\n\n![](statistic/asynccode-685.png)\n\n![](statistic/asynccode-675.png)\n\n![](statistic/asynccode-680.png)\n\n![](statistic/asynccode-686.png)\n\n## 其他操作\n\n![](statistic/asynccode-674.png)\n\n![](statistic/asynccode-681.png)\n\n  \n\n# ByteBufHolder\n\n- 除了实际的数据负载之外，我们还需要**存储各种属性值**。 为了处理这种常见的用例，Netty 提供了 ByteBufHolder\n    \n- ByteBufHolder 也为 Netty 的 高级特性提供了支持，如缓冲区池化，其中可以从池中借用 ByteBuf，并且在需要时自动释放\n    \n- ByteBufHolder 只有几种用于访问底层数据和引用计数的方法\n    \n\n![](statistic/asynccode-676.png)\n\n  \n\n# ByteBuf分配\n\n## 按需分配ByteBufAllocator\n\n，Netty 通过 interface ByteBufAllocator 实现了 （ByteBuf 的）**池化**，它可以用来分配我们所描述过的任意类型的 ByteBuf 实例\n\n![](statistic/asynccode-682.png)\n\n可以通过 Channel（每个都可以有一个不同的 ByteBufAllocator 实例）或者绑定到 ChannelHandler 的 ChannelHandlerContext 获取一个到 ByteBufAllocator 的引用。\n\n![](statistic/asynccode-686.png)\n\nNetty提供了两种ByteBufAllocator的实现：PooledByteBufAllocator和UnpooledByteBufAllocator。\n\n- PooledByteBufAllocator，池化了ByteBuf的实例以提高性能并最大限度地减少内存碎片\n    \n- UnpooledByteBufAllocator，非池化ByteBuf实例，并且在每次它被调用时都会返回一个新的实例\n    \n\n**Netty默认使用了PooledByteBufAllocator**，但这可以很容易地通过ChannelConfig API或者在引导你的应用程序时指定一个不同的分配器来更改\n\n  \n\n## Unpooled 缓冲区\n\n你未能获取一个到 ByteBufAllocator 的引用。对于这种情况，Netty 提供了一个简单的称为 Unpooled 的工具类，它提供了静态的辅助方法来创建未池化的 ByteBuf 实例。\n\n![](statistic/asynccode-678.png)\n\n## ByteBufUtil\n\nByteBufUtil 提供了用于操作 ByteBuf 的静态的辅助方法。因为这个 API 是通用的，并\n\n且和池化无关，所以这些方法已然在分配类的外部实现\n\n  \n\n# 引用计数\n\nNetty 在第 4 版中为 ByteBuf 和 ByteBufHolder 引入了引用计数技术，它们都实现了 interface ReferenceCounted。\n\n它主要涉及跟踪到某个特定对象的活动引用的数量。一个 ReferenceCounted 实现的实例将通常以活动的引用计数为 1 作为开始。\n\n只要引用计数大于 0，就能保证对象不会被释放。当活动引用的数量减少到 0 时，该实例就会被释放\n\n引用计数对于池化实现（如 PooledByteBufAllocator）来说是至关重要的，它降低了内存分配的开销。","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/netty/ChannelHandlerChannelPipelineChannelContext":{"title":"ChannelHandler、ChannelPipeline、ChannelContext","content":"当我们在本章中探讨 Netty 的数据流以及处理组件\n\n在 ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑。我们将会研究涉及这些类的各种用例，以及一个重要的关系—ChannelHandlerContext\n\n  \n\n# ChannelHandler\n\n  \n\n## Channel 的生命周期\n\n![](statistic/asynccode-731.png)\n\n![](statistic/asynccode-731.png)\n\n![](statistic/asynccode-689.png)\n\n  \n\nChannelHandler的生命周期\n\n![](statistic/asynccode-731.png)\n\n  \n\nNetty定义了两个重要的ChannelHandler\n\n-  ChannelInboundHandler——处理入站数据以及各种状态变化；\n    \n-  ChannelOutboundHandler——处理出站数据并且允许拦截所有的操作。\n    \n\n  \n\n## ChannelInBoundHandler接口\n\nChannelInBoundHandler的生命周期方法。这些方法将会在数据被接收时或者与其对应的 Channel 状态发生改变时被调用。\n\n![](statistic/asynccode-721.png)\n\n当某个 ChannelInboundHandler 的实现重写 channelRead()方法时，它将负责显式地释放与池化的 ByteBuf 实例相关的内存。Netty 为此提供了一个实用方法 ReferenceCountUtil.release()\n\n![](statistic/asynccode-711.png)\n\n一个更加简单的方式是使用 SimpleChannelInboundHandler\n\n![](statistic/asynccode-722.png)\n\n由于 SimpleChannelInboundHandler 会自动释放资源，所以你不应该存储指向任何消 息的引用供将来使用\n\n## ChannelOutboundHandler\n\n出站操作和数据将由 ChannelOutboundHandler 处理。它的方法将被 Channel、ChannelPipeline 以及 ChannelHandlerContext 调用。\n\nChannelOutboundHandler 的一个强大的功能**是可以按需推迟操作或者事件**，这使得可以通过一些复杂的方法来处理请求。例如，如果到远程节点的写入被暂停了，那么你可以推迟冲刷操作并在稍后继续\n\n![](statistic/asynccode-714.png)\n\n  \n\n\u003e **ChannelPromise**与**ChannelFuture** ChannelOutboundHandler中的大部分方法都需要一个\n\u003e \n\u003e ChannelPromise参数，以便在操作完成时得到通知。ChannelPromise是ChannelFuture的一个\n\u003e \n\u003e 子类，其定义了一些可写的方法，如setSuccess()和setFailure()，从而使ChannelFuture不\n\u003e \n\u003e 可变\n\n  \n\n## 适配器\n\n![](statistic/asynccode-723.png)\n\n## 资源管理\n\n- 每当通过调用 ChannelInboundHandler.channelRead()或者 ChannelOutboundHandler.write()方法来处理数据时，你都需要确保没有任何的资源泄漏。\n    \n- Netty 使用引用计数来处理池化的 ByteBuf。所以在完全使用完某个ByteBuf 后，**调整其引用计数是很重要的**\n    \n\n  \n\nNetty提供了class ResourceLeakDetector①，它将对你应用程序的缓冲区分配做大约 1%的采样来检测内存泄露\n\nNetty的泄露级别\n\n![](statistic/asynccode-713.png)\n\n消费并释放入站消息\n\n![](statistic/asynccode-737.png)\n\n丢弃并释放出站消息\n\n![](statistic/asynccode-731.png)\n\n  \n\n# ChannelPipeline\n\n\u003e - ChannelPipeline 保存了与 Channel 相关联的 ChannelHandler；\n\u003e     \n\u003e - ChannelPipeline 可以根据需要，通过添加或者删除 ChannelHandler 来动态地修改；\n\u003e     \n\u003e - ChannelPipeline 有着丰富的 API 用以被调用，以响应入站和出站事件。\n\u003e     \n\n  \n\n- 每一个新创建的 Channel 都将会被分配一个新的 ChannelPipeline。这项关联是永久性的；\n    \n- Channel 既不能附加另外一个 ChannelPipeline，也不能分离其当前的\n    \n- 根据事件的起源，事件将会被 ChannelInboundHandler 或者 ChannelOutboundHandler处理，随后，通过调用 ChannelHandlerContext 实现，它将被转发给同一超类型的下一个 ChannelHandler\n    \n\n  \n\n\u003e ChannelHandlerContext使得ChannelHandler能够和它的ChannelPipeline以及其他的ChannelHandler 交互，\n\u003e \n\u003e - ChannelHandler 可以通知其所属的 ChannelPipeline 中的下一 个ChannelHandler，\n\u003e     \n\u003e - 甚至可以动态修改它所属的ChannelPipeline\n\u003e     \n\n  \n\n了一个典型的同时具有入站和出站 ChannelHandler 的 ChannelPipeline 的布 局，并且印证了我们之前的关于 ChannelPipeline 主要由一系列的 ChannelHandler 所组成的说法。\n\n  \n\n![](statistic/asynccode-718.png)\n\n- ChannelPipeline 还提供了通过 ChannelPipeline 本身传播事件的方法。\n    \n    - 如果一个入站事件被触发，它将被从 ChannelPipeline 的头部开始一直被传播到 Channel Pipeline 的尾端\n        \n    - 一个出站 I/O 事件将从 ChannelPipeline 的最右边开始，然后向左传播。\n        \n- 在 ChannelPipeline 传播事件时，它会测试 ChannelPipeline 中的下一个 ChannelHandler 的类型是否和事件的运动方向相匹配。\n    \n    - 如果不匹配，ChannelPipeline 将跳过该ChannelHandler 并前进到下一个，直到它找到和该事件所期望的方向相匹配的为止。\n        \n\n  \n\n## 修改ChannelPipeline\n\nChannelHandler 可以通过添加、删除或者替换其他的 ChannelHandler 来实时地修改ChannelPipeline 的布局。\n\n![](statistic/asynccode-697.png)\n\n![](statistic/asynccode-749.png)\n\n  \n\n## ChannelHandler 的阻塞和执行\n\n- 通常 ChannelPipeline 中的每一个 ChannelHandler 都是通过它的 EventLoop（I/O 线程）来处 理传递给它的事件的。所以**至关重要的是不要阻塞这个线程**，因为这会对整体的 I/O 处理产生负面的影响。\n    \n- 但有时可能需要与那些使用阻塞 API 的遗留代码进行交互。对于这种情况，ChannelPipeline 有一些接受一个 EventExecutorGroup 的 add()方法。如果一个事件被传递给一个自定义的 EventExecutorGroup，它将被包含在这个 EventExecutorGroup 中的某个 **EventExecutor** 所处理，**从而被从该 Channel 本身的 EventLoop 中移除**。对于这种用例，Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。\n    \n\n## 其他获取ChannelHandler的方法\n\n还有别的通过类型或者名称来访问 ChannelHandler 的方法\n\n![](statistic/asynccode-731.png)\n\n## 触发事件\n\n入站操作，用于通知 ChannelInboundHandler 在 ChannelPipeline 中所发生的事件\n\n![](statistic/asynccode-712.png)\n\nChannelPipeline API 的出站操作。\n\n![](statistic/asynccode-719.png)\n\n  \n\n# ChannelHandlerContext\n\n- ChannelHandlerContext 代表了 ChannelHandler 和 ChannelPipeline 之间的关联，每当有 ChannelHandler 添加到 ChannelPipeline 中时，都会创建 ChannelHandlerContext\n    \n- ChannelHandlerContext 的主要功能是管理它所关联的 ChannelHandler 和在 同一个 ChannelPipeline 中的其他 ChannelHandler 之间的交互\n    \n\n  \n\nChannelHandlerContext 有很多的方法，其中一些方法也存在于 Channel 和 ChannelPipeline 本身上，但是有一点重要的不同。\n\n- 如果调用 Channel 或者 ChannelPipeline 上的这些方法，它们将沿着整个 ChannelPipeline 进行传播\n    \n- 而调用位于 ChannelHandlerContext 上的相同方法，则将从当前所关联的 ChannelHandler 开始，并且**只会传播给位于该 ChannelPipeline 中的下一个能够处理该事件的 ChannelHandler**\n    \n\n![](statistic/asynccode-698.png)\n\n![](statistic/asynccode-731.png)\n\n当使用 ChannelHandlerContext 的 API 的时候，请牢记以下两点：\n\n-  ChannelHandlerContext 和 ChannelHandler 之间的关联（绑定）是永远不会改变的，所以缓存对它的引用是安全的；\n    \n-  如同我们在本节开头所解释的一样，相对于其他类的同名方法，ChannelHandleContext 的方法将产生更短的事件流，应该尽可能地利用这个特性来获得最大的性能\n    \n\n## 使用\n\n![](statistic/asynccode-730.png)\n\n  \n\n### **ChannelHandlerContext** 访问 **Channel**\n\n![](statistic/asynccode-741.png)\n\n### 通过 **ChannelHandlerContext** 访问 **ChannelPipeline**\n\n![](statistic/asynccode-715.png)\n\n- 虽然被调用的 Channel 或 ChannelPipeline 上的 write()方法将**一直传播事件通过整个 ChannelPipeline**，\n    \n- 但是在 ChannelHandler 的级别上，事件从一个 ChannelHandler 到下一个 ChannelHandler 的移动是由ChannelHandlerContext 上的调用完成的\n    \n\n![](statistic/asynccode-716.png)\n\n### 调用 **ChannelHandlerContext** 的 **write()**方法\n\n![](statistic/asynccode-731.png)\n\n消息将从下一个 ChannelHandler 开始流经 ChannelPipeline，绕过了所有前面的 ChannelHandler\n\n![](statistic/asynccode-736.png)\n\n### 可共享的ChannelHandler\n\n![](statistic/asynccode-738.png)\n\n  \n\n# 异常处理\n\n## 处理入站点异常\n\n- 如果在处理入站事件的过程中有异常被抛出，那么它将从它在 ChannelInboundHandler里被触发的那一点开始流经 ChannelPipeline\n    \n- 要想处理这种类型的入站异常，你需要在你的 ChannelInboundHandler 实现中重写下面的方法\n    \n\n![](statistic/asynccode-742.png)\n\n- ChannelHandler.exceptionCaught()的默认实现是简单地将当前异常转发给 ChannelPipeline 中的下一个 ChannelHandler；\n    \n- 如果异常到达了 ChannelPipeline 的尾端，它将会被记录为未被处理；\n    \n- 要想定义自定义的处理逻辑，你需要重写 exceptionCaught()方法。然后你需要决定是否需要将该异常传播出去。\n    \n\n  \n\n## 处理出站异常\n\n用于处理出站操作中的正常完成以及异常的选项，都基于以下的通知机制。\n\n- 每个出站操作都将返回一个 ChannelFuture。注册到 ChannelFuture 的 ChannelFutureListener 将在操作完成时被通知该操作是成功了还是出错了\n    \n\n![](statistic/asynccode-720.png)\n\n- 几乎所有的 ChannelOutboundHandler 上的方法都会传入一个 ChannelPromise 的实例。作为 ChannelFuture 的子类，ChannelPromise 也可以被分配用于异步通知的监听器。但是，ChannelPromise 还具有提供立即通知的可写方法：\n    \n    - ChannelPromise setSuccess();\n        \n    - ChannelPromise setFailure(Throwable cause)\n        \n\n![](statistic/asynccode-690.png)","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/netty/EventLoop-%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B":{"title":"EventLoop 和线程模型","content":"# 常见的线程模型\n\n基本的线程池化模式\n\n- 从池的空闲线程列表中选择一个 Thread，并且指派它去运行一个已提交的任务（一个 Runnable 的实现）；\n    \n- 当任务完成时，将该 Thread 返回给该列表，使其可被重用\n    \n\n![](statistic/asynccode-717.png)\n\n虽然池化和重用线程相对于简单地为每个任务都创建和销毁线程是一种进步，但是它并不能消除由上下文切换所带来的开销\n\n# EventLoop\n\n运行任务来**处理在连接的生命周期内发生的事件是任何网络框架的基本功能**。与之相应的编程上的构造通常被称为**事件循环**—一个 Netty 使用了 interface io.netty.channel.EventLoop 来适配的术语。\n\n  \n\nNetty 的 EventLoop 是协同设计的一部分，它采用了两个基本的 API：并发和网络编程。\n\n- 首先，io.netty.util.concurrent 包构建在 JDK 的 java.util.concurrent 包上，用来提供线程执行器。\n    \n- 其次，io.netty.channel 包中的类，为了与 Channel 的事件进行交互，扩展了这些接口/类\n    \n\n![](statistic/asynccode-691.png)\n\n线程的关系\n\n- 在这个模型中，一个 EventLoop 将由一个永远都不会改变的 Thread 驱动\n    \n- 同时任务 （Runnable 或者 Callable）可以直接提交给 EventLoop 实现，以立即执行或者调度执行。\n    \n- 根据配置和可用核心的不同，可能会创建多个 EventLoop 实例用以优化资源的使用，\n    \n- 并且单个 EventLoop 可能会被指派用于服务多个 Channel\n    \n- netty的EventLoop在继承了ScheduledExecutorService的同时，只定 义了一个方法，parent(),用于返回到当前EventLoopGroup的引用。\n    \n\n```Go\npublic interface EventLoop extends EventExecutor, EventLoopGroup { \n    @Override \n    EventLoopGroup parent(); \n}\n```\n\n  \n\n## Netty4 中的I/O和事件处理\n\nI/O 操作触发的事件将流经安装了一个或者多个 ChannelHandler 的 ChannelPipeline。**传播这些事件的方法调用可以随后被 ChannelHandler 所拦截，并且可以按需地处理事件**\n\n- 在Netty 4 中，所有的I/O操作和事件都由已经被分配给了EventLoop的那个Thread来处理\n    \n\n不同于 Netty 3 中所使用的模型\n\n  \n\n## Netty3 中的I/O操作\n\n在以前的版本中\n\n- 所使用的线程模型只保证了入站（之前称为上游）事件会在所谓的 I/O 线程（对应于 Netty 4 中的 EventLoop）中执行。\n    \n- 所有的出站（下游）事件都由调用线程处理，其可能是 I/O 线程也可能是别的线程\n    \n\n  \n\n已经被发现**是有问题的**， **因为需要在 ChannelHandler 中对出站事件进行仔细的同步。简而言之，不可能保证多个线程不会在同一时刻尝试访问出站事件**\n\n  \n\nNetty 4 中所采用的线程模型，通过在**同一个线程中处理某个给定的 EventLoop 中所产生的所有事件**，解决了这个问题。这提供了一个更加简单的执行体系架构，并且消除了在多个 ChannelHandler 中进行同步的需要（除了任何可能需要在多个 Channel 中共享的）\n\n  \n\n# 任务调度\n\n  \n\n偶尔，你将需要调度一个任务**以便稍后（延迟）执行或者周期性地执行**。例如，你可能想要注册一个在客户端已经**连接了 5 分钟之后触发的任务。**\n\n  \n\nJDK 的任务调度\n\n- 在 Java 5 之前，任务调度是建立在 java.util.Timer 类之上的，其使用了一个后台 Thread，并且具有与标准线程相同的限制。、\n    \n- 随后，JDK 提供了 java.util.concurrent 包，它定义了 interface ScheduledExecutorService。表 7-1 展示了 java.util.concurrent.Executors的相关工厂方法。\n    \n\n![](statistic/asynccode-724.png)\n\n![](statistic/asynccode-746.png)\n\n  \n\n## EventLoop调度任务\n\nNetty 通 过 Channel 的 EventLoop 实现任务调度解决了这一问题\n\n  \n\n- 使用EventLoop调度任务\n    \n\n![](statistic/asynccode-692.png)\n\n- 使用 **EventLoop** 调度周期性的任务\n    \n\n![](statistic/asynccode-695.png)\n\n- 使用 **ScheduledFuture** 取消任务\n    \n\n![](statistic/asynccode-725.png)\n\n  \n\n# 实现细节\n\n## 线程管理\n\nNetty线程模型的卓越性能取决于对于当前执行的Thread的身份的确定 ，**它是否是分配给当前Channel以及它的EventLoop的那一个线程，**\n\n- 如果（当前）调用线程正是支撑 EventLoop 的线程，那么所提交的代码块将会被（直接执行）\n    \n- 否则，EventLoop 将调度该任务以便稍后执行，**并将它放入到内部队列中**。当 EventLoop 下次处理它的事件时，它会执行队列中的那些任务/事件。这也就解释了任何的\n    \n\n![](statistic/asynccode-696.png)\n\n  \n\n- 每个 EventLoop 都有它自已的任务队列，独立于任何其他的 EventLoop\n    \n- “永 远不要将一个长时间运行的任务放入到执行队列中，因为它将阻塞需要在同一线程上执行的任何其他任务。”如果必须要进行阻塞调用或者执行长时间运行的任务，我们建议使用一个专门的EventExecutor\n    \n\n## EventLoop线程的分配\n\n服务于 Channel 的 I/O 和事件的 EventLoop 包含在 EventLoopGroup 中。根据不同的传输实现，EventLoop 的创建和分配方式也不同\n\n#### 异步传输\n\n异步传输实现**只使用了少量的 EventLoop**（以及和它们相关联的 Thread），而且在当前的线程模型中，**它们可能会被多个 Channel 所共享**，这使得可以通过**尽可能少量的 Thread 来支撑大量的 Channel**，而不是每个 Channel 分配一个 Thread\n\n![](statistic/asynccode-693.png)\n\n- EventLoopGroup 负责为**每个新创建的 Channel 分配一个 EventLoop**。\n    \n    - 在当前实现中，使用顺序循环（round-robin）的方式进行分配以获取一个均衡的分布，并且相同的 **EventLoop可能会被分配给多个 Channel。**（这一点在将来的版本中可能会改变。）\n        \n- 一旦一个 Channel 被分配给一个 EventLoop，它将在它的整个生命周期中都使用这个 EventLoop（以及相关联的 Thread）\n    \n- EventLoop 的分配方式对 ThreadLocal 的使用的影响。因为一个EventLoop 通常会被用于支撑多个 Channel，所以对于所有相关联的 Channel 来说， ThreadLocal 都将是一样的\n    \n\n  \n\n#### 阻塞传输\n\n用于像 OIO（旧的阻塞 I/O）这样的其他传输的设计略有不同\n\n![](statistic/asynccode-694.png)\n\n这里每一个 Channel 都将被分配给一个 EventLoop（以及它的 Thread）。如果你开发的应用程序使用过 java.io 包中的阻塞 I/O 实现，你可能就遇到过这种模型\n\n但是，正如同之前一样，得到的保证是**每个 Channel 的 I/O 事件都将只会被一个 Thread** （用于支撑该 Channel 的 EventLoop 的那个 Thread）处理。","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/spring/spring-MVC/J2EE":{"title":"J2EE","content":"## Servlet 总结\n\n在 Java Web 程序中，**Servlet**主要负责接收用户请求 `HttpServletRequest`,在 `doGet()`, `doPost()` 中做相应的处理，并将回应 `HttpServletResponse` 反馈给用户。**Servlet** 可以设置初始化参数，供 Servlet 内部使用。一个 Servlet 类只会有一个实例，在它初始化时调用 `init()` 方法，销毁时调用 `destroy()` 方法**。**Servlet 需要在 web. Xml 中配置（MyEclipse 中创建 Servlet 会自动配置），**一个 Servlet 可以设置多个 URL 访问**。**Servlet 不是线程安全**，因此要谨慎使用类变量。\n\n## 阐述 Servlet 和 CGI 的区别?\n\n### CGI 的不足之处\n\n1，需要为每个请求启动一个操作 CGI 程序的系统进程。如果请求频繁，这将会带来很大的开销。\n\n2，需要为每个请求加载和运行一个 CGI 程序，这将带来很大的开销\n\n3，需要重复编写处理网络协议的代码以及编码，这些工作都是非常耗时的。\n\n### Servlet 的优点\n\n1，只需要启动一个操作系统进程以及加载一个 JVM，大大降低了系统的开销\n\n2，如果多个请求需要做同样处理的时候，这时候只需要加载一个类，这也大大降低了开销\n\n3，所有动态加载的类可以实现对网络协议以及请求解码的共享，大大降低了工作量。\n\n4，Servlet 能直接和 Web 服务器交互，而普通的 CGI 程序不能。Servlet 还能在各个程序之间共享数据，使数据库连接池之类的功能很容易实现。\n\n补充：Sun Microsystems 公司在 1996 年发布 Servlet 技术就是为了和 CGI 进行竞争，Servlet 是一个特殊的 Java 程序，一个基于 Java 的 Web 应用通常包含一个或多个 Servlet 类。Servlet 不能够自行创建并执行，它是在 Servlet 容器中运行的，容器将用户的请求传递给 Servlet 程序，并将 Servlet 的响应回传给用户。通常一个 Servlet 会关联一个或多个 JSP 页面。以前 CGI 经常因为性能开销上的问题被诟病，然而 Fast CGI 早就已经解决了 CGI 效率上的问题，所以面试的时候大可不必信口开河的诟病 CGI，事实上有很多你熟悉的网站都使用了 CGI 技术。\n\n参考：《javaweb 整合开发王者归来》P 7\n\n## Servlet 接口中有哪些方法及 Servlet 生命周期探秘\n\nServlet 接口定义了 5 个方法，其中**前三个方法与 Servlet 生命周期相关**：\n\n- `void init(ServletConfig config) throws ServletException`\n- `void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException`\n- `void destroy()`\n- `java.lang.String getServletInfo()`\n- `ServletConfig getServletConfig()`\n\n**生命周期：** **Web 容器加载 Servlet 并将其实例化后，Servlet 生命周期开始**，容器运行其**init ()方法**进行 Servlet 的初始化；请求到达时调用 Servlet 的**service ()方法**，service ()方法会根据需要调用与请求对应的**doGet 或 doPost**等方法；当服务器关闭或项目被卸载时服务器会将 Servlet 实例销毁，此时会调用 Servlet 的**destroy ()方法**。**init 方法和 destroy 方法只会执行一次，service 方法客户端每次请求 Servlet 都会执行**。Servlet 中有时会用到一些需要初始化与销毁的资源，因此可以把初始化资源的代码放入 init 方法中，销毁资源的代码放入 destroy 方法中，这样就不需要每次处理客户端的请求都要初始化与销毁资源。\n\n参考：《javaweb 整合开发王者归来》P 81\n\n## Get 和 post 请求的区别\n\nget 和 post 请求实际上是没有区别，大家可以自行查询相关文章（参考文章：[https://www.cnblogs.com/logsharing/p/8448446.html](https://www.cnblogs.com/logsharing/p/8448446.html)，知乎对应的问题链接：[get 和 post 区别？](https://www.zhihu.com/question/28586791)）！\n\n可以把 get 和 post 当作两个不同的行为，两者并没有什么本质区别，底层都是 TCP 连接。 Get 请求用来从服务器上获得资源，而 post 是用来向服务器提交数据。比如你要获取人员列表可以用 get 请求，你需要创建一个人员可以用 post 。这也是 Restful API 最基本的一个要求。\n\n推荐阅读：\n\n- \u003chttps://www.zhihu.com/question/28586791\u003e\n- \u003chttps://mp.weixin.qq.com/s?__biz=MzI3NzIzMzg3Mw==\u0026mid=100000054\u0026idx=1\u0026sn=71f6c214f3833d9ca20b9f7dcd9d33e4#rd\u003e\n\n## 什么情况下调用 doGet ()和 doPost ()\n\nForm 标签里的 method 的属性为 get 时调用 doGet ()，为 post 时调用 doPost ()。\n\n## 转发 (Forward)和重定向 (Redirect)的区别\n\n**转发是服务器行为，重定向是客户端行为。**\n\n**转发（Forward）**\n通过 RequestDispatcher 对象的 forward（HttpServletRequest request, HttpServletResponse response）方法实现的。RequestDispatcher 可以通过 HttpServletRequest 的 getRequestDispatcher ()方法获得。例如下面的代码就是跳转到 login_success. Jsp 页面。\n\n```java\n     request.getRequestDispatcher(\"login_success.jsp\").forward(request, response);\n```\n\n**重定向（Redirect）** 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过 `HttpServletResponse` 的 `setStatus(int status)` 方法设置状态码。如果服务器返回 301 或者 302，则浏览器会到新的网址重新请求该资源。\n\n1. **从地址栏显示来说**\n\n   Forward 是服务器请求资源, 服务器直接访问目标地址的 URL, 把那个 URL 的响应内容读取过来, 然后把这些内容再发给浏览器. 浏览器根本不知道服务器发送的内容从哪里来的, 所以它的地址栏还是原来的地址.\n   Redirect 是服务端根据逻辑, 发送一个状态码, 告诉浏览器重新去请求那个地址. 所以地址栏显示的是新的 URL.\n\n2. **从数据共享来说**\n\n   Forward: 转发页面和转发到的页面可以共享 request 里面的数据.\n   Redirect: 不能共享数据.\n\n3. **从运用地方来说**\n\n   Forward: 一般用于用户登陆的时候, 根据角色转发到相应的模块.\n   Redirect: 一般用于用户注销登陆时返回主页面和跳转到其它的网站等\n\n4. 从效率来说\n\n   Forward: 高.\n   Redirect: 低.\n\n## 自动刷新 (Refresh)\n\n自动刷新不仅可以实现一段时间之后自动跳转到另一个页面，还可以实现一段时间之后自动刷新本页面。Servlet 中通过 HttpServletResponse 对象设置 Header 属性实现自动刷新例如：\n\n```java\nResponse.setHeader(\"Refresh\",\"5;URL=http://localhost:8080/servlet/example.htm\");\n```\n\n其中 5 为时间，单位为秒。URL 指定就是要跳转的页面（如果设置自己的路径，就会实现每过 5 秒自动刷新本页面一次）\n\n## Servlet 与线程安全\n\n**Servlet 不是线程安全的，多线程并发的读写会导致数据不同步的问题。** 解决的办法是尽量不要定义 name 属性，而是要把 name 变量分别定义在 doGet ()和 doPost ()方法内。虽然使用 synchronized (name){}语句块可以解决问题，但是会造成线程的等待，不是很科学的办法。\n注意：多线程的并发的读写 Servlet 类属性会导致数据不同步。但是如果只是并发地读取属性而不写入，则不存在数据不同步的问题。因此 Servlet 里的只读属性最好定义为 final 类型的。\n\n参考：《javaweb 整合开发王者归来》P 92\n\n## JSP 和 Servlet 是什么关系\n\n其实这个问题在上面已经阐述过了，Servlet 是一个特殊的 Java 程序，它运行于服务器的 JVM 中，能够依靠服务器的支持向浏览器提供显示内容。JSP 本质上是 Servlet 的一种简易形式，JSP 会被服务器处理成一个类似于 Servlet 的 Java 程序，可以简化页面内容的生成。Servlet 和 JSP 最主要的不同点在于，Servlet 的应用逻辑是在 Java 文件中，并且完全从表示层中的 HTML 分离开来。而 JSP 的情况是 Java 和 HTML 可以组合成一个扩展名为. Jsp 的文件。有人说，Servlet 就是在 Java 中写 HTML，而 JSP 就是在 HTML 中写 Java 代码，当然这个说法是很片面且不够准确的。JSP 侧重于视图，Servlet 更侧重于控制逻辑，在 MVC 架构模式中，JSP 适合充当视图（view）而 Servlet 适合充当控制器（controller）。\n\n## JSP 工作原理\n\nJSP 是一种 Servlet，但是与 HttpServlet 的工作方式不太一样。HttpServlet 是先由源代码编译为 class 文件后部署到服务器下，为先编译后部署。而 JSP 则是先部署后编译。JSP 会在客户端第一次请求 JSP 文件时被编译为 HttpJspPage 类（接口 Servlet 的一个子类）。该类会被服务器临时存放在服务器工作目录里面。下面通过实例给大家介绍。\n工程 JspLoginDemo 下有一个名为 login. Jsp 的 Jsp 文件，把工程第一次部署到服务器上后访问这个 Jsp 文件，我们发现这个目录下多了下图这两个东东。\n.class 文件便是 JSP 对应的 Servlet。编译完毕后再运行 class 文件来响应客户端请求。以后客户端访问 login. Jsp 的时候，Tomcat 将不再重新编译 JSP 文件，而是直接调用 class 文件来响应客户端请求。\n\n![JSP工作原理](statistic/JSP工作原理.jpg)\n\n由于 JSP 只会在客户端第一次请求的时候被编译，因此第一次请求 JSP 时会感觉比较慢，之后就会感觉快很多。如果把服务器保存的 class 文件删除，服务器也会重新编译 JSP。\n\n开发 Web 程序时经常需要修改 JSP。Tomcat 能够自动检测到 JSP 程序的改动。如果检测到 JSP 源代码发生了改动。Tomcat 会在下次客户端请求 JSP 时重新编译 JSP，而不需要重启 Tomcat。这种自动检测功能是默认开启的，检测改动会消耗少量的时间，在部署 Web 应用的时候可以在 web. Xml 中将它关掉。\n\n参考：《javaweb 整合开发王者归来》P 97\n\n## JSP 有哪些内置对象、作用分别是什么\n\n[JSP 内置对象 - CSDN 博客](http://blog.csdn.net/qq_34337272/article/details/64310849)\n\nJSP 有 9 个内置对象：\n\n- Request：封装客户端的请求，其中包含来自 GET 或 POST 请求的参数；\n- Response：封装服务器对客户端的响应；\n- PageContext：通过该对象可以获取其他对象；\n- Session：封装用户会话的对象；\n- Application：封装服务器运行环境的对象；\n- Out：输出服务器响应的输出流对象；\n- Config：Web 应用的配置对象；\n- Page：JSP 页面本身（相当于 Java 程序中的 this）；\n- Exception：封装页面抛出异常的对象。\n\n## Request 对象的主要方法有哪些\n\n- SetAttribute (String name, Object)：设置名字为 name 的 request 的参数值\n- GetAttribute (String name)：返回由 name 指定的属性值\n- GetAttributeNames ()：返回 request 对象所有属性的名字集合，结果是一个枚举的实例\n- GetCookies ()：返回客户端的所有 Cookie 对象，结果是一个 Cookie 数组\n- GetCharacterEncoding ()：返回请求中的字符编码方式 = getContentLength ()：返回请求的 Body 的长度\n- GetHeader (String name)：获得 HTTP 协议定义的文件头信息\n- GetHeaders (String name)：返回指定名字的 request Header 的所有值，结果是一个枚举的实例\n- GetHeaderNames ()：返回所以 request Header 的名字，结果是一个枚举的实例\n- GetInputStream ()：返回请求的输入流，用于获得请求中的数据\n- GetMethod ()：获得客户端向服务器端传送数据的方法\n- GetParameter (String name)：获得客户端传送给服务器端的有 name 指定的参数值\n- GetParameterNames ()：获得客户端传送给服务器端的所有参数的名字，结果是一个枚举的实例\n- GetParameterValues (String name)：获得有 name 指定的参数的所有值\n- GetProtocol ()：获取客户端向服务器端传送数据所依据的协议名称\n- GetQueryString ()：获得查询字符串\n- GetRequestURI ()：获取发出请求字符串的客户端地址\n- GetRemoteAddr ()：获取客户端的 IP 地址\n- GetRemoteHost ()：获取客户端的名字\n- GetSession ([Boolean create])：返回和请求相关 Session\n- GetServerName ()：获取服务器的名字\n- GetServletPath ()：获取客户端所请求的脚本文件的路径\n- GetServerPort ()：获取服务器的端口号\n- RemoveAttribute (String name)：删除请求中的一个属性\n\n## Request.GetAttribute ()和 request.GetParameter ()有何区别\n\n**从获取方向来看：**\n\n`getParameter()` 是获取 POST/GET 传递的参数值；\n\n`getAttribute()` 是获取对象容器中的数据值；\n\n**从用途来看：**\n\n`getParameter()` 用于客户端重定向时，即点击了链接或提交按扭时传值用，即用于在用表单或 url 重定向传值时接收数据用。\n\n`getAttribute()` 用于服务器端重定向时，即在 sevlet 中使用了 forward 函数, 或 struts 中使用了\nMapping. FindForward。 GetAttribute 只能收到程序用 setAttribute 传过来的值。\n\n另外，可以用 `setAttribute()`, `getAttribute()` 发送接收对象. 而 `getParameter()` 显然只能传字符串。\n`setAttribute()` 是应用服务器把这个对象放在该页面所对应的一块内存中去，当你的页面服务器重定向到另一个页面时，应用服务器会把这块内存拷贝另一个页面所对应的内存中。这样 `getAttribute()` 就能取得你所设下的值，当然这种方法可以传对象。Session 也一样，只是对象在内存中的生命周期不一样而已。`getParameter()` 只是应用服务器在分析你送上来的 request 页面的文本时，取得你设在表单或 url 重定向时的值。\n\n**总结：**\n\n`getParameter()` 返回的是 String, 用于读取提交的表单中的值;（获取之后会根据实际需要转换为自己需要的相应类型，比如整型，日期类型啊等等）\n\n`getAttribute()` 返回的是 Object，需进行转换, 可用 `setAttribute()` 设置成任意对象，使用很灵活，可随时用\n\n## Include 指令 include 的行为的区别\n\n**include 指令：** JSP 可以通过 include 指令来包含其他文件。被包含的文件可以是 JSP 文件、HTML 文件或文本文件。包含的文件就好像是该 JSP 文件的一部分，会被同时编译执行。语法格式如下：\n\u003c%@ include file=\"文件相对 url 地址\" %\u003e\n\ni**nclude 动作：** `\u003cjsp:include\u003e` 动作元素用来包含静态和动态的文件。该动作把指定文件插入正在生成的页面。语法格式如下：\n\u003cjsp:include page=\"相对 URL 地址\" flush=\"true\" /\u003e\n\n## JSP 九大内置对象，七大动作，三大指令\n\n[JSP 九大内置对象，七大动作，三大指令总结](http://blog.csdn.net/qq_34337272/article/details/64310849)\n\n## 讲解 JSP 中的四种作用域\n\nJSP 中的四种作用域包括 page、request、session 和 application，具体来说：\n\n- **page**代表与一个页面相关的对象和属性。\n- **request**代表与 Web 客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个 Web 组件；需要在页面显示的临时数据可以置于此作用域。\n- **session**代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的 session 中。\n- **application**代表与整个 Web 应用程序相关的对象和属性，它实质上是跨越整个 Web 应用程序，包括多个页面、请求和会话的一个全局作用域。\n\n## 如何实现 JSP 或 Servlet 的单线程模式\n\n对于 JSP 页面，可以通过 page 指令进行设置。\n` \u003c%@page isThreadSafe=\"false\"%\u003e `\n\n对于 Servlet，可以让自定义的 Servlet 实现 SingleThreadModel 标识接口。\n\n说明：如果将 JSP 或 Servlet 设置成单线程工作模式，会导致每个请求创建一个 Servlet 实例，这种实践将导致严重的性能问题（服务器的内存压力很大，还会导致频繁的垃圾回收），所以通常情况下并不会这么做。\n\n## 实现会话跟踪的技术有哪些\n\n1. **使用 Cookie**\n\n   向客户端发送 Cookie\n\n   ```java\n   Cookie c =new Cookie(\"name\",\"value\"); //创建Cookie\n   c.setMaxAge(60*60*24); //设置最大时效，此处设置的最大时效为一天\n   response.addCookie(c); //把Cookie放入到HTTP响应中\n   ```\n\n   从客户端读取 Cookie\n\n   ```java\n   String name =\"name\";\n   Cookie[]cookies =request.getCookies();\n   if(cookies !=null){\n      for(int i= 0;i\u003ccookies.length;i++){\n       Cookie cookie =cookies[i];\n       if(name.equals(cookis.getName()))\n       //something is here.\n       //you can get the value\n       cookie.getValue();\n\n      }\n    }\n\n   ```\n\n   **优点:** 数据可以持久保存，不需要服务器资源，简单，基于文本的 Key-Value\n\n   **缺点:** 大小受到限制，用户可以禁用 Cookie 功能，由于保存在本地，有一定的安全风险。\n\n2. URL 重写\n\n   在 URL 中添加用户会话的信息作为请求的参数，或者将唯一的会话 ID 添加到 URL 结尾以标识一个会话。\n\n   **优点：** 在 Cookie 被禁用的时候依然可以使用\n\n   **缺点：** 必须对网站的 URL 进行编码，所有页面必须动态生成，不能用预先记录下来的 URL 进行访问。\n\n3. 隐藏的表单域\n\n   ```html\n   \u003cinput type=\"hidden\" name=\"session\" value=\"...\" /\u003e\n   ```\n\n   **优点：** Cookie 被禁时可以使用\n\n   **缺点：** 所有页面必须是表单提交之后的结果。\n\n4. HttpSession\n\n   在所有会话跟踪技术中，HttpSession 对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建 HttpSession，每个用户可以访问他自己的 HttpSession。可以通过 HttpServletRequest 对象的 getSession 方法获得 HttpSession，通过 HttpSession 的 setAttribute 方法可以将一个值放在 HttpSession 中，通过调用 HttpSession 对象的 getAttribute 方法，同时传入属性名就可以获取保存在 HttpSession 中的对象。与上面三种方式不同的是，HttpSession 放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的 Servlet 容器可以在内存将满时将 HttpSession 中的对象移到其他存储设备中，但是这样势必影响性能。添加到 HttpSession 中的值可以是任意 Java 对象，这个对象最好实现了 Serializable 接口，这样 Servlet 容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。\n\n## Cookie 和 Session 的区别\n\nCookie 和 Session 都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。\n\n**Cookie 一般用来保存用户信息** 比如 ① 我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；② 一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可 (为了安全考虑，重新登录一般要将 Token 重写)；③ 登录一次网站后访问网站其他页面不需要重新登录。**Session 的主要作用就是通过服务端记录用户的状态。** 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。\n\nCookie 数据保存在客户端 (浏览器端)，Session 数据保存在服务器端。\n\nCookie 存储在客户端中，而 Session 存储在服务器上，相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/spring/spring-framework/spring-%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F":{"title":"spring 中的设计模式","content":"“JDK 中用到了哪些设计模式? Spring 中用到了哪些设计模式? ”这两个问题，在面试中比较常见。\n\n我在网上搜索了一下关于 Spring 中设计模式的讲解几乎都是千篇一律，而且大部分都年代久远。所以，花了几天时间自己总结了一下。\n\n由于我的个人能力有限，文中如有任何错误各位都可以指出。另外，文章篇幅有限，对于设计模式以及一些源码的解读我只是一笔带过，这篇文章的主要目的是回顾一下 Spring 中的设计模式。\n\n## [#](#控制反转-ioc-和依赖注入-di) 控制反转(IoC)和依赖注入(DI)\n\n**IoC(Inversion of Control,控制反转)** 是 Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。IoC 的主要目的是借助于“第三方”(Spring 中的 IoC 容器) 实现具有依赖关系的对象之间的解耦(IOC 容器管理对象，你只管使用即可)，从而降低代码之间的耦合度。\n\n**IoC 是一个原则，而不是一个模式，以下模式（但不限于）实现了 IoC 原则。**\n\n![ioc-patterns](statistic/ioc-patterns.png)\n\nioc-patterns\n\n**Spring IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。** IoC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。\n\n在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。\n\n\u003e 关于 Spring IOC 的理解，推荐看这一下知乎的一个回答：[https://www.zhihu.com/question/23277575/answer/169698662open in new window](https://www.zhihu.com/question/23277575/answer/169698662) ，非常不错。\n\n**控制反转怎么理解呢?** 举个例子：\"对象 a 依赖了对象 b，当对象 a 需要使用 对象 b 的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象 a 和对象 b 之间就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b 的时候， 我们可以指定 IOC 容器去创建一个对象 b 注入到对象 a 中\"。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权反转，这就是控制反转名字的由来。\n\n**DI(Dependency Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。**\n\n## [#](#工厂设计模式) 工厂设计模式\n\nSpring 使用工厂模式可以通过 `BeanFactory` 或 `ApplicationContext` 创建 bean 对象。\n\n**两者对比：**\n\n- `BeanFactory`：延迟注入(使用到某个 bean 的时候才会注入),相比于`ApplicationContext` 来说会占用更少的内存，程序启动速度更快。\n- `ApplicationContext`：容器启动的时候，不管你用没用到，一次性创建所有 bean 。`BeanFactory` 仅提供了最基本的依赖注入支持，`ApplicationContext` 扩展了 `BeanFactory` ,除了有`BeanFactory`的功能还有额外更多功能，所以一般开发人员使用`ApplicationContext`会更多。\n\n`ApplicationContext` 的三个实现类：\n\n1. `ClassPathXmlApplication`：把上下文文件当成类路径资源。\n2. `FileSystemXmlApplication`：从文件系统中的 XML 文件载入上下文定义信息。\n3. `XmlWebApplicationContext`：从 Web 系统中的 XML 文件载入上下文定义信息。\n\nExample:\n\n```\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.FileSystemXmlApplicationContext;\n\npublic class App {\n\tpublic static void main(String[] args) {\n\t\tApplicationContext context = new FileSystemXmlApplicationContext(\n\t\t\t\t\"C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml\");\n\n\t\tHelloApplicationContext obj = (HelloApplicationContext) context.getBean(\"helloApplicationContext\");\n\t\tobj.getMsg();\n\t}\n}\n```\n\n## [#](#单例设计模式) 单例设计模式\n\n在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。\n\n**使用单例模式的好处** :\n\n- 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销；\n- 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。\n\n**Spring 中 bean 的默认作用域就是 singleton(单例)的。** 除了 singleton 作用域，Spring 中 bean 还有下面几种作用域：\n\n- **prototype** : 每次获取都会创建一个新的 bean 实例。也就是说，连续 `getBean()` 两次，得到的是不同的 Bean 实例。\n- **request** （仅 Web 应用可用）: 每一次 HTTP 请求都会产生一个新的 bean（请求 bean），该 bean 仅在当前 HTTP request 内有效。\n- **session** （仅 Web 应用可用） : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean（会话 bean），该 bean 仅在当前 HTTP session 内有效。\n- **application/global-session** （仅 Web 应用可用）：每个 Web 应用在启动时创建一个 Bean（应用 Bean），，该 bean 仅在当前应用启动时间内有效。\n- **websocket** （仅 Web 应用可用）：每一次 WebSocket 会话产生一个新的 bean。\n\nSpring 通过 `ConcurrentHashMap` 实现单例注册表的特殊方式实现单例模式。\n\nSpring 实现单例的核心代码如下：\n\n```\n// 通过 ConcurrentHashMap（线程安全） 实现单例注册表\nprivate final Map\u003cString, Object\u003e singletonObjects = new ConcurrentHashMap\u003cString, Object\u003e(64);\n\npublic Object getSingleton(String beanName, ObjectFactory\u003c?\u003e singletonFactory) {\n        Assert.notNull(beanName, \"'beanName' must not be null\");\n        synchronized (this.singletonObjects) {\n            // 检查缓存中是否存在实例\n            Object singletonObject = this.singletonObjects.get(beanName);\n            if (singletonObject == null) {\n                //...省略了很多代码\n                try {\n                    singletonObject = singletonFactory.getObject();\n                }\n                //...省略了很多代码\n                // 如果实例对象在不存在，我们注册到单例注册表中。\n                addSingleton(beanName, singletonObject);\n            }\n            return (singletonObject != NULL_OBJECT ? singletonObject : null);\n        }\n    }\n    //将对象添加到单例注册表\n    protected void addSingleton(String beanName, Object singletonObject) {\n            synchronized (this.singletonObjects) {\n                this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT));\n\n            }\n        }\n}\n```\n\n**单例 Bean 存在线程安全问题吗？**\n\n大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例 Bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。\n\n常见的有两种解决办法：\n\n1. 在 Bean 中尽量避免定义可变的成员变量。\n2. 在类中定义一个 `ThreadLocal` 成员变量，将需要的可变成员变量保存在 `ThreadLocal` 中（推荐的一种方式）。\n\n不过，大部分 Bean 实际都是无状态（没有实例变量）的（比如 Dao、Service），这种情况下， Bean 是线程安全的。\n\n## [#](#代理设计模式) 代理设计模式\n\n### [#](#代理模式在-aop-中的应用) 代理模式在 AOP 中的应用\n\n**AOP(Aspect-Oriented Programming，面向切面编程)** 能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。\n\n**Spring AOP 就是基于动态代理的**，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 **JDK Proxy** 去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 **Cglib** 生成一个被代理对象的子类来作为代理，如下图所示：\n\n![SpringAOPProcess](statistic/SpringAOPProcess.jpg)\n\nSpringAOPProcess\n\n当然，你也可以使用 AspectJ ,Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。\n\n使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。\n\n### [#](#spring-aop-和-aspectj-aop-有什么区别) Spring AOP 和 AspectJ AOP 有什么区别?\n\n**Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。** Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。\n\nSpring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，\n\n如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比 Spring AOP 快很多。\n\n## [#](#模板方法) 模板方法\n\n模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。\n\n```\npublic abstract class Template {\n    //这是我们的模板方法\n    public final void TemplateMethod(){\n        PrimitiveOperation1();\n        PrimitiveOperation2();\n        PrimitiveOperation3();\n    }\n\n    protected void  PrimitiveOperation1(){\n        //当前类实现\n    }\n\n    //被子类实现的方法\n    protected abstract void PrimitiveOperation2();\n    protected abstract void PrimitiveOperation3();\n\n}\npublic class TemplateImpl extends Template {\n\n    @Override\n    public void PrimitiveOperation2() {\n        //当前类实现\n    }\n\n    @Override\n    public void PrimitiveOperation3() {\n        //当前类实现\n    }\n}\n\n```\n\nSpring 中 `JdbcTemplate`、`HibernateTemplate` 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用 Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。\n\n## [#](#观察者模式) 观察者模式\n\n观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，依赖这个对象的所有对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。\n\n### [#](#spring-事件驱动模型中的三种角色) Spring 事件驱动模型中的三种角色\n\n#### [#](#事件角色) 事件角色\n\n`ApplicationEvent` (`org.springframework.context`包下)充当事件的角色,这是一个抽象类，它继承了`java.util.EventObject`并实现了 `java.io.Serializable`接口。\n\nSpring 中默认存在以下事件，他们都是对 `ApplicationContextEvent` 的实现(继承自`ApplicationContextEvent`)：\n\n- `ContextStartedEvent`：`ApplicationContext` 启动后触发的事件;\n- `ContextStoppedEvent`：`ApplicationContext` 停止后触发的事件;\n- `ContextRefreshedEvent`：`ApplicationContext` 初始化或刷新完成后触发的事件;\n- `ContextClosedEvent`：`ApplicationContext` 关闭后触发的事件。\n\n![ApplicationEvent-Subclass](statistic/ApplicationEvent-Subclass.png)\n\nApplicationEvent-Subclass\n\n#### [#](#事件监听者角色) 事件监听者角色\n\n`ApplicationListener` 充当了事件监听者角色，它是一个接口，里面只定义了一个 `onApplicationEvent（）`方法来处理`ApplicationEvent`。`ApplicationListener`接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 `ApplicationEvent`就可以了。所以，在 Spring 中我们只要实现 `ApplicationListener` 接口的 `onApplicationEvent()` 方法即可完成监听事件\n\n```\npackage org.springframework.context;\nimport java.util.EventListener;\n@FunctionalInterface\npublic interface ApplicationListener\u003cE extends ApplicationEvent\u003e extends EventListener {\n    void onApplicationEvent(E var1);\n}\n```\n\n#### [#](#事件发布者角色) 事件发布者角色\n\n`ApplicationEventPublisher` 充当了事件的发布者，它也是一个接口。\n\n```\n@FunctionalInterface\npublic interface ApplicationEventPublisher {\n    default void publishEvent(ApplicationEvent event) {\n        this.publishEvent((Object)event);\n    }\n\n    void publishEvent(Object var1);\n}\n\n```\n\n`ApplicationEventPublisher` 接口的`publishEvent（）`这个方法在`AbstractApplicationContext`类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过`ApplicationEventMulticaster`来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。\n\n### [#](#spring-的事件流程总结) Spring 的事件流程总结\n\n1. 定义一个事件: 实现一个继承自 `ApplicationEvent`，并且写相应的构造函数；\n2. 定义一个事件监听者：实现 `ApplicationListener` 接口，重写 `onApplicationEvent()` 方法；\n3. 使用事件发布者发布消息: 可以通过 `ApplicationEventPublisher` 的 `publishEvent()` 方法发布消息。\n\nExample:\n\n```\n// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数\npublic class DemoEvent extends ApplicationEvent{\n    private static final long serialVersionUID = 1L;\n\n    private String message;\n\n    public DemoEvent(Object source,String message){\n        super(source);\n        this.message = message;\n    }\n\n    public String getMessage() {\n         return message;\n          }\n\n\n// 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法；\n@Component\npublic class DemoListener implements ApplicationListener\u003cDemoEvent\u003e{\n\n    //使用onApplicationEvent接收消息\n    @Override\n    public void onApplicationEvent(DemoEvent event) {\n        String msg = event.getMessage();\n        System.out.println(\"接收到的信息是：\"+msg);\n    }\n\n}\n// 发布事件，可以通过ApplicationEventPublisher  的 publishEvent() 方法发布消息。\n@Component\npublic class DemoPublisher {\n\n    @Autowired\n    ApplicationContext applicationContext;\n\n    public void publish(String message){\n        //发布事件\n        applicationContext.publishEvent(new DemoEvent(this, message));\n    }\n}\n\n```\n\n当调用 `DemoPublisher` 的 `publish()` 方法的时候，比如 `demoPublisher.publish(\"你好\")` ，控制台就会打印出:`接收到的信息是：你好` 。\n\n## [#](#适配器模式) 适配器模式\n\n适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作。\n\n### [#](#spring-aop-中的适配器模式) Spring AOP 中的适配器模式\n\n我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是`AdvisorAdapter` 。\n\nAdvice 常用的类型有：`BeforeAdvice`（目标方法调用前,前置通知）、`AfterAdvice`（目标方法调用后,后置通知）、`AfterReturningAdvice`(目标方法执行结束后，return 之前)等等。每个类型 Advice（通知）都有对应的拦截器:`MethodBeforeAdviceInterceptor`、`AfterReturningAdviceInterceptor`、`ThrowsAdviceInterceptor` 等等。\n\nSpring 预定义的通知要通过对应的适配器，适配成 `MethodInterceptor` 接口(方法拦截器)类型的对象（如：`MethodBeforeAdviceAdapter` 通过调用 `getInterceptor` 方法，将 `MethodBeforeAdvice` 适配成 `MethodBeforeAdviceInterceptor` ）。\n\n### [#](#spring-mvc-中的适配器模式) Spring MVC 中的适配器模式\n\n在 Spring MVC 中，`DispatcherServlet` 根据请求信息调用 `HandlerMapping`，解析请求对应的 `Handler`。解析到对应的 `Handler`（也就是我们平常说的 `Controller` 控制器）后，开始由`HandlerAdapter` 适配器处理。`HandlerAdapter` 作为期望接口，具体的适配器实现类用于对目标类进行适配，`Controller` 作为需要适配的类。\n\n**为什么要在 Spring MVC 中使用适配器模式？**\n\nSpring MVC 中的 `Controller` 种类众多，不同类型的 `Controller` 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，`DispatcherServlet` 直接获取对应类型的 `Controller`，需要的自行来判断，像下面这段代码一样：\n\n```\nif(mappedHandler.getHandler() instanceof MultiActionController){\n   ((MultiActionController)mappedHandler.getHandler()).xxx\n}else if(mappedHandler.getHandler() instanceof XXX){\n    ...\n}else if(...){\n   ...\n}\n```\n\n假如我们再增加一个 `Controller`类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。\n\n## [#](#装饰者模式) 装饰者模式\n\n装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个 Decorator 套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 `InputStream`家族，`InputStream` 类下有 `FileInputStream` (读取文件)、`BufferedInputStream` (增加缓存,使读取文件速度大大提升)等子类都在不修改`InputStream` 代码的情况下扩展了它的功能。\n\n![装饰者模式示意图](statistic/装饰者模式示意图.png)\n\n装饰者模式示意图\n\nSpring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 `Wrapper`或者 `Decorator`。这些类基本上都是动态地给一个对象添加一些额外的职责\n\n## [#](#总结) 总结\n\nSpring 框架中用到了哪些设计模式？\n\n- **工厂设计模式** : Spring 使用工厂模式通过 `BeanFactory`、`ApplicationContext` 创建 bean 对象。\n- **代理设计模式** : Spring AOP 功能的实现。\n- **单例设计模式** : Spring 中的 Bean 默认都是单例的。\n- **模板方法模式** : Spring 中 `jdbcTemplate`、`hibernateTemplate` 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。\n- **包装器设计模式** : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。\n- **观察者模式:** Spring 事件驱动模型就是观察者模式很经典的一个应用。\n- **适配器模式** :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配`Controller`。\n- ......\n\n## [#](#参考) 参考\n\n- 《Spring 技术内幕》\n- [https://blog.eduonix.com/java-programming-2/learn-design-patterns-used-spring-framework/open in new window](https://blog.eduonix.com/java-programming-2/learn-design-patterns-used-spring-framework/)\n- [http://blog.yeamin.top/2018/03/27/单例模式-Spring单例实现原理分析/open in new window](http://blog.yeamin.top/2018/03/27/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F-Spring%E5%8D%95%E4%BE%8B%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/)\n- [https://www.tutorialsteacher.com/ioc/inversion-of-controlopen in new window](https://www.tutorialsteacher.com/ioc/inversion-of-control)\n- [https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.htmlopen in new window](https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html)\n- [https://juejin.im/post/5a8eb261f265da4e9e307230open in new window](https://juejin.im/post/5a8eb261f265da4e9e307230)\n- [https://juejin.im/post/5ba28986f265da0abc2b6084](https://juejin.im/post/5ba28986f265da0abc2b6084)\n\n---\n\n著作权归Guide所有 原文链接：https://javaguide.cn/system-design/framework/spring/spring-design-patterns-summary.html#%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F","lastmodified":"2023-08-01T17:34:29.088243973Z","tags":[]},"/statistic/%E5%BA%94%E7%94%A8%E5%B1%82.png":{"title":"应用层.png","content":"","lastmodified":"2023-08-01T17:34:29.608260364Z","tags":[]},"/statistic/%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E5%B1%82.png":{"title":"网络接口层.png","content":"","lastmodified":"2023-08-01T17:34:29.61226049Z","tags":[]}}